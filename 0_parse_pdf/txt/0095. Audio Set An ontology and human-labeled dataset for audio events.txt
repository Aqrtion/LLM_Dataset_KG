978-1-5090-4117-6/17/$31.00 ©2017 IEEE

776

ICASSP 2017

Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 07:02:33 UTC from IEEE Xplore.  Restrictions apply.

AUDIOSET:ANONTOLOGYANDHUMAN-LABELEDDATASETFORAUDIOEVENTSJortF.Gemmeke,DanielP.W.Ellis,DylanFreedman,ArenJansen,WadeLawrence,R.ChanningMoore,ManojPlakal,MarvinRitterGoogle,Inc.,MountainView,CA,andNewYork,NY,USA{jgemmeke,dpwe,freedmand,arenjansen,wadelawrence,channingmoore,plakal,marvinritter}@google.comABSTRACTAudioeventrecognition,thehuman-likeabilitytoidentifyandre-latesoundsfromaudio,isanascentprobleminmachinepercep-tion.Comparableproblemssuchasobjectdetectioninimageshavereapedenormousbeneﬁtsfromcomprehensivedatasets–principallyImageNet.ThispaperdescribesthecreationofAudioSet,alarge-scaledatasetofmanually-annotatedaudioeventsthatendeavorstobridgethegapindataavailabilitybetweenimageandaudiore-search.Usingacarefullystructuredhierarchicalontologyof632audioclassesguidedbytheliteratureandmanualcuration,wecol-lectdatafromhumanlabelerstoprobethepresenceofspeciﬁcaudioclassesin10secondsegmentsofYouTubevideos.Segmentsarepro-posedforlabelingusingsearchesbasedonmetadata,context(e.g.,links),andcontentanalysis.Theresultisadatasetofunprecedentedbreadthandsizethatwill,wehope,substantiallystimulatethede-velopmentofhigh-performanceaudioeventrecognizers.IndexTerms—Audioeventdetection,soundontology,audiodatabases,datacollection1.INTRODUCTIONWithinthedomainofmachineperception,weareinterestedinartiﬁ-cialsoundunderstanding.Wewouldliketoproduceanaudioeventrecognizerthatcanlabelhundredsorthousandsofdifferentsoundeventsinreal-worldrecordingswithatimeresolutionbetterthanonesecond–justashumanlistenerscanrecognizeandrelatethesoundstheyhear.Recently,therehavebeenastonishingresultsintheanalogousproblemofimagerecognition[1,2,3].ThesemakeuseoftheIm-ageNetdataset,whichprovidesmorethan1millionimageslabeledwith1000objectcategories[4].ImageNetappearstohavebeenamajorfactordrivingthesedevelopments,yetnothingofthisscaleexistsforsoundsources.ThispaperdescribesthecreationofAudioSet,adatasetandontologyofaudioeventsthatendeavorstoprovidecomprehensivecoverageofreal-worldsoundsatImageNet-likescale.Ourinten-tionistousethedatasettocreateautomaticaudioeventrecognitionsystemsthatarecomparabletothestate-of-the-artinrecognizingob-jectsinreal-worldimages.Audioeventrecognitionhasbeenstudiedfromperceptualandengineeringperspectives.Warren&Verbrugge[5]wereamongtheﬁrsttoconnectperceptualpropertiestoacousticfeaturesintheirstudyofbouncingandbreakingsounds.Ballas[6]probedtheper-ceptionof41shorteventsobtainedfromasoundeffectscollectionbyrelatingidentiﬁcationtimetoacousticfeatures,measuresoffamil-iarity,andenvironmentalprevalence.Gygi,Kidd&Watson[7]usedmultidimensionalscalingtoidentifyacousticfactorssuchastem-poralenvelopeandpitchmeasuresthatpredictedsimilarityratingsamong50environmentalsounds.LeMaitreandHeller[8]proposedataxonomyofsoundeventsdistinguishingobjectsandactions,andusedidentiﬁcationtimeandprimingeffectstoshowthatlistenersﬁnda“middlerange”ofabstractionmostnatural.Engineering-orientedtaxonomiesanddatasetsbeganwithGaver[9]whousedperceptualfactorstoguidethedesignofsyntheticsoundeffectsconveyingdifferentactionsandmaterials(tapping,scraping,etc.).Nakatani&Okuno[10]devisedasoundontol-ogytosupportreal-worldcomputationalauditorysceneanalysis.Burgeretal.[11]developedasetof42“noisemes”(byanal-ogywithphonemes)toprovideapracticalbasisforﬁne-grainedmanualannotationof5.6hoursofwebvideosoundtrack.Shar-ingmanyofthemotivationsofthispaper,Salamonetal.[12]re-leasedadatasetof18.5hoursofurbansoundrecordingsselectedfromfreesound.org,labeledatﬁnetemporalresolutionwith10low-levelsoundcategorieschosenfromtheirurbansoundtaxon-omyof101categories.Mostrecently,S¨ageretal.[13]systemati-callyconstructedadjective-nounandverb-nounpairsfromtagsap-pliedtoentirefreesound.orgrecordingstoconstructAudioSen-tiBank,1,267hoursofaudiolabeledwith1,123adjective-nounorverb-noun“sentiment”tags.Thelabels’timeresolutionislimitedwholeclips,whichcanbeupto15minlong,andthereisnoguaran-teethattheword-pairsactuallybelongtogether–e.g.,“talkingbird”yieldsmostlytrackscontainingboth“talking”and“bird”,butveryfewbirdsdoingthetalking.AutomaticsystemsforaudioeventclassiﬁcationgobacktotheMuscleFishcontent-basedsoundeffectsretrievalsystem[14].EUProjectCHILconductedanAcousticEventDetectionevaluation[15]overexamplesof16“meetingroomacousticevents”comparingresultsofthreesystems.AsimilartaskwasincludedintheIEEE-sponsoredDCASE2013[16]whichattracted7submissionstodetect16“ofﬁce”audioevents.F-measures(harmonicmeanofprecisionandrecallofdetectedevents)werebelow0.2,leavingmuchroomtoimprove.DCASE2016[17],includesanaudioeventdetectiontaskbasedon90minutesofrealhomeandoutdoorrecordings,carefullyannotatedwith13audioeventcategoriesspanning“birdsinging”to“washingdishes”.Unlikepreviouswork,AudioSetconsidersallsoundeventsratherthanalimiteddomain.Webelievethatalarge-scaletask,intermsofbothcategoriesanddata,willenablemorepowerfullearn-ingtechniques,andhenceastep-changeinsystemquality.2.THEAUDIOSETONTOLOGYGiventhegoalofcreatingageneral-purposeaudioeventrecognizer,weneedtodeﬁnethesetofeventsthesystemshouldrecognize.Thissetofclasseswillallowustocollectlabeleddatafortrainingandevaluation.Inturn,human-labeleddataprovidesacrispdeﬁnitionofourresearchgoal:tocreateasystemabletopredictthehumanAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 07:02:33 UTC from IEEE Xplore.  Restrictions apply.

777

labelingsfromaudioalone.Ratherthanaﬂatlistofaudioevents,wewanttohaveeventsstructuredintoanabstractionhierarchy.Intraining,thisindicatesclasseswithnonexclusiverelationships;forinstancewedonotwanttoourclassiﬁerstoattempttoseparate“dogsounds”from“bark”.Duringrecognition,hierarchicalrelationsallowbacking-offtomoregeneraldescriptionswhentheclassiﬁerencountersambiguityamongseveralsubcategories(e.g.,asoundthatisrecognizedambiguouslyas“growl”,“bark”and“howl”canfallbacktoaclassiﬁcationof“dogsounds”).Finally,awell-structuredhierarchycanaidhumanlabelingbyallowingalabelertoquicklyanddirectlyﬁndthesetoftermsthatbestdescribeasound;thiswasalsoimportantduringthedevelopmentoftheeventsetwhentryingtoaddcategorieswithoutoverlapandduplication.ThisstructuredsetofaudioeventcategoriesiscalledtheAudioSetOntology.Thespeciﬁcprinciplesthatguideditsdevelopmentwere:•Thecategoriesshouldprovideacomprehensivesetthatcanbeusedtodescribetheaudioeventsencounteredinreal-worldrecordings.•Thecategoryofaparticularaudioeventshouldcorrespondtotheideaorunderstandingthatimmediatelycomestothemindofalistenerhearingthesound.•Individualcategoriesshouldbedistinguishablebya“typical”listener.Thatis,iftwocategoriescorrespondtosoundsthatalistenercannoteasilyorreliablydistinguish,thecategoriesshouldbemerged.Soundsthatcanbedistinguishedonlybyanexpert(suchasparticularbirdspecies,orﬁnedistinctionsinmusicalinstruments)shouldnotbeseparated.Thisisanat-uralconditiontopreventthesetbecomingunwieldy,althoughwerecognizethepossibilityofextensionsthatexpandcertainnodeswithmoredetailed,expert,distinctions.•Ideally,individualcategoriesaredistinctbasedontheirsoundalone,i.e.withoutrelyingonaccompanyingvisualinforma-tionordetailsofcontext.Thus,asoundisa“thump”,ratherthan“barefootstampingonawoodenﬂoor”.•Thehierarchicalstructureallowsannotatorstoidentifythebest,mostspeciﬁccategoriesforgivenaudioeventsaseasilyaspossible.Thismeansthatthehierarchyshouldnotbetoodeep,whilethenumberofchildreninanynodeshouldrarelybemorethan10,tofacilitaterapidscanning.Toavoidbiasingthecategorysetbytheorientationofapar-ticularresearcher,orthelimiteddiversityofsoundsinaparticulardatasetortask,wesoughttostartfromaneutral,large-scaleanalysisofwebtext.Weseededouraudioeventlexiconusingamodiﬁedformofthe“Hearstpatterns”[18]toidentifyhyponymsof“sound”,i.e.termsthatfrequentlyoccurinconstructionsoftheform“...sounds,suchasXandY...”or“X,Y,andothersounds...”,etc.Ap-plyingtheserulesoverweb-scaletextyieldsaverylargesetofterms,whicharethensortedintermsofhowwelltheyrepresentsounds-calculatedasacombinationofoverallfrequencyofoccurrencewithhowexclusivelytheyareidentiﬁedashyponymsof“sound”ratherthanotherterms.Thisgaveusastartinglistofover3000terms;aspartofthematchingprocess,theywerealsoresolvedagainstFree-base/KnowledgeGraphentity/machineIDs(MIDs)[19],whichweuseasstableidentiﬁers.Startingfromthetopofthesortedlist,wemanuallyassembledahierarchytocontainthesetermsinawaythatbestagreedwithourintuitiveunderstandingofthesounds.Thisstepissubjective,butitbestcapturestheroleofthehierarchyinsupportinghumanlabelers.Westoppedwhenthelistseemedtobesupplyingtermsthatwereallobscureorill-deﬁned(e.g.,“Wilhelmscream”,“TheOakRidgeBoys”,“Earcon”,“Whump”).Theresultingstructureisnotastricthierarchyasnodesmayoccurinseveralplaces;forinstance,“Hiss”appearsunder“Cat”,“Steam”,and“Onomatopoeia”.Thereareintotal33categoriesthatappearmorethanonce.Wereﬁnedthisinitialstructurebycomparingitwithexistingaudioeventlistsortaxonomies,including[9,20,11,12,21,8,17].Ourhopewasthattheinitiallistwouldsubsumetheseearlieref-forts;infact,numerousgapswereexposed,althougheventuallywereachedapointwherenearlyeveryclassfromothersetswascov-ered.Someclassesfromothersetswerenotincorporatedbecausetheyweretoospeciﬁc,orotherwisedidnotmeetourcriteriaofbe-ingreadilyidentiﬁable.Forinstance,theurbansoundstaxonomyof[12]includes“Carradio”asasourceofrecordedmusic,whichmakessensewhenlabelingcitystreetambiencerecordingsbutistoospecializedorcontext-dependentforourset(althoughwedohave“Radio”forsoundsthatareclearlybeingproducedbyaradio).The165veriﬁed-distinctsoundclipsusedin[21]includedetailedex-amplessuch“Trumpetjazzsolo”and“Walkingonleaves”;inourscheme,thesewouldbesimply“Trumpet”and“Walk,footsteps”,respectively.Wethenbeganusingthissettolabelrecordings,aswellastry-ingtoﬁndexamplesforeverycategory(asdescribedinsection2.1below).Feedbackfromthelargergroupofpeopleinvolvedledtofurthermodiﬁcationstothecategoryset.Ourﬁnallistcontains632audioeventcategories,arrangedinahierarchywithamaximumdepthof6levels;anexamplefromamongtheeightlevel-6nodesis“Soundsofthings”→“Vehicle”→“Motorvehicle”→“Emergencyvehicle”→“Siren”→“Ambulance(siren)”(acategorythatwasappropriatedfromtheUrbanSoundtax-onomy[12]).Figure1showsthe50ﬁrst-andsecond-levelnodesinthishierarchy.2.1.OntologyDataReleaseTheontologyisreleased1asaJSONﬁlecontainingthefollowingﬁeldsforeachcategory:•ID:TheKnowledgeGraphMachineID(MID)bestmatch-ingthesoundorsource,usedastheprimaryidentiﬁerfortheclass.InKnowledgeGraph,manyMIDsrefertospeciﬁcob-jects(e.g.,/m/0gy1t2s,“Bicyclebell”or/m/02y763,“Slidingdoor”);whenusedintheontology,theyareunder-stoodtomean“thesoundreadilyassociatedwiththespeciﬁedobject”.Forinstance,inthecaseof“Slidingdoor”,thesoundmadebysomeonecrashingintoacloseddoorwouldbeema-natingfromthedoor;however,itwouldnotbethecharacter-isicshortburstofbearingnoisethatsuggests“Slidingdoor”,soitshouldinsteadbelabeled“Smash”or“Thump”.•Displayname:Abriefoneortwowordnameforthesound,sometimeswithasmallnumberofcomma-separatedalterna-tives(e.g.,“Burst,pop”),andsometimeswithparenthesizeddisambiguation(e.g.“Fill(withliquid)”).Displaynamesareintendedtobeunambiguousevenwhenshownwithouttheirancestornodes.•Description:Alongerdescription,typicallyoneortwosen-tences,usedtoprovidemoreexplanationofthemeaningandlimitsoftheclass.Inmanycasesthesearebasedon1g.co/audiosetAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 07:02:33 UTC from IEEE Xplore.  Restrictions apply.

778

Human soundsHuman voiceWhistlingRespiratory soundsHuman locomotionDigestiveHandsHeart sounds, heartbeatOtoacoustic emissionHuman group actionsAnimal soundsDomestic animals, petsLivestock, farm animals,working animalsWild animalsNatural soundsWindThunderstormWaterFireMusicMusical instrumentMusic genreMusical conceptsMusic roleMusic moodSounds of thingsVehicleEngineDomestic sounds, home soundsBellAlarmMechanismsToolsExplosionWoodGlassLiquidMiscellaneous sourcesSpeciﬁc impact soundsSource-ambiguoussoundsGeneric impact soundsSurface contactDeformable shellOnomatopoeiaSilenceOther sourcelessChannel, environmentand backgroundAcoustic environmentNoiseSound reproductionFig.1:ThetoptwolayersoftheAudioSetontology.WikipediaorWordNetdescriptions(withappropriatecita-tionURIs),adaptedtoemphasizetheirspeciﬁcuseforaudioevents.•Examples:Asanalternativetothetextualdescriptions,wealsocollectedatleastoneexampleofthesound(excepting“Abstract”classes,describedbelow).Atpresent,allexam-plesareprovidedasURLsindicatingshortexcerptsfrompub-licYouTubevideos.Theexerciseofﬁndingconcreteexam-plesforeachclasswashelpfulinﬂushingoutindistinctorambiguouscategoriesfromearlierversionsoftheontology.•Children:ThehierarchyisencodedbyincludingwithineachnodetheMIDsofalltheimmediatechildrenofthatcategory.•Restrictions:Ofthe632categories,56are“blacklisted”,meaningtheyarenotexposedtolabelersbecausetheyhaveturnedouttobeobscure(e.g.,“Altosaxophone”)orconfus-ing(e.g.,“Soundsofthings”).Another22nodesaremarked“Abstract”(e.g.,“Onomatopoeia”),meaningthattheyexistpurelyasintermediatenodestohelpstructuretheontology,andarenotexpectedtoeverbeuseddirectlyaslabels.TheseﬂagsappearintheRestrictionsﬁeld.Figure2showsthecompleterecordforasingleexamplecate-gory,“Birdvocalization,birdcall,birdsong”.3.AUDIOSETDATASETTheAudioSetYouTubeCorpusconsistsoflabeledYouTubeseg-ments,structuredasaCSVﬁle2comprisingYouTubeidentiﬁers,starttime,endtimeandoneormorelabels.Datasetsegmentsare2g.co/audioset{id:’/m/020bb7’,name:’Birdvocalization,birdcall,birdsong’,description:’Birdsoundsthataremelodioustothehumanear.’,citationuri:’http://en.wikipedia.org/wiki/Birdvocalization’,examples:’youtu.be/vRg6EQm8pBw?start=30&end=40’,children:’/m/07pggtf,/m/07pggtn,/m/07sx8x’,restrictions:’’}Fig.2:ThefulldatarecordforonesoundcategoryfromtheAudioSetOntology.Thechildrencorrespondto“Tweet”,“Chirp”,and“Squawk”.all10secondslong(exceptwhenthatexceedsthelengthoftheun-derlyingvideo).Eachdatasetsegmentcarriesoneormoreontologyclasslabels.3.1.HumanratingHumanraterswerepresentedwitha10-secondsegmentsincludingboththevideoandaudiocomponents,butdidnothaveaccesstothetitleorothermeta-informationoftheunderlyingYouTubevideo.Ifthedurationofthevideowaslessthan10seconds,theentirevideowasusedforrating.Weexperimentedbothwithshortersegmentsandaudio-onlypresentation,butratersfoundtheseconditionsfarmoredifﬁcult,possiblyduetotheﬁne-grainednatureofouraudioeventontology.Foreachsegment,raterswereaskedtoindependentlyratethepresenceofoneormorelabels.Thepossibleratingswere“present”,“notpresent”and“unsure”.Eachsegmentwasratedbythreeratersandamajorityvotewasrequiredtorecordanoverallrating.Forspeed,asegment’sthirdratingwasnotcollectediftheﬁrsttworat-ingsagreedforalllabels.Theraterswereunanimousin76.2%ofvotes.The“unsure”ratingwasrare,representingonly0.5%ofresponses,so2:1majorityvotesaccountfor23.6%ofthedecisions.Categoriesthatachievedthehighestrateragreementinclude“Christmasmusic”,“Accordion”and“Babbling”(>0.92);whilesomecategorieswithlowagreementinclude“Basketballbounce”,“Boiling”and“Bicycle”(<0.17).Spotcheckingrevealedasmallnumberoflabelingerrorswhichweremostlyattributedto:1)confusinglabels,2)humanerror,and3)differenceindetectionoffaint/non-salientaudioevents.Asanadditionalcheckweanalyzedcorrelationsbetween“present”labelsandwordsinthevideo’smetadata.Thisexposedsomecommonly-misinterpretedlabels,whichwerethenremovedfromtheontology.Duetothescaleofthedataandsincemajorityagreementwasveryhigh,nootherothercorrectiveactionsweretaken.3.2.SelectingsegmentsforratingInordertoobtainasufﬁcientnumberofpositiveratingsfromamod-eratevolumeoflabelingeffort,itisimportanttosendforratingonlythosethathaveagoodchanceofcontainingparticularaudioevents.Weusedavarietyofmethodstoidentifyvideosthatwerelikelytoberelevanttoparticularlabels:Abouthalfoftheaudioeventscorrespondedtolabelsalreadypredictedbyaninternalvideo-levelautomaticannotationsystem,andweusedthevideosbearingthelabeltoprovidesegmentsforrating.Thelabelsfromtheinter-nalvideolabelingsystemareautomaticallygeneratedandthusin-Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 07:02:33 UTC from IEEE Xplore.  Restrictions apply.

779

MusicSpeechVehicleMusical InstrumentGuitarSizzleSqueakShatterZingSlapScreechHeart sounds, heartbeatRoosterTire squealFixed-wing aircraft1,006,88280,42274,72930,466969127126126123121963955955953893,911Number of examplesAudio class235 classes omitted ...235 classes omitted ...1001,00010,000100,0001,000,000Fig.3:Histogramoflabelcountsovertheentire1,789,621segmentdataset.evitablyimperfect.Theyare,however,basedonmultiplecomple-mentarysourcesofinformation,suchasmetadata,anchortextofincominglinks,comments,anduserengagementsignals,andtheirreliabilityisgreatestforpopularvideoswhichhavemoreoftheseattributes.Wethereforeselectonlyvideoswithatleast1,000views.Inaddition,videoswereselectedbysearchingtitlesandothermetadataforrelevantkeywords.Weusedaranking-basedapproachwherevideoswerescoredbymatchingnotonlyagainstthedisplaynameoftheaudioevent,butalso,withdecreasingweight,itspar-entsintheontology.Includingtheterm“sound”asarootnodeinthesesearchesimprovedtheprecisionofresults.Temporalselectionofsegmentswiththereturnedvideoswasarbitrary,typicallytak-ingsegmentswithastarttimeat30sectoavoidanyintroductionorchannelbrandingatthestartofthevideo.Forsomeofthedata,temporalselectionwasbasedoncontentanalysissuchasnearest-neighboursearchbetweenthemanually-veriﬁedclipsclipsandthelabel-speciﬁccandidatevideos.Usingallthesetechniques,thereweresomecategoriesforwhichwewereunabletoﬁndenoughpositiveexamplestofullypopulatethedataset,butthisproportionisnowverysmall(andshrinking).Ingeneralwefoundthatsegmentsnominatedbytheautomaticvideoannotationsystemperformedthebest(49%ofsegmentsratedpresent,versus41%forthemetadata-basedapproach),withthecaveatthatnotallaudioclassesareincluded.Foraudioclassesnotincludedintheautomaticannotations,36%ofthemetadata-basedsegmentswereratedpresent.Theeffectivenessofthecontent-similarityapproachesvariedwidelydependingonthequalityoftheexampleandthecandidatesetofvideos.Regardlessofthemechanismbywhichasegmentwasnom-inated,wegatheredratingsforalllabelsassociatedwiththeseg-mentbyanymethod.Additionally,wealwayscollectedratingsfor“Speech”and“Music”(exceptingafewexperimentalruns).3.3.DatasetconstructionandcharacteristicsThereleaseddatasetconstitutesasubsetofthecollectedmaterial:Only“present”ratingsarerepresentedintherelease.Ratingwasconductedwithanefforttomaximizebalanceacrossaudioeventlabels.However,sincesegmentscanbelabeledwithmultipleau-dioevents(includingthealways-rated“Speech”and“Music”),cer-tainlabelsappearmuchmorefrequently.Asecondobjectivewastoavoiddrawingmorethanonesegmentfromanygivenvideo,toavoidcorrelationofexamples.Theseobjectiveswereachievedbyiterativelyaddingsegmentsfortheleast-representedclass(forwhichfurtherexamplesareavail-able).Outofthesetofcandidatesegmentsforthisaudioeventclass,preferenceisgiventosegmentsbearingthegreatestnumberofla-bels.Wealsoprovidemaximally-balancedtrainandtestsubsets(fromdisjointvideos),chosentoprovideatleast50positiveexam-ples(inbothsubsets)forasmanyclassesaspossible.Thesesetswereconstructedbyﬁrstcollectingexamplesfortherarestclasses,thenmovingontoless-rareclassesandaddingmoresegmentsonlywheretheyhadnotalreadypassedthethresholdof50.Evenso,verycommonlabelssuchas“Music”endedupwithmorethan5000labels.Theresultingdatasetincludes1,789,621segments(4,971hours),comprisingatleast100instancesfor485audioeventcat-egories.Theremainingcategoriesareeitherexcluded(blacklisted/abstractasdescribedinsection2.1),ordifﬁculttoﬁndusingourcur-rentapproaches.Wewillcontinuetodevelopmethodsforproposingsegmentsforrating,andaimeventuallytocoverallnon-excludedclasses.Theunbalancedtrainsetcontains1,771,873segmentsandtheevaluationsetcontains17,748.Becausesinglesegmentscanhavemultiplelabels(onaverage2.7labelspersegment),theoverallcountoflabelsisnotuniform,andisdistributedasshowninFig.3.“Music”isparticularlycommon,presentin56%ofthesegments.4.BENCHMARKTogiveasenseoftheperformancepossiblewiththisdata,wehavetrainedasimplebaselinesystem.Usingtheembeddinglayerrepre-sentationofadeep-networkclassiﬁertrainedonalargesetofgenericvideotopiclabels[22],weusedthetrainingportionoftheAudioSetYouTubeCorpustotrainashallowfully-connectedneuralnetworkclassiﬁerforthe485categoriesinthereleasedsegments.Weeval-uatedonthetestpartitionbyapplyingtheclassiﬁerto1secframestakenfromeachsegment,averagingthescores,thenforeachcate-goryrankingallsegmentsbytheirscores.Thissystemgaveabal-ancedmeanAveragePrecisionacrossthe485categoriesof0.314,andanaverageAUCof0.959(correspondingtoad-primeclasssep-arationof2.452).ThecategorywiththebestAPwas“Music”withAP/AUC/d-primeof0.896/0.951/2.34(reﬂectingitshighprior);theworstAPwasfor“Rattle”with0.020/0.796/1.168.5.CONCLUSIONWehaveintroducedtheAudioSetdatasetofgenericaudioevents,comprisinganontologyof632audioeventcategoriesandacollec-tionof1,789,621labeled10secexcerptsfromYouTubevideos.Theontologyishierarchicallystructuredwiththegoalofcoveringallacousticdistinctionsmadebya‘typical’listener.Wearereleasingthisdatatoaccelerateresearchintheareaofacousticeventdetection,justasImageNethasdrivenresearchinimageunderstanding.Inthefuture,wehopetobeabletomakelargerandimprovedreleases,ideallyincludingcontributionsfromthewidercommunity.Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 07:02:33 UTC from IEEE Xplore.  Restrictions apply.

780

6.REFERENCES[1]AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton,“Ima-genetclassiﬁcationwithdeepconvolutionalneuralnetworks,”inAdvancesinneuralinformationprocessingsystems,2012,pp.1097–1105.[2]ChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet,ScottReed,DragomirAnguelov,DumitruErhan,VincentVan-houcke,andAndrewRabinovich,“Goingdeeperwithconvo-lutions,”inProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,2015,pp.1–9.[3]KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun,“Deepresiduallearningforimagerecognition,”arXivpreprintarXiv:1512.03385,2015.[4]OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,San-jeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,etal.,“Imagenetlargescalevisualrecognitionchallenge,”InternationalJournalofCom-puterVision,vol.115,no.3,pp.211–252,2015.[5]WilliamHWarrenandRobertRVerbrugge,“Auditorypercep-tionofbreakingandbouncingevents:acasestudyinecolog-icalacoustics,”JournalofExperimentalPsychology:Humanperceptionandperformance,vol.10,no.5,pp.704,1984.[6]JamesABallas,“Commonfactorsintheidentiﬁcationofanassortmentofbriefeverydaysounds,”Journalofexperimentalpsychology:humanperceptionandperformance,vol.19,no.2,pp.250,1993.[7]BrianGygi,GaryRKidd,andCharlesSWatson,“Similarityandcategorizationofenvironmentalsounds,”Perception&psychophysics,vol.69,no.6,pp.839–855,2007.[8]GuillaumeLemaitreandLaurieM.Heller,“Evidenceforabasiclevelinataxonomyofeverydayactionsounds,”Experi-mentalBrainResearch,vol.226,pp.253–264,2013.[9]WilliamWGaver,“Whatintheworlddowehear?anecolog-icalapproachtoauditoryeventperception,”Ecologicalpsy-chology,vol.5,no.1,pp.1–29,1993.[10]TohomiroNakataniandHiroshiGOkuno,“Soundontologyforcomputationalauditorysceneanalysis,”inProceedingforthe1998conferenceoftheAmericanAssociationforArtiﬁcialIntelligence,1998.[11]SusanneBurger,QinJin,PeterFSchulam,andFlorianMetze,“Noisemes:Manualannotationofenvironmentalnoiseinau-diostreams,”Tech.Rep.CMU-LTI-12-07,CarnegieMellonUniversity,2012.[12]JustinSalamon,ChristopherJacoby,andJuanPabloBello,“Adatasetandtaxonomyforurbansoundresearch,”inProceed-ingsofthe22ndACMinternationalconferenceonMultimedia.ACM,2014,pp.1041–1044.[13]SebastianSager,DamianBorth,BenjaminElizalde,Chris-tianSchulze,BhikshaRaj,IanLane,andAndreasDengel,“Audiosentibank:Large-scalesemanticontologyofacous-ticconceptsforaudiocontentanalysis,”arXivpreprintarXiv:1607.03766,2016.[14]ErlingWold,ThomBlum,DouglasKeislar,andJamesWheaten,“Content-basedclassiﬁcation,search,andretrievalofaudio,”IEEEmultimedia,vol.3,no.3,pp.27–36,1996.[15]AndreyTemko,RobertMalkin,ChristianZieger,DuˇsanMa-cho,ClimentNadeu,andMaurizioOmologo,“Clearevalua-tionofacousticeventdetectionandclassiﬁcationsystems,”inInternationalEvaluationWorkshoponClassiﬁcationofEvents,ActivitiesandRelationships.Springer,2006,pp.311–322.[16]DanStowell,DimitriosGiannoulis,EmmanouilBenetos,MathieuLagrange,andMarkDPlumbley,“Detectionandclas-siﬁcationofacousticscenesandevents,”IEEETransactionsonMultimedia,vol.17,no.10,pp.1733–1746,2015.[17]AnnamariaMesaros,ToniHeittola,andTuomasVirtanen,“TUTdatabaseforacousticsceneclassiﬁcationandsoundeventdetection,”in24thEuropeanSignalProcessingCon-ference2016(EUSIPCO2016),Budapest,Hungary,2016,http://www.cs.tut.fi/sgn/arg/dcase2016/.[18]MartiAHearst,“Automaticacquisitionofhyponymsfromlargetextcorpora,”inProceedingsofthe14thconferenceonComputationallinguistics-Volume2.AssociationforCompu-tationalLinguistics,1992,pp.539–545.[19]AmitSinghal,“Introducingtheknowledgegraph:things,notstrings,”2012,OfﬁcialGoogleblog,https://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.html.[20]GeorgeAMiller,“WordNet:alexicaldatabaseforEnglish,”CommunicationsoftheACM,vol.38,no.11,pp.39–41,1995.[21]SamNorman-Haignere,NancyGKanwisher,andJoshHMc-Dermott,“Distinctcorticalpathwaysformusicandspeechre-vealedbyhypothesis-freevoxeldecomposition,”Neuron,vol.88,no.6,pp.1281–1296,2015.[22]ShawnHershey,SourishChaudhury,DanielP.W.Ellis,JortGemmeke,ArenJansen,R.ChanningMoore,ManojPlakal,RifA.Saurous,BrianSeybold,MalcolmSlaney,andRonWeiss,“CNNarchitecturesforlarge-scaleaudioclassiﬁca-tion,”inIEEEICASSP2017,NewOrleans,2017.

