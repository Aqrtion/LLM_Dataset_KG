6 1 0 2

r p A 7

]

V C . s c [

2 v 5 8 6 1 0 . 4 0 6 1 : v i X r a

The Cityscapes Dataset for Semantic Urban Scene Understanding

Marius Cordts1,2

Mohamed Omran3

Sebastian Ramos1,4

Timo Rehfeld1,2

Markus Enzweiler1

Rodrigo Benenson3

Uwe Franke1

Stefan Roth2

Bernt Schiele3

1Daimler AG R&D, 2TU Darmstadt, 3MPI Informatics, 4TU Dresden www.cityscapes-dataset.net

train/val – ﬁne annotation – 3475 images

train – coarse annotation – 20000 images

test – ﬁne annotation – 1525 images

Abstract

Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has beneﬁted enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.

1. Introduction

Visual scene understanding has moved from an elusive goal to a focus of much recent research in computer vision [27]. Semantic reasoning about the contents of a scene is thereby done on several levels of abstraction. Scene recognition aims to determine the overall scene category by putting emphasis on understanding its global properties, e.g. [46, 82]. Scene labeling methods, on the other hand, seek to identify the individual constituent parts of a whole scene as well as their interrelations on a more local pixel-

and instance-level, e.g. [41,71]. Specialized object-centric methods fall somewhere in between by focusing on detecting a certain subset of (mostly dynamic) scene constituents, e.g.[6,12,13,15]. Despitesigniﬁcantadvances, visualscene understanding remains challenging, particularly when taking human performance as a reference.

The resurrection of deep learning [34] has had a major impact on the current state-of-the-art in machine learning and computer vision. Many top-performing methods in a variety of applications are nowadays built around deep neural networks [30, 41, 66]. A major contributing factor to their success is the availability of large-scale, publicly available datasets such as ImageNet [59], PASCAL VOC [14], PASCAL-Context [45], and Microsoft COCO [38] that allow deep neural networks to develop their full potential.

Despite the existing gap to human performance, scene understanding approaches have started to become essential components of advanced real-world systems. A particularly popular and challenging application involves selfdriving cars, which make extreme demands on system performance and reliability. Consequently, signiﬁcant research efforts have gone into new vision technologies for understanding complex trafﬁc scenes and driving scenarios [4,16–18,58,62]. Also in this area, research progress can be heavily linked to the existence of datasets such as theKITTIVisionBenchmarkSuite[19], CamVid [7], Leuven [35], and Daimler Urban Segmentation [61] datasets. These urban scene datasets are often much smaller than datasets addressing more general settings. Moreover, we argue that they do not fully capture the variability and complexity of real-world inner-city trafﬁc scenes. Both shortcomings currently inhibit further progress in visual understanding of street scenes. To this end, we propose the Cityscapes benchmark suite and a corresponding dataset, speciﬁcally

1

. d l i u b

2 g n i k r a p

2 k c a r t

l i a r

e c n e f

l l a w

2 e g d i r b

s l e x i p

f o

r e b m u n

1010

d a o r

k l a w e d i s

108

106

2 l e n n u t

2 l i a r

d r a u g

. t e g e v

1 r a n c i a r r e t

1 e l c y c i b

1 instance-level annotations are available 2 ignored for evaluation

1 s u b

1 k c u r t

1 n i a r t

1 e l c y c r o t o m

2 , 1 n a v a r a c

2 , 1 r e l i a r t

y k s

n g i s

c ﬁ f a r t

e l o p

t h g i l

c ﬁ f a r t

2 p u o r g

e l o p

1 n o s r e p

1 r e d i r

2 c i t a t s

2 d n u o r g

2 c i m a n y d

ﬂat

construction

nature

vehicle

sky

object

human

void

Figure 1. Number of ﬁnely annotated pixels (y-axis) per class and their associated categories (x-axis).

tailored for autonomous driving in an urban environment and involving a much wider range of highly complex innercity street scenes that were recorded in 50 different cities. Cityscapes signiﬁcantly exceeds previous efforts in terms of size, annotation richness, and, more importantly, regarding scene complexity and variability. We go beyond pixel-level semantic labeling by also considering instance-level semantic labeling in both our annotations and evaluation metrics. To facilitate research on 3D scene understanding, we also provide depth information through stereo vision.

Very recently, [75] announced a new semantic scene labeling dataset for suburban trafﬁc scenes. It provides temporally consistent 3D semantic instance annotations with 2D annotations obtained through back-projection. We consider our efforts to be complementary given the differences in the way that semantic annotations are obtained, and in the type of scenes considered, i.e. suburban vs. inner-city trafﬁc. To maximize synergies between both datasets, a common label deﬁnition that allows for cross-dataset evaluation has been mutually agreed upon and implemented.

2. Dataset

Designing a large-scale dataset requires a multitude of decisions, e.g. on the modalities of data recording, data preparation, and the annotation protocol. Our choices were guided by the ultimate goal of enabling signiﬁcant progress in the ﬁeld of semantic urban scene understanding.

2.1. Data speciﬁcations

Our data recording and annotation methodology was carefully designed to capture the high variability of outdoor street scenes. Several hundreds of thousands of frames were acquired from a moving vehicle during the span of several months, covering spring, summer, and fall in 50 cities, primarily in Germany but also in neighboring countries. We deliberately did not record in adverse weather conditions, such as heavy rain or snow, as we believe such conditions to require specialized techniques and datasets [51].

Our camera system and post-processing reﬂect the current state-of-the-art in the automotive domain. Images were recorded with an automotive-grade 22cm baseline

stereo camera using 1/3in CMOS 2MP sensors (OnSemi AR0331) with rolling shutters at a frame-rate of 17Hz. The sensors were mounted behind the windshield and yield high dynamic-range (HDR) images with 16bits linear color depth. Each 16bit stereo image pair was subsequently debayered and rectiﬁed. We relied on [31] for extrinsic and intrinsic calibration. To ensure calibration accuracy we recalibrated on-site before each recording session.

For comparability and compatibility with existing datasets we also provide low dynamic-range (LDR) 8bit RGB images that are obtained by applying a logarithmic compression curve. Such tone mappings are common in automotive vision, since they can be computed efﬁciently and independently for each pixel. To facilitate highest annotationquality, weapplied aseparatetone mappingtoeach image. The resulting images are less realistic, but visually more pleasing and proved easier to annotate. 5000 images were manually selected from 27 cities for dense pixel-level annotation, aiming for high diversity of foreground objects, background, and overall scene layout. The annotations (see Sec. 2.2) were done on the 20th frame of a 30-frame video snippet, which we provide in full to supply context information. For the remaining 23 cities, a single image every 20s or 20m driving distance (whatever comes ﬁrst) was selected for coarse annotation, yielding 20000 images in total.

In addition to the rectiﬁed 16bit HDR and 8bit LDR stereo image pairs and corresponding annotations, our dataset includes vehicle odometry obtained from in-vehicle sensors, outside temperature, and GPS tracks.

2.2. Classes and annotations

We provide coarse and ﬁne annotations at pixel level in-

cluding instance-level labels for humans and vehicles.

Our 5000 ﬁne pixel-level annotations consist of layered polygons (à la LabelMe [60]) and were realized in-house to guarantee highest quality levels. Annotation and quality control required more than 1.5h on average for a single image. Annotators were asked to label the image from back to front such that no object boundary was marked more than once. Each annotation thus implicitly provides a depth ordering of the objects in the scene. Given our label scheme,

2

n o i t r o p o r p

0.4

0.3

0.2

0.1

0

Our dataset DUS

CamVid KITTI

0.02

0.01

0

ﬂat

construction

nature

vehicle

sky

object

human

void

human

Figure 2. Proportion of annotated pixels (y-axis) per category (x-axis) for Cityscapes, CamVid [7], DUS [61], and KITTI [19].

annotations can be easily extended to cover additional or more ﬁne-grained classes.

For our 20000 coarse pixel-level annotations, accuracy on object boundaries was traded off for annotation speed. We aimed to correctly annotate as many pixels as possible within a given span of less than 7min of annotation time per image. This was achieved by labeling coarse polygons under the sole constraint that each polygon must only include pixels belonging to a single object class.

In two experiments we assessed the quality of our labeling. First, 30 images were ﬁnely annotated twice by difIt ferent annotators and passed the same quality control. turned out that 96% of all pixels were assigned to the same label. Since our annotators were instructed to choose a void label if unclear (such that the region is ignored in training and evaluation), we exclude pixels having at least one void label and recount, yielding 98% agreement. Second, all our ﬁne annotations were additionally coarsely annotated such that we can enable research on densifying coarse labels. We found that 97% of all labeled pixels in the coarse annotationswereassignedthesameclassasintheﬁneannotations. We deﬁned 30 visual classes for annotation, which are grouped into eight categories: ﬂat, construction, nature, vehicle, sky, object, human, and void. Classes were selected based on their frequency, relevance from an application standpoint, practical considerations regarding the annotation effort, as well as to facilitate compatibility with existing datasets, e.g. [7,19,75]. Classes that are too rare are excluded from our benchmark, leaving 19 classes for evaluation, see Fig. 1 for details. We plan to release our annotation tool upon publication of the dataset.

2.3. Dataset splits

We split our densely annotated images into separate training, validation, and test sets. The coarsely annotated images serve as additional training data only. We chose not to split the data randomly, but rather in a way that ensures each split to be representative of the variability of different street scene scenarios. The underlying split criteria involve a balanced distribution of geographic location and population size of the individual cities, as well as regarding the time of year when recordings took place. Speciﬁcally, each of the three split sets is comprised of data recorded with the

#pixels [109]

annot. density [%]

Ours (ﬁne) Ours (coarse) CamVid DUS KITTI

9.43 26.0 0.62 0.14 0.23

97.1 67.5 96.2 63.0 88.9

Table 1. Absolute number and density of annotated pixels for Cityscapes, DUS, KITTI, and CamVid (upscaled to 1280 × 720 pixels to maintain the original aspect ratio).

following properties in equal shares: (i) in large, medium, andsmallcities; (ii)in thegeographicwest, center, andeast; (iii)inthegeographicnorth, center, andsouth; (iv)atthebeginning, middle, and end of the year. Note that the data is split at the city level, i.e. a city is completely within a single split. Following this scheme, we arrive at a unique split consisting of 2975 training and 500 validation images with publicly available annotations, as well as 1525 test images with annotations withheld for benchmarking purposes.

In order to assess how uniform (representative) the splits are regarding the four split characteristics, we trained a fully convolutional network [41] on the 500 images in our validation set. This model was then evaluated on the whole test set, as well as eight subsets thereof that reﬂect the extreme values of the four characteristics. With the exception of the time of year, the performance is very homogeneous, varying less than 1.5% points (often much less). Interestingly, the performance on the end of the year subset is 3.8% points better than on the whole test set. We hypothesize that this is due to softer lighting conditions in the frequently cloudy fall. To verify this hypothesis, we additionally tested on images taken in low- or high-temperature conditions, ﬁnding a 4.5% point increase in low temperatures (cloudy) and a 0.9% point decrease in warm (sunny) weather. Moreover, speciﬁcally training for either condition leads to an improvement on the respective test set, but not on the balanced set. These ﬁndings support our hypothesis and underline the importance of a dataset covering a wide range of conditions encountered in the real world in a balanced way.

2.4. Statistical analysis

We compare Cityscapes to other datasets in terms of (i) annotation volume and density, (ii) the distribution of visual

3

#humans [103]

#vehicles [103]

#h/image

#v/image

Ours (ﬁne) KITTI Caltech

24.4 6.1 1921

41.0 30.3 -

7.0 0.8 1.5

11.8 4.1 -

Table 2. Absolute and average number of instances for Cityscapes, KITTI, and Caltech (1 via interpolation) on the respective training and validation datasets.

classes, and (iii) scene complexity. Regarding the ﬁrst two aspects, we compare Cityscapes to other datasets with semantic pixel-wise annotations, i.e. CamVid [7], DUS [62], and KITTI [19]. Note that there are many other datasets with dense semantic annotations, e.g. [2, 56, 65, 69, 70]. However, we restrict this part of the analysis to those with a focus on autonomous driving.

CamVid consists of ten minutes of video footage with pixel-wise annotations for over 700 frames. DUS consists of a video sequence of 5000 images from which 500 have been annotated. KITTI addresses several different tasks including semantic labeling and object detection. As no ofﬁcial pixel-wise annotations exist for KITTI, several independent groups have annotated approximately 700 frames [22,29,32,33,58,64,77,80]. We map those labels to our high-level categories and analyze this consolidated set. In comparison, Cityscapes provides signiﬁcantly more annotated images, i.e. 5000 ﬁne and 20000 coarse annotations. Moreover, the annotation quality and richness is notably better. As Cityscapes provides recordings from 50 different cities, it also covers a signiﬁcantly larger area than previous datasets that contain images from a single city only, e.g. Cambridge (CamVid), Heidelberg (DUS), and Karlsruhe (KITTI). In terms of absolute and relative numbers of semantically annotated pixels (training, validation, and test data), Cityscapes compares favorably to CamVid, DUS, and KITTI with up to two orders of magnitude more annotated pixels, c.f. Tab. 1. The majority of all annotated pixels in Cityscapes belong to the coarse annotations, providing many individual (but correlated) training samples, but missing information close to object boundaries.

Figures 1 and 2 compare the distribution of annotations across individual classes and their associated higher-level categories. Notable differences stem from the inherently different conﬁgurations of the datasets. Cityscapes involves dense inner-city trafﬁc with wide roads and large intersections, whereas KITTI is composed of less busy suburban trafﬁc scenes. As a result, KITTI exhibits signiﬁcantly fewer ﬂat ground structures, fewer humans, and more nature. In terms of overall composition, DUS and CamVid seem more aligned with Cityscapes. Exceptions are an abundance of sky pixels in CamVid due to cameras with a comparably large vertical ﬁeld-of-view and the absence of certain categories in DUS, i.e. nature and object.

105

103

101

s e g a m

i

f o

r e b m u n

Cityscapes MS COCO

KITTI Pascal

0

1-9

19-27

37-45

55-63

73-81

>90

number of trafﬁc participant instances per image

Figure 3. Dataset statistics regarding scene complexity. Only MS COCO and Cityscapes provide instance segmentation masks.

s e l c i h e v

f o

r e b m u n

4k

3k

2k

1k

0

Our dataset KITTI

1

2

3

4

5

7

9 11 14 18 23 29 37 46 59 75 95

121

154

196

249

Figure 4. Histogram of object distances in meters for class vehicle.

Finally, we assess scene complexity, where density and scale of trafﬁc participants (humans and vehicles) serve as proxy measures. Out of the previously discussed datasets, only Cityscapes and KITTI provide instance-level annotations for humans and vehicles. We additionally compare to the Caltech Pedestrian Dataset [12], which only contains annotationsforhumans, butnoneforvehicles. Furthermore, KITTI and Caltech only provide instance-level annotations in terms ofaxis-aligned boundingboxes. We usethe respective training and validation splits for our analysis, since test set annotations are not publicly available for all datasets. In absolute terms, Cityscapes contains signiﬁcantly more object instance annotations than KITTI, see Tab. 2. Being a specialized benchmark, the Caltech dataset provides the most annotations for humans by a margin. The major share of those labels was obtained, however, by interpolation between a sparse set of manual annotations resulting in signiﬁcantly degraded label quality. The relative statistics emphasize the much higher complexity of Cityscapes, as the average numbers of object instances per image notably exceed those of KITTI and Caltech. We extend our analysis to MS COCO [38] and PASCAL VOC [14] that also contain street scenes while not being speciﬁc for them. We analyze the frequency of scenes with a certain number of trafﬁc participant instances, see Fig. 3. We ﬁnd our dataset to cover a greatervarietyofscenecomplexityandtohaveahigherportion of highly complex scenes than previous datasets. Using stereo data, we analyze the distribution of vehicle distances to the camera. From Fig. 4 we observe, that in comparison to KITTI, Cityscapes covers a larger distance range. We attribute this to both our higher-resolution imagery and the careful annotation procedure. As a consequence, algorithms need to take a larger range of scales and object sizes into account to score well in our benchmark.

4

3. Semantic Labeling

The ﬁrst Cityscapes task involves predicting a per-pixel semantic labeling of the image without considering higherlevel object instance or boundary information.

3.1. Tasks and metrics

TP

To assess labeling performance, we rely on a standard and a novel metric. The ﬁrst is the standard Jaccard Index, commonly known as the PASCAL VOC intersection-overunion metric IoU = TP+FP+FN [14], where TP, FP, and FN are the numbers of true positive, false positive, and false negative pixels, respectively, determined over the whole test set. Owing to the two semantic granularities, i.e. classes and categories, we report two separate mean performance scores: IoUcategory and IoUclass. In either case, pixels labeled as void do not contribute to the score.

iTP

The global IoU measure is biased toward object instances that cover a large image area. In street scenes with their strong scale variation this can be problematic. Specifically for trafﬁc participants, which are the key classes in our scenario, we aim to evaluate how well the individual instances in the scene are represented in the labeling. To address this, we additionally evaluate the semantic labeling using an instance-level intersection-over-union metric iIoU = iTP+FP+iFN. Here, iTP, and iFN denote weighted counts of true positive and false negative pixels, respectively. In contrast to the standard IoU measure, the contribution of each pixel is weighted by the ratio of the class’ average instance size to the size of the respective ground truth instance. As before, FP is the number of false positive pixels. It is important to note here that unlike the instancelevel task in Sec. 4, we assume that the methods only yield a standard per-pixel semantic class labeling as output. Therefore, the false positive pixels are not associated with any instance and thus do not require normalization. The ﬁnal scores, iIoUcategory and iIoUclass, are obtained as the means for the two semantic granularities, while only classes with instance annotations are included.

3.2. Control experiments

We conduct several control experiments to put our baseline results below into perspective. First, we count the relative frequency of every class label at each pixel location of the ﬁne (coarse) training annotations. Using the most frequentlabelateachpixelasaconstantpredictionirrespective of the test image (called static ﬁne, SF, and static coarse, SC) results in roughly 10% IoUclass, as shown in Tab. 3. These low scores emphasize the high diversity of our data. SC and SF having similar performance indicates the value of our additional coarse annotations. Even if the ground truth (GT) segments are re-classiﬁed using the most frequent training label (SF or SC) within each segment mask, the performance does not notably increase.

Secondly, we re-classify each ground truth segment using FCN-8s [41], c.f. Sec. 3.4. We compute the average scores within each segment and assign the maximizing label. The performance is signiﬁcantly better than the static predictors but still far from 100%. We conclude that it is necessary to optimize both classiﬁcation and segmentation quality at the same time.

Thirdly, we evaluate the performance of subsampled ground truth annotations as predictors. Subsampling was done by majority voting of neighboring pixels, followed by resampling back to full resolution. This yields an upper bound on the performance at a ﬁxed output resolution and is particularly relevant for deep learning approaches that often apply downscaling due to constraints on time, memory, or the network architecture itself. Downsampling factors 2 and 4 correspond to the most common setting of our 3rd-party baselines (Sec. 3.4). Note that while subsampling by a factor of 2 hardly affects the IoU score, it clearly decreases the iIoU score given its comparatively large impact on small, but nevertheless important objects. This underlines the importance of the separate instance-normalized evaluation. The downsampling factors of 8, 16, and 32 are motivated by the corresponding strides of the FCN model. The performance of a GT downsampling by a factor of 64 is comparable to the current state of the art, while downsampling by a factor of 128 is the smallest (power of 2) downsampling for which all images have a distinct labeling.

Lastly, we employ 128-times subsampled annotations and retrieve the nearest training annotation in terms of the Hamming distance. The full resolution version of this training annotation is then used as prediction, resulting in 21% IoUclass. While outperforming the static predictions, the poor result demonstrates the high variability of our dataset and its demand for approaches that generalize well.

3.3. State of the art

Drawing on the success of deep learning algorithms, a number of semantic labeling approaches have shown very promising results and signiﬁcantly advanced the state of the art. These new approaches take enormous advantage from recently introduced large-scale datasets, e.g. PASCALContext [45] and Microsoft COCO [38]. Cityscapes aims to complement these, particularly in the context of understanding complex urban scenarios, in order to enable further research in this area.

The popular work of Long et al. [41] showed how a topperforming Convolutional Neural Network (CNN) for image classiﬁcation can be successfully adapted for the task of semantic labeling. Following this line, [9,37,40,63,81] propose different approaches that combine the strengths of CNNs and Conditional Random Fields (CRFs).

Other work takes advantage of deep learning for explicitly integrating global scene context in the prediction

5

Average over

Classes

Categories

3.4. Baselines

Metric [%]

IoU

iIoU

IoU

iIoU

static ﬁne (SF) static coarse (SC) GT segmentation with SF GT segmentation with SC

GT segmentation with [41]

GT subsampled by 2 GT subsampled by 4 GT subsampled by 8 GT subsampled by 16 GT subsampled by 32 GT subsampled by 64 GT subsampled by 128

nearest training neighbor

10.1 10.3 10.1 10.9

79.4

97.2 95.2 90.7 84.6 75.4 63.8 50.6

21.3

4.7 5.0 6.3 6.3

52.6

92.6 90.4 82.8 70.8 53.7 35.1 21.1

5.9

26.3 27.5 26.5 29.6

93.3

97.6 96.0 92.1 87.4 80.2 71.0 60.6

39.7

19.9 21.7 25.0 27.0

80.9

93.3 91.2 83.9 72.9 58.1 39.6 29.9

18.6

Table 3. Quantitative results of control experiments for semantic labeling using the metrics presented in Sec. 3.1.

n i a r t

l a v

e s r a o c

b u s

2

(cid:88)

(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)

FCN-32s FCN-16s FCN-8s FCN-8s FCN-8s FCN-8s [4] extended (cid:88) 4 (cid:88) 4 [4] basic (cid:88) (cid:88) (cid:88) 3 [40] (cid:88) 2 [81] (cid:88) (cid:88) 2 [9] (cid:88) (cid:88) (cid:88) 2 [48] (cid:88) [37] (cid:88) [79]

Classes

Categories

IoU

61.3 64.3 65.3 61.9 58.3 58.0

56.1 57.0 59.1 62.5 63.1 64.8 66.4 67.1

iIoU

38.2 41.1 41.7 33.6 37.4 31.8

34.2 32.0 28.1 34.4 34.5 34.9 46.7 42.0

IoU

82.2 84.5 85.7 81.6 83.4 78.2

79.8 79.1 79.5 82.7 81.2 81.3 82.8 86.5

iIoU

65.4 69.2 70.1 60.9 67.2 58.4

66.4 61.9 57.9 66.0 58.7 58.7 67.4 71.1

Table 4. Quantitative results of baselines for semantic labeling using the metrics presented in Sec. 3.1. The ﬁrst block lists results from our own experiments, the second from those provided by 3rd parties. All numbers are given in percent and we indicate the used training data for each method, i.e. train ﬁne, val ﬁne, coarse extra as well as a potential downscaling factor (sub) of the input image.

of pixel-wise semantic labels, in particular through CNNs [4,39,44,67] or Recurrent Neural Networks (RNNs) [8,52]. Furthermore, a novel CNN architecture explicitly designed for dense prediction has been proposed recently by [79].

Last but not least, several studies [5,11,48–50,53,74,76] lately have explored different forms of weak supervision, such as bounding boxes or image-level labels, for training CNNsforpixel-levelsemanticlabeling. Wehopeourcoarse annotations can further advance this area.

6

Our own baseline experiments (Tab. 4, top) rely on fully convolutional networks (FCNs), as they are central to most state-of-the-art methods [9, 37, 41, 63, 81]. We adopted VGG16 [68] and utilize the PASCAL-context setup [41] with a modiﬁed learning rate to match our image resolution under an unnormalized loss. According to the notation in [41], we denote the different models as FCN-32s, FCN16s, and FCN-8s, where the numbers are the stride of the ﬁnest heatmap. Since VGG16 training on 2MP images exceeds even the largest GPU memory available, we split each image into two halves with sufﬁciently large overlap. Additionally, we trained a model on images downscaled by a factor of 2. We ﬁrst train on our training set (train) until the performance on our validation set (val) saturates, and then retrain on train+val with the same number of epochs.

To obtain further baseline results, we asked selected groups that have proposed state-of-the-art semantic labeling approaches to optimize their methods on our dataset and evaluated their predictions on our test set. The resulting scores are given in Tab. 4 (bottom) and qualitative examples of three selected methods are shown in Fig. 5. Interestingly enough, the performance ranking in terms of the main IoUclass score on Cityscapes is highly different from PASCAL VOC [14]. While DPN [40] is the 2nd best method on PASCAL, it is only the 6th best on Cityscapes. FCN8s [41] is last on PASCAL, but 3rd best on Cityscapes. Adelaide [37] performs consistently well on both datasets with rank 1 on PASCAL and 2 on Cityscapes.

From studying these results, we draw several conclusions: (1) The amount of downscaling applied during training and testing has a strong and consistent negative inﬂuence on performance (c.f. FCN-8s vs. FCN-8s at half resolution, as well as the 2nd half of the table). The ranking according to IoUclass is strictly consistent with the degree of downscaling. We attribute this to the large scale variation present in our dataset, c.f. Fig. 4. This observation clearly indicates the demand for additional research in the direction of memory and computationally efﬁcient CNNs when facing such a large-scale dataset with high-resolution images. (2) Our novel iIoU metric treats instances of any size equally and is therefore more sensitive to errors in predicting small objects compared to the IoU. Methods that leverage a CRF for regularization [9, 40, 48, 81] tend to over smooth small objects, c.f. Fig. 5, hence show a larger drop from IoU to iIoU than [4] or FCN-8s [41]. [37] is the only exception; its speciﬁc FCN-derived pairwise terms apparently allow for a more selective regularization. (3) When considering IoUcategory, Dilated10 [79] and FCN8s [41] perform particularly well, indicating that these approaches produce comparatively many confusions between the classes within the same category, c.f. the buses in Fig. 5 (top). (4) Training FCN-8s [41] with 500 densely annotated

Dataset

Best reported result Our result

4.1. Tasks and metrics

Camvid [7] KITTI [58] KITTI [64]

62.9 [4] 61.6 [4] 82.2 [73]

72.6 70.9 81.2

Table 5. Quantitative results (avg. recall in percent) of our half-resolution FCN-8s model trained on Cityscapes images and tested on Camvid and KITTI.

images (750h of annotation) yields comparable IoU performance to a model trained on 20000 weakly annotated images (1300h annot.), c.f. rows 5 & 6 in Tab. 4. However, in both cases the performance is signiﬁcantly lower than FCN8s trained on all 3475 densely annotated images. Many ﬁne labels are thus important for training standard methods as well as for testing, but the performance using coarse annotationsonlydoesnotcollapseandpresentsaviableoption. (5) Since the coarse annotations do not include small or distant instances, their iIoU performance is worse. (6) Coarse labels can complement the dense labels if applying appropriate methods as evidenced by [48] outperforming [9], which it extends by exploiting both dense and weak annotations (e.g. bounding boxes). Our dataset will hopefully stimulate research on exploiting the coarse labels further, especially given the interest in this area, e.g. [25,43,47].

Overall, we believe that the unique characteristics of our dataset (e.g. scale variation, amount of small objects, focus on urban street scenes) allow for more such novel insights.

3.5. Cross-dataset evaluation

In order to show the compatibility and complementarity ofCityscapesregardingrelateddatasets, weappliedanFCN model trained on our data to Camvid [7] and two subsets of KITTI [58,64]. We use the half-resolution model (c.f. 4th row in Tab. 4) to better match the target datasets, but we do not apply any speciﬁc training or ﬁne-tuning. In all cases, we follow the evaluation of the respective dataset to be able to compare to previously reported results [4,73]. The obtained results in Tab. 5 show that our large-scale dataset enables us to train models that are on a par with or even outperforming methods that are speciﬁcally trained on another benchmark and specialized for its test data. Further, our analysis shows that our new dataset integrates well with existing ones and allows for cross-dataset research.

4. Instance-Level Semantic Labeling

The pixel-level task, c.f. Sec. 3, does not aim to segment individual object instances. In contrast, in the instancelevel semantic labeling task, we focus on simultaneously detecting objects and segmenting them. This is an extension to both traditional object detection, since per-instance segments must be provided, and semantic labeling, since each instance is treated as a separate label.

For instance-level semantic labeling, algorithms are required to deliver a set of detections of trafﬁc participants in the scene, each associated with a conﬁdence score and a per-instance segmentation mask. To assess instance-level performance, we compute the average precision on the region level (AP [23]) for each class and average it across a range of overlap thresholds to avoid a bias towards a speciﬁc value. Speciﬁcally, we follow [38] and use 10 different overlaps ranging from 0.5 to 0.95 in steps of 0.05. The overlap is computed at the region level, making it equivalent to the IoU of a single instance. We penalize multiple predictions of the same ground truth instance as false positives. To obtain a single, easy to compare compound score, we report the mean average precision AP, obtained by also averaging over the class label set. As minor scores, we add AP50% for an overlap value of 50%, as well as AP100m and AP50m where the evaluation is restricted to objects within 100m and 50m distance, respectively.

4.2. State of the art

As detection results have matured (70% mean AP on PASCAL [14,55]), the last years have seen a rising interest in more difﬁcult settings. Detections with pixel-level segments rather than traditional bounding boxes provide a richer output and allow (in principle) for better occlusion handling. We group existing methods into three categories. The ﬁrst encompasses segmentation, then detection and most prominently the R-CNN detection framework [21], relying on object proposals for generating detections. Many of the commonly used bounding box proposal methods [28, 54] ﬁrst generate a set of overlapping segments, e.g. Selective Search [72] or MCG [1]. In R-CNN, bounding boxes of each segment are then scored using a CNN-based classiﬁer, while each segment is treated independently.

The second category encompasses detection, then segmentation, where bounding-box detections are reﬁned to instance speciﬁc segmentations. Either CNNs [23, 24] or non-parametric methods [10] are typically used, however, in both cases without coupling between individual predictions.

Third, simultaneous detection and segmentation is signiﬁcantly more delicate. Earlier methods relied on Hough voting [36, 57]. More recent works formulate a joint inference problem on pixel and instance level using CRFs [11, 26, 42, 71, 78, 80]. Differences lie in the generation of proposals (exemplars, average class shape, direct regression), the cues considered (pixel-level labeling, depth ordering), and the inference method (probabilistic, heuristics).

4.3. Lower bounds, oracles & baselines

In Tab. 6, we provide lower-bounds that any sensible method should improve upon, as well as oracle-case results

7

Figure 5. Qualitative examples of selected baselines. From left to right: image with stereo depth maps partially overlayed, annotation, DeepLab [48], Adelaide [37], and Dilated10 [79]. The color coding of the semantic classes matches Fig. 1.

Proposals

Classif. AP

AP50% AP100m AP50m

5. Conclusion and Outlook

MCG regions MCG bboxes MCG hulls

GT bboxes GT regions

2.6 FRCN 3.8 FRCN FRCN 4.6

8.2 FRCN FRCN 41.3

MCG regions GT GT MCG bboxes GT MCG hulls

10.5 9.9 11.6

9.0 11.3 12.9

23.7 41.3

27.0 25.8 29.1

4.4 6.5 7.7

12.6 58.1

16.0 15.3 17.7

5.5 8.9 10.3

15.2 64.9

18.7 18.9 21.4

Table 6. Baseline results on instance-level semantic labeling task using the metrics described in Sec. 4. All numbers in %.

(i.e. using the test time ground truth). For our experiments, we rely on publicly available implementations. We train a Fast-R-CNN (FRCN) detector [20] on our training data in order to score MCG object proposals [1]. Then, we use either its output bounding boxes as (rectangular) segmentations, the associated region proposal, or its convex hull as a per-instance segmentation. The best main score AP is 4.6%, is obtained with convex hull proposals, and becomes larger when restricting the evaluation to 50% overlap or close instances. We contribute these rather low scores to our challenging dataset, biased towards busy and cluttered scenes, where many, often highly occluded, objects occur at various scales, c.f. Sec. 2. Further, the MCG bottom-up proposals seem to be unsuited for such street scenes and cause extremely low scores when requiring large overlaps. We conﬁrm this interpretation with oracle experiments, where we replace the proposals at test-time with ground truth segments or replace the FRCN classiﬁer with an oracle. In doing so, the task of object localization is decoupled from the classiﬁcation task. The results in Tab. 6 show that when bound to MCG proposals, the oracle classiﬁer is onlyslightlybetterthanFRCN.Ontheotherhand, whenthe proposals are perfect, FRCN achieves decent results. Overall, these observations unveil that the instance-level performance of our baseline is bound by the region proposals.

In this work, we presented Cityscapes, a comprehensive benchmark suite that has been carefully designed to spark progress in semantic urban scene understanding by: (i) creating the largest and most diverse dataset of street scenes with high-quality and coarse annotations to date; (ii) developing a sound evaluation methodology for pixel-level and instance-level semantic labeling; (iii) providing an in-depth analysis of the characteristics of our dataset; (iv) evaluating several state-of-the-art approaches on our benchmark. To keep pace with the rapid progress in scene understanding, we plan to adapt Cityscapes to future needs over time.

The signiﬁcance of Cityscapes is all the more apparent from three observations. First, the relative order of performance for state-of-the-art methods on our dataset is notably different than on more generic datasets such as PASCAL VOC. Our conclusion is that serious progress in urban scene understanding may not be achievable through such generic datasets. Second, the current state-of-the-art in semantic labeling on KITTI and CamVid is easily reached and to some extentevenoutperformedbyapplyinganoff-the-shelffullyconvolutional network [41] trained on Cityscapes only, as demonstrated in Sec. 3.5. This underlines the compatibility and unique beneﬁt of our dataset. Third, Cityscapes will pose a signiﬁcant new challenge for our ﬁeld given that it is currently far from being solved. The best performing baseline for pixel-level semantic segmentation obtains an IoU score of 67.1%, whereas the best current methods on PASCAL VOC and KITTI reach IoU levels of 77.9% [3] and 72.5% [73], respectively. In addition, the instance-level task is particularly challenging with an AP score of 4.6%.

Acknowledgments. S. Roth was supported in part by the European Research Council under the EU’s 7th Framework Programme (FP/2007-2013)/ERC Grant agreement no. 307942. The authors acknowledge the support of the Bundesministerium für Wirtschaft und Technologie (BMWi) in the context of the UR:BAN initiative. We thank the 3rd-party authors for their valuable submissions.

8

References

[1] P. Arbelaez, J. Pont-Tuset, J. Barron, F. Marqués, and J. Malik. Multiscale combinatorial grouping. In CVPR, 2014. 7, 8, ii

[2] S. Ardeshir, K. M. Collins-Sibley, and M. Shah. Geo-

semantic segmentation. In CVPR, 2015. 4, ii

[3] A. Arnab, S. Jayasumana, S. Zheng, and P. H. S. Torr. Higher order conditional random ﬁelds in deep neural networks. arXiv:1511.08119v3 [cs.CV], 2015. 8

[4] V. Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. arXiv:1511.00561v2 [cs.CV], 2015. 1, 6, 7, ii, xii, xv, xvii

[5] A. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei. What’s the point: Semantic segmentation with point supervision. arXiv:1506.02106v4 [cs.CV], 2015. 6

[6] R. Benenson, M. Mathias, R. Timofte, and L. Van Gool. In CVPR,

Pedestrian detection at 100 frames per second. 2012. 1

[7] G. J. Brostow, J. Fauqueur, and R. Cipolla. Semantic object classes in video: A high-deﬁnition ground truth database. Pattern Recognition Letters, 30(2):88–97, 2009. 1, 3, 4, 7, ii [8] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene Labeling with LSTM Recurrent Neural Networks. In CVPR, 2015. 6

[9] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep convolutional nets and fully connected CRFs. In ICLR, 2015. 5, 6, 7, ii, xii

[10] Y.-T. Chen, X. Liu, and M.-H. Yang. Multi-instance object

segmentation with occlusion handling. In CVPR, 2015. 7

[11] J. Dai, K. He, and J. Sun. Convolutional feature masking for joint object and stuff segmentation. In CVPR, 2015. 6, 7 [12] P. Dollár, C. Wojek, B. Schiele, and P. Perona. Pedestrian detection: An evaluation of the state of the art. Trans. PAMI, 34(4):743–761, 2012. 1, 4

[13] M. Enzweiler and D. M. Gavrila. Monocular pedestrian detection: Survey and experiments. 31(12):2179–2195, 2009. 1

[14] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL visual object classes challenge: A retrospective. IJCV, 111(1):98– 136, 2015. 1, 4, 5, 6, 7, ii

[15] P.F.Felzenszwalb, R.B.Girshick, D.McAllester, andD.Ramanan. Object detection with discriminatively trained partbased models. Trans. PAMI, 32(9):1627–1645, 2010. 1 [16] U. Franke, D. Pfeiffer, C. Rabe, C. Knöppel, M. Enzweiler, F. Stein, and R. G. Herrtwich. Making Bertha see. In ICCV Workshops, 2013. 1

[17] P. Furgale, U. Schwesinger, M. Ruﬂi, W. Derendarz, H. Grimmett, P. Mühlfellner, S. Wonneberger, B. Li, et al. Toward automated driving in cities using close-to-market sensors: An overview of the V-Charge project. In IV Symposium, 2013. 1

[18] A.Geiger, M.Lauer, C.Wojek, C.Stiller, andR.Urtasun. 3D trafﬁc scene understanding from movable platforms. Trans. PAMI, 36(5):1012–1025, 2014. 1

[19] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. IJRR, 32(11), 2013. 1, 3, 4, ii

[20] R. Girshick. Fast R-CNN. In ICCV, 2015. 8, ii [21] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 7

[22] F. Gueney and A. Geiger. Displets: Resolving stereo ambiguities using object knowledge. In CVPR, 2015. 4, ii [23] B. Hariharan, P. Arbeláez, R. B. Girshick, and J. Malik. Simultaneous detection and segmentation. In ECCV, 2014. 7 [24] B. Hariharan, P. A. Arbeláez, R. B. Girshick, and J. Malik. Hypercolumns for object segmentation and ﬁne-grained localization. In CVPR, 2015. 7

[25] H. Hattori, V. N. Boddeti, K. M. Kitani, and T. Kanade. Learning scene-speciﬁc pedestrian detectors without real data. In CVPR, 2015. 7

[26] X. He and S. Gould. An exemplar-based CRF for multi-

instance object segmentation. In CVPR, 2014. 7

[27] D. Hoiem, J. Hays, J. Xiao, and A. Khosla. Guest editorial:

Scene understanding. IJCV, 2015. 1

[28] J. Hosang, R. Benenson, P. Dollár, and B. Schiele. What Trans. PAMI,

makes for effective detection proposals? 38(4):814–830, 2015. 7

[29] H. Hu and B. Upcroft. Nonparametric semantic segmenta-

tion for 3D street scenes. In IROS, 2013. 4, ii [30] A. Krizhevsky, I. Sutskever, and G. E. Hinton.

classiﬁcation with deep convolutional neural networks. NIPS, 2012. 1

ImageNet In

[31] L. Krüger, C. Wöhler, A. Würz-Wessel, and F. Stein.

Infactory calibration of multiocular camera systems. In SPIE Photonics Europe (Optical Metrology in Production Engineering), 2004. 2

[32] A. Kundu, Y. Li, F. Dellaert, F. Li, and J. Rehg. Joint semantic segmentation and 3D reconstruction from monocular video. In ECCV, 2014. 4, ii

[33] L. Ladicky, J. Shi, and M. Pollefeys. Pulling things out of

perspective. In CVPR, 2014. 4, ii

[34] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature,

2015. 1

[35] B. Leibe, N. Cornelis, K. Cornelis, and L. Van Gool. Dynamic 3D scene analysis from a moving vehicle. In CVPR, 2007. 1, ii

[36] B. Leibe, A. Leonardis, and B. Schiele. Robust object detection with interleaved categorization and segmentation. IJCV, 77(1-3):259–289, 2008. 7

[37] G. Lin, C. Shen, A. van den Hengel, and I. Reid. Efﬁcient piecewise training of deep structured models for semantic segmentation. In CVPR, 2016, to appear. 5, 6, 8, i, ii, xii, xv, xvii

[38] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014. 1, 4, 5, 7, ii [39] W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking

wider to see better. arXiv:1506.04579v2 [cs.CV], 2015. 6

[40] Z. Liu, X. Li, P. Luo, C. C. Loy, and X. Tang. Semantic image segmentation via deep parsing network. In ICCV, 2015. 5, 6, ii, xii, xv, xvii

9

[41] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 1, 3, 5, 6, 8, i, xi, xiv, xvi

[42] M. Maire, S. X. Yu, and P. Perona. Object detection and segmentation from joint embedding of parts and pixels. In ICCV, 2011. 7

[43] I. Misra, A. Shrivastava, and M. Hebert. Watch and learn: Semi-supervised learning for object detectors from video. In CVPR, 2015. 7

[44] M. Mostajabi, P. Yadollahpour, and G. Shakhnarovich. Feedforward semantic segmentation with zoom-out features. In CVPR, 2015. 6

[45] R. Mottaghi, X. Chen, X. Liu, N.-G. Cho, S.-W. Lee, S. Fidler, R. Urtasun, and A. Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, 2014. 1, 5, i, ii

[46] A. Oliva and A. Torralba. Modeling the shape of the scene: IJCV,

A holistic representation of the spatial envelope. 42(3):145–175, 2001. 1

[47] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object localization for free? Weakly-supervised learning with convolutional neural networks. In CVPR, 2015. 7

[48] G. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille. Weakly- and semi-supervised learning of a DCNN for semantic image segmentation. In ICCV, 2015. 6, 7, 8, ii, xii, xv, xvii

[49] D. Pathak, P. Kraehenbuehl, and T. Darrell. Constrained convolutional neural networks for weakly supervised segmentation. In ICCV, 2015. 6

[50] D. Pathak, E. Shelhamer, J. Long, and T. Darrell. Fully convolutional multi-class multiple instance learning. In ICLR, 2015. 6

[51] D. Pfeiffer, S. K. Gehrig, and N. Schneider. Exploiting the

power of stereo conﬁdences. In CVPR, 2013. 2

[52] P. H. Pinheiro and R. Collobert. Recurrent convolutional neural networks for scene parsing. In ICML, 2014. 6 [53] P. H. Pinheiro and R. Collobert. From image-level to pixellevel labeling with convolutional networks. In CVPR, 2015. 6

[54] J. Pont-Tuset and L. Van Gool. Boosting object proposals:

From Pascal to COCO. In ICCV, 2015. 7

[55] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, 2015. 7

[56] H. Riemenschneider, A. Bódis-Szomorú, J. Weissenberg, and L. Van Gool. Learning where to classify in multi-view semantic segmentation. In ECCV. 2014. 4, ii

[57] H.Riemenschneider, S. Sternig, M.Donoser, P. M. Roth, and H. Bischof. Hough regions for joining instance localization and segmentation. In ECCV, 2012. 7

[58] G. Ros, S. Ramos, M. Granados, D. Vazquez, and A. M. Lopez. Vision-based ofﬂine-online perception paradigm for autonomous driving. In WACV, 2015. 1, 4, 7, ii

[59] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A.C.Berg, andL.Fei-Fei. Imagenetlargescalevisualrecognition challenge. IJCV, 115(3):211–252, 2015. 1, i, ii

[60] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. LabelMe: A database and web-based tool for image annotation. IJCV, 77(1-3):157–173, 2008. 2, i

[61] T. Scharwächter, M. Enzweiler, U. Franke, and S. Roth. Efﬁcient multi-cue scene segmentation. In GCPR, 2013. 1, 3, ii

[62] T. Scharwächter, M. Enzweiler, U. Franke, and S. Roth. Stixmantics: Amedium-levelmodelforreal-timesemanticscene understanding. In ECCV, 2014. 1, 4

[63] A. Schwing and R. Urtasun. Fully connected deep structured

networks. arXiv:1503.02351v1 [cs.CV], 2015. 5, 6

[64] S. Sengupta, E. Greveson, A. Shahrokni, and P. H. S. Torr. Semantic modelling of urban scenes. In ICRA, 2013. 4, 7, ii [65] S. Sengupta, P. Sturgess, L. Ladicky, and P. H. S. Torr. Automatic dense visual semantic mapping from street-level imagery. In IROS, 2012. 4, ii

[66] P.Sermanet, D.Eigen, X.Zhang, M.Mathieu, R.Fergus, and Y. LeCun. OverFeat: Integrated recognition, localization and detection using convolutional networks. In ICLR, 2014. 1

[67] A. Sharma, O. Tuzel, and D. W. Jacobs. Deep hierarchical parsing for semantic segmentation. In CVPR, 2015. 6

[68] K. Simonyan and A. Zisserman.

volutional networks for arXiv:1409.1556v6 [cs.CV], 2014. 6, i

Very deep conlarge-scale image recognition.

[69] S. Song, S. P. Lichtenberg, and J. Xiao. Sun RGB-D: A In CVPR,

RGB-D scene understanding benchmark suite. 2015. 4, ii

[70] J. Tighe and S. Lazebnik. Superparsing. IJCV, 101(2):329–

349, 2013. 4

[71] J. Tighe, M. Niethammer, and S. Lazebnik. Scene parsing with object instance inference using regions and perexemplar detectors. IJCV, 112(2):150–171, 2015. 1, 7 [72] J. R. R. Uijlings, K. E. A. van de Sande, T. Gevers, and A. W. M. Smeulders. Selective search for object recognition. International journal of computer vision, 104(2):154–171, 2013. 7

[73] V. Vineet, O. Miksik, M. Lidegaard, M. Niessner, S. Golodetz, V. A. Prisacariu, O. Kahler, D. W. Murray, S. Izadi, P. Perez, and P. H. S. Torr. Incremental dense semantic stereo fusion for large-scale semantic scene reconstruction. In ICRA, 2015. 7, 8

[74] Y. Wei, X. Liang, Y. Chen, X. Shen, M.-M. Cheng, Y. Zhao, and S. Yan. STC: A simple to complex framework for weakly-supervised semantic segmentation. arXiv:1509.03150v1 [cs.CV], 2015. 6

[75] J. Xie, M. Kiefel, M.-T. Sun, and A. Geiger. Semantic instance annotation of street scenes by 3D to 2D label transfer. In CVPR, 2016, to appear. 2, 3, ii

[76] J. Xu, A. G. Schwing, and R. Urtasun. Learning to segment under various forms of weak supervision. In CVPR, 2015. 6 [77] P. Xu, F. Davoine, J.-B. Bordes, H. Zhao, and T. Denoeux. Information fusion on oversegmented images: An application for urban scene understanding. In MVA, 2013. 4, ii [78] J. Yao, S. Fidler, and R. Urtasun. Describing the scene as a whole: Joint object detection, scene classiﬁcation and semantic segmentation. In CVPR, 2012. 7

10

[79] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016, to appear. 6, 8, i, ii, xii, xv, xvii

[80] Z. Zhang, A. Schwing, S. Fidler, and R. Urtasun. Monocular object instance segmentation and depth ordering with CNNs. In ICCV, 2015. 4, 7, ii

[81] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, and P. H. S. Torr. Conditional random ﬁelds as recurrent neural networks. In ICCV, 2015. 5, 6, ii, xii, xv, xvii

[82] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In NIPS, 2014. 1

11

A. Related Datasets

In Tab. 7 we provide a comparison to other related datasets in terms of the type of annotations, the meta information provided, the camera perspective, the type of scenes, and their size. The selected datasets are either of large scale or focus on street scenes.

B. Class Deﬁnitions

Table 8 provides precise deﬁnitions of our annotated classes. These deﬁnitions were used to guide our labeling process, as well as quality control. In addition, we include a typical example for each class.

The annotators were instructed to make use of the depth ordering and occlusions of the scene to accelerate labeling, analogously to LabelMe [60]; see Fig. 6 for an example. In doing so, distant objects are annotated ﬁrst, while occluded parts are annotated with a coarser, conservative boundary (possibly larger than the actual object). Subsequently, the occluder is annotated with a polygon that lies in front of the occluded part. Thus, the boundary between these objects is shared and consistent.

Holes in an object through which a background region can be seen are considered to be part of the object. This allows keeping the labeling effort within reasonable bounds such that objects can be described via simple polygons forming simply-connected sets.

C. Example Annotations

Figure 7 presents several examples of annotated frames from our dataset that exemplify its diversity and difﬁculty. All examples are taken from the train and val splits and were chosen by searching for the extremes in terms of the number of trafﬁc participant instances in the scene; see Fig. 7 for details.

D. Detailed Results

In this section, we present additional details regarding our control experiments and baselines. Speciﬁcally, we give individual class scores that complement the aggregated scores in the main paper. Moreover, we provide details on the training procedure for all baselines. Finally, we show additional qualitative results of all methods.

D.1. Semantic labeling

Tables 9 and 11 list all individual class-level IoU scores for all control experiments and baselines. Tables 10 and 12 give the corresponding instance-normalized iIoU scores. In addition, Figs. 8 and 9 contain qualitative examples of these methods.

Basic setup. All baselines relied on single frame, monocular LDR images and were pretrained on ImageNet [59], i.e.

Figure 6. Exemplary labeling process. Distant objects are annotated ﬁrst and subsequently their occluders. This ensures the boundary between these objects to be shared and consistent.

their underlying CNN was generally initialized with ImageNet VGG weights [68]. Subsequently, the CNNs were ﬁnetuned on Cityscapes using the respective portions listed in Tab. 4. In our own FCN [41] experiments, we additionally investigated ﬁrst pretraining on PASCAL-Context [45], but found this to not inﬂuence performance given a sufﬁ- ciently large number of training iterations. Most baselines applied a subsampling of the input image, c.f. Tab. 4, probably due to time or memory constraints. Only Adelaide [37], Dilated10 [79], and our FCN experiments were conducted on the full-resolution images. In the ﬁrst case, a new random patch of size 614 × 614 pixels was drawn at each iteration. In our FCN training, we split each image into two halves (left and right) with an overlap that is sufﬁciently large considering the network’s receptive ﬁeld.

Own baselines. The training procedure of all our FCN experiments follows [41]. We use three-stage training with subsequently smaller strides, i.e. ﬁrst FCN-32s, then FCN16s, and then FCN-8s, always initializing with the parameters from the previous stage. We add a 4th stage for which we reduce the learning rate by a factor of 10. The training parameters are identical to those publicly available for training on PASCAL-Context [45], except that we reduce the learning rate to account for the increased image resolution. Each stage is trained until convergence on the validation set; pixels with void ground truth are ignored such that they do not induce any gradient. Eventually, we retrain on train and val together with the same number of epochs, yielding 243250, 69500, 62550, and 5950 iterations for stages 1 through 4. Note that each iteration corresponds to half of an image (see above). For the variant with factor 2 downsampling, no image splitting is necessary, yielding 80325, 68425, 35700, and 5950 iterations in the respective stages. The variant only trained on val (full resolution) uses train for validation, leading to 130000, 35700, 47600, and 0 iterations in the 4 stages. Our last FCN variant is trained using the coarse annotations only, with 386750, 113050, 35700, and 0 iterations in the respective stage; pixels with void ground truth are ignored here as well.

3rd-party baselines. Note that for the following descrip-

i

Dataset

Labels

Color

Video

Depth

Camera

Scene

#images

#classes

B B, C D C D, C B, Da D D D D C D D D, C

[59] [14] [45] [38] [69] [19] [7] [35] [61] [2] [65] [56] [75] Ours a Including the annotations of 3rd party groups [22,29,32,33,58,64,77,80]

× × × × Kinect Laser, Stereo × Stereo, Manual Stereo × Stereo 3D mesh Laser Stereo

Mixed Mixed Mixed Mixed Pedestrian Car Car Car Car Pedestrian Car Pedestrian Car Car

× × × × × (cid:88) (cid:88) (cid:88) (cid:88) × × × (cid:88) (cid:88)

(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) × (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)

Mixed Mixed Mixed Mixed Indoor Suburban Urban Urban Urban Urban Facades Urban Suburban Urban

150k 20k (B), 10k (C) 20k 300k 10k 15k (B), 700 (D) 701 70 500 200 86 428 400k 5k (D), 20k (C)

1000 20 400 80 37 3 (B), 8 (D) 32 7 5 2 13 8 27 30

Table 7. Comparison to related datasets. We list the type of labels provided, i.e. object bounding boxes (B), dense pixel-level semantic labels (D), coarse labels (C) that do not aim to label the whole image. Further, we mark if color, video, and depth information are available. We list the camera perspective, the scene type, the number of images, and the number of semantic classes.

tions of the 3rd-party baselines, we have to rely on authorprovided information.

SegNet [4] training for both the basic and extended variant was performed until convergence, yielding approximately 50 epochs. Inference takes 0.12s per image.

DPN[40]wastrainedusingtheoriginalprocedure, while

using all available Cityscapes annotations.

For training CRF as RNN [81], an FCN-32s model was trained for 3 days on train using a GPU. Subsequently an FCN-8s model was trained for 2 days, and eventually the model was further ﬁnetuned including the CRF-RNN layers. Testing takes 0.7s on half-resolution images.

For training DeepLab on the ﬁne annotations, denoted DeepLab-LargeFOV-Strong, the authors applied the training procedure from [9]. The model was trained on train for 40000 iterations until convergence on val. Then val was included in the training set for another 40000 iterations. In both cases, a mini-batch size of 10 was applied. Each training iteration lasts 0.5s, while inference including the dense CRF takes 4s per image. The DeepLab variant including our coarse annotations, termed DeepLabLargeFOV-StrongWeak, followed the protocol in [48] and is initialized from the DeepLab-LargeFOV-Strong model. Each mini-batch consists of 5 ﬁnely and 5 coarsely annotated images and training is performed for 20000 iterations until convergence on val. Then, training was continued for another 20000 iterations on train and val.

Adelaide [37] was trained for 8 days using random crops of the input image as described above. Inference on a single image takes 35s.

The best performing baseline, Dilated10 [79], is a convolutionalnetworkthatconsistsofafront-endpredictionmodule and a context aggregation module. The front-end moduleisanadaptationoftheVGG-16networkbasedondilated

convolutions. The context module uses dilated convolutions to systematically expand the receptive ﬁeld and aggregate contextual information. This module is derived from the “Basic" network, where each layer has C = 19 feature maps. The total number of layers in the context module is 10, hence the name Dilation10. The increased number of layers in the context module (10 for Cityscapes versus 8 for PASCAL VOC) is due to the higher input resolution. The completeDilation10modelisapureconvolutionalnetwork: there is no CRF and no structured prediction. The Dilation10 network was trained in three stages. First, the frontend prediction module was trained for 40000 iterations on randomly sampled crops of size 628×628, with learning rate 10−4, momentum 0.99, and batch size 8. Second, the context module was trained for 24000 iterations on whole (uncropped) images, with learning rate 10−4, momentum 0.99, and batch size 100. Third, the complete model (front-end + context) was jointly trained for 60000 iterations on halves of images (input size 1396×1396, including padding), with learning rate 10−5, momentum 0.99, and batch size 1.

D.2. Instance-level semantic labeling

For our instance-level semantic labeling baselines and control experiments, we rely on Fast R-CNN [20] and proposal regions from either MCG (Multiscale Combinatorial Grouping [1]) or from the ground truth annotations.

We use the standard training and testing parameters for Fast R-CNN. Training starts with a model pre-trained on ImageNet [59]. We use a learning rate of 0.001 and stop when the validation error plateaus after 120000 iterations. Attesttime, onescoreperclassisassignedtoeachobject proposal. Subsequently, thresholding and non-maximum suppression is applied and either the bounding boxes, the original proposal regions or their convex hull are used to

ii

generate the predicted masks of each instance. Quantitative results of all classes can be found in Tables 13 to 16 and qualitative results in Fig. 12.

iii

Category

Class

Deﬁnition

Examples

human

person1

rider1

All humans that would primarily rely on their legs to move if necessary. Consequently, this label includes people who are standing/sitting, or otherwise stationary. This class also includes babies, people pushing a bicycle, or standing next to it with both legs on the same side of the bicycle.

Humans relying on some device for movement. This includes drivers, passengers, or riders of bicycles, motorcycles, scooters, skateboards, (inline) skates, wheelchairs, horses, Segways, road cleaning cars, or convertibles. Note that a visible driver of a closed car can only be seen through the window. Since holes are considered part of the surrounding object, the human is included in the car label.

vehicle

car1

This includes cars, jeeps, SUVs, vans with a continuous body shape (i.e. the driver’s cabin and cargo compartment are one). Does not include trailers, which have their own separate class.

truck1

Thisincludestrucks, vanswithabodythatisseparate from the driver’s cabin, pickup trucks, as well as their trailers.

bus1

This includes buses that are intended for 9+ persons for public or long-distance transport.

train1

All vehicles that move on rails, e.g. trams, trains.

1 Single instance annotation available. 2 Not included in challenges.

Table 8. List of annotated classes including their deﬁnition and typical example.

iv

Category

Class

Deﬁnition

Examples

vehicle

motorcycle1

This includes motorcycles, mopeds, and scooters without the driver or other passengers. The latter receive the label rider.

bicycle1

This includes bicycles without the cyclist or other passengers. The latter receive the label rider.

caravan1,2

Vehicles that (appear to) contain living quarters. This also includes trailers that are used for living and has priority over the trailer class.

trailer1,2

Includes trailers that can be attached to any vehicle, but excludes trailers attached to trucks. The latter are included in the truck label.

nature

vegetation

Trees, hedges, and all kinds of vertically growing vegetation. Plants attached to buildings/walls/fences are not annotated separately, and receive the same label as the surface they are supported by.

terrain

Grass, all kinds of horizontally spreading vegetation, soil, or sand. These are areas that are not meant to be driven on. This label may also include a possibly adjacent curb. Single grass stalks or very small patches of grass are not annotated separately and thus are assigned to the label of the region they are growing on.

1 Single instance annotation available. 2 Not included in challenges.

Table 8. List of annotated classes including their deﬁnition and typical example. (continued)

v

Category

Class

Deﬁnition

Examples

construction

building

Includes structures that house/shelter humans, e.g. low-rises, skyscrapers, bus stops, car ports. Translucent buildings made of glass still receive the label building. Also includes scaffolding attached to buildings.

wall

Individually standing walls that separate two (or more) outdoor areas, and do not provide support for a building.

fence

Structures with holes that separate two (or more) outdoor areas, sometimes temporary.

guard rail2

Metal structure located on the side of the road to prevent serious accidents. Rare in inner cities, but occur sometimes in curves. Includes the bars holding the rails.

bridge2

Bridges (on which the ego-vehicle is not driving) including everything (fences, guard rails) permanently attached to them.

tunnel2

Tunnel walls and the (typically dark) space encased by the tunnel, but excluding vehicles.

1 Single instance annotation available. 2 Not included in challenges.

Table 8. List of annotated classes including their deﬁnition and typical example. (continued)

vi

Category

Class

Deﬁnition

Examples

object

trafﬁc sign

Front part of signs installed by the state/city authority with the purpose of conveying information to drivers/cyclists/pedestrians, e.g. trafﬁc signs, parking signs, direction signs, or warning reﬂector posts.

trafﬁc light

The trafﬁc light box without its poles in all orientations and for all types of trafﬁc participants, e.g. regular trafﬁc light, bus trafﬁc light, train trafﬁc light.

pole

Small, mainly vertically oriented poles, e.g. sign poles or trafﬁc light poles. This does not include objects mounted on the pole, which have a largerdiameterthanthepoleitself(e.g.moststreet lights).

pole group2

Multiple poles that are cumbersome to label individually, but where the background can be seen in their gaps.

sky

sky

Open sky (without tree branches/leaves)

1 Single instance annotation available. 2 Not included in challenges.

Table 8. List of annotated classes including their deﬁnition and typical example. (continued)

vii

Category

Class

Deﬁnition

Examples

ﬂat

road

sidewalk

Horizontal surfaces on which cars usually drive, including road markings. Typically delimited by curbs, rail tracks, or parking areas. However, road is not delimited by road markings and thus may include bicycle lanes or roundabouts.

Horizontal surfaces designated for pedestrians or cyclists. Delimited from the road by some obstacle, e.g. curbs or poles (might be small), but not only by markings. Often elevated compared to the road and often located at the side of a road. The curbs are included in the sidewalk label. Also includes the walkable part of trafﬁc islands, as well as pedestrian-only zones, where cars are not allowed to drive during regular business hours. If it’s an all-day mixed pedestrian/car area, the correct label is ground.

parking2

Horizontal surfaces that are intended for parking and separated from the road, either via elevation or via a different texture/material, but not separated merely by markings.

rail track2

Horizontal surfaces on which only rail cars can normally drive. If rail tracks for trams are embedded in a standard road, they are included in the road label.

1 Single instance annotation available. 2 Not included in challenges.

Table 8. List of annotated classes including their deﬁnition and typical example. (continued)

viii

Category

Class

Deﬁnition

Examples

void

ground2

All other forms of horizontal ground-level structures that do not match any of the above, for example mixed zones (cars and pedestrians), roundabouts that are ﬂat but delimited from the road by a curb, or in general a fallback label for horizontal surfaces that are difﬁcult to classify, e.g. due to having a dual purpose.

dynamic2

Movable objects that do not correspond to any of the other non-void categories and might not be in thesamepositioninthenextday/hour/minute, e.g. movable trash bins, buggies, luggage, animals, chairs, or tables.

static2

This includes areas of the image that are difﬁ- cult to identify/label due to occlusion/distance, as well as non-movable objects that do not match any of the non-void categories, e.g. mountains, street lights, reverse sides of trafﬁc signs, or permanently mounted commercial signs.

ego vehicle2

Since a part of the vehicle from which our data was recorded is visible in all frames, it is assigned to this special label. This label is also available at test time.

unlabeled2

Pixels that were not explicitly assigned to a label.

out of roi2

rectiﬁcation border2

Narrow strip of 5pixels along the image borders that is not considered for training or evaluation. This label is also available at test-time.

Areas close to the image border that contain artifacts resulting from the stereo pair rectiﬁcation. This label is also available at test time.

1 Single instance annotation available. 2 Not included in challenges.

Table 8. List of annotated classes including their deﬁnition and typical example. (continued)

ix

Largest number of instances and persons

Largest number of riders

Largest number of cars

Largest number of bicycles

Largest number of buses

Largest number of trucks

Largest number of motorcycles

Large spatial variation of persons

Fewest number of instances

Figure 7. Examples of our annotations on various images of our train and val sets. The images were selected based on criteria overlayed on each image.

x

k l a w e d i s

g n i d l i u b

d a o r

l l a w

static ﬁne (SF) static coarse (SC) GT segmentation with SF GT segmentation with SC

80.0 13.2 40.3 0.0 80.1 9.5 39.5 0.0 80.8 11.1 44.5 0.0 79.6 5.1 46.6 0.0

t h g i l

c ﬁ f a r t

0.0 0.0 0.0 0.0

e c n e f

0.0 0.0 0.0 0.0

e l o p

0.0 0.0 0.0 0.0

n g i s

c ﬁ f a r t

n o i t a t e g e v

n i a r r e t

y k s

n o s r e p

r e d i r

r a c

k c u r t

0.0 12.5 0.0 22.1 0.0 0.0 16.4 0.0 24.3 0.0 0.0 17.9 0.0 0.0 0.0 11.8 0.0 29.2 0.0

4.2

0.0 23.4 0.0 0.0 26.2 0.0 0.0 32.9 0.0 0.0 34.1 0.0

e l c y c r o t o m

0.0 0.0 0.0 0.0

e l c y c i b

0.0 0.0 0.0 0.0

s u b

0.0 0.0 0.0 0.0

n i a r t

0.0 0.0 0.0 0.0

GT segmentation with [41]

99.3 91.9 94.8 44.9 62.0 66.1 81.2 84.3 96.5 80.1 99.1 90.6 69.2 98.0 59.0 66.9 71.6 66.8 85.8

GT subsampled by 2 GT subsampled by 4 GT subsampled by 8 GT subsampled by 16 GT subsampled by 32 GT subsampled by 64 GT subsampled by 128

99.6 98.1 98.6 97.8 97.4 90.4 94.1 95.2 98.7 97.6 98.3 96.5 95.7 98.9 98.9 99.1 98.9 96.5 95.8 99.4 96.8 98.0 96.1 95.5 83.1 89.7 91.6 98.0 96.0 97.9 94.1 92.5 98.2 98.1 98.5 98.1 94.1 93.0 98.6 93.4 95.4 92.3 91.1 69.5 80.9 84.2 95.5 92.1 94.5 88.9 86.1 96.2 95.9 96.7 96.1 88.7 86.8 97.8 88.8 93.1 86.9 84.9 50.9 68.4 73.0 93.4 86.5 93.1 81.0 76.0 93.5 93.0 94.4 93.4 80.8 78.0 96.0 80.9 88.7 77.6 75.2 30.9 51.6 56.8 89.2 77.3 88.7 69.4 62.3 88.0 87.4 89.8 88.5 68.6 65.6 92.1 69.6 83.0 65.5 61.0 14.8 32.1 37.6 83.3 65.2 81.6 55.1 46.4 78.8 78.9 82.4 80.2 54.2 50.7 86.2 55.0 75.2 51.3 45.9 5.7 13.6 17.9 75.2 51.6 69.9 41.1 31.5 67.3 66.3 70.1 68.3 36.0 33.3

nearest training neighbor

85.3 35.6 56.7 15.6 6.2

1.3

0.5

1.0 54.2 23.3 36.5 4.0

0.4 42.0 9.7 18.3 12.9 0.3

1.7

U o I n a e m

10.1 10.3 10.1 10.9

79.4

97.2 95.2 90.7 84.6 75.4 63.8 50.6

21.3

Table 9. Detailed results of our control experiments for the pixel-level semantic labeling task in terms of the IoU score on the class level. All numbers are given in percent. See the main paper for details on the listed methods.

n o s r e p

0.0 0.0 0.0 0.0

r e d i r

r a c

k c u r t

0.0 38.0 0.0 0.0 39.8 0.0 0.0 50.3 0.0 0.0 50.8 0.0

s u b

0.0 0.0 0.0 0.0

n i a r t

0.0 0.0 0.0 0.0

e l c y c r o t o m

0.0 0.0 0.0 0.0

e l c y c i b

0.0 0.0 0.0 0.0

static ﬁne (SF) static coarse (SC) GT segmentation with SF GT segmentation with SC

GT segmentation with [41]

68.3 44.4 92.8 32.3 38.7 41.5 39.5 63.1

GT subsampled by 2 GT subsampled by 4 GT subsampled by 8 GT subsampled by 16 GT subsampled by 32 GT subsampled by 64 GT subsampled by 128

91.4 91.9 95.1 93.3 94.1 94.3 91.4 89.6 88.1 86.4 94.4 91.8 93.1 93.0 88.9 87.2 78.4 75.6 89.7 85.7 87.8 88.8 79.4 76.8 63.5 58.5 82.6 73.4 78.2 81.5 66.4 62.3 45.5 38.0 71.0 57.7 62.1 66.0 46.2 43.5 28.4 19.1 51.0 37.0 42.0 51.4 27.6 24.4 19.1 10.5 41.9 18.9 24.5 30.7 11.0 11.8

nearest training neighbor

3.6

0.5 32.7 1.9

4.0

2.8

0.3

1.5

U o I i n a e m

4.7 5.0 6.3 6.3

52.6

92.6 90.4 82.8 70.8 53.7 35.1 21.1

5.9

Table 10. Detailed results of our control experiments for the pixel-level semantic labeling task in terms of the instance-normalized iIoU score on the class level. All numbers are given in percent. See the main paper for details on the listed methods.

xi

n i a r t

l a v

e s r a o c

b u s

d a o r

k l a w e d i s

g n i d l i u b

l l a w

e c n e f

e l o p

t h g i l

c ﬁ f a r t

n g i s

c ﬁ f a r t

n o i t a t e g e v

n i a r r e t

y k s

n o s r e p

r e d i r

r a c

k c u r t

s u b

n i a r t

e l c y c r o t o m

e l c y c i b

FCN-32s (cid:88) (cid:88) FCN-16s (cid:88) (cid:88) (cid:88) (cid:88) FCN-8s (cid:88) (cid:88) FCN-8s (cid:88) FCN-8s FCN-8s

2

(cid:88)

(cid:88) [4] ext. [4] basic (cid:88) [40] [81] [9] [48] [37] [79]

4 4 (cid:88) (cid:88) (cid:88) 3 (cid:88) 2 (cid:88) (cid:88) 2 (cid:88) (cid:88) (cid:88) 2 (cid:88) (cid:88)

97.1 76.0 87.6 33.1 36.3 35.2 53.2 58.1 89.5 66.7 91.6 71.1 46.7 91.0 33.3 46.6 43.8 48.2 59.1 97.3 77.6 88.7 34.7 44.0 43.0 57.7 62.0 90.9 68.6 92.9 75.4 50.5 91.9 35.3 49.1 45.9 50.7 65.2 97.4 78.4 89.2 34.9 44.2 47.4 60.1 65.0 91.4 69.3 93.9 77.1 51.4 92.6 35.3 48.6 46.5 51.6 66.8 97.0 75.4 87.3 37.4 39.0 35.1 47.7 53.3 89.3 66.1 92.5 69.5 46.0 90.8 41.9 52.9 50.1 46.5 58.4 95.9 69.7 86.9 23.1 32.6 44.3 52.1 56.8 90.2 60.9 92.9 73.3 42.7 89.9 22.8 39.2 29.6 42.5 63.1 95.3 67.7 84.6 35.9 41.0 36.0 44.9 52.7 86.6 60.2 90.2 59.6 37.2 86.1 35.4 53.1 39.7 42.6 52.6

95.6 70.1 82.8 29.9 31.9 38.1 43.1 44.6 87.3 62.3 91.7 67.3 50.7 87.9 21.7 29.0 34.7 40.5 56.6 96.4 73.2 84.0 28.5 29.0 35.7 39.8 45.2 87.0 63.8 91.8 62.8 42.8 89.3 38.1 43.1 44.2 35.8 51.9 96.3 71.7 86.7 43.7 31.7 29.2 35.8 47.4 88.4 63.1 93.9 64.7 38.7 88.8 48.0 56.4 49.4 38.3 50.0 96.3 73.9 88.2 47.6 41.3 35.2 49.5 59.7 90.6 66.1 93.5 70.4 34.7 90.1 39.2 57.5 55.4 43.9 54.6 97.3 77.7 87.7 43.6 40.5 29.7 44.5 55.4 89.4 67.0 92.7 71.2 49.4 91.4 48.7 56.7 49.1 47.9 58.6 97.4 78.3 88.1 47.5 44.2 29.5 44.4 55.4 89.4 67.3 92.8 71.0 49.3 91.4 55.9 66.6 56.7 48.1 58.1 97.3 78.5 88.4 44.5 48.3 34.1 55.5 61.7 90.1 69.5 92.2 72.5 52.3 91.0 54.6 61.6 51.6 55.0 63.1 97.6 79.2 89.9 37.3 47.6 53.2 58.6 65.2 91.8 69.4 93.7 78.9 55.0 93.3 45.5 53.4 47.7 52.2 66.0

U o I n a e m

61.3 64.3 65.3 61.9 58.3 58.0

56.1 57.0 59.1 62.5 63.1 64.8 66.4 67.1

Table 11. Detailed results of our baseline experiments for the pixel-level semantic labeling task in terms of the IoU score on the class level. All numbers are given in percent and we indicate the used training data for each method, i.e. train ﬁne, val ﬁne, coarse extra, as well as a potential downscaling factor (sub) of the input image. See the main paper and Sec. D.1 for details on the listed methods.

n i a r t

l a v

e s r a o c

b u s

n o s r e p

r e d i r

r a c

k c u r t

s u b

n i a r t

e l c y c r o t o m

e l c y c i b

2

(cid:88)

(cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)

FCN-32s FCN-16s FCN-8s FCN-8s FCN-8s FCN-8s [4] extended (cid:88) 4 (cid:88) 4 [4] basic (cid:88) (cid:88) (cid:88) 3 [40] (cid:88) 2 [81] (cid:88) (cid:88) 2 [9] (cid:88) (cid:88) (cid:88) 2 [48] (cid:88) [37] (cid:88) [79]

46.9 32.0 82.1 21.2 28.8 21.9 26.0 47.1 53.6 33.5 84.2 21.3 32.8 25.8 28.9 48.6 55.9 33.4 83.9 22.2 30.8 26.7 31.1 49.6 42.8 22.3 79.3 16.6 27.3 22.2 20.0 38.5 51.8 31.0 80.6 17.0 23.9 24.5 23.7 47.3 43.2 18.9 72.5 18.2 24.2 20.1 20.9 36.2

49.9 27.1 81.1 15.3 23.7 18.5 19.6 38.4 44.3 22.7 78.4 16.1 24.3 20.7 15.8 33.6 38.9 12.8 78.6 13.4 24.0 19.2 10.7 27.2 50.6 17.8 81.1 18.0 25.0 30.3 22.3 30.1 40.5 23.3 78.8 20.3 31.9 24.8 21.1 35.2 40.7 23.1 78.6 21.4 32.4 27.6 20.8 34.6 56.2 38.0 77.1 34.0 47.0 33.4 38.1 49.9 56.3 34.5 85.8 21.8 32.7 27.6 28.0 49.1

U o I i n a e m

38.2 41.1 41.7 33.6 37.4 31.8

34.2 32.0 28.1 34.4 34.5 34.9 46.7 42.0

Table 12. Detailed results of our baseline experiments for the pixel-level semantic labeling task in terms of the instance-normalized iIoU score on the class level. All numbers are given in percent and we indicate the used training data for each method, i.e. train ﬁne, val ﬁne, coarse extra, as well as a potential downscaling factor (sub) of the input image. See the main paper and Sec. D.1 for details on the listed methods.

Proposals

Classiﬁer

MCG regions FRCN MCG bboxes FRCN FRCN MCG hulls

GT bboxes GT regions

FRCN FRCN

MCG regions GT MCG bboxes GT GT MCG hulls

n o s r e p

r e d i r

r a c

k c u r t

s u b

n i a r t

e l c y c r o t o m

e l c y c i b

1.9 1.0 6.2 0.5 0.1 1.3 0.6 10.5 6.1

2.8 7.8 6.4 10.3 4.5

1.5 0.6 0.2 0.9 9.7 5.9 1.7 0.5

3.1

4.0

7.6 0.5 17.5 10.7 15.7 8.4 2.9 65.5 40.6 65.9 21.1 31.9 30.2 28.8 46.4

2.6

3.7 4.4 11.9 19.9 21.5 12.4 7.8 2.0 2.0 10.9 18.2 22.1 15.9 6.0 3.4 4.1 13.4 20.4 24.1 16.0 8.3

2.6 2.2 2.8

P A n a e m

2.6 3.8 4.6

8.2 41.3

10.5 9.9 11.6

Table 13. Detailed results of our baseline experiments for the instance-level semantic labeling task in terms of the region-level average precision scores AP on the class level. All numbers are given in percent. See the main paper and Sec. D.2 for details on the listed methods.

xii

n o s r e p

r e d i r

r a c

k c u r t

s u b

n i a r t

e l c y c r o t o m

e l c y c i b

3.0 6.7 5.4 19.3 10.3 11.9 7.8 2.7 0.6 23.3 15.4 27.2 15.2 4.8 1.4 5.6 3.9 26.0 13.8 26.3 15.8 8.6 3.1

7.6

35.4 4.3 44.9 19.3 29.9 26.7 11.9 16.7 65.5 40.6 65.9 21.1 31.9 30.2 28.8 46.4

12.3 18.1 29.6 43.9 44.6 31.4 25.9 10.0 9.2 11.5 29.0 41.8 46.0 36.0 23.3 9.6 12.0 18.4 31.4 46.1 46.3 40.7 27.7 10.7

% 0 5 P A n a e m

9.0 11.3 12.9

23.7 41.3

27.0 25.8 29.1

Proposals

Classiﬁer

MCG regions FRCN MCG bboxes FRCN FRCN MCG hulls

GT bboxes GT regions

FRCN FRCN

MCG regions GT MCG bboxes GT GT MCG hulls

Table 14. Detailed results of our baseline experiments for the instance-level semantic labeling task in terms of the region-level average precision scores AP50% for an overlap value of 50%. All numbers are given in percent. See the main paper and Sec. D.2 for details on the listed methods.

n o s r e p

r e d i r

r a c

k c u r t

s u b

n i a r t

e l c y c r o t o m

e l c y c i b

2.2 1.1 3.7 1.6 10.2 4.2 0.9 0.1 12.9 11.3 18.5 6.9 0.3 1.3 2.6 1.1 17.5 10.6 17.4 9.2 2.6 0.9

6.8

5.4

8.8 0.8 25.3 18.4 27.1 13.0 3.9 3.6 79.1 66.0 78.9 33.6 53.9 47.1 42.6 63.5

6.8 6.8 18.9 28.7 32.7 19.0 10.5 4.3 3.5 2.9 17.3 27.3 34.5 24.9 8.2 3.7 6.1 6.2 21.4 29.9 37.2 24.7 11.4 4.7

m 0 0 1 P A n a e m

4.4 6.5 7.7

12.6 58.1

16.0 15.3 17.7

Proposals

Classiﬁer

MCG regions FRCN MCG bboxes FRCN FRCN MCG hulls

GT bboxes GT regions

FRCN FRCN

MCG regions GT MCG bboxes GT GT MCG hulls

Table 15. Detailed results of our baseline experiments for the instance-level semantic labeling task in terms of the region-level average precision scores AP100m for objects within 100m. All numbers are given in percent. See the main paper and Sec. D.2 for details on the listed methods.

n o s r e p

r e d i r

r a c

k c u r t

s u b

n i a r t

e l c y c r o t o m

e l c y c i b

2.4 1.1 4.0 1.7 12.0 9.0 0.4 1.0 0.1 15.5 14.9 27.7 10.0 1.4 2.7 1.1 21.2 14.0 25.2 14.2 2.7 1.0

7.8

6.4

8.5 0.8 26.6 23.2 37.2 17.7 4.1 3.6 79.1 68.3 80.5 42.9 69.4 67.9 46.2 64.7

7.2 7.0 21.7 32.4 42.4 23.6 11.1 4.5 3.8 3.7 3.0 19.9 33.0 46.0 32.9 8.6 6.5 6.4 24.8 35.4 49.6 31.8 12.2 4.9

m 0 5 P A n a e m

5.5 8.9 10.3

15.2 64.9

18.7 18.9 21.4

Proposals

Classiﬁer

MCG regions FRCN MCG bboxes FRCN FRCN MCG hulls

GT bboxes GT regions

FRCN FRCN

MCG regions GT MCG bboxes GT GT MCG hulls

Table 16. Detailed results of our baseline experiments for the instance-level semantic labeling task in terms of the region-level average precision scores AP50m for objects within 50m. All numbers are given in percent. See the main paper and Sec. D.2 for details on the listed methods.

xiii

Image

Annotation

static ﬁne (SF)

static coarse (SC)

GT segmentation w/ SF

GT segmentation w/ SC

GT segmentation w/ [41]

GT subsampled by 2

GT subsampled by 8

GT subsampled by 32

GT subsampled by 128

nearest training neighbor

Figure 8. Exemplary output of our control experiments for the pixel-level semantic labeling task, see the main paper for details. The image is part of our test set and has both, the largest number of instances and persons.

xiv

Image

Annotation

FCN-32s

FCN-8s

FCN-8s half resolution

FCN-8s trained on coarse

SegNet basic [4]

DPN [40]

CRF as RNN [81]

DeepLab LargeFOV StrongWeak [48]

Adelaide [37]

Dilated10 [79]

Figure 9. Exemplary output of our baselines for the pixel-level semantic labeling task, see the main paper for details. The image is part of our test set and has both, the largest number of instances and persons.

xv

Image

Annotation

static ﬁne (SF)

static coarse (SC)

GT segmentation w/ SF

GT segmentation w/ SC

GT segmentation w/ [41]

GT subsampled by 2

GT subsampled by 8

GT subsampled by 32

GT subsampled by 128

nearest training neighbor

Figure 10. Exemplary output of our control experiments for the pixel-level semantic labeling task, see the main paper for details. The image is part of our test set and has the largest number of car instances.

xvi

Image

Annotation

FCN-32s

FCN-8s

FCN-8s half resolution

FCN-8s trained on coarse

SegNet basic [4]

DPN [40]

CRF as RNN [81]

DeepLab LargeFOV StrongWeak [48]

Adelaide [37]

Dilated10 [79]

Figure 11. Exemplary output of our baseline experiments for the pixel-level semantic labeling task, see the main paper for details. The image is part of our test set and has the largest number of car instances.

xvii

Largest number of instances and persons

Annotation

FRCN + MCG bboxes

FRCN + MCG regions

FRCN + GT bboxes

FRCN + GT regions

Largest number of cars

Annotation

FRCN + MCG bboxes

FRCN + MCG regions

FRCN + GT bboxes

FRCN + GT regions

Figure 12. Exemplary output of our control experiments and baselines for the instance-level semantic labeling task, see the main paper for details.

xviii

