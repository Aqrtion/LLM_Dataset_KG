ScanNet:Richly-annotated3DReconstructionsofIndoorScenesAngelaDai1AngelX.Chang2ManolisSavva2MaciejHalber2ThomasFunkhouser2MatthiasNießner1,31StanfordUniversity2PrincetonUniversity3TechnicalUniversityofMunichwww.scan-net.orgAbstractAkeyrequirementforleveragingsuperviseddeeplearn-ingmethodsistheavailabilityoflarge,labeleddatasets.Unfortunately,inthecontextofRGB-Dsceneunderstand-ing,verylittledataisavailable–currentdatasetscoverasmallrangeofsceneviewsandhavelimitedsemantican-notations.Toaddressthisissue,weintroduceScanNet,anRGB-Dvideodatasetcontaining2.5Mviewsin1513scenesannotatedwith3Dcameraposes,surfacereconstructions,andsemanticsegmentations.Tocollectthisdata,wede-signedaneasy-to-useandscalableRGB-Dcapturesystemthatincludesautomatedsurfacereconstructionandcrowd-sourcedsemanticannotation.Weshowthatusingthisdatahelpsachievestate-of-the-artperformanceonseveral3Dsceneunderstandingtasks,including3Dobjectclassiﬁca-tion,semanticvoxellabeling,andCADmodelretrieval.1.IntroductionSincetheintroductionofcommodityRGB-Dsensors,suchastheMicrosoftKinect,theﬁeldof3Dgeometrycap-turehasgainedsigniﬁcantattentionandopenedupawiderangeofnewapplications.Althoughtherehasbeensig-niﬁcantefforton3Dreconstructionalgorithms,general3DsceneunderstandingwithRGB-Ddatahasonlyveryre-centlystartedtobecomepopular.Researchalongseman-ticunderstandingisalsoheavilyfacilitatedbytherapidprogressofmodernmachinelearningmethods,suchasneu-ralmodels.Onekeytosuccessfullyapplyingthesesap-proachesistheavailabilityoflarge,labeleddatasets.Whilemuchefforthasbeenmadeon2Ddatasets[17,44,47],whereimagescanbedownloadedfromthewebanddirectlyannotated,thesituationfor3Ddataismorechallenging.Thus,manyofthecurrentRGB-Ddatasets[74,92,77,32]areordersofmagnitudesmallerthantheir2Dcounterparts.Typically,3Ddeeplearningmethodsusesyntheticdatatomitigatethislackofreal-worlddata[91,6].Oneofthereasonsthatcurrent3Ddatasetsaresmallisbecausetheircapturerequiresmuchmoreeffort,andefﬁ-Figure1.ExamplereconstructedspacesinScanNetannotatedwithinstance-levelobjectcategorylabelsthroughourcrowdsourcedannotationframework.cientlyproviding(dense)annotationsin3Disnon-trivial.Thus,existingworkon3Ddatasetsoftenfallbacktopoly-gonorboundingboxannotationson2.5DRGB-Dimages[74,92,77],ratherthandirectlyannotatingin3D.Inthelattercase,labelsareaddedmanuallybyexpertusers(typi-callybythepaperauthors)[32,71]whichlimitstheirover-allsizeandscalability.Inthispaper,weintroduceScanNet,adatasetofrichly-annotatedRGB-Dscansofreal-worldenvironmentscon-taining2.5MRGB-Dimagesin1513scansacquiredin707distinctspaces.Thesheermagnitudeofthisdatasetislargerthananyother[58,81,92,75,3,71,32].However,whatmakesitparticularlyvaluableforresearchinsceneunderstandingisitsannotationwithestimatedcalibrationparameters,cameraposes,3Dsurfacereconstructions,tex-turedmeshes,denseobject-levelsemanticsegmentations,andalignedCADmodels(seeFig.2).Thesemanticseg-mentationsaremorethananorderofmagnitudelargerthananypreviousRGB-Ddataset.Inthecollectionofthisdataset,wehaveconsideredtwomainresearchquestions:1)howcanwedesignaframe-workthatallowsmanypeopletocollectandannotatelarge1

DatasetSizeLabelsAnnotationToolReconstructionCADModelsNYUv2[58]464scans1449frames2DLabelMe-style[69]nonesome[25]TUM[81]47scansnone-alignedposes(Vicon)noSUN3D[92]415scans8scans2Dpolygonsalignedposes[92]noSUNRGB-D[75]10kframes10kframes2Dpolygons+boundingboxesalignedposes[92]noBuildingParser[3]265rooms265roomsCloudCompare[24]pointcloudnoPiGraphs[71]26scans26scansdense3D,bytheauthors[71]dense3D[62]noSceneNN[32]100scans100scansdense3D,bytheauthors[60]dense3D[9]noScanNet(ours)1513scans1513scansdense3D,crowd-sourcedMTurkdense3D[12]yes2.5Mframeslabelsalsoproj.to2DframesTable1.OverviewofRGB-Ddatasetsfor3Dreconstructionandsemanticsceneunderstanding.Notethatinadditiontothe1513scansinScanNet,wealsoprovideddense3DreconstructionandannotationsonallNYUv2sequences.amountsofRGB-Ddata,and2)canweusetherichannota-tionsanddataquantityprovidedinScanNettolearnbetter3Dmodelsforsceneunderstanding?Toinvestigatetheﬁrstquestion,webuiltacapturepipelinetohelpnovicesacquiresemantically-labeled3Dmodelsofscenes.ApersonusesanapponaniPadmountedwithadepthcameratoacquireRGB-Dvideo,andthenweprocessesthedataoff-lineandreturnacom-pletesemantically-labeled3Dreconstructionofthescene.Thechallengesindevelopingsuchaframeworkarenumer-ous,includinghowtoperform3Dsurfacereconstructionro-bustlyinascalablepipelineandhowtocrowdsourceseman-ticlabeling.ThepaperdiscussesourstudyoftheseissuesanddocumentsourexperiencewithscalingupRGB-Dscancollection(20people)andannotation(500crowdworkers).Toinvestigatethesecondquestion,wetrained3DdeepnetworkswiththedataprovidedbyScanNetandtestedtheirperformanceonseveralsceneunderstandingtasks,includ-ing3Dobjectclassiﬁcation,semanticvoxellabeling,andCADmodelretrieval.Forthesemanticvoxellabelingtask,weintroduceanewvolumetricCNNarchitecture.Overall,thecontributionsofthispaperare:•Alarge3Ddatasetcontaining1513RGB-Dscansofover707uniqueindoorenvironmentswithestimatedcameraparameters,surfacereconstructions,texturedmeshes,semanticsegmentations.WealsoprovideCADmodelplacementsforasubsetofthescans.•Adesignforefﬁcient3Ddatacaptureandannotationsuitablefornoviceusers.•NewRGB-Dbenchmarksandimprovedresultsforstate-of-theartmachinelearningmethodson3Dob-jectclassiﬁcation,semanticvoxellabeling,andCADmodelretrieval.•AcompleteopensourceacquisitionandannotationframeworkfordenseRGB-Dreconstructions.2.PreviousWorkAlargenumberofRGB-Ddatasetshavebeencapturedandmadepubliclyavailablefortrainingandbenchmarking[56,34,50,65,79,83,74,4,58,81,15,55,1,68,30,51,21,48,43,92,80,61,72,93,36,16,35,57,40,29,70,52,45,95,75,9,33,85,71,32,3,10,78,2].1Thesedatasetshavebeenusedtotrainmodelsformany3Dsceneunderstandingtasks,includingsemanticsegmentation[67,58,26,86],3Dobjectdetection[73,46,27,76,77],3Dobjectclassiﬁcation[91,53,66],andothers[94,22,23].MostRGB-Ddatasetscontainscansofindividualob-jects.Forexample,theRedwooddataset[10]containsover10,000scansofobjectsannotatedwithclasslabels,1,781ofwhicharereconstructedwithKinectFusion[59].Sincetheobjectsarescannedinisolationwithoutscenecontext,thedataset’sfocusismainlyonevaluatingsurfacereconstruc-tionqualityratherthansemanticunderstandingofcompletescenes.OneoftheearliestandmostpopulardatasetsforRGB-DsceneunderstandingisNYUv2[74].Itiscomposedof464shortRGB-Dsequences,fromwhich1449frameshavebeenannotatedwith2Dpolygonsdenotingsemanticseg-mentations,asinLabelMe[69].SUNRGB-D[75]followsuponthisworkbycollecting10,335RGB-Dframesan-notatedwithpolygonsin2Dandboundingboxesin3D.Thesedatasetshavescenediversitycomparabletoours,butincludeonlyalimitedrangeofviewpoints,anddonotpro-videcomplete3Dsurfacereconstructions,dense3Dseman-ticsegmentations,oralargesetofCADmodelalignments.OneoftheﬁrstRGB-DdatasetsfocusedonlongRGB-DsequencesinindoorenvironmentsisSUN3D.Itcontainsasetof415Kinectv1sequencesof254uniquespaces.Althoughsomeobjectswereannotatedmanuallywith2Dpolygons,and8scanshaveestimatedcameraposesbasedonuserinput,thebulkofthedatasetdoesnotincludecam-eraposes,3Dreconstructions,orsemanticannotations.Recently,Armenietal.[3,2]introducedanindoordatasetcontaining3Dmeshesfor265roomscapturedwithacustomMatterportcameraandmanuallylabeledwithse-manticannotations.Thedatasetishigh-quality,butthecap-1Acomprehensiveanddetailedoverviewofpublicly-accessibleRGB-Ddatasetsisgivenby[20]athttp://www0.cs.ucl.ac.uk/staff/M.Firman/RGBDdatasets/,whichisupdatedonaregularbasis.RGB-D Scanning3D ReconstructionUploadSegmentationSemantic LabelingRetrieval + AlignmentCrowd-sourcingFigure2.OverviewofourRGB-Dreconstructionandsemanticannotationframework.Left:anoviceuserusesahandheldRGB-Ddevicewithourscanninginterfacetoscananenvironment.Mid:RGB-Dsequencesareuploadedtoaprocessingserverwhichproduces3Dsurfacemeshreconstructionsandtheirsurfacesegmentations.Right:Semanticannotationtasksareissuedforcrowdsourcingtoobtaininstance-levelobjectcategoryannotationsand3DCADmodelalignmentstothereconstruction.turepipelineisbasedonexpensiveandlessportablehard-ware.Furthermore,onlyafusedpointcloudisprovidedasoutput.Duetothelackofrawcoloranddepthdata,itsapplicabilitytoresearchonreconstructionandsceneunder-standingfromrawRGB-Dinputislimited.ThedatasetsmostsimilartooursareSceneNN[32]andPiGraphs[71],whicharecomposedof100and26denselyreconstructedandlabeledscenesrespectively.Theanno-tationsaredonedirectlyin3D[60,71].However,bothscanningandlabelingareperformedonlybyexpertusers(i.e.theauthors),limitingthescalabilityofthesystemandthesizeofthedataset.Incontrast,wedesignourRGB-Dacquisitionframeworkspeciﬁcallyforease-of-usebyun-trainedusersandforscalableprocessingthroughcrowd-sourcing.Thisallowsustoacquireasigniﬁcantlylargerdatasetwithmoreannotations(currently,1513sequencesarereconstructedandlabeled).3.DatasetAcquisitionFrameworkInthissection,wefocusonthedesignoftheframeworkusedtoacquiretheScanNetdataset(Fig.2).Wediscussde-signtrade-offsinbuildingtheframeworkandrelayﬁndingsonwhichmethodswerefoundtoworkbestforlarge-scaleRGB-Ddatacollectionandprocessing.Ourmaingoaldrivingthedesignofourframeworkwastoallowuntraineduserstocapturesemanticallylabeledsur-facesofindoorsceneswithcommodityhardware.ThustheRGB-Dscanningsystemmustbetrivialtouse,thedataprocessingrobustandautomatic,thesemanticannotationscrowdsourced,andtheﬂowofdatathroughthesystemhan-dledbyatrackingserver.3.1.RGB-DScanningHardware.ThereisaspectrumofchoicesforRGB-Dsensorhardware.Ourrequirementfordeploymenttolargegroupsofinexperiencedusersnecessitatesaportableandlow-costRGB-Dsensorsetup.WeusetheStructuresen-sor[63],acommodityRGB-DsensorwithdesignsimilartotheMicrosoftKinectv1.WeattachthissensortoahandhelddevicesuchasaniPhoneoriPad(seeFig.2left)—resultsinthispaperwerecollectedusingiPadAir2devices.TheiPadRGBcameradataistemporallysynchronizedwiththedepthsensorviahardware,providingsynchronizeddepthandcolorcaptureat30Hz.Depthframesarecapturedataresolutionof640×480andcolorat1296×968pixels.Weenableauto-whitebalanceandauto-exposurebydefault.Calibration.OuruseofcommodityRGB-Dsensorsne-cessitatesunwarpingofdepthdataandalignmentofdepthandcolordata.Priorworkhasfocusedmostlyoncontrolledlabconditionswithmoreaccurateequipmenttoinformcal-ibrationforcommoditysensors(e.g.,Wangetal.[87]).However,thisisnotpracticalfornoviceusers.Thustheuseronlyneedstoprintoutacheckerboardpattern,placeitonalarge,ﬂatsurface,andcaptureanRGB-Dsequenceviewingthesurfacefromclosetofaraway.Thissequence,aswellasasetofinfraredandcolorframepairsviewingthecheckerboard,areuploadedbytheuserasinputtothecali-bration.Oursystemthenrunsacalibrationprocedurebasedon[84,14]toobtainintrinsicparametersforbothdepthandcolorsensors,andanextrinsictransformationofdepthtocolor.Weﬁndthatthiscalibrationprocedureiseasyforusersandresultsinimproveddataandconsequentlyen-hancedreconstructionquality.UserInterface.Tomakethecaptureprocesssimpleforuntrainedusers,wedesignedaniOSappwithasimpleliveRGB-DvideocaptureUI(seeFig.2left).Theuserprovidesanameandscenetypeforthecurrentscanandproceedstorecordasequence.Duringscanning,alog-scaleRGBfeaturedetectorpointmetricisshownasa“featurefulness”bartoprovidearoughmeasureoftrackingrobustnessandreconstructionqualityindifferentregionsbeingscanned.Thisfeaturewascriticalforprovidingintuitiontouserswhoarenotfamiliarwiththeconstraintsandlimitationsof3Dreconstructionalgorithms.Storage.WestorescansascompressedRGB-Ddataonthedeviceﬂashmemorysothatastableinternetconnec-tionisnotrequiredduringscanning.Theusercanuploadscanstotheprocessingserverwhenconvenientbypress-ingan“upload”button.Oursensorunitsused128GBiPadAir2devices,allowingforseveralhoursofrecordedRGB-Dvideo.Inpractice,thebottleneckwasbatteryliferatherthanstoragespace.Depthisrecordedas16-bitunsignedshortvaluesandstoredusingstandardzLibcompression.RGBdataisencodedwiththeH.264codecwithahighbi-trateof15Mbpstopreventencodingartifacts.InadditiontotheRGB-Dframes,wealsorecordInertialMeasurementUnit(IMU)data,includingacceleration,andangularveloc-ities,fromtheAppleSDK.TimestampsarerecordedforIMU,color,anddepthimages.3.2.SurfaceReconstructionOncedatahasbeenuploadedfromtheiPadtoourserver,theﬁrstprocessingstepistoestimateadensely-reconstructed3Dsurfacemeshand6-DoFcameraposesforallRGB-Dframes.Toconformwiththegoalforanau-tomatedandscalableframework,wechoosemethodsthatfavorrobustnessandprocessingspeedsuchthatuploadedrecordingscanbeprocessedatnearreal-timerateswithlit-tlesupervision.DenseReconstruction.Weusevolumetricfusion[11]toperformthedensereconstruction,sincethisapproachiswidelyusedinthecontextofcommodityRGB-Ddata.Thereisalargevarietyofalgorithmstargetingthissce-nario[59,88,7,62,37,89,42,9,90,38,12].WechosetheBundleFusionsystem[12]asitwasdesignedandevalu-atedforsimilarsensorsetupsasours,andprovidesreal-timespeedwhilebeingreasonablyrobustgivenhandheldRGB-Dvideodata.Foreachinputscan,weﬁrstrunBundleFusion[12]atavoxelresolutionof1cm3.BundleFusionproducesaccu-rateposealignmentswhichwethenusetoperformvolu-metricintegrationthroughVoxelHashing[62]andextractahighresolutionsurfacemeshusingtheMarchingCubesal-gorithmontheimplicitTSDF(4mm3voxels).Themeshisthenautomaticallycleanedupwithasetofﬁlteringstepstomergeclosevertices,deleteduplicateandisolatedmeshparts,andﬁnallytodownsamplethemeshtohigh,medium,andlowresolutionversions(eachlevelreducingthenumberoffacesbyafactoroftwo).Orientation.Afterthesurfacemeshisextracted,weau-tomaticallyalignitandallcameraposestoacommonco-ordinateframewiththez-axisastheupvector,andthexyplanealignedwiththeﬂoorplane.Toperformthisalign-ment,weﬁrstextractallplanarregionsofsufﬁcientsize,mergeregionsdeﬁnedbythesameplane,andsortthembynormal(weuseanormalthresholdof25◦andaplanaroff-setthresholdof5cm).WethendetermineapriorfortheupvectorbyprojectingtheIMUgravityvectorsofallframesintothecoordinatesoftheﬁrstframe.Thisallowsustose-lecttheﬂoorplanebasedonthescanboundingboxandthenormalmostsimilartotheIMUupvectordirection.Finally,weuseaPCAonthemeshverticestodeterminetherotationaroundthez-axisandtranslatethescansuchthatitsboundsarewithinthepositiveoctantofthecoordinatesystem.Figure3.Ourweb-basedcrowdsourcinginterfaceforannotatingascenewithinstance-levelobjectcategorylabels.Therightpanellistsobjectinstancesalreadyannotatedinthescenewithmatchingpaintedcolors.Thisannotationisinprogressat≈35%,withgrayregionsindicatingunannotatedsurfaces.Validation.Thisreconstructionprocessisautomaticallytriggeredwhenascanisuploadedtotheprocessingserverandrunsunsupervised.Inordertoestablishacleansnap-shottoconstructtheScanNetdatasetreportedinthispaper,weautomaticallydiscardscansequencesthatareshort,havehighresidualreconstructionerror,orhavelowpercentageofalignedframes.Wethenmanuallycheckforanddiscardreconstructionswithnoticeablemisalignments.3.3.SemanticAnnotationAfterareconstructionisproducedbytheprocessingserver,annotationHITs(HumanIntelligenceTasks)areis-suedontheAmazonMechanicalTurkcrowdsourcingmar-ket.ThetwoHITsthatwecrowdsourceare:i)instance-levelobjectcategorylabelingofallsurfacesintherecon-struction,andii)3DCADmodelalignmenttotherecon-struction.Theseannotationsarecrowdsourcedusingweb-basedinterfacestoagainmaintaintheoverallscalabilityoftheframework.Instance-levelSemanticLabeling.Ourﬁrstannotationstepistoobtainasetofobjectinstance-levellabelsdirectlyoneachreconstructed3Dsurfacemesh.Thisisincontrasttomuchpriorworkthatuses2DpolygonannotationsonRGBorRGB-Dimages,or3Dboundingboxannotations.WedevelopedaWebGLinterfacethattakesasinputthelow-resolutionsurfacemeshofagivenreconstructionandaconservativeover-segmentationofthemeshusinganormal-basedgraphcutmethod[19,39].Thecrowdworkerthenselectssegmentstoannotatewithinstance-levelobjectcate-gorylabels(seeFig.3).Eachworkerisrequiredtoannotateatleast25%ofthesurfacesinareconstruction,andencour-agedtoannotatemorethan50%beforesubmission.Eachscanisannotatedbymultipleworkers(scansinScanNetareannotatedby2.3workersonaverage).Akeychallengeindesigningthisinterfaceistoenableefﬁcientannotationbyworkerswhohavenopriorexperi-encewiththetask,or3Dinterfacesingeneral.Ourinterfaceusesasimplepaintingmetaphorwhereclickinganddrag-Figure4.CrowdsourcinginterfaceforaligningCADmodelstoobjectsinareconstruction.ObjectscanbeclickedtoinitiateanassistedsearchforCADmodels(seelistofbookshelvesinmid-dle).Asuggestedmodelisplacedatthepositionoftheclickedobject,andtheuserthenreﬁnesthepositionandorientation.Adesk,chair,andnightstandhavebeenalreadyplacedhere.gingoversurfacespaintssegmentswithagivenlabelandcorrespondingcolor.Thisfunctionssimilarlyto2Dpaint-ingandallowsforerasingandmodifyingexistingregions.Anotherdesignrequirementistoallowforfreeformtextlabels,toreducetheinherentbiasandscalabilityissuesofpre-selectedlabellists.Atthesametime,itisdesirabletoguideusersforconsistencyandcoverageofbasicobjecttypes.Toachievethis,theinterfaceprovidesautocompletefunctionalityoveralllabelspreviouslyprovidedbyotherworkersthatpassafrequencythreshold(>5annotations).Workersarealwaysallowedtoaddarbitrarytextlabelstoensurecoverageandallowexpansionofthelabelset.Severaladditionaldesigndetailsareimportanttoensureusabilitybynoviceworkers.First,asimpledistancecheckforconnectednessisusedtodisallowlabelingofdiscon-nectedsurfaceswiththesamelabel.Earlierexperimentswithoutthisconstraintresultedintwoundesirablebehav-iors:cheatingbypaintingmanysurfaceswithafewlabels,andlabelingofmultipleobjectinstanceswiththesamela-bel.Second,the3Dnatureofthedataischallengingfornoviceusers.Therefore,weﬁrstshowafullturntablerota-tionofeachreconstructionandinstructworkerstochangetheviewusingarotatingturntablemetaphor.Withouttheturntablerotationanimation,manyworkersonlyannotatedfromtheinitialviewandneverusedcameracontrolsdespitetheprovidedinstructions.CADModelRetrievalandAlignment.Inthesecondan-notationtask,acrowdworkerwasgivenareconstructionalreadyannotatedwithobjectinstancesandaskedtoplaceappropriate3DCADmodelstorepresentmajorobjectsinthescene.Thechallengeofthistaskliesintheselectionofcloselymatching3Dmodelsfromalargedatabase,andinpreciselyaligningeachmodeltothe3Dpositionofthecorrespondingobjectinthereconstruction.WeimplementedanassistedobjectretrievalinterfaceStatisticSceneNN[32]ScanNet#ofscans1001513#ofRGB-Dframes2,475,9052,492,518ﬂoorarea(avg/summ2)22.6/2,12422.6/34,453surfacearea(avg/summ2)75.3/7,07851.6/78,595labeledobjects(avg/sum)15.8/148224.1/36,213Table2.SummarystatisticsforScanNetcomparedtothemostsimilarexistingdataset(SceneNN[32]).ScanNethasanorderofmagnitudemorescans,with3Dsurfacemeshreconstructionscoveringmorethantentimestheﬂoorandsurfacearea,andwithmorethan36,000annotatedobjectinstances.whereclickingonapreviouslylabeledobjectinarecon-structionimmediatelysearchedforCADmodelswiththesamecategorylabelintheShapeNetCore[6]dataset,andplacedoneexamplemodelsuchthatitoverlapswiththeori-entedboundingboxoftheclickedobject(seeFig.4).Theworkerthenusedkeyboardandmouse-basedcontrolstoad-justthealignmentofthemodel,andwasallowedtosubmitthetaskonceatleastthreeCADmodelswereplaced.Usingthisinterface,wecollectedsetsofCADmod-elsalignedtoeachScanNetreconstruction.Preliminaryresultsindicatethatdespitethechallengingnatureofthistask,workersselectsemanticallyappropriateCADmodelstomatchobjectsinthereconstructions.Themainlimitationofthisinterfaceisduetothemismatchbetweenthecor-pusofavailableCADmodelsandtheobjectsobservedintheScanNetscans.DespitethediversityoftheShapeNetCADmodeldataset(55Kobjects),itisstillhardtoﬁndex-actinstance-levelmatchesforchairs,desksandmorerareobjectcategories.Apromisingwaytoalleviatethislimi-tationistoalgorithmicallysuggestcandidateretrievedandalignedCADmodelssuchthatworkerscanperformaneas-ierveriﬁcationandadjustmenttask.4.ScanNetDatasetInthissection,wesummarizethedatawecollectedus-ingourframeworktoestablishtheScanNetdataset.Thisdatasetisasnapshotofavailabledatafromroughlyonemonthofdataacquisitionby20usersatlocationsinseveralcountries.Ithasannotationsbymorethan500crowdwork-ersontheMechanicalTurkplatform.Sincethepresentedframeworkrunsinanunsupervisedfashionandpeoplearecontinuouslycollectingdata,thisdatasetcontinuestogroworganically.Here,wereportsomestatisticsforaninitialsnapshotof1513scans,whicharesummarizedinTable2.Fig.5plotsthedistributionofscannedscenesoverdiffer-enttypesofreal-worldspaces.ScanNetcontainsavarietyofspacessuchasofﬁces,apartments,andbathrooms.Thedatasetcontainsadiversesetofspacesrangingfromsmall(e.g.,bathrooms,closets,utilityrooms)tolarge(e.g.,apart-ments,classrooms,andlibraries).Eachscanhasbeenanno-tatedwithinstance-levelsemanticcategorylabelsthroughFigure5.DistributionofthescansinScanNetorganizedbytype.ourcrowdsourcingtask.Intotal,wedeployed3,391anno-tationtaskstoannotateall1513scans.ThetextlabelsusedbycrowdworkerstoannotateobjectinstancesareallmappedtotheobjectcategorysetsofNYUv2[58],ModelNet[91],ShapeNet[6],andWordNet[18]synsets.Thismappingismademorerobustbyapreprocessthatcollapsestheinitialtextlabelsthroughsynonymandmisspellingdetection.Inadditiontoreconstructingandannotatingthe1513ScanNetscans,wehaveprocessedalltheNYUv2RGB-Dsequenceswithourframework.TheresultisasetofdensereconstructionsoftheNYUv2spaceswithinstance-levelobjectannotationsin3Dthatarecomplementaryinnaturetotheexistingimage-basedannotations.WealsodeployedtheCADmodelalignmentcrowd-sourcingtasktocollectatotalof107virtualsceneinter-pretationsconsistingofalignedShapeNetmodelsplacedonasubsetof52ScanNetscansby106workers.Therewereatotalof681CADmodelinstances(of296uniquemodels)retrievedandplacedonthereconstructions,withanaverageof6.4CADmodelinstancesperannotatedscan.FormoredetailedstatisticsonthisﬁrstScanNetdatasetsnapshot,pleaseseetheappendix.5.TasksandBenchmarksInthissection,wedescribethethreetaskswedevelopedasbenchmarksfordemonstratingthevalueofScanNetdata.Train/Testsplitstatistics.Table3showsthetestandtrainingsplitsofScanNetinthecontextoftheobjectclassi-ﬁcationanddensevoxelpredictionbenchmarks.Notethatourdataissigniﬁcantlylargerthananyexistingcompara-bledataset.WeusethesetaskstodemonstratethatScan-Netenablestheuseofdeeplearningmethodsfor3Dsceneunderstandingtaskswithsupervisedtraining,andcompareperformancetothatusingdatafromotherexistingdatasets.5.1.3DObjectClassiﬁcationWiththeavailabilityoflarge-scalesynthetic3Ddatasetssuchas[91,6]andrecentadvancesin3Ddeeplearn-ScansInstances#Train#Test#Train#TestObjectClassiﬁcationScanNet120531293052606NYU452803260613SceneNN701237766SemanticVoxelLabelingScanNet12013128055421300Table3.Train/Testsplitforobjectclassiﬁcationanddensevoxelpredictiontasks.Notethatthenumberofinstancesdoesnotin-cludetherotationaugmentation.ing,researchhasdevelopedapproachestoclassifyob-jectsusingonlygeometricdatawithvolumetricdeepnets[91,82,52,13,66].Allofthesemethodstrainonpurelysyntheticdataandfocusonisolatedobjects.Althoughtheyshowlimitedevaluationonreal-worlddata,alargerevalu-ationonrealisticscanningdataislargelymissing.Whentrainingdataissyntheticandtestisperformedonrealdata,thereisalsoasigniﬁcantdiscrepancyoftestperformance,asdatacharacteristics,suchasnoiseandocclusionspat-terns,areinherentlydifferent.WithScanNet,weclosethisgapaswehavecapturedasufﬁcientlylargeamountof3Ddatatousereal-worldRGB-Dinputforbothtrainingandtestsets.Forthistask,weusetheboundingboxesofannotatedobjectsinScanNet,andisolatethecontainedgeometry.Asaresult,weobtainlocalvolumesaroundeachobjectinstanceforwhichweknowtheannotatedcategory.Thegoalofthetaskistoclassifytheobjectrepresentedbyasetofscannedpointswithinagivenboundingbox.Forthisbenchmark,weuse17categories,with9,677traininstancesand2,606testinstances.Networkandtraining.Forobjectclassiﬁcation,wefol-lowthenetworkarchitectureofthe3DNetwork-in-Networkof[66],withoutthemulti-orientationpoolingstep.Inordertoclassifypartialdata,weaddasecondchanneltothe303occupancygridinput,indicatingknownandunknownre-gions(with1and0,respectively)accordingtothecamerascanningtrajectory.AsinQietal.[66],weuseanSGDsolverwithlearningrate0.01andmomentum0.9,decayingthelearningratebyhalfevery20epochs,andtrainingthemodelfor200epochs.Weaugmenttrainingsampleswith12instancesofdifferentrotations(includingbothelevationandtilt),resultinginatotaltrainingsetof111,660samples.Benchmarkperformance.Asabaselineevaluation,werunthe3DCNNapproachofQietal.[66].Table4showstheperformanceof3Dshapeclassiﬁcationwithdifferenttrainandtestsets.TheﬁrsttwocolumnsshowresultsonsynthetictestdatafromShapeNet[6]includingbothcom-pleteandpartialdata.Naturally,trainingwiththecorre-spondingsyntheticcounterpartsofShapeNetprovidesthebestperformance,asdatacharacteristicsareshared.How-ever,themoreinterestingcaseisreal-worldtestdata(right-mosttwocolumns);here,weshowresultsontestsetsofSceneNN[32]andScanNet.First,weseethattrainingonsyntheticdataallowsonlyforlimitedknowledgetransfer(ﬁrsttworows).Second,althoughtherelativelysmallSce-neNNdatasetisabletolearnwithinitsowndatasettoareasonabledegree,itdoesnotgeneralizetothelargervari-etyofenvironmentsfoundinScanNet.Ontheotherhand,trainingonScanNettranslateswelltotestingonSceneNN;asaresult,thetestresultsonSceneNNaresigniﬁcantlyimprovedbyusingthetrainingdatafromScanNet.In-terestinglyenough,theseresultscanbeslightlyimprovedwhenmixingtrainingdataofScanNetwithpartialscansofShapeNet(lastrow).SyntheticTestSetsRealTestSetsTrainingSetShapeNetShapeNetPartialSceneNNScanNetShapeNet92.537.668.239.5ShapeNetPartial88.592.172.745.7SceneNN19.927.769.848.2NYU26.226.672.753.2ScanNet21.431.078.874.9ScanNet+ShapeNetPar.79.789.881.276.6Table4.3Dobjectclassiﬁcationbenchmarkperformance.Per-centagesgivetheclassiﬁcationaccuracyoverallmodelsineachtestset(averageinstanceaccuracy).RetrievalfromShapeNetTrainTop1NNTop3NNsShapeNet10.4%8.0%ScanNet12.7%11.7%ShapeNet+ScanNet77.5%77.0%Table6.3Dmodelretrievalbenchmarkperformance.NearestneighbormodelsareretrievedforScanNetobjectsfromShapeNet-Core.Percentagesindicateaverageinstanceaccuracyofretrievedmodeltoqueryregion.5.2.SemanticVoxelLabelingAcommontaskonRGBdataissemanticsegmentation(i.e.labelingpixelswithsemanticclasses)[49].Withourdata,wecanextendthistaskto3D,wherethegoalistopredictthesemanticobjectlabelonaper-voxelbasis.Thistaskofpredictingasemanticclassforeachvisible3Dvoxelhasbeenaddressedbysomepriorwork,butusinghand-craftedfeaturestopredictasmallnumberofclasses[41,86],orfocusingonoutdoorenvironments[8,5].DataGeneration.Weﬁrstvoxelizeasceneandobtainadensevoxelgridwith2cm3voxels,whereeveryvoxelstoresitsTSDFvalueandobjectclassannotation(emptyspaceandunlabeledsurfacepointshavetheirownrespec-tiveclasses).Wenowextractsubvolumesofthescenevol-ume,ofdimension2×31×31×62andspatialextent1.5m×1.5m×3m;i.e.,avoxelsizeof≈4.8cm3;thetwochannelsrepresenttheoccupancyandknown/unknownspaceaccordingtothecameratrajectory.Thesesamplevol-umesarealignedwiththexy-groundplane.Forgroundtruthdatageneration,voxellabelsarepropagatedfromthescenevoxelizationtothesesamplevolumes.Thesamplesarecho-senthat≥2%ofthevoxelsareoccupied(i.e.,onthesur-face),and≥70%ofthesesurfacevoxelshavevalidan-notations;samplesnotmeetingthesecriteriaarediscarded.AcrossScanNet,wegenerate93,721subvolumeexamplesfortraining,augmentedby8rotationseach(i.e.,749,768trainingsamples),from1201trainingscenes.Inaddition,weextract18,750samplevolumesfortesting,whicharealsoaugmentedby8rotationseach(i.e.,150,000testsam-ples)from312testscenes.Wehave20objectclasslabelsplus1classforfreespace.Networkandtraining.Forthesemanticvoxellabelingtask,weproposeanetworkwhichpredictsclasslabelsforacolumnofvoxelsinasceneaccordingtotheoccupancycharacteristicsofthevoxels’neighborhood.Inordertoin-ferlabelsforanentirescene,weusethenetworktopredictalabelforeveryvoxelcolumnattesttime(i.e.,everyxypositionthathasvoxelsonthesurface).Thenetworktakesasinputa2×31×31×62volumeandusesaseriesoffullyconvolutionallayerstosimultaneouslypredictclassscoresforthecentercolumnof62voxels.WeuseReLUandbatchnormalizationforalllayers(exceptthelast)inthenetwork.Toaccountfortheunbalancedtrainingdataovertheclasslabels,weweightthecrossentropylosswiththeinverselogofthehistogramofthetraindata.WeuseanSGDsolverwithlearningrate0.01andmo-mentum0.9,decayingthelearningratebyhalfevery20epochs,andtrainthemodelfor100epochs.QuantitativeResults.Thegoalofthistaskistopredictsemanticlabelsforallvisiblesurfacevoxelsinagiven3Dscene;i.e.,everyvoxelonavisiblesurfacereceivesoneofthe20objectclasslabels.WeuseNYU2labels,andlistvoxelclassiﬁcationresultsonScanNetinTable7.Weachieveanvoxelclassiﬁcationaccuracyof73.0%overthesetof312testscenes,whichisbasedpurelyonthegeomet-ricinput(nocolorisused).InTable5,weshowoursemanticvoxellabelingresultsontheNYU2dataset[58].Weareabletooutperformprevi-ousmethodswhicharetrainedonlimitedsetsofreal-worlddatausingourvolumetricclassiﬁcationnetwork.Forin-stance,Hermansetal.[31]classifyRGB-Dframesusingadenserandomdecisionforestincombinationwithacon-ditionalrandomﬁeld.Additionally,SemanticFusion[54]usesadeepnettrainedonRGB-Dframes,andregularizethepredictionswithaCRFovera3Dreconstructionoftheframes;notethatwecomparetotheirclassiﬁcationresultsﬂoorwallchairtablewindowbedsofatvobjs.furn.ceil.avg.Hermansetal.[31]91.571.841.927.746.168.428.538.48.637.183.449.4SemanticFusion[54]∗92.686.058.434.060.561.747.333.959.163.743.458.2SceneNet[28]96.285.361.043.830.072.562.819.450.060.474.159.6Ours(ScanNet+NYU)99.055.867.650.963.181.467.235.834.665.646.260.7Table5.DensepixelclassiﬁcationaccuracyonNYU2[58].NotethatbothSemanticFusion[54]andHermanset.al.[31]usebothgeometryandcolor,andthatHermansetal.usesaCRF,unlikeourapproachwhichisgeometry-onlyandhasonlyunarypredictions.ThereportedSemanticFusionclassiﬁcationisonthe13classtask(13classaverageaccuracyof58.9%).Class%ofTestScenesAccuracyFloor35.7%90.3%Wall38.8%70.1%Chair3.8%69.3%Sofa2.5%75.7%Table3.3%68.4%Door2.2%48.9%Cabinet2.4%49.8%Bed2.0%62.4%Desk1.7%36.8%Toilet0.2%69.9%Sink0.2%39.4%Window0.4%20.1%Picture0.2%3.4%Bookshelf1.6%64.6%Curtain0.7%7.0%ShowerCurtain0.04%46.8%Counter0.6%32.1%Refrigerator0.3%66.4%Bathtub0.2%74.3%OtherFurniture2.9%19.5%Total-73.0%Table7.SemanticvoxellabelpredictionaccuracyonScanNettestscenes.beforetheCRFregularization.SceneNettrainsonalargesyntheticdatasetandﬁne-tunesonNYU2.Notethatincon-trasttoHermansetal.andSemanticFusion,neitherwenorSceneNetuseRGBinformation.Notethatwedonotexplicitlyenforcepredictioncon-sistencybetweenneighboringvoxelcolumnswhenthetestvolumeisslidacrossthexyplane.ThiscouldbeachievedwithavolumetricCRF[64],asusedin[86];however,ourgoalinthistasktofocusexclusivelyontheper-voxelclas-siﬁcationaccuracy.5.3.3DObjectRetrievalAnotherimportanttaskisretrievalofsimilarCADmod-elsgiven(potentiallypartial)RGB-Dscans.Tothisend,onewantstolearnashapeembeddingwhereafeaturede-scriptordeﬁnesgeometricsimilaritybetweenshapes.Thecoreideaistotrainanetworkonashapeclassiﬁcationtaskwhereashapeembeddingcanbelearnedasbyproductoftheclassiﬁcationtask.Forinstance,Wuetal.[91]andQietal.[66]usethistechniquetoperformshaperetrievalquerieswithintheShapeNetdatabase.WithScanNet,wehaveestablishedcategory-levelcorre-spondencesbetweenreal-worldobjectsandShapeNetmod-els.Thisallowsustotrainonaclassiﬁcationproblemwherebothrealandsyntheticdataaremixedinsideofeachcate-goryusingrealandsyntheticdatawithinsharedclasslabels.Thus,wecanlearnanembeddingbetweenrealandsyn-theticdatainordertoperformmodelretrievalforRGB-Dscans.Tothisend,weusethevolumetricshapeclassiﬁ-cationnetworkbyQietal.[66],weusethesametrainingprocedureasinSec.5.1.Nearestneighborsareretrievedbasedonthe(cid:96)2distancebetweentheextractedfeaturede-scriptors,andmeasuredagainstthegroundtruthprovidedbytheCADmodelretrievaltask.InTable6,weshowob-jectretrievalresultsusingobjectsfromScanNettoqueryfornearestneighbormodelsfromShapeNetCore.NotethattrainingonShapeNetandScanNetindependentlyresultsinpoorretrievalperformance,asneitherareabletobridgethegapbetweenthedifferingcharacteristicsofsyntheticandreal-worlddata.TrainingonbothShapeNetandScanNettogetherisabletoﬁndanembeddingofshapesimilaritiesbetweenbothdatamodalities,resultinginmuchhigherre-trievalaccuracy.6.ConclusionThispaperintroducesScanNet:alarge-scaleRGB-Ddatasetof1513scanswithsurfacereconstructions,instance-levelobjectcategoryannotations,and3DCADmodelplacements.Tomakethecollectionofthisdatapos-sible,wedesignedascalableRGB-Dacquisitionandse-manticannotationframeworkthatweprovidefortheben-eﬁtofthecommunity.Wedemonstratedthattherichly-annotatedscandatacollectedsofarinScanNetisusefulinachievingstate-of-the-artperformanceonseveral3Dsceneunderstandingtasks;wehopethatScanNetwillinspirefu-tureworkonmanyothertasks.AcknowledgmentsThisprojectisfundedbyGoogleTango,Intel,NSF(IIS-1251217andVEC1539014/1539099),andaStanfordGraduatefellowship.WealsothankOccipitalfordonat-ingstructuresensorsandNvidiaforhardwaredonations,aswellassupportbytheMax-PlanckCenterforVisualCom-putingandtheStanfordCURISprogram.Further,wethankToanVuong,JosephChang,andHelenJiangforhelponthemobilescanningappandthescanningprocess,andHopeCasey-AllenandDucNugyenforearlyprototypesoftheannotationinterfaces.Lastbutnotleast,wewouldliketothankallthevolunteerswhohelpedwithscanningandget-tingusaccesstoscanningspaces.References[1]A.Aldoma,F.Tombari,L.DiStefano,andM.Vincze.Aglobalhypothesesveriﬁcationmethodfor3Dobjectrecog-nition.InEuropeanConferenceonComputerVision,pages511–524.Springer,2012.2[2]I.Armeni,S.Sax,A.R.Zamir,andS.Savarese.Joint2d-3d-semanticdataforindoorsceneunderstanding.arXivpreprintarXiv:1702.01105,2017.2[3]I.Armeni,O.Sener,A.R.Zamir,H.Jiang,I.Brilakis,M.Fischer,andS.Savarese.3Dsemanticparsingoflarge-scaleindoorspaces.CVPR,2016.1,2[4]I.B.Barbosa,M.Cristani,A.DelBue,L.Bazzani,andV.Murino.Re-identiﬁcationwithRGB-Dsensors.InEu-ropeanConferenceonComputerVision,pages433–442.Springer,2012.2[5]M.Blaha,C.Vogel,A.Richard,J.D.Wegner,T.Pock,andK.Schindler.Large-scalesemantic3dreconstruction:anadaptivemulti-resolutionmodelformulti-classvolumetriclabeling.InProceedingsoftheIEEEConferenceonCom-puterVisionandPatternRecognition,pages3176–3184,2016.7[6]A.X.Chang,T.Funkhouser,L.Guibas,P.Hanrahan,Q.Huang,Z.Li,S.Savarese,M.Savva,S.Song,H.Su,etal.ShapeNet:Aninformation-rich3Dmodelrepository.arXivpreprintarXiv:1512.03012,2015.1,5,6,21,22[7]J.Chen,D.Bautembach,andS.Izadi.Scalablereal-timevolumetricsurfacereconstruction.ACMTransactionsonGraphics(TOG),32(4):113,2013.4[8]I.Cherabier,C.H¨ane,M.R.Oswald,andM.Pollefeys.Multi-labelsemantic3dreconstructionusingvoxelblocks.In3DVision(3DV),2016FourthInternationalConferenceon,pages601–610.IEEE,2016.7[9]S.Choi,Q.-Y.Zhou,andV.Koltun.Robustreconstructionofindoorscenes.In2015IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pages5556–5565.IEEE,2015.2,4,15[10]S.Choi,Q.-Y.Zhou,S.Miller,andV.Koltun.Alargedatasetofobjectscans.arXiv:1602.02481,2016.2,15[11]B.CurlessandM.Levoy.Avolumetricmethodforbuildingcomplexmodelsfromrangeimages.InProceedingsofthe23rdannualconferenceonComputergraphicsandinterac-tivetechniques,pages303–312.ACM,1996.4[12]A.Dai,M.Nießner,M.Zoll¨ofer,S.Izadi,andC.Theobalt.BundleFusion:Real-timegloballyconsistent3Dreconstruc-tionusingon-the-ﬂysurfacere-integration.arXivpreprintarXiv:1604.01093,2016.2,4,20[13]A.Dai,C.R.Qi,andM.Nießner.Shapecompletionus-ing3d-encoder-predictorcnnsandshapesynthesis.arXivpreprintarXiv:1612.00101,2016.6[14]M.DiCicco,L.Iocchi,andG.Grisetti.Non-parametriccal-ibrationfordepthsensors.RoboticsandAutonomousSys-tems,74:309–317,2015.3,20[15]F.Endres,J.Hess,N.Engelhard,J.Sturm,D.Cremers,andW.Burgard.AnevaluationoftheRGB-DSLAMsystem.InRoboticsandAutomation(ICRA),2012IEEEInternationalConferenceon,pages1691–1696.IEEE,2012.2[16]N.ErdogmusandS.Marcel.Spooﬁngin2Dfacerecognitionwith3Dmasksandanti-spooﬁngwithKinect.InBiometrics:Theory,ApplicationsandSystems(BTAS),2013IEEESixthInternationalConferenceon,pages1–6.IEEE,2013.2[17]M.Everingham,L.VanGool,C.K.Williams,J.Winn,andA.Zisserman.ThePASCALvisualobjectclasses(VOC)challenge.Internationaljournalofcomputervision,88(2):303–338,2010.1[18]C.Fellbaum.WordNet.WileyOnlineLibrary,1998.6,12,21[19]P.F.FelzenszwalbandD.P.Huttenlocher.Efﬁcientgraph-basedimagesegmentation.InternationalJournalofCom-puterVision,59(2):167–181,2004.4[20]M.Firman.RGBDdatasets:Past,presentandfuture.InCVPRWorkshoponLargeScale3DData:Acquisition,Mod-ellingandAnalysis,2016.2[21]S.Fothergill,H.Mentis,P.Kohli,andS.Nowozin.Instruct-ingpeoplefortraininggesturalinteractivesystems.InPro-ceedingsoftheSIGCHIConferenceonHumanFactorsinComputingSystems,pages1737–1746.ACM,2012.2[22]D.F.Fouhey,A.Gupta,andM.Hebert.Data-driven3Dprimitivesforsingleimageunderstanding.InProceedingsoftheIEEEInternationalConferenceonComputerVision,pages3392–3399,2013.2[23]D.F.Fouhey,A.Gupta,andM.Hebert.Unfoldinganindoororigamiworld.InEuropeanConferenceonComputerVision,pages687–702.Springer,2014.2[24]D.Girardeau-Montaut.CloudCompare3Dpointcloudandmeshprocessingsoftware.OpenSourceProject,2011.2[25]R.GuoandD.Hoiem.Supportsurfacepredictioninindoorscenes.InProceedingsoftheIEEEInternationalConferenceonComputerVision,pages2144–2151,2013.2[26]S.Gupta,P.Arbelaez,andJ.Malik.Perceptualorganiza-tionandrecognitionofindoorscenesfromRGB-Dimages.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages564–571,2013.2[27]S.Gupta,R.Girshick,P.Arbel´aez,andJ.Malik.LearningrichfeaturesfromRGB-Dimagesforobjectdetectionandsegmentation.InEuropeanConferenceonComputerVision,pages345–360.Springer,2014.2[28]A.Handa,V.Patraucean,V.Badrinarayanan,S.Stent,andR.Cipolla.Scenenet:Understandingrealworldindoorsceneswithsyntheticdata.arXivpreprintarXiv:1511.07041,2015.8[29]A.Handa,T.Whelan,J.McDonald,andA.J.Davison.AbenchmarkforRGB-Dvisualodometry,3DreconstructionandSLAM.In2014IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages1524–1531.IEEE,2014.2[30]V.Hedau,D.Hoiem,andD.Forsyth.Recoveringfreespaceofindoorscenesfromasingleimage.InComputerVisionandPatternRecognition(CVPR),2012IEEEConferenceon,pages2807–2814.IEEE,2012.2[31]A.Hermans,G.Floros,andB.Leibe.Dense3DsemanticmappingofindoorscenesfromRGB-Dimages.InRoboticsandAutomation(ICRA),2014IEEEInternationalConfer-enceon,pages2631–2638.IEEE,2014.7,8[32]B.-S.Hua,Q.-H.Pham,D.T.Nguyen,M.-K.Tran,L.-F.Yu,andS.-K.Yeung.SceneNN:Ascenemeshesdatasetwithannotations.InInternationalConferenceon3DVision(3DV),volume1,2016.1,2,3,5,7,12,15,16[33]M.Innmann,M.Zollh¨ofer,M.Nießner,C.Theobalt,andM.Stamminger.VolumeDeform:Real-timevolumetricnon-rigidreconstruction.arXivpreprintarXiv:1603.08161,2016.2[34]C.Ionescu,F.Li,andC.Sminchisescu.Latentstructuredmodelsforhumanposeestimation.In2011InternationalConferenceonComputerVision,pages2220–2227.IEEE,2011.2[35]C.Ionescu,D.Papava,V.Olaru,andC.Sminchisescu.Human3.6M:Largescaledatasetsandpredictivemethodsfor3Dhumansensinginnaturalenvironments.IEEEtransactionsonpatternanalysisandmachineintelligence,36(7):1325–1339,2014.2[36]A.Janoch,S.Karayev,Y.Jia,J.T.Barron,M.Fritz,K.Saenko,andT.Darrell.Acategory-level3Dobjectdataset:PuttingtheKinecttowork.InConsumerDepthCamerasforComputerVision,pages141–165.Springer,2013.2[37]O.Kahler,V.AdrianPrisacariu,C.YuhengRen,X.Sun,P.Torr,andD.Murray.Veryhighframeratevol-umetricintegrationofdepthimagesonmobiledevices.IEEETransactionsonVisualizationandComputerGraph-ics,21(11):1241–1250,2015.4[38]O.K¨ahler,V.A.Prisacariu,andD.W.Murray.Real-timelarge-scaledense3Dreconstructionwithloopclosure.InEuropeanConferenceonComputerVision,pages500–516.Springer,2016.4[39]A.Karpathy,S.Miller,andL.Fei-Fei.Objectdiscoveryin3Dscenesviashapeanalysis.InRoboticsandAutoma-tion(ICRA),2013IEEEInternationalConferenceon,pages2088–2095.IEEE,2013.4[40]M.KepskiandB.Kwolek.Falldetectionusingceiling-mounted3Ddepthcamera.InComputerVisionTheoryandApplications(VISAPP),2014InternationalConferenceon,volume2,pages640–647.IEEE,2014.2[41]B.-s.Kim,P.Kohli,andS.Savarese.3dsceneunderstand-ingbyvoxel-crf.InProceedingsoftheIEEEInternationalConferenceonComputerVision,pages1425–1432,2013.7[42]M.Klingensmith,I.Dryanovski,S.Srinivasa,andJ.Xiao.Chisel:Realtimelargescale3Dreconstructiononboardamobiledeviceusingspatiallyhashedsigneddistanceﬁelds.InRobotics:ScienceandSystems,2015.4[43]H.S.Koppula,R.Gupta,andA.Saxena.LearninghumanactivitiesandobjectaffordancesfromRGB-Dvideos.TheInternationalJournalofRoboticsResearch,32(8):951–970,2013.2[44]A.Krizhevsky,I.Sutskever,andG.E.Hinton.ImageNetclassiﬁcationwithdeepconvolutionalneuralnetworks.InAdvancesinneuralinformationprocessingsystems,pages1097–1105,2012.1[45]Y.Li,A.Dai,L.Guibas,andM.Nießner.Database-assistedobjectretrievalforreal-time3Dreconstruction.InComputerGraphicsForum,volume34,pages435–446.WileyOnlineLibrary,2015.2[46]D.Lin,S.Fidler,andR.Urtasun.Holisticsceneunderstand-ingfor3DobjectdetectionwithRGBDcameras.InPro-ceedingsoftheIEEEInternationalConferenceonComputerVision,pages1417–1424,2013.2[47]T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ra-manan,P.Doll´ar,andC.L.Zitnick.MicrosoftCOCO:Com-monobjectsincontext.InEuropeanConferenceonCom-puterVision,pages740–755.Springer,2014.1[48]L.LiuandL.Shao.LearningdiscriminativerepresentationsfromRGB-Dvideodata.InIJCAI,volume1,page3,2013.2[49]J.Long,E.Shelhamer,andT.Darrell.Fullyconvolutionalnetworksforsemanticsegmentation.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecogni-tion,pages3431–3440,2015.7[50]M.Luber,L.Spinello,andK.O.Arras.PeopletrackinginRGB-Ddatawithon-lineboostedtargetmodels.In2011IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,pages3844–3849.IEEE,2011.2[51]J.Mason,B.Marthi,andR.Parr.Objectdisappearanceforobjectdiscovery.In2012IEEE/RSJInternationalConfer-enceonIntelligentRobotsandSystems,pages2836–2843.IEEE,2012.2[52]O.Mattausch,D.Panozzo,C.Mura,O.Sorkine-Hornung,andR.Pajarola.Objectdetectionandclassiﬁcationfromlarge-scaleclutteredindoorscans.InComputerGraphicsFo-rum,volume33,pages11–21.WileyOnlineLibrary,2014.2,6[53]D.MaturanaandS.Scherer.VoxNet:A3Dconvolutionalneuralnetworkforreal-timeobjectrecognition.InIntelligentRobotsandSystems(IROS),2015IEEE/RSJInternationalConferenceon,pages922–928.IEEE,2015.2[54]J.McCormac,A.Handa,A.Davison,andS.Leutenegger.Semanticfusion:Dense3dsemanticmappingwithconvo-lutionalneuralnetworks.arXivpreprintarXiv:1609.05130,2016.7,8[55]S.Meister,S.Izadi,P.Kohli,M.H¨ammerle,C.Rother,andD.Kondermann.WhencanweuseKinectFusionforgroundtruthacquisition.InWorkshoponColor-DepthCameraFu-sioninRobotics,IROS,volume2,2012.2[56]A.Mian,M.Bennamoun,andR.Owens.Ontherepeatabil-ityandqualityofkeypointsforlocalfeature-based3Dob-jectretrievalfromclutteredscenes.InternationalJournalofComputerVision,89(2-3):348–361,2010.2[57]R.Min,N.Kose,andJ.-L.Dugelay.KinectFaceDB:AKinectdatabaseforfacerecognition.IEEETransactionsonSystems,Man,andCybernetics:Systems,44(11):1534–1548,2014.2[58]P.K.NathanSilberman,DerekHoiemandR.Fergus.IndoorsegmentationandsupportinferencefromRGBDimages.InECCV,2012.1,2,6,7,8,15,21[59]R.A.Newcombe,S.Izadi,O.Hilliges,D.Molyneaux,D.Kim,A.J.Davison,P.Kohi,J.Shotton,S.Hodges,andA.Fitzgibbon.KinectFusion:Real-timedensesurfacemap-pingandtracking.InMixedandaugmentedreality(ISMAR),201110thIEEEinternationalsymposiumon,pages127–136.IEEE,2011.2,4[60]D.T.Nguyen,B.-S.Hua,L.-F.Yu,andS.-K.Yeung.Aro-bust3D-2Dinteractivetoolforscenesegmentationandan-notation.arXivpreprintarXiv:1610.05883,2016.2,3[61]B.Ni,G.Wang,andP.Moulin.RGBD-HuDaAct:Acolor-depthvideodatabaseforhumandailyactivityrecognition.InConsumerDepthCamerasforComputerVision,pages193–208.Springer,2013.2[62]M.Nießner,M.Zollh¨ofer,S.Izadi,andM.Stamminger.Real-time3Dreconstructionatscaleusingvoxelhashing.ACMTransactionsonGraphics(TOG),32(6):169,2013.2,4,15[63]Occipital.Occipital:Thestructuresensor,2016.3[64]K.PhillipandV.Koltun.Efﬁcientinferenceinfullycon-nectedcrfswithgaussianedgepotentials.Adv.NeuralInf.Process.Syst,2011.8[65]F.Pomerleau,S.Magnenat,F.Colas,M.Liu,andR.Sieg-wart.Trackingadepthcamera:ParameterexplorationforfastICP.In2011IEEE/RSJInternationalConferenceonIn-telligentRobotsandSystems,pages3824–3829.IEEE,2011.2[66]C.R.Qi,H.Su,M.Niessner,A.Dai,M.Yan,andL.J.Guibas.Volumetricandmulti-viewCNNsforobjectclassi-ﬁcationon3Ddata.arXivpreprintarXiv:1604.03265,2016.2,6,8[67]X.Ren,L.Bo,andD.Fox.RGB-(D)scenelabeling:Fea-turesandalgorithms.InComputerVisionandPatternRecog-nition(CVPR),2012IEEEConferenceon,pages2759–2766.IEEE,2012.2[68]A.Richtsfeld,T.M¨orwald,J.Prankl,M.Zillich,andM.Vincze.Segmentationofunknownobjectsinindooren-vironments.In2012IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,pages4791–4796.IEEE,2012.2[69]B.C.Russell,A.Torralba,K.P.Murphy,andW.T.Free-man.LabelMe:adatabaseandweb-basedtoolforimageannotation.Internationaljournalofcomputervision,77(1-3):157–173,2008.2[70]M.Savva,A.X.Chang,P.Hanrahan,M.Fisher,andM.Nießner.SceneGrok:Inferringactionmapsin3Denvironments.ACMTransactionsonGraphics(TOG),33(6):212,2014.2[71]M.Savva,A.X.Chang,P.Hanrahan,M.Fisher,andM.Nießner.PiGraphs:Learninginteractionsnapshotsfromobservations.ACMTransactionsonGraphics(TOG),35(4),2016.1,2,3,15,16[72]J.Shotton,B.Glocker,C.Zach,S.Izadi,A.Criminisi,andA.Fitzgibbon.Scenecoordinateregressionforestsforcam-erarelocalizationinRGB-Dimages.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecogni-tion,pages2930–2937,2013.2[73]A.ShrivastavaandA.Gupta.Buildingpart-basedobjectde-tectorsvia3Dgeometry.InProceedingsoftheIEEEInter-nationalConferenceonComputerVision,pages1745–1752,2013.2[74]N.SilbermanandR.Fergus.Indoorscenesegmentationus-ingastructuredlightsensor.InProceedingsoftheInter-nationalConferenceonComputerVision-Workshopon3DRepresentationandRecognition,2011.1,2[75]S.Song,S.P.Lichtenberg,andJ.Xiao.SUNRGB-D:ARGB-Dsceneunderstandingbenchmarksuite.InProceed-ingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages567–576,2015.1,2[76]S.SongandJ.Xiao.Slidingshapesfor3Dobjectdetectionindepthimages.InEuropeanConferenceonComputerVision,pages634–651.Springer,2014.2[77]S.SongandJ.Xiao.Deepslidingshapesforamodal3DobjectdetectioninRGB-Dimages.arXivpreprintarXiv:1511.02300,2015.1,2[78]S.Song,F.Yu,A.Zeng,A.X.Chang,M.Savva,andT.Funkhouser.Semanticscenecompletionfromasingledepthimage.arXivpreprintarXiv:1611.08974,2016.2[79]L.SpinelloandK.O.Arras.PeopledetectioninRGB-Ddata.In2011IEEE/RSJInternationalConferenceonIntel-ligentRobotsandSystems,pages3838–3843.IEEE,2011.2[80]S.SteinandS.J.McKenna.Combiningembeddedac-celerometerswithcomputervisionforrecognizingfoodpreparationactivities.InProceedingsofthe2013ACMinter-nationaljointconferenceonPervasiveandubiquitouscom-puting,pages729–738.ACM,2013.2[81]J.Sturm,N.Engelhard,F.Endres,W.Burgard,andD.Cre-mers.AbenchmarkfortheevaluationofRGB-DSLAMsystems.In2012IEEE/RSJInternationalConferenceonIn-telligentRobotsandSystems,pages573–580.IEEE,2012.1,2[82]H.Su,S.Maji,E.Kalogerakis,andE.G.Learned-Miller.Multi-viewconvolutionalneuralnetworksfor3Dshaperecognition.InProc.ICCV,2015.6[83]J.Sung,C.Ponce,B.Selman,andA.Saxena.Humanactiv-itydetectionfromRGBDimages.plan,activity,andintentrecognition,64,2011.2[84]A.Teichman,S.Miller,andS.Thrun.UnsupervisedintrinsiccalibrationofdepthsensorsviaSLAM.InRobotics:ScienceandSystems,volume248,2013.3,20[85]J.Valentin,A.Dai,M.Nießner,P.Kohli,P.Torr,S.Izadi,andC.Keskin.Learningtonavigatetheenergylandscape.arXivpreprintarXiv:1603.05772,2016.2[86]J.Valentin,V.Vineet,M.-M.Cheng,D.Kim,J.Shotton,P.Kohli,M.Nießner,A.Criminisi,S.Izadi,andP.Torr.Se-manticPaint:Interactive3Dlabelingandlearningatyourﬁn-gertips.ACMTransactionsonGraphics(TOG),34(5):154,2015.2,7,8[87]H.Wang,J.Wang,andW.Liang.OnlinereconstructionofindoorscenesfromRGB-Dstreams.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecogni-tion,pages3271–3279,2016.3[88]T.Whelan,M.Kaess,M.Fallon,H.Johannsson,J.Leonard,andJ.McDonald.Kintinuous:SpatiallyextendedKinectFu-sion.2012.4[89]T.Whelan,S.Leutenegger,R.F.Salas-Moreno,B.Glocker,andA.J.Davison.ElasticFusion:DenseSLAMwithoutaposegraph.Proc.Robotics:ScienceandSystems,Rome,Italy,2015.4[90]T.Whelan,R.F.Salas-Moreno,B.Glocker,A.J.Davison,andS.Leutenegger.ElasticFusion:Real-timedenseSLAMandlightsourceestimation.TheInternationalJournalofRoboticsResearch,page0278364916669237,2016.4[91]Z.Wu,S.Song,A.Khosla,F.Yu,L.Zhang,X.Tang,andJ.Xiao.3DShapeNets:Adeeprepresentationforvolumetricshapes.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages1912–1920,2015.1,2,6,8,21[92]J.Xiao,A.Owens,andA.Torralba.SUN3D:Adatabaseofbigspacesreconstructedusingsfmandobjectlabels.InComputerVision(ICCV),2013IEEEInternationalConfer-enceon,pages1625–1632.IEEE,2013.1,2[93]B.Zeisl,K.Koser,andM.Pollefeys.AutomaticregistrationofRGB-Dscansviasalientdirections.InProceedingsoftheIEEEinternationalconferenceoncomputervision,pages2808–2815,2013.2[94]J.Zhang,C.Kan,A.G.Schwing,andR.Urtasun.Estimatingthe3Dlayoutofindoorscenesanditsclutterfromdepthsen-sors.InProceedingsoftheIEEEInternationalConferenceonComputerVision,pages1273–1280,2013.2[95]M.Zollh¨ofer,A.Dai,M.Innmann,C.Wu,M.Stamminger,C.Theobalt,andM.Nießner.Shading-basedreﬁnementonvolumetricsigneddistancefunctions.ACMTransactionsonGraphics(TOG),34(4):96,2015.2A.DatasetStatisticsandComparisonsInthissection,weprovidethoroughstatisticsontheconstructionandcompositionofScanNetdataset,andalsocompareittothemostsimilardatasetsfrompriorwork.A.1.ExampleAnnotatedReconstructionsFig.6showssixexampleannotatedreconstructionsforavarietyofspaces.Foreachreconstruction,thesurfacemeshwithcolorsisshown,aswellasavisualizationwithcategorylabelsforeachobjectcollectedusingourcrowd-sourcedannotationinterface.CategorylabelsareconsistentbetweenspacesandaremappedtoWordNet[18]synsets.Inadditiontothecategorylabel,separateobjectinstancelabelsarealsoavailabletoindicatemultipleinstancesofagivencategory,suchasdistinctchairsaroundaconferencetableinthefourthrowofFig.6.Fig.7showsalargersetofreconstructedspacesinScan-Nettoillustratethevarietyofspacesthatarepartofthedataset.Thescansrangefromsmallspaceswithjustafewobjects(e.g.,toilets),tolargeareaswithdozensofobjects(e.g.,classroomsandstudioapartments).A.2.DatasetConstructionStatisticsTheconstructionofScanNetwascarriedoutwiththeRGB-Dacquisitionandannotationframeworkdescribedinthemainpaper.InordertoprovideanintuitionoftheScanNetCategoryCountwall6226chair4279ﬂoor3212table2223door1181couch1048cabinet937desk733shelf732bed699ofﬁcechair669trashcan561pillow490sink470window398toilet397picture351bookshelf328monitor308curtain280computer274armchair264bathtub253coffeetable239box231diningchair230refrigerator226book221lamp218towel216kitchencabinet203drawer202tv187nightstand182counter179dresser177clothes164countertop163stool130plant130cushion116ceiling114bedframe111keyboard107endtable105toiletpaper104bag104backpack100blanket94diningtable94SceneNN[32]CategoryCountchair194table53ﬂoor44seat41desk39monitor31sofa25cabinet25door24box23keyboard23trashbin21wall20pillow19fridge18stand18bag17bed16window14sink13printer12computer12chair0112desk111monitor0110shelves10shelf10chair110chair0210fan9basket9desk29laptop9trashbin9kettle9microwave9monitor18stove8chair28bike7blanket7drawer7lamp7wall027wall017wall047backpack7cup7chair37whiteboard7Table8.Totalcountsofannotatedobjectinstancesofthe50largestcategoriesinScanNet(left),andinSceneNN[32](right),themostsimilarannotatedRGB-Dreconstructiondataset.ScanNetcon-tainsfarmoreannotatedobjectinstances,andtheannotatedla-belsareprocessedforconsistencytoremoveduplicatessuchas“chair01”inSceneNN.scalabilityofourframework,wereporttimingstatisticsforboththereconstructionandannotationsteps.Theme-dianreconstructionprocessingtime(includingdataconver-sion,densevoxelfusion,surfacemeshextraction,align-Figure6.ExampleannotatedscansinScanNet.Left:reconstructedsurfacemeshwithoriginalcolors.Middle:colorindicatescategorylabelconsistentlyacrossallscans.Right:eachobjectinstanceshownwithadifferentrandomlyassignedcolor.Figure7.AvarietyofexampleannotatedscansinScanNet.Colorsindicatecategoryconsistentlyacrossallscans.ment,cleanup,andpreviewthumbnailimagerendering)is11.3minforeachscene.Afewoutliersexistwithsigniﬁ-cantlyhigherprocessingtimes(ontheorderofhours),duetounplannedprocessingserverdowntimeduringourdatacollection(mainlysoftwareupdates),resultinginahighermeanreconstructiontimeof14.9min.Afterreconstructioniscomplete,eachscanisannotatedbyseveralcrowdworkersonAmazonMechanicalTurk(2.3workersonaverageperscan).Themedianannotationtimepercrowdworkeris12.0min(meantimeis17.3min,againduetoafewoutlierworkerswhotakesigniﬁcantlylonger).Aggregatingthetimetakenacrossworkersforannotatingeachofthe1513scansinScanNet,themediantimeperscanis16.8min,andthemeantimeperscanis22.3min.A.3.DatasetCompositionStatisticsTheconstructionoftheScanNetdatasetismotivatedbythelackoflarge,annotated,denselyreconstructedRGB-Ddatasetof3Dscenesthatarepubliclyavailableintheaca-demiccommunity.ExistingRGB-Ddatasetseitherhavefullscene-levelannotationsonlyforasubsetofRGB-Dframes(e.g.,NYUv2depth[58]),ortheyfocusonanno-tatingdecontextualizedobjectsandnotscenes(e.g.,Choietal.[10]).Thetwodatasetsthatdoannotatedenselyrecon-structedRGB-DspacesatthescenelevelaretheSceneNNdatasetbyHuaetal.[32]andthesmallerPiGraphsdatasetbySavvaetal.[71].SceneNNconsistsof94RGB-DscanscapturedusingAsusXtionProdevicesandreconstructedwiththemethodofChoietal.[9].Theresultingdensely-fusedsurfacemeshesarefullysegmentedatthelevelofmeaningfulob-jects.However,onlyasmallsetofsegmentsareannotatedwithsemanticlabels.Ontheotherhand,thePiGraphs[71]datasetconsistsof26RGB-DscanscapturedwithKinectv1devicesandreconstructedwiththeVoxelHashingapproachofNießneretal.[62].Thisdatasethasmorecompleteandcleansemanticlabels,includingobjectpartsandobjectin-stances.However,itcontainsveryfewscenesandislimitedinthevarietyofenvironments,consistingmostlyofofﬁcesandconferencerooms.Toillustratethelargegapinquan-tityofannotatedsemanticlabelsbetweenthesetwodatasetsandScanNet,Fig.8plotshistogramsofthetotalnumberoflabeledobjectinstancesandthetotalnumbersofuniquese-manticlabelsforeachscan.Inordertodemonstratehowourcategorylabelsmaptootherdata,weplotthedistributionofannotatedobjectlabelscorrespondedtotheShapeNetCore3DCADmodelcategoriesinFig.9.ThismappingisleveragedduringourCADmodelalignmentandretrievaltasktoautomati-callysuggestinstancesofCADmodelsfromShapeNetthatmatchthelabelofagivenobjectcategoryinthereconstruc-tion.Wecanalsoobtain2DannotationsontheinputRGB-DFigure9.Top25mostfrequentannotationlabelsinScanNetscansmappedtoShapeNetCoreclasses.ScanNethasthousandsof3Dreconstructedinstancesofcommonobjectssuchaschairs,tables,andcabinets.sequencesbyprojectingour3Dannotationsintoeachframeusingthecorrespondingcamerapose.Thisway,weob-tainanaverageof76%annotationcoverageofallpixelsperscenebyusingthepreviouslyobtained3Dannotations.A.4.NYUv2ReconstructionandComparisonHere,wediscusshowScanNetrelatestoNYUv2,oneofthemostpopularRGB-Ddatasetwithannotations.InordertocomparethedatainScanNetwiththedatainNYUv2,wereconstructedandannotatedalltheRGB-DsequencesinNYUv2usingourframework.(Notethatfor9sequencesoftheNYUv2dataset,ourframeworkdidnotobtainvalidcameraposesfor>50%oftheframes,sowedidnotcom-putereconstructionsandannotationsforthesesequences.)Moreover,wecreatedasetofsurfacemeshsemanticanno-tationsfortheNYUv2reconstructionsbyprojectingeverypixeloftheannotatedRGB-Dframeswithvaliddepthandlabelintoworldspaceusingourcomputedcameraposes,andassigningthecorrespondingobjectlabeltotheclos-estsurfacemeshvertices(within0.04cm,usingakd-treelookup).Wethencomparethetotalsurfaceareaoftherecon-structedmeshesthatwasannotatedusingprojectionfromtheannotatedNYUv2frames,andusingourannotationpipeline.Fig.10plotsthepercentageofreconstructedsur-facesinNYUv2thatwereannotatedwitheachapproach,aswellasthepercentagedistributionfortheScanNetre-constructionsforcomparison.Notethatweexcludethe9sequencesforwhichwedonothaveenoughvalidcameraposes.AnoticeabledifferencebetweentheRGB-DsequencesinNYUv2andthoseinScanNetisthatoverall,theScanNetsequencesaremorecompletesurfacereconstructionsofthereal-worldspaces.Mostimportantly,theNYUv2originalframesingeneraldonotcoverasufﬁcientnumberofview-pointsofthespacetoensurefullreconstructionofsemanti-callymeaningfulcompleteobjects.Fig.11showsacompar-isonofseveralreconstructedscenesfromNYUv2RGB-DPiGraphsSceneNNScanNet01020304050Labeled Objects01020304050Labeled Objects01020304050Labeled Objects050100150200250300350400450500550600Number of ScansPiGraphsSceneNNScanNet01020304050607080Unique Labels01020304050607080Unique Labels01020304050607080Unique Labels050100150200250300350400450Number of ScansFigure8.Histogramsofthetotalnumberofobjectslabeledperscan(top)andtotalnumberofuniquelabelsperscan(bottom)inthePiGraphs[71],SceneNN[32]andourdataset(ScanNet).ThehistogramsshowthatScanNethasmanyannotatedobjectsoveralargernumberofscans,rangingincomplexitywithregardstothetotalnumberofobjetsperscan.0.10.20.30.40.50.60.70.80.9NYU Surface Coverage (Original Frames)0.10.20.30.40.50.60.70.80.91.0NYU Surface Coverage (3D Re-annotation)0%2%4%6%8%10%12%14%% of Scans0.10.20.30.40.50.60.70.80.9ScanNet Surface Coverage (3D Annotation)0%2%4%6%8%10%12%14%16%18%% of ScansFigure10.Histogramsofthepercentageoftotalreconstructionsurfaceareaperscanthatissemanticallylabeledfor:NYUv2reconstruc-tionsusingprojectionofRGB-Dannotatedframes(left),forNYUv2reconstructionsusingour3Dannotationinterface(middle),andforScanNetreconstructionssimilarlyannotatedwithourinterface(right).sequencesvscomparablereconstructionsfromScanNet.Asshowninthetop-downviews,theNYUreconstructionsaremuchmoresparsethantheScanNetreconstructions.ThisdisparitymakesamoredirectcomparisonwithScanNetre-constructionshardtoquantify.However,wecanconcludethatprojectingtheannotatedNYUv2RGB-Dframestore-constructionsisnotsufﬁcienttosemanticallyannotatethespaces,asisclearfromthefarlowersurfacecoveragedis-tributionforNYUv2inFig.10.B.TasksHereweprovidemoredetailsaboutthe3Dsceneunder-standingtasksandbenchmarksdiscussedinthemainpaper.B.1.SemanticVoxelLabelingForthesemanticvoxellabelingtask,weproposeanet-workwhichpredictsclasslabelsforeachcolumnofavox-elizedscene.AsshowninFig.12,ournetworktakesasinputa2×31×31×62volumeandusesaseriesoffullyconvolutionallayerstosimultaneouslypredictclassscoresforthecentercolumnof62voxels.Weleverageinforma-tionfromthevoxelneighborhoodofbothoccupiedspace(voxelsonthesurface)andknownspace(voxelsinfrontofasurfaceaccordingtothecameratrajectory)todescribetheinputpartialdatafromascan.Attesttime,weslidethenetworkthroughascanthroughavoxelizedscanalongthexy-plane,andeachcolumnispredictedindependently.Fig.13visualizesseveralScanNettestscanswithvoxellabelpredictions,alongsidethegroundtruthannotationsfromourcrowdsourcedlabelingtask.Figure11.ComparisonofreconstructedBathroom(top),Bedroom(middle),andKitchen(bottom)fromNYUv2RGB-Dframes(left),andacomparablereconstructionfromScanNet(right).ForeachNYUscene,weshowanexamplecolorframe,theroughcorrespondingregionoftheviewinthereconstructedscene(lightbluebox),andatopdownviewofthereconstruction.WhileNYUv2reconstructionlookcompletefromsomeview-points,muchofthesceneisleftuncovered(seetopdownviews).Inconstrast,ScanNetreconstructionhaveamuchmorecompletecoverageofthespaceandallowfordenserannotation.C.DatasetAcquisitionFrameworkThissectionprovidesmoredetailsforspeciﬁcstepsinourRGB-Ddataacquisitionframeworkwhichwasde-scribedinthemainpaper.Toenablescalabledatasetac-quisition,wedesignedourdataacquisitionframeworkfor1)easeofuseduringcapture,2)robustreconstruction,3)rapidcrowdsourcing,4)visibilityintothecollecteddataanditsmetadata.For1)wedevelopedaniPadapp(seeAp-Figure12.DeepNeuralNetworkarchitectureforoursemanticvoxellabelpredictiontask.Thenetworkismainlycomposedof3Dconvolutionsthatprocessthegeometryofasceneusinga3Dvoxelgridrepresentation.Figure14.OurRGB-DrecordingapponaniPadAir2withat-tachedStructuresensor(showingcolorstreamatthetopanddepthstreamatthebottom).TheappallowsnoviceuserstorecordRGB-Dvideosanduploadtoaserverforreconstructionandannotation.pendixC.1)withaneasy-to-useinterface,reasonablescan-ningpresets,andminimalisticusercontrols.Toensuregoodreconstructionwithminimaluserinteractionduringscan-ning,wetesteddifferentexposuretimesettingsandenabledautowhitebalancing(seeAppendixC.1).Wealsoestab-lishedasimplecalibrationprocessthatnoviceuserscouldcarryout(seeAppendixC.2),andofﬂoadedRGB-Drecon-structiontothecloud(seeAppendixC.3).Finally,wede-velopedweb-basedUIsforcrowdsourcingsemanticanno-tationtasksasdescribedinAppendixC.4,andformanagingthecollecteddataasdescribedinAppendixC.6.C.1.RGB-DAcquisitionUIFig.14showsourRGB-DrecordingappontheiPad.WedesignedaniPadappwithasimplecamera-basedUIandaminimalisticsetofcontrols.Beforescanning,theuseren-tersausername,ascenename,andselectsthetypeofroombeingscanned.Theuserthenpressesasinglebuttontostartandstopascanrecording.Theinterfacecanbetoggledbetweenvisualizingthecolorstreamandthedepthstreamoverlaidonthecolor.Wefoundthatthemostchallengingpartofscanningfornoviceuserswasacquiringanintuitionastowhatre-gionsduringscanningarelikelytoresultinpoortrack-ingandfailedreconstruction.Toalleviatethis,weaddeda“progressbar”-stylevisualizationduringactivescan-ningwhichindicatesthefeaturefulnessoftheregionbeingscanned.Thebarrangesfromfullgreen,indicatinghighfeaturecount,tonear-emptyblack,indicatinglowfeaturecountandhighlikelihoodoftrackingloss.ThisUIelementwashelpfulforquicklyfamiliarizinguserswiththescan-ningprocess.Afterscanning,theusercanviewalistofscansonthedeviceandselecttouploadthescandatatoaprocessingserver.Duringupload,aprogressbarisshownandscanningisdisabled.Uponcompletionoftheupload,thechecksumsofscandataontheserverareveriﬁedagainstlocaldataandthescansareautomaticallydeletedtoprovidemorememoryforscanning.AutowhitebalancingandExposureSettingsAnotherchallengetowardsperformingreconstructioninuncon-trolledscenariosisthewidevarietyofilluminationcon-ditions.Sinceourscanningappwasdesignedfornoviceusers,weoptedtoprovideareasonablesetofpresetsandal-lowformanualoverrideonlywhendeemednecessary.Bydefault,weenabledcontinuousautomaticwhitepointbal-ancingasimplementedbytheiOSSDK.WealsoenableddynamicexposureselectionagainasimplementedbytheiOSSDK,butinstructedusersthattheycouldmanuallyad-justexposureifnecessarytomakeoverlydarklocationsbrighter,oroverlybrightlocationsdarker.Theexposuresettingcanhaveasigniﬁcantimpactontheamountofmo-tionblurduringscanning.However,wefoundthatinex-perienceduserspreferredtorelyondynamicexposure,andtypicallymovedrelativelyslowlyduringscanning,makingmotionblurlessofanissue.Theaverageexposuretimeduringscanswithdynamicexposurewascloseto30ms.Figure13.Semanticvoxellabelingof3DscansinScanNetusingour3DCNNarchitecture.Voxelcolorsindicatepredictedorgroundtruthcategory.C.2.SensorCalibrationSensorcalibrationisacritical,yetoftenoverlookedpartofRGB-Ddataacquisition.Ourexperimentsshowedthatdepth-to-colorcalibrationisanimportantstepinacquir-inggood3DreconstructionsfromRGB-Dsequences(seeFig.15).DepthToColorCalibrationToalignadepthimageDtocolorimageC,weneedtoestimateintrinsicparametersofbothsensors,theinfraredcameraKDandcolorcam-eraKC,aswellasextrinsictransformationTD→C.Inourexperimentswehavefoundthatusingthesetofintrinsicpa-rametersoffocallength,centerofprojection,andtwobar-reldistortioncoefﬁcientsmodelsworkedwellfortheusedcameras.ToobtaincalibrationparametersKDandKCwecaptureaseriesofcolor-infraredpairsshowinganasym-metriccheckerboardgrid.Wethenestimatecalibrationpa-rametersforeachcamerawithMatlab’sCameraCalibratorapplication.Duringthisprocedureweadditionallyobtaintheworldpositionsofcalibrationgridcorners,andusethemtoestimatethetransformationTD→C.DepthDistortionCalibrationPreviousworksuggeststhatforconsumer-leveldepthcamerasthereexistsdepth-dependentdistortionthatincreasesascameramovesawayfromthesurface.Thus,wedecidedtoaugmentoursetofintrinsicparametersfordepthcameraswithaundistortionlookuptable,asﬁrstsuggestedinTeichmanetal.[84].Thislookuptableisafunctionf(x,y,d),ofspatialcoordinatesx,yandobserveddepthd,returningamultiplicationfac-tormusedtoobtainundistorteddepthd(cid:48)=md.Theta-bleiscomputedfromtrainingpairsofobservedandgroundtruthdepthsdanddt.However,unlikeTeichman’sunsuper-visedapproach,whichproducestrainingpairsusingcare-fullytaken’calibrationsequences’,wedecidedtodesignasupervisedapproachsimilartothatofDiCicco[14].How-ever,wefoundthatatlargedistancesthedepthdistortionbecomessoseverethatapproachesbasedonﬁttingplanestodepthdataareboundtofail.Thustoobtaintrainingpairs{d,dt},wecaptureacolor-depthvideosequenceofalargeﬂatwallwithacalibrationtargetatthecenter,astheusermovesawayandtowardsthewall.Toensuresuccessfullcalibrationprocessuserneedstoensurethattheviewedwallistheonlyobservedsurfaceandthatitcoverstheentireﬁeldofview.Withthecapturedcolor-depthsequenceandprevi-ouslyestimatedKD,KC,TD→Cwecanrecoverthetheworldpositionsofthecalibrationgridcorners,effectivelyobtainingthegroundtruthplanelocationsforeachofthecaptureddepthimages.Foreachpixelx,ywithdepthd,wethenshootaraythroughx,ytointersectwiththerelatedplane.dtcanberecoveredfromthepointofintersection.TherestofourundistortionpipelinefollowscloselythethatFigure15.Comparisonofcalibrationresults.Inthetoprow,weshowresultsofcalibrationonaﬂatwall.Asthedistanceincreasesthedistortionbecomesquitesevere,motivatingtheneedfordepthdistortioncalibration.Inthebottomrow,weshowresultsofframe-to-frametrackingonrawandcalibrateddata.ofTeichmanetal.[84].WefoundthatundistortingdepthimagesobtainedbyaStructuresensorleadstosigniﬁcantlyimprovedtracking.C.3.SurfaceReconstructionGivenacalibratedRGB-Dsequenceasinput,afused3DsurfacereconstructionisobtainedusingtheBundleFu-sionframework[12],asdescribedinthemainpaper.Thereconstructionisthencleanedbymergingverticeswithin1mmofeachother,andremovingconnectedcomponentswithfewerthan7500triangles.Followingthiscleanupstep,twoquadricedgecollapsedecimationstepsareperformedtoproducelowertrianglecountversionsofeachsurfacemesh.Eachdecimationhalvesthenumberoftrianglesinthesurfacemesh,reducingthesizeoftheoriginalmeshesfromanaverageof146MBto5.82MBforthelowres-olutionmesh.Themeshdecimationstepisimportantforreducingdatatransferrequirementsandimprovingloadingtimesduringthecrowdsourcedannotationusingourweb-basedUI.C.4.CrowdsourcedAnnotationUIWedeployedoursemanticannotationtasktocrowdworkersontheAmazonMechanicalTurkplatform.Eachannotationtaskbeganwithanintroduction(seeFig.16)providingabasicoverviewofthetask.Theworkerwasthenshownareconstructionandaskedtopaintallobjectinstanceswithacolorandcorrespondinglabel.Theworkerwasrequiredtoannotateatleast25%ofthesurfaceareaofthereconstruction,andencouragedtocoveratleast50%.Oncetheworkerwasdone,theycouldsubmitbypressingabutton.Workerswerecompensatedwith$0.50foreachannotationtaskperformed.TheCADmodelretrievalandalignmenttaskbeganwithaviewofanalreadysemanticallyannotatedreconstruc-tionandaskedworkerstoclickonobjectstoretrieveandplaceappropriateCADmodels.Fig.17showstheinitialin-structionsforanexamplereconstructionwithseveralchairs.Workersforthistaskwererequiredtoplaceatleastthreeobjectsbeforesubmitting.Oncetheworkerwasdone,theywerecompensatedwith$1.00foreachcompletedtask.C.5.LabelcleaningandpropagationLabelingisperformedonthesurfacemeshreconstruc-tion,withseveralworkerslabelingeachscan.Toensurethatlabelsareconsistentacrossworkers,weusestandardNLPtechniquestocleanupthelabels.First,weuseamanuallycuratedlistofgoodlabelsandtheirsynonymstocomputeamaptoasinglecanonicallabelforeachset,alsoincludingcommonmisspellingsbyasmalleditdistancethresholdofthegivenlabel.Labelswithlessthan5countsaredeemedunreliableandignoredinallstatistics.Labelswithmorethan20countsaremanuallyexaminedandaddedtothelistofgoodlabelsorcollapsedasasynonymofagoodlabel.ThelistofthesefrequentcollapsedlabelsisalsomappedtoWordNet[18]synsetswhenpossible,andtoothercommonlabelsetsthatarecommonlyusedforRGB-Dand3DCADdata(NYUv2[58],ModelNet[91],andShapeNetCore[6]).Usingthecleanedlabels,wethencomputeanaggre-gatedconsensuslabelingofeachscene,sinceanyindividualcrowdsourcedannotationofascenemaynotcovertheen-tirescene,ormaycontainsomeerrors.Foreachsegmentintheover-segmentationofascenemesh,weﬁrsttakethema-jorityvotelabel.Thisgroupstogetherinstancesofthesameclassofobjects,sowealsocomputealabelingpurelybasedongeometricoverlap;thatis,wegreedilytaketheunionsofannotationswhichhave≥50%overlapofsegments.Wethentakethemaximalintersectionsbetweenthesetwola-belingstoobtaintheﬁnalconsensus.Afterwehaveobtainedtheaggregatedconsensusseman-ticannotationforascene,wethenpropagatetheselabelstothehigh-resolutionmeshaswellastothe2DframesoftheinputRGB-Dsequence.Topropagatethelabelstothehighresolutionmesh,wecomputeakd-treeoverthemeshver-ticesofthelabeledcoarsemesh,andwelabeleachvertexofthehighresolutionmeshaccordingtoanearestneighborlookupinthekd-tree.Weprojectthe3Dsemanticannota-tionstotheinput2Dframesbyrenderingthelabeledmeshfromthecameraposesofeachframe,andfollowthiswithajointdilationﬁlterwiththeoriginalRGBimageandjointerosionﬁlterwiththeoriginalRGBimage.C.6.ManagementUIToenablescalabilityofourRGB-Dacquisitionandan-notation,andcontinualtransparencyintotheprogressofscansthroughoutourframework,wecreatedaweb-basedmanagementUItotrackandorganizealldata(seeFig.19).WhenauserisﬁnishedscanningandpressestheuploadbuttononaniPaddevice,theirscandataisautomaticallyFigure16.Instructionsprovidedtocrowdworkersforoursemanticannotationtask.Top:instructionsbeforethebeginningofthetask.Bottom:interfaceinstructionsduringannotation.uploadedtoourprocessingserver,placedintoareconstruc-tionqueue,andimmediatelymadevisibleinthemanage-mentUI.Asthereconstructionproceedsthroughthevar-Figure17.InstructionsprovidedtocrowdworkersforourCADmodelalignmenttask.TheworkerclicksoncoloredobjectstoretrieveandplaceCADmodels.Figure18.ShapeNetCore[6]CADmodelsretrievedandplacedonScanNetscansbycrowdworkers(scanmeshistransparentandCADmodelsareopaque).Fromtopleftclockwise:aclassroom,bedroom,bathroom,andloungescan.iousstagesofdataconversion,calibration,poseoptimiza-tionandRGB-Dfusion,alignment,cleanup,decimation,andsegmentation,progressisvisualizedinthemanage-mentUI.Thumbnailrenderingsofthegeneratedsurfacere-construction,andstatisticssuchastotalnumberofframes,reconstructedﬂoorareaetc.areautomaticallycomputedandcanbeusedforﬁlteringandsortingofthereconstruc-tions.Similarly,duringcrowdsourcedannotation,workerprogressandaggregatedannotatedsurfaceareastatisticsarevisibleandusableforsortingandﬁlteringofthescandatabase.Figure19.Ourweb-baseddatamanagementUIforScanNetscandata.

