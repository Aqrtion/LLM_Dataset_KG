Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, 18-21 October 2013. c(cid:13)2013 Association for Computational Linguistics

RecursiveDeepModelsforSemanticCompositionalityOveraSentimentTreebankRichardSocher,AlexPerelygin,JeanY.Wu,JasonChuang,ChristopherD.Manning,AndrewY.NgandChristopherPottsStanfordUniversity,Stanford,CA94305,USArichard@socher.org,{aperelyg,jcchuang,ang}@cs.stanford.edu{jeaneis,manning,cgpotts}@stanford.eduAbstractSemanticwordspaceshavebeenveryuse-fulbutcannotexpressthemeaningoflongerphrasesinaprincipledway.Furtherprogresstowardsunderstandingcompositionalityintaskssuchassentimentdetectionrequiresrichersupervisedtrainingandevaluationre-sourcesandmorepowerfulmodelsofcom-position.Toremedythis,weintroduceaSentimentTreebank.Itincludesﬁnegrainedsentimentlabelsfor215,154phrasesintheparsetreesof11,855sentencesandpresentsnewchallengesforsentimentcomposition-ality.Toaddressthem,weintroducetheRecursiveNeuralTensorNetwork.Whentrainedonthenewtreebank,thismodelout-performsallpreviousmethodsonseveralmet-rics.Itpushesthestateoftheartinsinglesentencepositive/negativeclassiﬁcationfrom80%upto85.4%.Theaccuracyofpredictingﬁne-grainedsentimentlabelsforallphrasesreaches80.7%,animprovementof9.7%overbagoffeaturesbaselines.Lastly,itistheonlymodelthatcanaccuratelycapturetheeffectsofnegationanditsscopeatvarioustreelevelsforbothpositiveandnegativephrases.1IntroductionSemanticvectorspacesforsinglewordshavebeenwidelyusedasfeatures(TurneyandPantel,2010).Becausetheycannotcapturethemeaningoflongerphrasesproperly,compositionalityinsemanticvec-torspaceshasrecentlyreceivedalotofattention(MitchellandLapata,2010;Socheretal.,2010;Zanzottoetal.,2010;YessenalinaandCardie,2011;Socheretal.,2012;Grefenstetteetal.,2013).How-ever,progressisheldbackbythecurrentlackoflargeandlabeledcompositionalityresourcesand–00This0ﬁlm–––0does0n’t0+care+0about+++++cleverness0,0wit0or+00any00other+kind+0of++intelligent++humor0.Figure1:ExampleoftheRecursiveNeuralTensorNet-workaccuratelypredicting5sentimentclasses,veryneg-ativetoverypositive(––,–,0,+,++),ateverynodeofaparsetreeandcapturingthenegationanditsscopeinthissentence.modelstoaccuratelycapturetheunderlyingphe-nomenapresentedinsuchdata.Toaddressthisneed,weintroducetheStanfordSentimentTreebankandapowerfulRecursiveNeuralTensorNetworkthatcanaccuratelypredictthecompositionalsemanticeffectspresentinthisnewcorpus.TheStanfordSentimentTreebankistheﬁrstcor-puswithfullylabeledparsetreesthatallowsforacompleteanalysisofthecompositionaleffectsofsentimentinlanguage.ThecorpusisbasedonthedatasetintroducedbyPangandLee(2005)andconsistsof11,855singlesentencesextractedfrommoviereviews.ItwasparsedwiththeStanfordparser(KleinandManning,2003)andincludesatotalof215,154uniquephrasesfromthoseparsetrees,eachannotatedby3humanjudges.Thisnewdatasetallowsustoanalyzetheintricaciesofsenti-mentandtocapturecomplexlinguisticphenomena.Fig.1showsoneofthemanyexampleswithclearcompositionalstructure.Thegranularityandsizeof1632

thisdatasetwillenablethecommunitytotraincom-positionalmodelsthatarebasedonsupervisedandstructuredmachinelearningtechniques.Whilethereareseveraldatasetswithdocumentandchunklabelsavailable,thereisaneedtobettercapturesentimentfromshortcomments,suchasTwitterdata,whichprovidelessoverallsignalperdocument.Inordertocapturethecompositionaleffectswithhigheraccuracy,weproposeanewmodelcalledtheRecursiveNeuralTensorNetwork(RNTN).Recur-siveNeuralTensorNetworkstakeasinputphrasesofanylength.Theyrepresentaphrasethroughwordvectorsandaparsetreeandthencomputevectorsforhighernodesinthetreeusingthesametensor-basedcompositionfunction.Wecomparetoseveralsuper-vised,compositionalmodelssuchasstandardrecur-siveneuralnetworks(RNN)(Socheretal.,2011b),matrix-vectorRNNs(Socheretal.,2012),andbase-linessuchasneuralnetworksthatignorewordorder,NaiveBayes(NB),bi-gramNBandSVM.Allmod-elsgetasigniﬁcantboostwhentrainedwiththenewdatasetbuttheRNTNobtainsthehighestperfor-mancewith80.7%accuracywhenpredictingﬁne-grainedsentimentforallnodes.Lastly,weuseatestsetofpositiveandnegativesentencesandtheirre-spectivenegationstoshowthat,unlikebagofwordsmodels,theRNTNaccuratelycapturesthesentimentchangeandscopeofnegation.RNTNsalsolearnthatsentimentofphrasesfollowingthecontrastiveconjunction‘but’dominates.Thecompletetrainingandtestingcode,alivedemoandtheStanfordSentimentTreebankdatasetareavailableathttp://nlp.stanford.edu/sentiment.2RelatedWorkThisworkisconnectedtoﬁvedifferentareasofNLPresearch,eachwiththeirownlargeamountofrelatedworktowhichwecannotdofulljusticegivenspaceconstraints.SemanticVectorSpaces.Thedominantap-proachinsemanticvectorspacesusesdistributionalsimilaritiesofsinglewords.Often,co-occurrencestatisticsofawordanditscontextareusedtode-scribeeachword(TurneyandPantel,2010;BaroniandLenci,2010),suchastf-idf.Variantsofthisideausemorecomplexfrequenciessuchashowoftenawordappearsinacertainsyntacticcontext(PadoandLapata,2007;ErkandPad´o,2008).However,distributionalvectorsoftendonotproperlycapturethedifferencesinantonymssincethoseoftenhavesimilarcontexts.Onepossibilitytoremedythisistouseneuralwordvectors(Bengioetal.,2003).Thesevectorscanbetrainedinanunsupervisedfashiontocapturedistributionalsimilarities(CollobertandWeston,2008;Huangetal.,2012)butthenalsobeﬁne-tunedandtrainedtospeciﬁctaskssuchassen-timentdetection(Socheretal.,2011b).Themodelsinthispapercanusepurelysupervisedwordrepre-sentationslearnedentirelyonthenewcorpus.CompositionalityinVectorSpaces.Mostofthecompositionalityalgorithmsandrelateddatasetscapturetwowordcompositions.MitchellandLa-pata(2010)usee.g.two-wordphrasesandanalyzesimilaritiescomputedbyvectoraddition,multiplica-tionandothers.Somerelatedmodelssuchasholo-graphicreducedrepresentations(Plate,1995),quan-tumlogic(Widdows,2008),discrete-continuousmodels(ClarkandPulman,2007)andtherecentcompositionalmatrixspacemodel(RudolphandGiesbrecht,2010)havenotbeenexperimentallyval-idatedonlargercorpora.YessenalinaandCardie(2011)computematrixrepresentationsforlongerphrasesanddeﬁnecompositionasmatrixmultipli-cation,andalsoevaluateonsentiment.Grefen-stetteandSadrzadeh(2011)analyzesubject-verb-objecttripletsandﬁndamatrix-basedcategoricalmodeltocorrelatewellwithhumanjudgments.Wecomparetotherecentlineofworkonsupervisedcompositionalmodels.Inparticularwewillde-scribeandexperimentallycompareournewRNTNmodeltorecursiveneuralnetworks(RNN)(Socheretal.,2011b)andmatrix-vectorRNNs(Socheretal.,2012)bothofwhichhavebeenappliedtobagofwordssentimentcorpora.LogicalForm.Arelatedﬁeldthattacklescom-positionalityfromaverydifferentangleisthatoftryingtomapsentencestologicalform(ZettlemoyerandCollins,2005).Whilethesemodelsarehighlyinterestingandworkwellincloseddomainsandondiscretesets,theycouldonlycapturesentimentdistributionsusingseparatemechanismsbeyondthecurrentlyusedlogicalforms.DeepLearning.Apartfromtheabovementioned1633

workonRNNs,severalcompositionalityideasre-latedtoneuralnetworkshavebeendiscussedbyBot-tou(2011)andHinton(1990)andﬁrstmodelssuchasRecursiveAuto-associativememoriesbeenexper-imentedwithbyPollack(1990).Theideatorelateinputsthroughthreewayinteractions,parameterizedbyatensorhavebeenproposedforrelationclassiﬁ-cation(Sutskeveretal.,2009;Jenattonetal.,2012),extendingRestrictedBoltzmannmachines(RanzatoandHinton,2010)andasaspeciallayerforspeechrecognition(Yuetal.,2012).SentimentAnalysis.Apartfromtheabove-mentionedwork,mostapproachesinsentimentanal-ysisusebagofwordsrepresentations(PangandLee,2008).SnyderandBarzilay(2007)analyzedlargerreviewsinmoredetailbyanalyzingthesentimentofmultipleaspectsofrestaurants,suchasfoodoratmosphere.Severalworkshaveexploredsentimentcompositionalitythroughcarefulengineeringoffea-turesorpolarityshiftingrulesonsyntacticstructures(PolanyiandZaenen,2006;MoilanenandPulman,2007;Rentoumietal.,2010;Nakagawaetal.,2010).3StanfordSentimentTreebankBagofwordsclassiﬁerscanworkwellinlongerdocumentsbyrelyingonafewwordswithstrongsentimentlike‘awesome’or‘exhilarating.’How-ever,sentimentaccuraciesevenforbinaryposi-tive/negativeclassiﬁcationforsinglesentenceshasnotexceeded80%forseveralyears.Forthemoredifﬁcultmulticlasscaseincludinganeutralclass,accuracyisoftenbelow60%forshortmessagesonTwitter(Wangetal.,2012).Fromalinguisticorcognitivestandpoint,ignoringwordorderinthetreatmentofasemantictaskisnotplausible,and,aswewillshow,itcannotaccuratelyclassifyhardex-amplesofnegation.Correctlypredictingthesehardcasesisnecessarytofurtherimproveperformance.InthissectionwewillintroduceandprovidesomeanalysesforthenewSentimentTreebankwhichin-cludeslabelsforeverysyntacticallyplausiblephraseinthousandsofsentences,allowingustotrainandevaluatecompositionalmodels.Weconsiderthecorpusofmoviereviewexcerptsfromtherottentomatoes.comwebsiteorig-inallycollectedandpublishedbyPangandLee(2005).Theoriginaldatasetincludes10,662sen-nerdy  folks|Verynegative|Negative|Somewhatnegative|Neutral|Somewhatpositive|Positive|Verypositivephenomenal  fantasy  best  sellers|Verynegative|Negative|Somewhatnegative|Neutral|Somewhatpositive|Positive|Verypositive    Figure3:Thelabelinginterface.Randomphraseswereshownandannotatorshadasliderforselectingthesenti-mentanditsdegree.tences,halfofwhichwereconsideredpositiveandtheotherhalfnegative.Eachlabelisextractedfromalongermoviereviewandreﬂectsthewriter’sover-allintentionforthisreview.Thenormalized,lower-casedtextisﬁrstusedtorecover,fromtheorigi-nalwebsite,thetextwithcapitalization.RemainingHTMLtagsandsentencesthatarenotinEnglisharedeleted.TheStanfordParser(KleinandMan-ning,2003)isusedtoparsesall10,662sentences.Inapproximately1,100casesitsplitsthesnippetintomultiplesentences.WethenusedAmazonMe-chanicalTurktolabeltheresulting215,154phrases.Fig.3showstheinterfaceannotatorssaw.Thesliderhas25differentvaluesandisinitiallysettoneutral.Thephrasesineachhitarerandomlysampledfromthesetofallphrasesinordertopreventlabelsbeinginﬂuencedbywhatfollows.Formoredetailsonthedatasetcollection,seesupplementarymaterial.Fig.2showsthenormalizedlabeldistributionsateachn-gramlength.Startingatlength20,thema-jorityarefullsentences.Oneoftheﬁndingsfromlabelingsentencesbasedonreader’sperceptionisthatmanyofthemcouldbeconsideredneutral.Wealsonoticethatstrongersentimentoftenbuildsupinlongerphrasesandthemajorityoftheshorterphrasesareneutral.Anotherobservationisthatmostannotatorsmovedtheslidertooneoftheﬁvepo-sitions:negative,somewhatnegative,neutral,posi-tiveorsomewhatpositive.Theextremevalueswererarelyusedandthesliderwasnotoftenleftinbe-tweentheticks.Hence,evena5-classclassiﬁcationintothesecategoriescapturesthemainvariabilityofthelabels.Wewillnamethisﬁne-grainedsenti-mentclassiﬁcationandourmainexperimentwillbetorecovertheseﬁvelabelsforphrasesofalllengths.1634

51015202530354045N-Gram Length0%20%40%60%80%100%% of Sentiment ValuesNeutralSomewhat PositivePositiveVery PositiveSomewhat NegativeNegativeVery Negative(a)(a)(b)(b)(c)(c)(d)(d)Distributions of sentiment values for (a) unigrams, (b) 10-grams, (c) 20-grams, and (d) full sentences.Figure2:Normalizedhistogramofsentimentannotationsateachn-gramlength.Manyshortern-gramsareneutral;longerphrasesarewelldistributed.Fewannotatorsusedsliderpositionsbetweenticksortheextremevalues.Hencethetwostrongestlabelsandintermediatetickpositionsaremergedinto5classes.4RecursiveNeuralModelsThemodelsinthissectioncomputecompositionalvectorrepresentationsforphrasesofvariablelengthandsyntactictype.Theserepresentationswillthenbeusedasfeaturestoclassifyeachphrase.Fig.4displaysthisapproach.Whenann-gramisgiventothecompositionalmodels,itisparsedintoabinarytreeandeachleafnode,correspondingtoaword,isrepresentedasavector.Recursiveneuralmod-elswillthencomputeparentvectorsinabottomupfashionusingdifferenttypesofcompositional-ityfunctionsg.Theparentvectorsareagaingivenasfeaturestoaclassiﬁer.Foreaseofexposition,wewillusethetri-graminthisﬁguretoexplainallmodels.Weﬁrstdescribetheoperationsthatthebelowre-cursiveneuralmodelshaveincommon:wordvectorrepresentationsandclassiﬁcation.ThisisfollowedbydescriptionsoftwopreviousRNNmodelsandourRNTN.Eachwordisrepresentedasad-dimensionalvec-tor.Weinitializeallwordvectorsbyrandomlysamplingeachvaluefromauniformdistribution:U(−r,r),wherer=0.0001.Allthewordvec-torsarestackedinthewordembeddingmatrixL∈Rd×|V|,where|V|isthesizeofthevocabulary.Ini-tiallythewordvectorswillberandombuttheLma-trixisseenasaparameterthatistrainedjointlywiththecompositionalitymodels.Wecanusethewordvectorsimmediatelyasparameterstooptimizeandasfeatureinputstoasoftmaxclassiﬁer.Forclassiﬁcationintoﬁveclasses,wecomputetheposteriorprobabilityover    not      very       good ...        a          b             c p1 =g(b,c)p2 = g(a,p1)00+++-Figure4:ApproachofRecursiveNeuralNetworkmod-elsforsentiment:Computeparentvectorsinabottomupfashionusingacompositionalityfunctiongandusenodevectorsasfeaturesforaclassiﬁeratthatnode.Thisfunc-tionvariesforthedifferentmodels.labelsgiventhewordvectorvia:ya=softmax(Wsa),(1)whereWs∈R5×disthesentimentclassiﬁcationmatrix.Forthegiventri-gram,thisisrepeatedforvectorsbandc.Themaintaskofanddifferencebetweenthemodelswillbetocomputethehiddenvectorspi∈Rdinabottomupfashion.4.1RNN:RecursiveNeuralNetworkThesimplestmemberofthisfamilyofneuralnet-workmodelsisthestandardrecursiveneuralnet-work(GollerandK¨uchler,1996;Socheretal.,2011a).First,itisdeterminedwhichparentalreadyhasallitschildrencomputed.Intheabovetreeex-ample,p1hasitstwochildren’svectorssincebotharewords.RNNsusethefollowingequationstocomputetheparentvectors:1635

p1=f(cid:18)W(cid:20)bc(cid:21)(cid:19),p2=f(cid:18)W(cid:20)ap1(cid:21)(cid:19),wheref=tanhisastandardelement-wisenonlin-earity,W∈Rd×2disthemainparametertolearnandweomitthebiasforsimplicity.ThebiascanbeaddedasanextracolumntoWifanadditional1isaddedtotheconcatenationoftheinputvectors.Theparentvectorsmustbeofthesamedimensionalitytoberecursivelycompatibleandbeusedasinputtothenextcomposition.Eachparentvectorpi,isgiventothesamesoftmaxclassiﬁerofEq.1tocomputeitslabelprobabilities.Thismodelusesthesamecompositionalityfunc-tionastherecursiveautoencoder(Socheretal.,2011b)andrecursiveauto-associatememories(Pol-lack,1990).Theonlydifferencetotheformermodelisthatweﬁxthetreestructuresandignorethere-constructionloss.Ininitialexperiments,wefoundthatwiththeadditionalamountoftrainingdata,thereconstructionlossateachnodeisnotnecessarytoobtainhighperformance.4.2MV-RNN:Matrix-VectorRNNTheMV-RNNislinguisticallymotivatedinthatmostoftheparametersareassociatedwithwordsandeachcompositionfunctionthatcomputesvec-torsforlongerphrasesdependsontheactualwordsbeingcombined.ThemainideaoftheMV-RNN(Socheretal.,2012)istorepresenteverywordandlongerphraseinaparsetreeasbothavectorandamatrix.Whentwoconstituentsarecombinedthematrixofoneismultipliedwiththevectoroftheotherandviceversa.Hence,thecompositionalfunc-tionisparameterizedbythewordsthatparticipateinit.Eachword’smatrixisinitializedasad×didentitymatrix,plusasmallamountofGaussiannoise.Sim-ilartotherandomwordvectors,theparametersofthesematriceswillbetrainedtominimizetheclas-siﬁcationerrorateachnode.Forthismodel,eachn-gramisrepresentedasalistof(vector,matrix)pairs,togetherwiththeparsetree.Forthetreewith(vec-tor,matrix)nodes:(p2,P2)(a,A)(p1,P1)(b,B)(c,C)theMV-RNNcomputestheﬁrstparentvectoranditsmatrixviatwoequations:p1=f(cid:18)W(cid:20)CbBc(cid:21)(cid:19),P1=f(cid:18)WM(cid:20)BC(cid:21)(cid:19),whereWM∈Rd×2dandtheresultisagainad×dmatrix.Similarly,thesecondparentnodeiscom-putedusingthepreviouslycomputed(vector,matrix)pair(p1,P1)aswellas(a,A).ThevectorsareusedforclassifyingeachphraseusingthesamesoftmaxclassiﬁerasinEq.1.4.3RNTN:RecursiveNeuralTensorNetworkOneproblemwiththeMV-RNNisthatthenumberofparametersbecomesverylargeanddependsonthesizeofthevocabulary.Itwouldbecognitivelymoreplausibleiftherewasasinglepowerfulcom-positionfunctionwithaﬁxednumberofparameters.ThestandardRNNisagoodcandidateforsuchafunction.However,inthestandardRNN,theinputvectorsonlyimplicitlyinteractthroughthenonlin-earity(squashing)function.Amoredirect,possiblymultiplicative,interactionwouldallowthemodeltohavegreaterinteractionsbetweentheinputvectors.Motivatedbytheseideasweaskthequestion:Canasingle,morepowerfulcompositionfunctionper-formbetterandcomposeaggregatemeaningfromsmallerconstituentsmoreaccuratelythanmanyin-putspeciﬁcones?Inordertoanswerthisquestion,weproposeanewmodelcalledtheRecursiveNeu-ralTensorNetwork(RNTN).Themainideaistousethesame,tensor-basedcompositionfunctionforallnodes.Fig.5showsasingletensorlayer.Wedeﬁnetheoutputofatensorproducth∈Rdviathefollow-ingvectorizednotationandtheequivalentbutmoredetailednotationforeachsliceV[i]∈Rd×d:h=(cid:20)bc(cid:21)TV[1:d](cid:20)bc(cid:21);hi=(cid:20)bc(cid:21)TV[i](cid:20)bc(cid:21).whereV[1:d]∈R2d×2d×disthetensorthatdeﬁnesmultiplebilinearforms.1636

Slices of       Standard                   Tensor Layer         Layerp = f             V[1:2]        +   WNeural Tensor LayerbcbcbcTp = f                             +          Figure5:AsinglelayeroftheRecursiveNeuralTen-sorNetwork.Eachdashedboxrepresentsoneofd-manyslicesandcancaptureatypeofinﬂuenceachildcanhaveonitsparent.TheRNTNusesthisdeﬁnitionforcomputingp1:p1=f (cid:20)bc(cid:21)TV[1:d](cid:20)bc(cid:21)+W(cid:20)bc(cid:21)!,whereWisasdeﬁnedinthepreviousmodels.Thenextparentvectorp2inthetri-gramwillbecom-putedwiththesameweights:p2=f (cid:20)ap1(cid:21)TV[1:d](cid:20)ap1(cid:21)+W(cid:20)ap1(cid:21)!.ThemainadvantageoverthepreviousRNNmodel,whichisaspecialcaseoftheRNTNwhenVissetto0,isthatthetensorcandirectlyrelatein-putvectors.Intuitively,wecaninterpreteachsliceofthetensorascapturingaspeciﬁctypeofcompo-sition.AnalternativetoRNTNswouldbetomakethecompositionalfunctionmorepowerfulbyaddingasecondneuralnetworklayer.However,initialexper-imentsshowedthatitishardtooptimizethismodelandvectorinteractionsarestillmoreimplicitthanintheRNTN.4.4TensorBackpropthroughStructureWedescribeinthissectionhowtotraintheRNTNmodel.Asmentionedabove,eachnodehasasoftmaxclassiﬁertrainedonitsvectorrepresenta-tiontopredictagivengroundtruthortargetvectort.Weassumethetargetdistributionvectorateachnodehasa0-1encoding.IfthereareCclasses,thenithaslengthCanda1atthecorrectlabel.Allotherentriesare0.Wewanttomaximizetheprobabilityofthecor-rectprediction,orminimizethecross-entropyerrorbetweenthepredicteddistributionyi∈RC×1atnodeiandthetargetdistributionti∈RC×1atthatnode.Thisisequivalent(uptoaconstant)tomini-mizingtheKL-divergencebetweenthetwodistribu-tions.TheerrorasafunctionoftheRNTNparame-tersθ=(V,W,Ws,L)forasentenceis:E(θ)=XiXjtijlogyij+λkθk2(2)Thederivativefortheweightsofthesoftmaxclas-siﬁerarestandardandsimplysumupfromeachnode’serror.Wedeﬁnexitobethevectoratnodei(intheexampletrigram,thexi∈Rd×1’sare(a,b,c,p1,p2)).WeskipthestandardderivativeforWs.EachnodebackpropagatesitserrorthroughtotherecursivelyusedweightsV,W.Letδi,s∈Rd×1bethesoftmaxerrorvectoratnodei:δi,s=(cid:0)WTs(yi−ti)(cid:1)⊗f0(xi),where⊗istheHadamardproductbetweenthetwovectorsandf0istheelement-wisederivativeoffwhichinthestandardcaseofusingf=tanhcanbecomputedusingonlyf(xi).Theremainingderivativescanonlybecomputedinatop-downfashionfromthetopnodethroughthetreeandintotheleafnodes.ThefullderivativeforVandWisthesumofthederivativesateachofthenodes.Wedeﬁnethecompleteincomingerrormessagesforanodeiasδi,com.Thetopnode,inourcasep2,onlyreceivederrorsfromthetopnode’ssoftmax.Hence,δp2,com=δp2,swhichwecanusetoobtainthestandardbackpropderivativeforW(GollerandK¨uchler,1996;Socheretal.,2010).Forthederivativeofeachslicek=1,...,d,weget:∂Ep2∂V[k]=δp2,comk(cid:20)ap1(cid:21)(cid:20)ap1(cid:21)T,whereδp2,comkisjustthek’thelementofthisvector.Now,wecancomputetheerrormessageforthetwo1637

childrenofp2:δp2,down=(cid:18)WTδp2,com+S(cid:19)⊗f0(cid:18)(cid:20)ap1(cid:21)(cid:19),wherewedeﬁneS=dXk=1δp2,comk(cid:18)V[k]+(cid:16)V[k](cid:17)T(cid:19)(cid:20)ap1(cid:21)Thechildrenofp2,willtheneachtakehalfofthisvectorandaddtheirownsoftmaxerrormessageforthecompleteδ.Inparticular,wehaveδp1,com=δp1,s+δp2,down[d+1:2d],whereδp2,down[d+1:2d]indicatesthatp1istherightchildofp2andhencetakesthe2ndhalfoftheerror,fortheﬁnalwordvectorderivativefora,itwillbeδp2,down[1:d].ThefullderivativeforsliceV[k]forthistrigramtreethenisthesumateachnode:∂E∂V[k]=Ep2∂V[k]+δp1,comk(cid:20)bc(cid:21)(cid:20)bc(cid:21)T,andsimilarlyforW.Forthisnonconvexoptimiza-tionweuseAdaGrad(Duchietal.,2011)whichcon-vergesinlessthan3hourstoalocaloptimum.5ExperimentsWeincludetwotypesofanalyses.Theﬁrsttypein-cludesseverallargequantitativeevaluationsonthetestset.Thesecondtypefocusesontwolinguisticphenomenathatareimportantinsentiment.Forallmodels,weusethedevsetandcross-validateoverregularizationoftheweights,wordvectorsizeaswellaslearningrateandminibatchsizeforAdaGrad.Optimalperformanceforallmod-elswasachievedatwordvectorsizesbetween25and35dimensionsandbatchsizesbetween20and30.Performancedecreasedatlargerorsmallervec-torandbatchsizes.ThisindicatesthattheRNTNdoesnotoutperformthestandardRNNduetosim-plyhavingmoreparameters.TheMV-RNNhasor-dersofmagnitudesmoreparametersthananyothermodelduetothewordmatrices.TheRNTNwouldusuallyachieveitsbestperformanceonthedevsetaftertrainingfor3-5hours.InitialexperimentsModelFine-grainedPositive/NegativeAllRootAllRootNB67.241.082.681.8SVM64.340.784.679.4BiNB71.041.982.783.1VecAvg73.332.785.180.1RNN79.043.286.182.4MV-RNN78.744.486.882.9RNTN80.745.787.685.4Table1:Accuracyforﬁnegrained(5-class)andbinarypredictionsatthesentencelevel(root)andforallnodes.showedthattherecursivemodelsworkedsigniﬁ-cantlyworse(over5%dropinaccuracy)whennononlinearitywasused.Weusef=tanhinallex-periments.WecomparetocommonlyusedmethodsthatusebagofwordsfeatureswithNaiveBayesandSVMs,aswellasNaiveBayeswithbagofbigramfeatures.WeabbreviatethesewithNB,SVMandbiNB.Wealsocomparetoamodelthataveragesneuralwordvectorsandignoreswordorder(VecAvg).Thesentencesinthetreebankweresplitintoatrain(8544),dev(1101)andtestsplits(2210)andthesesplitsaremadeavailablewiththedatarelease.Wealsoanalyzeperformanceononlypositiveandnegativesentences,ignoringtheneutralclass.Thisﬁltersabout20%ofthedatawiththethreesetshav-ing6920/872/1821sentences.5.1Fine-grainedSentimentForAllPhrasesThemainnovelexperimentandevaluationmetricanalyzetheaccuracyofﬁne-grainedsentimentclas-siﬁcationforallphrases.Fig.2showedthataﬁnegrainedclassiﬁcationinto5classesisareasonableapproximationtocapturemostofthedatavariation.Fig.6showstheresultonthisnewcorpus.TheRNTNgetsthehighestperformance,followedbytheMV-RNNandRNN.Therecursivemodelsworkverywellonshorterphrases,wherenegationandcompositionareimportant,whilebagoffeaturesbaselinesperformwellonlywithlongersentences.TheRNTNaccuracyupperboundsothermodelsatmostn-gramlengths.Table1(left)showstheoverallaccuracynumbersforﬁnegrainedpredictionatallphraselengthsandfullsentences.1638

510152025N­Gram Length0.20.40.60.81.0Accuracy510152025N­Gram Length0.60.70.80.91.0Cumulative AccuracyModelRNTNMV­RNNRNNbiNBNBFigure6:Accuracycurvesforﬁnegrainedsentimentclassiﬁcationateachn-gramlengths.Left:Accuracyseparatelyforeachsetofn-grams.Right:Cumulativeaccuracyofall≤n-grams.5.2FullSentenceBinarySentimentThissetupiscomparabletopreviousworkontheoriginalrottentomatoesdatasetwhichonlyusedfullsentencelabelsandbinaryclassiﬁcationofpos-itive/negative.Hence,theseexperimentsshowtheimprovementevenbaselinemethodscanachievewiththesentimenttreebank.Table1showsresultsofthisbinaryclassiﬁcationforbothallphrasesandforonlyfullsentences.Thepreviousstateoftheartwasbelow80%(Socheretal.,2012).Withthecoarsebagofwordsannotationfortraining,manyofthemorecomplexphenomenacouldnotbecaptured,evenbymorepowerfulmodels.ThecombinationofthenewsentimenttreebankandtheRNTNpushesthestateoftheartonshortphrasesupto85.4%.5.3ModelAnalysis:ContrastiveConjunctionInthissection,weuseasubsetofthetestsetwhichincludesonlysentenceswithan‘XbutY’structure:AphraseXbeingfollowedbybutwhichisfollowedbyaphraseY.Theconjunctionisinterpretedasanargumentforthesecondconjunct,withtheﬁrstfunctioningconcessively(Lakoff,1971;Blakemore,1989;Merin,1999).Fig.7containsanexample.Weanalyzeastrictsetting,whereXandYarephrasesofdifferentsentiment(includingneutral).Theex-ampleiscountedascorrect,iftheclassiﬁcationsforbothphrasesXandYarecorrect.Furthermore,thelowestnodethatdominatesbothofthewordbutandthenodethatspansYalsohavetohavethesamecorrectsentiment.Fortheresulting131cases,theRNTNobtainsanaccuracyof41%comparedtoMV-RNN(37),RNN(36)andbiNB(27).5.4ModelAnalysis:HighLevelNegationWeinvestigatetwotypesofnegation.Foreachtype,weuseaseparatedatasetforevaluation.++–––0There–0are––0–slow0and–repetitive0parts0,0but+0it+00has00just0enough++spice+0to+0keep+0it+interesting0.Figure7:ExampleofcorrectpredictionforcontrastiveconjunctionXbutY.Set1:NegatingPositiveSentences.Theﬁrstsetcontainspositivesentencesandtheirnegation.Inthisset,thenegationchangestheoverallsentimentofasentencefrompositivetonegative.Hence,wecomputeaccuracyintermsofcorrectsentimentre-versalfrompositivetonegative.Fig.9showstwoexamplesofpositivenegationtheRNTNcorrectlyclassiﬁed,evenifnegationislessobviousinthecaseof‘least’.Table2(left)givestheaccuraciesover21positivesentencesandtheirnegationforallmodels.TheRNTNhasthehighestreversalaccuracy,show-ingitsabilitytostructurallylearnnegationofposi-tivesentences.Butwhatifthemodelsimplymakesphrasesverynegativewhennegationisinthesen-tence?Thenextexperimentsshowthatthemodelcapturesmorethansuchasimplisticnegationrule.Set2:NegatingNegativeSentences.Thesec-ondsetcontainsnegativesentencesandtheirnega-tion.Whennegativesentencesarenegated,thesen-timenttreebankshowsthatoverallsentimentshouldbecomelessnegative,butnotnecessarilypositive.Forinstance,‘Themoviewasterrible’isnegativebutthe‘Themoviewasnotterrible’saysonlythatitwaslessbadthanaterribleone,notthatitwasgood(Horn,1989;Israel,2001).Hence,weevaluateac-1639

++00Roger0Dodger++0is+0one+0of++0the++0most+compelling0variations00on00this0theme0.–00Roger0Dodger––0is–0one–0of––0the–––least+compelling0variations00on00this0theme0.+0I+++liked000every00single0minute00of00this0ﬁlm0.–0I––00did0n’t00like000a00single0minute00of00this0ﬁlm0.–0It––00’s0just–+incredibly––dull0.00It00000’s+deﬁnitely–not––dull0.Figure9:RNTNpredictionofpositiveandnegative(bottomright)sentencesandtheirnegation.ModelAccuracyNegatedPositiveNegatedNegativebiNB19.027.3RNN33.345.5MV-RNN52.454.6RNTN71.481.8Table2:Accuracyofnegationdetection.Negatedposi-tiveismeasuredascorrectsentimentinversions.Negatednegativeismeasuredasincreasesinpositiveactivations.curacyintermsofhowofteneachmodelwasabletoincreasenon-negativeactivationinthesentimentofthesentence.Table2(right)showstheaccuracy.Inover81%ofcases,theRNTNcorrectlyincreasesthepositiveactivations.Fig.9(bottomright)showsatypicalcaseinwhichsentimentwasmademorepositivebyswitchingthemainclassfromnegativetoneutraleventhoughbothnotanddullwerenega-tive.Fig.8showsthechangesinactivationforbothsets.Negativevaluesindicateadecreaseinaver-­0.6­0.4­0.20.00.20.4biNBRRNMV­RNNRNTN­0.57­0.34­0.16­0.5                    Negated Positive Sentences: Change in Activation­0.6­0.4­0.20.00.20.4biNBRRNMV­RNNRNTN+0.35+0.01­0.01­0.01                    Negated Negative Sentences: Change in ActivationFigure8:Changeinactivationsfornegations.OnlytheRNTNcorrectlycapturesbothtypes.Itdecreasespositivesentimentmorewhenitisnegatedandlearnsthatnegat-ingnegativephrases(suchasnotterrible)shouldincreaseneutralandpositiveactivations.agepositiveactivation(forset1)andpositivevaluesmeananincreaseinaveragepositiveactivation(set2).TheRNTNhasthelargestshiftsinthecorrectdi-rections.ThereforewecanconcludethattheRNTNisbestabletoidentifytheeffectofnegationsuponbothpositiveandnegativesentimentsentences.1640

nMostpositiven-gramsMostnegativen-grams1engaging;best;powerful;love;beautifulbad;dull;boring;fails;worst;stupid;painfully2excellentperformances;Amasterpiece;masterfulﬁlm;wonderfulmovie;marvelousperformancesworstmovie;verybad;shapelessmess;worstthing;instantlyforgettable;completefailure3anamazingperformance;wonderfulall-agestri-umph;awonderfulmovie;mostvisuallystunningforworstmovie;Alousymovie;acompletefail-ure;mostpainfullymarginal;verybadsign5nicelyactedandbeautifullyshot;gorgeousim-agery,effectiveperformances;thebestoftheyear;aterriﬁcAmericansportsmovie;refresh-inglyhonestandultimatelytouchingsilliestandmostincoherentmovie;completelycrassandforgettablemovie;justanotherbadmovie.Acumbersomeandcliche-riddenmovie;ahumorless,disjointedmess8oneofthebestﬁlmsoftheyear;Aloveforﬁlmsshinesthrougheachframe;createdamasterfulpieceofartistryrighthere;Amasterfulﬁlmfromamasterﬁlmmaker,Atrashy,exploitative,thoroughlyunpleasantex-perience;thissloppydramaisanemptyves-sel.;quicklydragsonbecomingboringandpre-dictable.;betheworstspecial-effectscreationoftheyearTable3:Examplesofn-gramsforwhichtheRNTNpredictedthemostpositiveandmostnegativeresponses.12345678910N­Gram Length0.70.80.91.0Average Ground Truth SentimentModelRNTNMV­RNNRNNFigure10:Averagegroundtruthsentimentoftop10mostpositiven-gramsatvariousn.TheRNTNcorrectlypicksthemorenegativeandpositiveexamples.5.5ModelAnalysis:MostPositiveandNegativePhrasesWequeriedthemodelforitspredictionsonwhatthemostpositiveornegativen-gramsare,measuredasthehighestactivationofthemostnegativeandmostpositiveclasses.Table3showssomephrasesfromthedevsetwhichtheRNTNselectedfortheirstrongestsentiment.DuetolackofspacewecannotcomparetopphrasesoftheothermodelsbutFig.10showsthattheRNTNselectsmorestronglypositivephrasesatmostn-gramlengthscomparedtoothermodels.Forthisandthepreviousexperiment,pleaseﬁndadditionalexamplesanddescriptionsinthesupple-mentarymaterial.6ConclusionWeintroducedRecursiveNeuralTensorNetworksandtheStanfordSentimentTreebank.Thecombi-nationofnewmodelanddataresultsinasystemforsinglesentencesentimentdetectionthatpushesstateoftheartby5.4%forpositive/negativesen-tenceclassiﬁcation.Apartfromthisstandardset-ting,thedatasetalsoposesimportantnewchallengesandallowsfornewevaluationmetrics.Forinstance,theRNTNobtains80.7%accuracyonﬁne-grainedsentimentpredictionacrossallphrasesandcapturesnegationofdifferentsentimentsandscopemoreac-curatelythanpreviousmodels.AcknowledgmentsWethankRukmaniRavisundaramandTayyabTariqfortheﬁrstversionoftheonlinedemo.RichardispartlysupportedbyaMicrosoftRe-searchPhDfellowship.Theauthorsgratefullyac-knowledgethesupportoftheDefenseAdvancedRe-searchProjectsAgency(DARPA)DeepExplorationandFilteringofText(DEFT)ProgramunderAirForceResearchLaboratory(AFRL)primecontractno.FA8750-13-2-0040,theDARPADeepLearningprogramundercontractnumberFA8650-10-C-7020andNSFIIS-1159679.Anyopinions,ﬁndings,andconclusionsorrecommendationsexpressedinthismaterialarethoseoftheauthorsanddonotneces-sarilyreﬂecttheviewofDARPA,AFRL,ortheUSgovernment.1641

ReferencesM.BaroniandA.Lenci.2010.Distributionalmem-ory:Ageneralframeworkforcorpus-basedsemantics.ComputationalLinguistics,36(4):673–721.Y.Bengio,R.Ducharme,P.Vincent,andC.Janvin.2003.Aneuralprobabilisticlanguagemodel.J.Mach.Learn.Res.,3,March.D.Blakemore.1989.Denialandcontrast:Arelevancetheoreticanalysisof‘but’.LinguisticsandPhiloso-phy,12:15–37.L.Bottou.2011.Frommachinelearningtomachinereasoning.CoRR,abs/1102.1808.S.ClarkandS.Pulman.2007.Combiningsymbolicanddistributionalmodelsofmeaning.InProceedingsoftheAAAISpringSymposiumonQuantumInteraction,pages52–55.R.CollobertandJ.Weston.2008.Auniﬁedarchitecturefornaturallanguageprocessing:deepneuralnetworkswithmultitasklearning.InICML.J.Duchi,E.Hazan,andY.Singer.2011.Adaptivesub-gradientmethodsforonlinelearningandstochasticop-timization.JMLR,12,July.K.ErkandS.Pad´o.2008.Astructuredvectorspacemodelforwordmeaningincontext.InEMNLP.C.GollerandA.K¨uchler.1996.Learningtask-dependentdistributedrepresentationsbybackpropaga-tionthroughstructure.InProceedingsoftheInterna-tionalConferenceonNeuralNetworks(ICNN-96).E.GrefenstetteandM.Sadrzadeh.2011.Experimentalsupportforacategoricalcompositionaldistributionalmodelofmeaning.InEMNLP.E.Grefenstette,G.Dinu,Y.-Z.Zhang,M.Sadrzadeh,andM.Baroni.2013.Multi-stepregressionlearningforcompositionaldistributionalsemantics.InIWCS.G.E.Hinton.1990.Mappingpart-wholehierarchiesintoconnectionistnetworks.ArtiﬁcialIntelligence,46(1-2).L.R.Horn.1989.Anaturalhistoryofnegation,volume960.UniversityofChicagoPressChicago.E.H.Huang,R.Socher,C.D.Manning,andA.Y.Ng.2012.ImprovingWordRepresentationsviaGlobalContextandMultipleWordPrototypes.InACL.M.Israel.2001.Minimizers,maximizers,andtherhetoricofscalarreasoning.JournalofSemantics,18(4):297–331.R.Jenatton,N.LeRoux,A.Bordes,andG.Obozinski.2012.Alatentfactormodelforhighlymulti-relationaldata.InNIPS.D.KleinandC.D.Manning.2003.Accurateunlexical-izedparsing.InACL.R.Lakoff.1971.If’s,and’s,andbut’saboutconjunction.InCharlesJ.FillmoreandD.TerenceLangendoen,ed-itors,StudiesinLinguisticSemantics,pages114–149.Holt,Rinehart,andWinston,NewYork.A.Merin.1999.Information,relevance,andsocialdeci-sionmaking:Someprinciplesandresultsofdecision-theoreticsemantics.InLawrenceS.Moss,JonathanGinzburg,andMaartendeRijke,editors,Logic,Lan-guage,andInformation,volume2.CSLI,Stanford,CA.J.MitchellandM.Lapata.2010.Compositionindis-tributionalmodelsofsemantics.CognitiveScience,34(8):1388–1429.K.MoilanenandS.Pulman.2007.Sentimentcomposi-tion.InInProceedingsofRecentAdvancesinNaturalLanguageProcessing.T.Nakagawa,K.Inui,andS.Kurohashi.2010.Depen-dencytree-basedsentimentclassiﬁcationusingCRFswithhiddenvariables.InNAACL,HLT.S.PadoandM.Lapata.2007.Dependency-basedcon-structionofsemanticspacemodels.ComputationalLinguistics,33(2):161–199.B.PangandL.Lee.2005.Seeingstars:Exploitingclassrelationshipsforsentimentcategorizationwithrespecttoratingscales.InACL,pages115–124.B.PangandL.Lee.2008.Opinionminingandsenti-mentanalysis.FoundationsandTrendsinInformationRetrieval,2(1-2):1–135.T.A.Plate.1995.Holographicreducedrepresentations.IEEETransactionsonNeuralNetworks,6(3):623–641.L.PolanyiandA.Zaenen.2006.Contextualvalenceshifters.InW.BruceCroft,JamesShanahan,YanQu,andJanyceWiebe,editors,ComputingAttitudeandAf-fectinText:TheoryandApplications,volume20ofTheInformationRetrievalSeries,chapter1.J.B.Pollack.1990.Recursivedistributedrepresenta-tions.ArtiﬁcialIntelligence,46,November.M.RanzatoandA.KrizhevskyG.E.Hinton.2010.Factored3-WayRestrictedBoltzmannMachinesForModelingNaturalImages.AISTATS.V.Rentoumi,S.Petrakis,M.Klenner,G.A.Vouros,andV.Karkaletsis.2010.Unitedwestand:Improvingsentimentanalysisbyjoiningmachinelearningandrulebasedmethods.InProceedingsoftheSeventhconferenceonInternationalLanguageResourcesandEvaluation(LREC’10),Valletta,Malta.S.RudolphandE.Giesbrecht.2010.Compositionalmatrix-spacemodelsoflanguage.InACL.B.SnyderandR.Barzilay.2007.Multipleaspectrank-ingusingtheGoodGriefalgorithm.InHLT-NAACL.R.Socher,C.D.Manning,andA.Y.Ng.2010.Learningcontinuousphraserepresentationsandsyntacticpars-ingwithrecursiveneuralnetworks.InProceedingsoftheNIPS-2010DeepLearningandUnsupervisedFea-tureLearningWorkshop.1642

R.Socher,C.Lin,A.Y.Ng,andC.D.Manning.2011a.ParsingNaturalScenesandNaturalLanguagewithRecursiveNeuralNetworks.InICML.R.Socher,J.Pennington,E.H.Huang,A.Y.Ng,andC.D.Manning.2011b.Semi-SupervisedRecursiveAutoencodersforPredictingSentimentDistributions.InEMNLP.R.Socher,B.Huval,C.D.Manning,andA.Y.Ng.2012.Semanticcompositionalitythroughrecursivematrix-vectorspaces.InEMNLP.I.Sutskever,R.Salakhutdinov,andJ.B.Tenenbaum.2009.ModellingrelationaldatausingBayesianclus-teredtensorfactorization.InNIPS.P.D.TurneyandP.Pantel.2010.Fromfrequencytomeaning:Vectorspacemodelsofsemantics.JournalofArtiﬁcialIntelligenceResearch,37:141–188.H.Wang,D.Can,A.Kazemzadeh,F.Bar,andS.Narayanan.2012.Asystemforreal-timetwit-tersentimentanalysisof2012u.s.presidentialelec-tioncycle.InProceedingsoftheACL2012SystemDemonstrations.D.Widdows.2008.Semanticvectorproducts:Someini-tialinvestigations.InProceedingsoftheSecondAAAISymposiumonQuantumInteraction.A.YessenalinaandC.Cardie.2011.Composi-tionalmatrix-spacemodelsforsentimentanalysis.InEMNLP.D.Yu,L.Deng,andF.Seide.2012.Largevocabularyspeechrecognitionusingdeeptensorneuralnetworks.InINTERSPEECH.F.M.Zanzotto,I.Korkontzelos,F.Fallucchi,andS.Man-andhar.2010.Estimatinglinearmodelsforcomposi-tionaldistributionalsemantics.InCOLING.L.ZettlemoyerandM.Collins.2005.Learningtomapsentencestologicalform:Structuredclassiﬁca-tionwithprobabilisticcategorialgrammars.InUAI.

