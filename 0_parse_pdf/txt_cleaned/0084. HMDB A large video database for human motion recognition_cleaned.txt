2011 IEEE International Conference on Computer Vision 978-1-4577-1102-2/11/$26.00 c(cid:13)2011 IEEE

Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:55:30 UTC from IEEE Xplore.  Restrictions apply.

HMDB:ALargeVideoDatabaseforHumanMotionRecognitionH.KuehneKarlsruheInstit.ofTech.Karlsruhe,Germanykuehne@kit.eduH.JhuangE.GarroteT.PoggioMassachusettsInstituteofTechnologyCambridge,MA02139hueihan@mit.edu,tp@ai.mit.eduT.SerreBrownUniversityProvidence,RI02906thomasserre@brown.eduAbstractWithnearlyonebilliononlinevideosviewedeveryday,anemergingnewfrontierincomputervisionresearchisrecognitionandsearchinvideo.Whilemuchefforthasbeendevotedtothecollectionandannotationoflargescal-ablestaticimagedatasetscontainingthousandsofimagecategories,humanactiondatasetslagfarbehind.Cur-rentactionrecognitiondatabasescontainontheorderoftendifferentactioncategoriescollectedunderfairlycon-trolledconditions.State-of-the-artperformanceonthesedatasetsisnownearceilingandthusthereisaneedforthedesignandcreationofnewbenchmarks.Toaddressthisis-suewecollectedthelargestactionvideodatabaseto-datewith51actioncategories,whichintotalcontainaround7,000manuallyannotatedclipsextractedfromavarietyofsourcesrangingfromdigitizedmoviestoYouTube.Weusethisdatabasetoevaluatetheperformanceoftworepresen-tativecomputervisionsystemsforactionrecognitionandexploretherobustnessofthesemethodsundervariouscon-ditionssuchascameramotion,viewpoint,videoqualityandocclusion.1.IntroductionWithseveralbillionvideoscurrentlyavailableonthein-ternetandapproximately24hoursofvideouploadedtoYouTubeeveryminute,thereisanimmediateneedforro-bustalgorithmsthatcanhelporganize,summarizeandre-trievethismassiveamountofdata.Whilemuchefforthasbeendevotedtothecollectionofrealisticinternet-scalestaticimagedatabases[17,23,27,4,5],currentac-tionrecognitiondatasetslagfarbehind.Themostpopularbenchmarkdatasets,suchasKTH[20],Weizmann[3]ortheIXMASdataset[25],containaround6-11actionseach.Atypicalvideoclipinthesedatasetscontainsasinglestagedactorwithnoocclusionandverylimitedclutter.Astheyarealsolimitedintermsofilluminationandcameraposi-tionvariation,thesedatabasesarenotquiterepresentativeoftherichnessandcomplexityofreal-worldactionvideos.Figure1.SampleframesfromtheproposedHMDB51[1](fromtoplefttolowerright,actionsare:hand-waving,drinking,swordﬁghting,diving,runningandkicking).Someofthekeychallengesarelargevariationsincameraviewpointandmotion,theclutteredbackground,andchangesintheposition,scale,andappearancesoftheactors.Recognitionratesonthesedatasetstendtobeveryhigh.Arecentsurveyofactionrecognitionsystems[26]reportedthat12outofthe21testedsystemsperformbetterthan90%ontheKTHdataset.FortheWeizmanndataset,14ofthe16testedsystemsperformat90%orbetter,8ofthe16betterthan95%,and3outof16scoredaperfect100%recogni-tionrate.Inthiscontext,wedescribeanefforttoadvancetheﬁeldwiththedesignofalargevideodatabasecontain-ing51distinctactioncategories,dubbedtheHumanMo-tionDataBase(HMDB51),thattriestobettercapturetherichnessandcomplexityofhumanactions(seeFigure1).1Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:55:30 UTC from IEEE Xplore.  Restrictions apply.

Relatedwork.AnoverviewofexistingdatasetsisshowninTable1.Inthislist,theHollywood[11]andUCF50[2]datasetsaretwoexamplesofrecenteffortstobuildmorere-alisticactionrecognitiondatasetsbyconsideringvideoclipstakenfromrealmoviesandYouTube.Thesedatasetsaremorechallengingduetolargevariationsincameramotion,objectappearanceandchangesintheposition,scaleandviewpointoftheactors,aswellasclutteredbackground.TheUCF50datasetextendsthe11actioncategoriesfromtheUCFYouTubedatasetforatotalof50actioncategorieswithreal-lifevideostakenfromYouTube.Eachcategoryhasbeenfurtherorganizedby25groupscontainingvideoclipsthatsharecommonfeatures(e.g.background,cameraposition,etc.).TheUCF50,itsclosecousin,theUCFSportsdataset[16],andtherecentlyintroducedOlympicSportsdataset[14],containmostlysportsvideosfromYouTube.Asare-sultofsearchingforspeciﬁctitlesonYouTube,thesetypesofactionsareusuallyunambiguousandhighlydistinguish-ablefromshapecuesalone(e.g.,therawpositionsofthejointsorthesilhouetteextractedfromsingleframes).Todemonstratethispoint,weconductedasimpleexper-iment:usingAmazonMechanicalTurk,14jointlocationsweremanuallyannotatedateveryframefor5randomlyse-lectedclipsfromeachofthe9actioncategoriesoftheUCFSportsdataset.Usingaleave-one-clip-outprocedure,clas-sifyingthefeaturesderivedfromthejointlocationsatsin-gleframesresultsinarecognitionrateabove98%(chancelevel11%).Thissuggeststhattheinformationofstaticjointlocationsaloneissufﬁcientfortherecognitionofthoseac-tionswhiletheuseofjointkinematicsisnotnecessary.Thisseemsunlikelytobetrueformorereal-worldscenarios.ItisalsoincompatiblewithpreviousresultsofJohanssonetal.[9],whodemonstratedthatjointkinematicsplayacriticalrolefortherecognitionofbiologicalmotion.WeconductedasimilarexperimentontheproposedHMDB51wherewepicked10actioncategoriessimilartothoseoftheUCF50(e.g.climb,climb-stairs,run,walk,jump,etc.)andobtainedmanualannotationsforthe14jointlocationsinasetofover1,100randomclips.Theclassiﬁ-cationaccuracyoffeaturesderivedfromthejointlocationsatsingleframesnowreachesonly35%(chancelevel10%)andismuchlowerthanthe54%obtainedusingmotionfea-turesfromtheentireclip(Section4.1).Wealsocomputedtheclassiﬁcationaccuracyofthe10actioncategoriesoftheUCF50usingthemotionfeaturesandobtainedanaccuracyof66%.ThesesmallexperimentssuggestthattheproposedHMDB51isanactiondatasetwhoseactioncategoriesmainlydifferinmotionratherthanstaticposesandcanthusbeseenasavalidcontributionfortheevaluationofactionrecognitionsystemsaswellasforthestudyofrelativecon-tributionsofmotionvs.shapecues,acurrenttopicinbio-Table1.Alistofexistingdatasets,thenumberofcategories,andthenumberofclipspercategorysortedbyyear.DatasetRefYearActionsClipsKTH[20]20046100Weizmann[3]200599IXMAS[25]20061133Hollywood[11]2008830-129UCFSports[16]2009914-35Hollywood2[13]20091261-278UCFYouTube[12]200911100Olympic[14]20101650UCF50[2]201050min.100HMDB51[1]201151min.101logicalmotionperceptionandrecognition[22].Contributions.TheproposedHMDB51contains51dis-tinctactioncategories,eachcontainingatleast101clipsforatotalof6,766videoclipsextractedfromawiderangeofsources.Tothebestofourknowledge,itisto-datethelargestandperhapsmostrealisticavailabledataset.Eachclipwasvalidatedbyatleasttwohumanobserverstoen-sureconsistency.Additionalmetainformationallowsforapreciseselectionoftestingdata,aswellastrainingandevaluationofrecognitionsystems.Themetatagsforeachclipincludethecameraview-point,thepresenceorabsenceofcameramotion,thevideoquality,andthenumberofac-torsinvolvedintheaction.Thisshouldpermitthedesignofmoreﬂexibleexperimentstoevaluatetheperformanceofcomputervisionsystemsusingselectedsubsetsofthedatabase.WeusetheproposedHMDB51toevaluatetheperfor-manceoftworepresentativeactionrecognitionsystems.Weconsiderthebiologically-motivatedactionrecognitionsys-tembyJhuangetal.[8],whichisbasedonamodelofthedorsalstreamofthevisualcortexandwasrecentlyshowntoachieveon-parwithhumansfortherecognitionofrodentbehaviorsinthehomecageenvironment[7].Wealsocon-siderthespatio-temporalbag-of-wordssystembyLaptevandcolleagues[10,11,24].Wecomparetheperformanceofthetwosystems,eval-uatetheirrobustnesstovarioussourcesofimagedegrada-tionsanddiscusstherelativeroleofshapevs.motioninfor-mationforactionrecognition.Wealsostudytheinﬂuenceofvariousnuisances(cameramotion,position,videoqual-ity,etc.)ontherecognitionperformanceofthesesystemsandsuggestpotentialavenuesforfutureresearch.2.TheHumanMotionDataBase(HMDB51)2.1.DatabasecollectionInordertocollecthumanactionsthatarerepresentativeofeverydayactions,westartedbyaskingagroupofstu-dentstowatchvideosfromvariousinternetsourcesanddig-itizedmoviesandannotateanysegmentofthesevideosthatAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:55:30 UTC from IEEE Xplore.  Restrictions apply.

00.10.20.30.40.50.60.70.80.91a)b)c)d) full (56.3%)  upper (30.5%)  lower(0.8%)  head(12.3%)  camera motion (59.9%)  no motion (40.1%)  front (40.8%)  back (18.2%)  left (22.1%)  right (19.0%)  low (20.8%)  medium (62.1%)  good (17.1%) Figure2.DistributionofthevariousconditionsfortheHMDB51:a)visiblebodypart,b)cameramotion,c)cameraviewpoint,andd)clipquality.representsasinglenon-ambiguoushumanaction.Studentswereaskedtoconsideraminimumqualitystandardlikeasingleactionperclip,aminimumof60pixelsinheightforthemainactor,minimumcontrastlevel,minimum1sec-ondofcliplength,andacceptablecompressionartifacts.Thefollowingsourceswereused:digitizedmovies,publicdatabasessuchasthePrelingerarchive,othervideosavail-ableontheinternet,andYouTubeandGooglevideos.Thus,aﬁrstsetofannotationswasgeneratedwithover60actioncategories.Itwasreducedto51categoriesbyretainingonlythosewithatleast101clips.Theactionscategoriescanbegroupedinﬁvetypes:1)Generalfacialactions:smile,laugh,chew,talk;2)Fa-cialactionswithobjectmanipulation:smoke,eat,drink;3)Generalbodymovements:cartwheel,claphands,climb,climbstairs,dive,fallontheﬂoor,backhandﬂip,hand-stand,jump,pullup,pushup,run,sitdown,situp,som-ersault,standup,turn,walk,wave;4)Bodymovementswithobjectinteraction:brushhair,catch,drawsword,drib-ble,golf,hitsomething,kickball,pick,pour,pushsome-thing,ridebike,ridehorse,shootball,shootbow,shootgun,swingbaseballbat,swordexercise,throw;5)Bodymovementsforhumaninteraction:fencing,hug,kicksome-one,kiss,punch,shakehands,swordﬁght.2.2.AnnotationsInadditiontoactioncategorylabels,eachclipwasan-notatedwithmetainformationtoallowforamorepreciseevaluationofthelimitationofcurrentcomputervisionsys-tems.Themetainformationcontainsthefollowingﬁelds:visiblebodyparts/occlusionsindicatingifthehead,upperbody,lowerbodyorthefullbodyisvisible,cameramotionindicatingwhetherthecameraismovingorstatic,cameraviewpointrelativetotheactor(labeledfront,back,leftorright),andthenumberofpeopleinvolvedintheaction(one,twoormultiplepeople).Theclipswerealsoannotatedaccordingtotheirvideoquality.Weconsiderthreelevels:1)High–detailedvisualelementssuchastheﬁngersandeyesofthemainactoriden-tiﬁablethroughmostoftheclip,limitedmotionblurandlimitedcompressionartifacts;2)Medium–largebodypartsliketheupperandlowerarmsandlegsidentiﬁablethroughmostoftheclip;3)Low–largebodypartsnotidentiﬁabledueinparttothepresenceofmotionblurandcompressionartifacts.ThedistributionofthemetatagsfortheentiredatasetisshowninFigure2.2.3.TrainingandtestingsetgenerationForevaluationpurposes,threedistincttrainingandtest-ingsplitsweregeneratedfromthedatabase.Thesetswerebuilttoensurethatclipsfromthesamevideowerenotusedforbothtrainingandtestingandthattherelativeproportionsofmetatagssuchascameraposition,videoquality,motion,etc.wereevenlydistributedacrossthetrainingandtestingsets.Foreachactioncategoryinourdatasetweselectedsetsof70trainingand30testingclipssothattheyfulﬁllthe70/30balanceforeachmetatagwiththeaddedconstraintthatclipsinthetrainingandtestingsetcouldnotcomefromthesamevideoﬁle.Tothisend,weselectedthethreebestresultsbythede-ﬁnedcriteriafromaverylargenumberofrandomlygener-atedsplits.Toensurethatselectedsplitsarenotcorrelatedwitheachother,weimplementedagreedyapproachbyﬁrstpickingthesplitwiththemostbalancedmetatagdistribu-tionandsubsequentlychoosingthesecondandthirdsplitwhichareleastcorrelatedwiththeprevioussplits.Thecor-relationwasmeasuredbynormalizedHammingdistance.Becauseofthehardconstraintofnotusingclipsfromthesamesourcefortrainingandtesting,itisnotalwayspos-sibletoﬁndanoptimalsplitthathasperfectmetatagdis-tribution,butwefoundthatinpracticethesimpleapproachdescribedaboveprovidesreasonablesplits.2.4.VideonormalizationTheoriginalvideosourcesusedtoextracttheactionclipsvaryinsizeandframerate.Toensureconsistencyacrossthedatabase,theheightofalltheframeswasscaledto240pixels.Tewidthwasscaledaccordinglytomaintaintheoriginalaspectratio.Theframeratewasconvertedto30fpsforalltheclips.AlltheclipswerecompressedusingtheDivX5.0codecwiththeffmpegvideolibrary.2.5.VideostabilizationAmajorchallengeaccompanyingtheuseofvideoclipsextractedfromreal-worldvideosisthepotentialpresenceofsigniﬁcantcameramotion,whichisthecaseforapproxi-mately2/3oftheclipsinourdatabaseasshowninFigure2.Ascameramotionisassumedtointerferewiththelocalmo-tioncomputationandshouldbecorrected,itfollowsthatvideostabilizationisakeypre-processingstep.Toremovethecameramotion,weusedstandardimagestitchingtech-niquestoalignframesofaclip.Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:55:30 UTC from IEEE Xplore.  Restrictions apply.

Figure3.Examplesofaclipstabilizedover50framesshowingfromthetoptothebottom,the1st,30thand50thframeoftheoriginal(leftcolumn)andstabilizedclip(rightcolumn).Table2.Therecognitionaccuracyoflow-levelcolor/gistcuesfordifferentactiondatasets.DatasetNColor+Gray+PCAPercentdropGistPercentdropHOG/HOFHollywood826.9%16.7%27.4%15.2%32.3%UCFSports947.7%18.6%60.0%-2.4%58.6%UCFYouTube1138.3%35.0%53.8%8.7%58.9%Hollywood21216.2%68.7%21.8%57.8%51.7%UCF505041.3%13.8%38.8%19.0%47.9%HMDB51518.8%56.4%13.4%33.7%20.2%Todothis,abackgroundplaneisestimatedbydetectingandmatchingsalientfeaturesintwoadjacentframes.Cor-respondingfeaturesarecomputedusingadistancemeasurethatincludesboththeabsolutepixeldifferencesandtheEu-lerdistanceofthedetectedpoints.PointswithaminimumdistancearethenmatchedandtheRANSACalgorithmisusedtoestimatethegeometrictransformationbetweenallneighboringframes.Thisisdoneindependentlyforeverypairofframes.Usingthisestimatedtransformation,allframesofthecliparewarpedandcombinedtoachieveastabilizedclip.Wevisuallyinspectedalargenumberoftheresultingstabilizedclipsandfoundthattheimagestitch-ingtechniquesworksurprisinglywell.Figure3showsanexample.Fortheevaluationoftheactionrecognitionsys-tems,theperformancewasreportedfortheoriginalclipsaswellasthestabilizedclips.3.ComparisonwithotheractiondatasetsTocomparetheproposedHMDB51withexistingreal-worldactiondatasetssuchasHollywood,Hollywood2,UCFSports,andtheUCFYouTubedataset,weevaluatethediscriminativepowerofvariouslow-levelfeatures.Foranidealunbiasedactiondataset,low-levelfeaturessuchascolorshouldnotbepredictiveofthehigh-levelactioncate-gory.Forlow-levelfeaturesweconsideredthemeancolorintheHSVcolorspacecomputedforeachframeovera12×16spatialgridaswellasthecombinationofcolorandgrayvalueandtheuseofPCAtoreducethefeaturedimen-sionofthosedescriptors.Herewereporttheresults“color+gray+PCA”.Wefurtherconsideredthelow-levelglobalsceneinfor-mation(gist)[15]computedforthreeframesofaclip.Gistisacoarseorientation-basedrepresentationofanimagethathasbeenshowntocapturewellthecontextualinformationinasceneandshowntoperformquitewellonavarietyofrecognitiontasks,see[15].Weusedthesourcecodepro-videdbytheauthors.Lastly,wecomparetheselow-levelcueswithacommonmid-levelspatio-temporalbag-of-wordscue(HOG/HOF)bycomputingspatialtemporalinterestpointsforallclips.Astandardbagofwordsapproachwith2,000,3,000,4,000,and5,000visualwordswasusedforclassiﬁcationandthebestresultisreported.Forevaluationweusedthetestingandtrainingsplitsthatcamewiththedatasets,otherwisea3-or5-foldcrossvalidationwasusedfordatasetswithoutspeciﬁedsplits.Table2showstheresultssortedbythenum-berofclasses(N)ineachdataset.PercentdropiscomputedfortheperformancedownfromHOG/HOFfeaturestoeachofthetwotypesoflow-levelfeatures.Asmallpercentagedropmeansthatthelow-levelfeaturesperformaswellasthemid-levelmotionfeatures.Resultsobtainedbyclassifyingtheseverysimplefea-turesshowthattheUCFSportsdatasetcanbeclassiﬁedbyscenedescriptorsratherthanbyactiondescriptorsasgistismorepredictivethanmid-levelspatio-temporalfeatures.Weconjecturethatgistfeaturesarepredictiveofthesportsactions(i.e.,UCFSports)becausemostsportsarelocation-speciﬁc.Forexample,ballgamesusuallyoccurongrassﬁeld,swimmingisalwaysinwater,andmostskiinghap-pensonsnow.Theresultsalsorevealthatlow-levelfeaturesarefairlypredictiveascomparedtomid-levelfeaturesfortheUCFYouTubeandUCF50dataset.Thismightbeduetolow-levelbiasesforvideosonYouTube,e.g.,preferredvantagepointsandcamerapositionsforamateurdirectors.ForthedatasetcollectedfromgeneralmoviesorHollywoodmovies,theperformanceofvariouslow-levelcuesisonav-eragelowerthanthatofthemid-levelspatio-temporalfea-tures.ThisimpliesthatthedatasetscollectedfromYouTubetendtobebiasedandcaptureonlyasmallrangeofcolorsandscenesacrossactioncategoriescomparedtothosecol-lectedfrommovies.Thesimilarperformanceusinglow-levelandmid-levelfeaturesfortheHollywooddatasetislikelyduetothelownumberofsourcemovies(12).Clipsextractedfromthesamemovieusuallyhavesimilarscenes.Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:55:30 UTC from IEEE Xplore.  Restrictions apply.

4.BenchmarksystemsToevaluatethediscriminabilityofour51actioncate-gorieswefocusontheclassofalgorithmsforactionrecog-nitionbasedontheextractionoflocalspace-timeinforma-tionfromvideos,whichhavebecomethedominanttrendinthepastﬁveyears[24].Variouslocalspace-timebasedap-proachesmainlydifferinthetypeofdetectors(e.g.,theim-plementationofthespatio-temporalﬁlters),thefeaturede-scriptors,andthenumberofspatio-temporalpointssampled(densevs.sparse).Wangetal.havegroupedthesedetectorsanddescriptorsintosixtypesandevaluatedtheirperfor-manceontheKTH,UCFSportsandHollywood2datasetsinacommonexperimentalsetup[24].TheresultshaveshownthatLaptev’scombinationofahistogramoforientedgradient(HOG)andhistogramofori-entedﬂow(HOF)descriptorsperformedbestfortheHol-lywood2andUCFSports.AsHMDB51containsmoviesandYouTubevideos,thesedatasetsareconsideredthemostsimilarintermsofvideosources.Therefore,weselectedthealgorithmbyLatptevandcolleagues[11]asoneofourbenchmarks.Toexpandbeyond[24],wechoseforoursec-ondbenchmarkapproachesdevelopedbyourgroup[21,8].Itusesahierarchicalarchitecturemodeledaftertheventralanddorsalstreamsoftheprimatevisualcortexforthetaskofobjectandactionrecognition,respectively.Inthefollowingweprovideadetailedcomparisonbe-tweenthesealgorithms,lookinginparticularattherobust-nessofthetwoapproacheswithrespecttovariousnuisancefactorsincludingthequalityofthevideoandthecameramotion,aswellaschangesintheposition,scaleandview-pointofthemainactors.4.1.HOG/HOFfeaturesThecombinationofHOG,whichhasbeenusedfortherecognitionofobjectsandscenes,andHOF,a3Dﬂow-basedversionofHOG,hasbeenshowntoachievestate-of-the-artperformanceonseveralcommonlyusedactiondatasets[11,24].Weusedthebinariesprovidedby[11]toextractfeaturesusingtheHarris3DasfeaturedetectorandtheHOG/HOFfeaturedescriptors.Foreveryclipasetof3DHarriscornersisdetectedandalocaldescriptoriscom-putedasaconcatenationoftheHOGandHOFaroundthecorner.Forclassiﬁcation,weimplementedabag-of-wordssys-temasdescribedin[11].Toevaluatethebestcodebooksize,wesampled100,000space-timeinterest-pointdescrip-torsfromthetrainingsetandappliedthek-meansclusteringtoobtainasetofk=[2,000,4,000,6,000,8,000]visualwords.Foreveryclip,eachofthelocalpointdescriptorsismatchedtothenearestprototypereturnedbyk-meansclus-teringandaglobalfeaturedescriptorisobtainedbycomput-ingahistogramovertheindexofthematchedcodebooken-tries.Thisresultsinak-dimensionalfeaturevectorwherekisthenumberofvisualwordslearnedfromk-means.Theseclipdescriptorsarethenusedtotrainandtestasupportvec-tormachine(SVM)intheclassiﬁcationstage.WeusedaSVMwithanRBFkernelK(u,v)=exp(−γ∗|u−v|2)).TheparametersoftheRBFkernel(thecosttermCandkernelbandwidthγ)wereoptimizedusingagreedysearchwitha5-foldcross-validationonthetrainingset.Thebestresultfortheoriginalclipswasreachedfork=8,000whereasthebestresultforthestabilizedclipswasforatk=2000(seeSection5.1).Tovalidateourre-implementationofLaptev’ssystem,weevaluatedtheper-formanceofthesystemontheKTHdatasetandwereabletoreproducetheresultsfortheHOG(81.4%)andHOFde-scriptors(90.7%)asreportedin[24].4.2.C2featuresTwotypesofC2featureshavebeendescribedinthelit-erature.Oneisfromamodelthatwasdesignedtomimicthehierarchicalorganizationandfunctionsoftheventralstreamofthevisualcortex[21].Theventralstreamisbelievedtobecriticallyinvolvedintheprocessingofshapeinforma-tionandthescale-and-position-invariantobjectrecognition.ThemodelstartswithapyramidofGaborﬁlters(S1unitsatdifferentorientationsandscales),whichcorrespondsimplecellsintheprimaryvisualcortex.Thenextlayer(C1)mod-elsthecomplexcellsintheprimaryvisualcortexbypoolingtogethertheactivityofS1unitsinalocalspatialregionandacrossscalestobuildsometoleranceto2Dtransformations(translationandsize)ofinputs.Thethirdlayer(S2)responsesarecomputedbymatchingtheC1inputswithadictionaryofnprototypeslearnedfromasetoftrainingimages.Asopposedtothebag-of-wordsapproachthatusesvectorquantizationandsummarizestheindicesofthematchedcodebookentries,weretainthesim-ilarity(rangingfrom0to1)witheachofthenprototypes.Inthetoplayerofthefeaturehierarchy,an-dimensionalC2vectorisobtainedforeachimagebypoolingthemax-imumofS2responsesacrossscalesandpositionsforeachofthenprototypes.TheC2featureshavebeenshowntoperformcomparablytostate-of-the-artalgorithmsappliedtotheproblemofobjectrecognition[21].Theyhavealsobeenshowntoaccountwellforthepropertiesofcellsintheinferotemporalcortex(IT),whichisthehighestpurelyvisualareaintheprimatebrain.Basedontheworkdescribedabove,Jhuangetal.[8]proposedamodelofthedorsalstreamofthevisualcor-tex.Thedorsalstreamisthoughttobecriticallyinvolvedintheprocessingofmotioninformationandtheperceptionofmotion.Themodelstartswithspatio-temporalGaborﬁltersthatmimicthedirection-sensitivesimplecellsintheprimaryvisualcortex.Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:55:30 UTC from IEEE Xplore.  Restrictions apply.

Thedorsalstreammodelisa3D(space-time)extensionoftheventralstreammodel.TheS1unitsintheventralstreammodelrespondbesttoorientationinspace,whereasS1unitsinthedorsalstreammodelhavenon-separablespatio-temporalreceptiveﬁeldsandrespondbesttodirec-tionsofmotion,whichcouldbeseenasorientationinspace-time.Ithasbeensuggestedthatmotion-directionsensitivecellsandshape-orientationcellsperformtheinitialﬁlteringfortwoparallelchannelsoffeatureprocessing,oneformo-tioninthedorsalstream,andanotherforshapeintheventralstream.BeyondtheS1layer,thedorsalsteammodelfollowsthesamearchitectureastheventralstreammodel.ItcontainstheC1,S2,C2layers,whichperformsimilaroperationsasitsventralstreamcounterpart.TheS2unitsinthedorsalstreammodelarenowtunedtooptic-ﬂowpatternsthatcor-respondtocombinationsofdirectionsofmotionwhereastheventralS2unitsaretunedtoshapepatternscorrespond-ingtocombinationsoforientations.Ithasbeensuggestedthatboththeshapefeaturesprocessedintheventralstreamandthemotionfeaturesprocessedinthedorsalstreamcon-tributetotherecognitionofactions.Inthiswork,wecon-sidertheircombinationbycomputingbothtypesofC2fea-turesindependentlyandthenconcatenatingthem.5.Evaluation5.1.OverallrecognitionperformanceWeﬁrstevaluatedtheoverallperformanceofbothsys-temsontheproposedHMDB51averagedoverthreesplits(seeSection2.3).Bothsystemsexhibitedcomparablelevelsofperformanceslightlyover20%(chancelevel2%).TheconfusionmatrixforbothsystemsontheoriginalclipsisshowninFigure4.Errorsseemtoberandomlydistributedacrosscategorylabelswithnoapparenttrends.Themostsurprisingresultisthattheperformanceofthetwosystemsimprovedonlymarginallyafterstabilizationforcameramo-tion(Table3).Asrecognitionresultsforbothsystemsappearrela-tivelylowcomparedtopreviouslypublishedresultsonotherdatasets[8,11,24],weconductedasimpleexperimenttoﬁndoutwhetherthisdecreaseinperformancesimplyre-sultsfromanincreaseinthenumberofobjectcategoriesandacorrespondingdecreaseinchancelevelrecognitionoranactualincreaseinthecomplexityofthedatasetdueforinstancetothepresenceofcomplexbackgroundclutterandmoreintra-classvariations.Weselected10commonac-tionsintheHMDB51thatweresimilartoactioncategoriesintheUCF50andcomparedtherecognitionperformanceoftheHOG/HOFonvideoclipsfromthetwodatasets.Thefollowingisalistofmatchedcategories:basketball/shootball,biking/ridebike,diving/dive,fencing/stab,golfswing/golf,horseriding/ridehorse,pullups/pull-HOG/HOF − Original Clips  chewlaughsmiletalkdrinkeatsmokecartwheelclapclimbclimb_stairsdivefall_floorflic_flachandstandjumppulluppushuprunsitsitupsomersaultstandturnwalkwavebrush_haircatchdraw_sworddribblegolfhitkick_ballpickpourpushride_bikeride_horseshoot_ballshoot_bowshoot_gunswing_baseballsword_exercisethrowfencinghugkickkisspunchshake_handsswordchewlaughsmiletalkdrinkeatsmokecartwheelclapclimbclimb_stairsdivefall_floorflic_flachandstandjumppulluppushuprunsitsitupsomersaultstandturnwalkwavebrush_haircatchdraw_sworddribblegolfhitkick_ballpickpourpushride_bikeride_horseshoot_ballshoot_bowshoot_gunswing_baseballsword_exercisethrowfencinghugkickkisspunchshake_handssword0.10.20.30.40.50.60.7C2 − Original Clips  chewlaughsmiletalkdrinkeatsmokecartwheelclapclimbclimb_stairsdivefall_floorflic_flachandstandjumppulluppushuprunsitsitupsomersaultstandturnwalkwavebrush_haircatchdraw_sworddribblegolfhitkick_ballpickpourpushride_bikeride_horseshoot_ballshoot_bowshoot_gunswing_baseballsword_exercisethrowfencinghugkickkisspunchshake_handsswordchewlaughsmiletalkdrinkeatsmokecartwheelclapclimbclimb_stairsdivefall_floorflic_flachandstandjumppulluppushuprunsitsitupsomersaultstandturnwalkwavebrush_haircatchdraw_sworddribblegolfhitkick_ballpickpourpushride_bikeride_horseshoot_ballshoot_bowshoot_gunswing_baseballsword_exercisethrowfencinghugkickkisspunchshake_handssword0.10.20.30.40.50.60.7Figure4.ConfusionMatrixforHOG/HOFandtheC2featuresonthesetoforiginal(notstabilized)clips.up,push-ups/push-up,rockclimbingindoor/climbaswellaswalkingwithdog/walk.Overall,wefoundamilddropinperformancefromtheUCF50with66.3%accuracydownto54.3%forsimilarcat-egoriesontheHMDB51(chancelevel10%forbothsets).TheseresultsarealsocomparabletotheperformanceofthesameHOG/HOFsystemonsimilarsizeddatasetsofdif-ferentactionswith51.7%over12categoriesoftheHolly-wood2datasetand58.9%over11categoriesoftheUCFYouTubedatasetasshowninTable2.TheseresultssuggestthattherelativelylowperformanceofthebenchmarksontheproposedHMDB51ismostlikelytheconsequenceoftheincreaseinthenumberofactioncategoriescomparedtoolderdatasets.Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:55:30 UTC from IEEE Xplore.  Restrictions apply.

Table3.PerformanceofthebenchmarksystemsontheHMDB51.SystemOriginalclipsStabilizedclipsHOG/HOF20.44%21.96%C222.83%23.18%Table4.Meanrecognitionperformanceasafunctionofcameramotionandclipquality.CameramotionQualityyesnolowmedhighHOG/HOF19.84%19.99%17.18%18.68%27.90%C225.20%19.13%17.54%23.10%28.62%5.2.RobustnessofthebenchmarksInordertoassesstherelativestrengthsandweaknessesofthetwobenchmarksystemsontheHMDB51inthecontextofvariousnuisancefactors,webrokedowntheirperformanceintermsof1)visiblebodypartsorequiv-alentlythepresence/absenceofocclusions,2)thepres-ence/absenceofcameramotion,3)viewpoint/camerapo-sition,and4)thequalityofthevideoclips.Wefoundthatthepresence/absenceofocclusionsandthecamerapositiondidnotseemtoinﬂuenceperformance.Amajorfactorfortheperformanceofthetwosystemswastheclipquality.AsshownonTable4,fromhightolowqualityvideos,thetwosystemsregisteredadropinperformanceofabout10%(from27.90%/28.62%fortheHOG+HOF/C2featuresforthehighqualityclipsdownto17.18%/17.54%forthelowqualityclips).Afactorthataffectedthetwosystemsdifferentlywascameramotion:WhereastheHOG/HOFperformancewasstablewiththepresenceorabsenceofcameramotion,sur-prisingly,theperformanceoftheC2featuresactuallyim-provedwiththepresenceofcameramotion.Wesuspectthatcameramotionmightactuallyincreasetheresponseofthelow-levelS1motiondetectors.Analternativeexplana-tionisthatthecameramotionbyitselfmightbecorrelatedwiththeactioncategory.Toevaluatewhethercameramo-tionalonecanbepredictiveoftheactioncategory,wetriedtoclassifythemeanparametersoftheestimatedframe-by-framemotionreturnedbythevideostabilizationalgorithm.Theresultof5.29%recognitionshowsthatatleastcam-eramotionalonedoesnotprovidesigniﬁcantinformationinthiscase.Tofurtherinvestigatehowvariousnuisancefactorsmayaffecttherecognitionperformanceofthetwosystems,weconductedalogisticregressionanalysistopredictwhethereachofthetwosystemswillbecorrectvs.incorrectforspe-ciﬁcconditions.Thelogisticregressionmodelwasbuiltasfollows:thecorrectnessofthepredictedlabelwasusedasbinarydependentvariable,thecameraviewpointsweresplitintoonegroupforfrontandbackviews(becauseofsimi-larappearances;front,back=0)andanothergroupforsideviews(left,right=1).Theocclusionconditionwassplitintofullbodyview(=0)andoccludedviews(head,upperorlowerbodyonly=1).Thevideoqualitylabelwascon-Table5.Resultsofthelogisticregressionanalysisonthekeyfac-torsinﬂuencingtheperformanceofthetwosystems.HOG/HOFCoefﬁcientCoef.est.βpoddsratioIntercept-1.600.0000.20Occluders0.070.4271.06Cameramotion-0.120.1320.88Viewpoint0.090.2671.09Med.quality0.110.2541.12Highquality0.650.0001.91C2CoefﬁcientCoef.est.βpoddsratioIntercept-1.520.0000.22Occluders-0.220.0070.81Cameramotion-0.430.0000.65Viewpoint0.190.0091.21Med.quality0.470.0001.60Highquality0.970.0002.65vertedintobinaryvariableswhereasthelabels10,01and00correspondedtoahigh,medium,andlowqualityvideorespectively.TheestimatedβcoefﬁcientsforthetwosystemsareshowninTable5.Thelargestfactorinﬂuencingperfor-manceforbothsystemsremainedthequalityofthevideoclips.Onaveragethesystemswerepredictedtobenearlytwiceaslikelytobecorrectonhighvs.mediumqualityvideos.Thisisthestrongestinﬂuencefactorbyfar.How-evertheregressionanalysisalsoconﬁrmedtheassumptionthatcameramotionimprovesclassiﬁcationperformance.Consistentwiththepreviousanalysisbasedonerrorrates,thistrendisonlysigniﬁcantfortheC2features.Theaddi-tionalfactors,occlusionandcameraviewpoint,didnothaveasigniﬁcantinﬂuenceontheresultsoftheHOG/HOForC2approach.5.3.Shapevs.motioninformationTheroleofshapevs.motioncuesfortherecognitionofbiologicalmotionhasbeenthesubjectofanintensedebate.Computervisioncouldprovidecriticalinsighttothisques-tionasvariousapproacheshavebeenproposedthatrelynotjustonmotioncueslikethetwosystemswehavetestedbutalsoonsingle-frameshape-basedcues,suchasposture[18]andshape[19],andcontextualinformation[13,28].Weherestudytherelativecontributionsofshapevs.mo-tioncuesfortherecognitionofactionsontheHMDB51.WecomparedtheHOG/HOFdescriptorwiththerecogni-tionofashape-onlyHOGdescriptorandamotion-onlyHOFdescriptor.Wealsocomparedtheperformanceofthepreviouslymentionedmotion-basedC2tothoseofshape-basedC2.Table6showstheperformanceofthevariousdescriptors.Ingeneralweﬁndthatshapecuesaloneperformmuchworsethanmotioncuesalone,andtheircombinationtendstoimproverecognitionperformanceverymoderately.Thiscombinationseemstoaffecttherecognitionoftheoriginalclipsratherthantherecognitionofthestabilizedclips.AnAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:55:30 UTC from IEEE Xplore.  Restrictions apply.

Table6.Averageperformanceforshapevs.motioncues.HOG/HOFHOGHOFHOGHOFOriginal20.44%15.01%17.95%Stabilized21.96%15.47%22.48%C2Motion+ShapeShapeMotionOriginal22.83%13.40%21.96%Stabilized23.18%13.44%22.73%earlierstudy[19]suggestedthat“localshapeandﬂowforasingleframeisenoughtorecognizeactions”.OurresultssuggestthatthestatementmightbetrueforsimpleactionsasisthecasefortheKTHdatasetbutmotioncuesdoseemtobemorepowerfulthanshapecuesfortherecognitionofcomplexactionsliketheonesintheHMDB51.6.ConclusionWedescribedanefforttoadvancetheﬁeldofactionrecognitionwiththedesignofwhatis,toourknowledge,currentlythelargestactiondataset.With51actioncat-egoriesandjustunder7,000videoclips,theproposedHMDB51isstillfarfromcapturingtherichnessandthefullcomplexityofvideoclipscommonlyfoundinthemoviesoronlinevideos.Howevergiventhelevelofperformanceofrepresentativestate-of-the-artcomputervisionalgorithmswithaccuracyabout23%,thisdatasetisarguablyagoodplacetostart(performanceontheCalTech-101databaseforobjectrecognitionstartedaround16%[6]).Further-moreourexhaustiveevaluationoftwostate-of-the-artsys-temssuggestthatperformanceisnotsigniﬁcantlyaffectedoverarangeoffactorssuchascamerapositionandmotionaswellasocclusions.Thissuggeststhatcurrentmethodsarefairlyrobustwithrespecttotheselow-levelvideodegra-dationsbutremainlimitedintheirrepresentativepowerinordertocapturethecomplexityofhumanactions.AcknowledgementsThispaperdescribesresearchdoneinpartattheCenterforBi-ological&ComputationalLearning,afﬁliatedwithMIBR,BCS,CSAILatMIT.ThisresearchwassponsoredbygrantsfromDARPA(IPTOandDSO),NSF(NSF-0640097,NSF-0827427),AFSOR-THRL(FA8650-05-C-7262).Additionalsupportwasprovidedby:Adobe,KingAbdullahUniversityScienceandTech-nologygranttoB.DeVore,NEC,SonyandbytheEugeneMc-DermottFoundation.ThisworkisalsodoneandsupportedbyBrownUniversity,CenterforComputationandVisualization,andtheRobertJ.andNancyD.CarneyFundforScientiﬁcInnova-tion,byDARPA(DARPA-BAA-09-31),andONR(ONR-BAA-11-001).H.K.wassupportedbyagrantfromtheMinistryofSci-ence,ResearchandtheArtsofBadenW¨urttemberg,Germany.References[1]http://serre-lab.clps.brown.edu/resources/HMDB/.1,2[2]http://server.cs.ucf.edu/˜vision/data.html.2[3]M.Blank,L.Gorelick,E.Shechtman,M.Irani,andR.Basri.Actionsasspace-timeshapes.ICCV,2005.1,2[4]J.Deng,W.Dong,R.Socher,L.Li,K.Li,andL.Fei-Fei.Imagenet:Alarge-scalehierarchicalimagedatabase.CVPR,2009.1[5]M.Everingham,L.VanGool,C.K.I.Williams,J.Winn,andA.Zisserman.ThePASCALVisualObjectClassesChallenge2010(VOC2010)results.http://www.pascal-network.org/challenges/voc/voc2010/workshop/index.html.1[6]L.Fei-Fei,R.Fergus,andP.Perona.Learninggenerativevisualmod-elsfromfewtrainingexamples:anincrementalbayesianapproachtestedon101objectcategories.CVPRWorkshoponGenerative-ModelBasedVision,2004.8[7]H.Jhuang,E.Garrote,J.Mutch,X.Yu,V.Khilnani,T.Poggio,A.D.Steele,andT.Serre.Automatedhome-cagebehavioralphenotypingofmice.NatureCommunications,1(5):1–9,2010.2[8]H.Jhuang,T.Serre,L.Wolf,andT.Poggio.Abiologicallyinspiredsystemforactionrecognition.ICCV,2007.2,5,6[9]G.Johansson,S.Bergstr¨om,andW.Epstein.Perceivingeventsandobjects.LawrenceErlbaumAssociates,1994.2[10]I.Laptev.Onspace-timeinterestpoints.Int.J.ofComput.Vision,64(2-3):107–123,2005.2[11]I.Laptev,M.Marszałek,C.Schmid,andB.Rozenfeld.Learningrealistichumanactionsfrommovies.CVPR,2008.2,5,6[12]J.Liu,J.Luo,andM.Shah.Recognizingrealisticactionsfromvideos”inthewild”.CVPR,2009.2[13]M.Marszałek,I.Laptev,andC.Schmid.Actionsincontext.CVPR,2009.2,7[14]J.Niebles,C.Chen,andL.Fei-Fei.Modelingtemporalstructureofdecomposablemotionsegmentsforactivityclassiﬁcation.ECCV,2010.2[15]A.OlivaandA.Torralba.Modelingtheshapeofthescene:Aholis-ticrepresentationofthespatialenvelope.Int.J.Comput.Vision,42:145–175,2001.4[16]M.Rodriguez,J.Ahmed,andM.Shah.Actionmach:Aspatio-temporalmaximumaveragecorrelationheightﬁlterforactionrecog-nition.CVPR,2008.2[17]B.Russell,A.Torralba,K.Murphy,andW.Freeman.Labelme:adatabaseandweb-basedtoolforimageannotation.Int.J.Comput.Vision,77(1):157–173,2008.1[18]J.M.S.Maji,L.Bourdev.Actionrecognitionfromadistributedrepresentationofposeandappearance.CVPR,2011.7[19]K.SchindlerandL.V.Gool.Actionsnippets:Howmanyframesdoeshumanactionrecognitionrequire.CVPR,2008.7,8[20]C.Schuldt,I.Laptev,andB.Caputo.Recognizinghumanactions:AlocalSVMapproach.ICPR,2004.1,2[21]T.Serre,L.Wolf,S.Bileschi,M.Riesenhuber,andT.Poggio.Robustobjectrecognitionwithcortex-likemechanisms.IEEETrans.PatternAnal.Mach.Intell.,29(3):411–26,2007.5[22]M.Thirkettle,C.Benton,andN.Scott-Samuel.Contributionsofform,motionandtasktobiologicalmotionperception.JournalofVision,9(3):1–11,2009.2[23]A.Torralba,R.Fergus,andW.Freeman.80milliontinyimages:Alargedatasetfornonparametricobjectandscenerecognition.IEEETrans.PatternAnal.Mach.Intell.,11(30):1958–1970,2008.1[24]H.Wang,M.Ullah,A.Kl¨aser,I.Laptev,andC.Schmid.Evalua-tionoflocalspatio-temporalfeaturesforactionrecognition.BMVC,2009.2,5,6[25]D.Weinland,E.Boyer,andR.Ronfard.Actionrecognitionfromarbitraryviewsusing3Dexemplars.ICCV,2007.1,2[26]D.Weinland,R.Ronfard,andE.Boyer.Asurveyofvision-basedmethodsforactionrepresentation,segmentationandrecognition.Comput.Vis.ImageUnd.,115(2):224–241,2010.1[27]J.Xiao,J.Hays,K.Ehinger,A.Oliva,andA.Torralba.Sundatabase:Large-scalescenerecognitionfromabbeytozoo.CVPR,2010.1[28]B.YaoandL.Fei-Fei.Grouplet:Astructuredimagerepresentationforrecognizinghumanandobjectinteractions.CVPR,2010.7

