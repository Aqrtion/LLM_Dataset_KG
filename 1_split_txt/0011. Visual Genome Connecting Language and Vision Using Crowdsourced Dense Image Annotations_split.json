{
    "title_author_abstract_introduction": "Noname manuscript No. (will be inserted by the editor)\nVisual Genome Connecting Language and Vision Using Crowdsourced Dense Image Annotations\nRanjay Krishna · Yuke Zhu · Oliver Groth · Justin Johnson · Kenji Hata · Joshua Kravitz · Stephanie Chen · Yannis Kalantidis · Li-Jia Li · David A. Shamma · Michael S. Bernstein · Li Fei-Fei\nReceived: date / Accepted: date\nAbstract Despite progress in perceptual tasks such as image classiﬁcation, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in\nRanjay Krishna Stanford University, Stanford, CA, USA E-mail: ranjaykrishna@cs.stanford.edu\nYuke Zhu Stanford University, Stanford, CA, USA\nOliver Groth Dresden University of Technology, Dresden, Germany\nJustin Johnson Stanford University, Stanford, CA, USA\nKenji Hata Stanford University, Stanford, CA, USA\nJoshua Kravitz Stanford University, Stanford, CA, USA\nStephanie Chen Stanford University, Stanford, CA, USA\nYannis Kalantidis Yahoo Inc., San Francisco, CA, USA\nLi-Jia Li Snapchat Inc., Los Angeles, CA, USA\nDavid A. Shamma Yahoo Inc., San Francisco, CA, USA\nMichael S. Bernstein Stanford University, Stanford, CA, USA\nLi Fei-Fei Stanford University, Stanford, CA, USA\nan image. When asked “What vehicle is the person riding?”, computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that “the person is riding a horse-drawn carriage.”\nIn this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Speciﬁ- cally, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.\nKeywords Computer Vision · Dataset · Image · Scene Graph · Question Answering · Objects · Attributes · Relationships · Knowledge · Language · Crowdsourcing\n1 Introduction\nA holy grail of computer vision is the complete understanding of visual scenes: a model that is able to name and detect objects, describe their attributes, and recognize their relationships and interactions. Understanding scenes would enable important applications such as image search, question answering, and robotic interactions. Much progress has been made in recent including image classiﬁcayears towards this goal, tion (Deng et al., 2009, Perronnin et al., 2010, Simonyan and Zisserman, 2014, Krizhevsky et al., 2012, Szegedy\nRanjay Krishna et al.\nFig. 1: An overview of the data needed to move from perceptual awareness to cognitive understanding of images. We present a dataset of images densely annotated with numerous region descriptions, objects, attributes, and relationships. Region descriptions (e.g. “girl feeding large elephant” and “a man taking a picture behind girl”) are shown (top). The objects (e.g. elephant), attributes (e.g. large) and relationships (e.g. feeding) are shown (bottom). Our dataset also contains image related question answer pairs (not shown).\net al., 2014) and object detection (Everingham et al., 2010, Girshick et al., 2014, Sermanet et al., 2013, Girshick, 2015, Ren et al., 2015b). An important contributing factor is the availability of a large amount of data that drives the statistical models that underpin today’s advances in computational visual understanding. While the progress is exciting, we are still far from reaching the goal of comprehensive scene understanding. As Figure 1 shows, existing models would be able to detect discreet objects in a photo but would not be able to explain their interactions or the relationships between them. Such explanations tend to be cognitive in nature, integrating perceptual information into conclusions about the relationships between objects in a scene (Bruner, 1990, Firestone and Scholl, 2015). A cognitive understanding of our visual world thus requires that we complement computers’ ability to detect objects with abilities to describe those objects (Isola et al., 2015) and\nunderstand their interactions within a scene (Sadeghi and Farhadi, 2011).\nThere is an increasing eﬀort to put together the next generation of datasets to serve as training and benchmarking datasets for these deeper, cognitive scene understanding and reasoning tasks, the most notable being MS-COCO (Lin et al., 2014) and VQA (Antol et al., 2015). The MS-COCO dataset consists of 300K real-world photos collected from Flickr. For each image, there is pixel-level segmentation of 91 object classes (when present) and 5 independent, user-generated sentences describing the scene. VQA adds to this a set of 614K question-answer pairs related to the visual contents of each image (see more details in Section 3.1). With this information, MS-COCO and VQA provide a fertile training and testing ground for models aimed at tasks for accurate object detection, segmentation, and summary-level image captioning (Kiros et al., 2014,\nVisual Genome\nMao et al., 2014, Karpathy and Fei-Fei, 2014, Vinyals et al., 2014) as well as basic QA (Ren et al., 2015a, Antol et al., 2015, Malinowski et al., 2015, Gao et al., 2015, Malinowski and Fritz, 2014). For example, a stateof-the-art model (Karpathy and Fei-Fei, 2014) provides a description of one MS-COCO image in Figure 1 as “two men are standing next to an elephant.” But what is missing is the further understanding of where each object is, what each person is doing, what the relationship between the person and elephant is, etc. Without such relationships, these models fail to diﬀerentiate this image from other images of people next to elephants.\nTo understand images thoroughly, we believe three key elements need to be added to existing datasets: a grounding of visual concepts to language (Kiros et al., 2014), a more complete set of descriptions and QAs for each image based on multiple image regions (Johnson et al., 2015), and a formalized representation of the components of an image (Hayes, 1978). In the spirit of mapping out this complete information of the visual world, we introduce the Visual Genome dataset. The ﬁrst release of the Visual Genome dataset uses 108,249 images from the intersection of the YFCC100M (Thomee et al., 2016) and MS-COCO (Lin et al., 2014). Section 5 provides a more detailed description of the dataset. We highlight below the motivation and contributions of the three key elements that set Visual Genome apart from existing datasets.\nThe Visual Genome dataset regards relationships and attributes as ﬁrst-class citizens of the annotation space, in addition to the traditional focus on objects. Recognition of relationships and attributes is an important part of the complete understanding of the visual scene, and in many cases, these elements are key to the story of a scene (e.g., the diﬀerence between “a dog chasing a man” versus “a man chasing a dog”). The Visual Genome dataset is among the ﬁrst to provide a detailed labeling of object interactions and attributes, grounding visual concepts to language1.\nAn image is often a rich scenery that cannot be fully described in one summarizing sentence. The scene in Figure 1 contains multiple “stories”: “a man taking a photo of elephants,” “a woman feeding an elephant,” “a river in the background of lush grounds,” etc. Existing datasets such as Flickr 30K (Young et al., 2014) and MS-COCO (Lin et al., 2014) focus on high-level descriptions of an image2. Instead, for each image in the Visual Genome dataset, we collect more than 42 descriptions for diﬀerent regions in the image, providing a much denser and complete set of descriptions of the scene. In addition, inspired by VQA (Antol et al.,\n1 The Lotus Hill Dataset (Yao et al., 2007) also provides a\nsimilar annotation of object relationships, see Sec 3.1.",
    "data_related_paragraphs": [
        "With a set of dense descriptions of an image and the explicit correspondences between visual pixels (i.e. bounding boxes of objects) and textual descriptors (i.e. relationships, attributes), the Visual Genome dataset is poised to be the ﬁrst image dataset that is capable of providing a structured formalized representation of an image, in the form that is widely used in knowledge base representations in NLP (Zhou et al., 2007, GuoDong et al., 2005, Culotta and Sorensen, 2004, Socher et al., 2012). For example, in Figure 1, we can formally express the relationship holding between the woman and food as holding(woman, food)). Putting together all the objects and relations in a scene, we can represent each image as a scene graph (Johnson et al., 2015). The scene graph representation has been shown to improve semantic image retrieval (Johnson et al., 2015, Schuster et al., 2015) and image captioning (Farhadi et al., 2009, Chang et al., 2014, Gupta and Davis, 2008). Furthermore, all objects, attributes and relationships in each image in the Visual Genome dataset are canonicalized to its corresponding WordNet (Miller, 1995) ID (called a synset ID). This mapping connects all images in Visual Genome and provides an eﬀective way to consistently query the same concept (object, attribute, or relationship) in the dataset. It can also potentially help train models that can learn from contextual information from multiple images.",
        "In this paper, we introduce the Visual Genome dataset with the aim of training and benchmarking the next generation of computer models for comprehensive scene understanding. The paper proceeds as follows: In Section 2, we provide a detailed description of each component of the dataset. Section 3 provides a literature review of related datasets as well as related recognition tasks. Section 4 discusses the crowdsourcing strategies we deployed in the ongoing eﬀort of collecting this dataset. Section 5 is a collection of data analysis statistics, showcasing the key properties of the Visual Genome dataset. Last but not least, Section 6 provides a set of experimental results that use Visual Genome as a benchmark.",
        "Further visualizations, API, and additional information on the Visual Genome dataset can be found online3.",
        "Fig. 2: An example image from the Visual Genome dataset. We show 3 region descriptions and their corresponding region graphs. We also show the connected scene graph collected by combining all of the image’s region graphs. The top region description is “a man and a woman sit on a park bench along a river.” It contains the objects: man, woman, bench and river. The relationships that connect these objects are: sits on(man, bench), in front of (man, river), and sits on(woman, bench).",
        "Fig. 3: An example image from our dataset along with its scene graph representation. The scene graph contains objects (child, instructor, helmet, etc.) that are localized in the image as bounding boxes (not shown). These objects also have attributes: large, green, behind, etc. Finally, objects are connected to each other through relationships: wears(child, helmet), wears(instructor, jacket), etc.",
        "Fig. 4: A representation of the Visual Genome dataset. Each image contains region descriptions that describe a localized portion of the image. We collect two types of question answer pairs (QAs): freeform QAs and region-based QAs. Each region is converted to a region graph representation of objects, attributes, and pairwise relationships. Finally, each of these region graphs are combined to form a scene graph with all the objects grounded to the image. Best viewed in color",
        "2 Visual Genome Data Representation",
        "The Visual Genome dataset consists of seven main components: region descriptions, objects, attributes, relationships, region graphs, scene graphs, and questionanswer pairs. Figure 4 shows examples of each component for one image. To enable research on comprehensive understanding of images, we begin by collecting descriptions and question answers. These are raw texts without any restrictions on length or vocabulary. Next, we extract objects, attributes and relationships from our descriptions. Together, objects, attributes and relationships fabricate our scene graphs that represent a formal representation of an image. In this section, we break down Figure 4 and explain each of the seven components. In Section 4, we will describe in more detail how data from each component is collected through a crowdsourcing platform.",
        "In a real-world image, one simple summary sentence is often insuﬃcient to describe all the contents of and interactions in an image. Instead, one natural way to extend this might be a collection of descriptions based on diﬀerent regions of a scene. In Visual Genome, we collect human-generated image region descriptions, with each region localized by a bounding box. In Figure 5, we show three examples of region descriptions. Regions are allowed to have a high degree of overlap with each other when the descriptions diﬀer. For example, “yellow ﬁre hydrant” and “woman in shorts is standing behind the man” have very little overlap, while “man jumping over ﬁre hydrant” has a very high overlap with the other two regions. Our dataset contains on average a total of 42 region descriptions per image. Each description is a phrase ranging from 1 to 16 words in length describing that region.",
        "Each image in our dataset consists of an avarege of 21 objects, each delineated by a tight bounding box (Figure 6). Furthermore, each object is canonicalized to a synset ID in WordNet (Miller, 1995). For example, man and person would get mapped to man.n.03 (the generic use of the word to refer to any human being). Similarly, person gets mapped to person.n.01 (a human being). Afterwards, these two concepts can be joined to person.n.01 since this is a hypernym of man.n.03. This is an standardization step to avoid multiple important",
        "Fig. 5: To describe all the contents of and interactions in an image, the Visual Genome dataset includes multiple human-generated image regions descriptions, with each region localized by a bounding box. Here, we show three regions descriptions: “man jumping over a ﬁre hydrant,” “yellow ﬁre hydrant,” and “woman in shorts is standing beghind the man.”",
        "Fig. 8: Our dataset also captures the relationships and interactions between objects in our images. In this example, we show the relationship jumping over between the objects man and fire hydrant.",
        "over on the object fire hydrant. Each relationship is canonicalized to a WordNet (Miller, 1995) synset ID; i.e. jumping is canonicalized to jump.a.1 (move forward by leaps and bounds). On average, each image in our dataset contains 18 relationships.",
        "We have two types of QA pairs associated with each image in our dataset: freeform QAs, based on the entire image, and region-based QAs, based on selected regions of the image. We collect 6 diﬀerent types of questions per image: what, where, how, when, who, and why. In Figure 4, “Q. What is the woman standing next to?; A. Her belongings” is a freeform QA. Each image has at least one question of each type listed above. Regionbased QAs are collected by prompting workers with region descriptions. For example, we use the region “yellow ﬁre hydrant” to collect the region-based QA: “Q. What color is the ﬁre hydrant?; A. Yellow.” Region based QAs allow us to independently study methods that use NLP and vision priors to answer questions.",
        "We discuss existing datasets that have been released and used by the vision community for classiﬁcation and object detection. We also mention work that has improved object and attribute detection models. Then, we explore existing work that has utilized representations similar to our relationships between objects. In addition, we dive into literature related to cognitive tasks like image description, question answering, and knowledge representation.",
        "3.1 Datasets",
        "Datasets (Table 1) have been growing in size as researchers have begun tackling increasingly complicated problems. Caltech 101 (Fei-Fei et al., 2007) was one of the ﬁrst datasets hand-curated for image classiﬁcation, with 101 object categories and 15-30 of examples per category. One of the biggest criticisms of Caltech 101 was the lack of variability in its examples. Caltech 256 (Griﬃn et al., 2007) increased the number of categories to 256, while also addressing some of the shortcomings of Caltech 101. However, it still had only a handful of examples per category, and most of its images contained only a single object. LabelMe (Russell",
        "et al., 2008) introduced a dataset with multiple objects per category. They also provided a web interface that experts and novices could use to annotate additional images. This web interface enabled images to be labeled with polygons, helping create datasets for image segmentation. The Lotus Hill dataset (Yao et al., 2007) contains a hierarchical decomposition of objects (vehicles, man-made objects, animals, etc.) along with segmentations. Only a small part of this dataset is freely available. SUN (Xiao et al., 2010), just like LabelMe (Russell et al., 2008) and Lotus Hill (Yao et al., 2007), was curated for object detection. Pushing the size of datasets even further, 80 Million Tiny Images (Torralba et al., 2008) created a signiﬁcantly larger dataset than its predecessors. It contains tiny (i.e. 32 × 32 pixels) images that were collected using WordNet (Miller, 1995) synsets as queries. However, because the data in 80 Million Images were not human-veriﬁed, they contain numerous errors. YFCC100M (Thomee et al., 2016) is another large database of 100 million images that is still largely unexplored. It contains human generated and machine generated tags.",
        "Pascal VOC (Everingham et al., 2010) pushed research from classiﬁcation to object detection with a dataset containing 20 semantic categories in 11,000 images. Imagenet (Deng et al., 2009) took WordNet synsets and crowdsourced a large dataset of 14 million images. They started the ILSVRC (Russakovsky et al., 2015) challenge for a variety of computer vision tasks. ILSVRC and PASCAL provide a test bench for object detection, image classiﬁcation, object segmentation, person layout, and action classiﬁcation. MSCOCO (Lin et al., 2014) recently released its dataset, with over 328,000 images with sentence descriptions and segmentations of 91 object categories. The current largest dataset for QA, VQA (Antol et al., 2015), contains 204,721 images annotated with one or more question answers. They collected a dataset of 614,163 freeform questions with 6.1M ground truth answers and provided a baseline approach in answering questions using an image and a textual question as the input.",
        "Visual Genome aims to bridge the gap between all these datasets, collecting not just annotations for a large number of objects but also scene graphs, region descriptions, and question answer pairs for image regions. Unlike previous datasets, which were collected for a single task like image classiﬁcation, the Visual Genome dataset was collected to be a general-purpose representation of the visual world, without bias toward a particular task. Our images contain an average of 21 objects, which is almost an order of magnitude more dense than any existing vision dataset. Similarly, we contain an average of 18 attributes and 18 relationships",
        "per image. We also have an order of magnitude more unique objects, attributes, and relationships than any other dataset. Finally, we have 1.7 million question answer pairs, also larger than any other dataset for visual question answering.",
        "One of the core contributions of Visual Genome is its descriptions for multiple regions in an image. As such, we mention other image description datasets and models in this subsection. Most work related to describing images can be divided into two categories: retrieval of human-generated captions and generation of novel captions. Methods in the ﬁrst category use similarity metrics between image features from predeﬁned models to retrieve similar sentences (Ordonez et al., 2011, Hodosh et al., 2013). Other methods map both sentences and their images to a common vector space (Ordonez et al., 2011) or map them to a space of triples (Farhadi et al., 2010). Among those in the second category, a common theme has been to use recurrent neural networks to produce novel captions (Kiros et al., 2014, Mao et al., 2014, Karpathy and Fei-Fei, 2014, Vinyals et al., 2014). More recently, researchers have also used a visual attention model (Xu et al., 2015).",
        "One drawback of these approaches is their attention to describing only the most salient aspect of the image. This problem is ampliﬁed by datasets like Flickr 30K (Young et al., 2014) and MS-COCO (Lin et al., 2014), whose sentence desriptions tend to focus, somewhat redundantly, on these salient parts. For example, “an elephant is seen wandering around on a sunny day,” “a large elephant in a tall grass ﬁeld,” and “a very large elephant standing alone in some brush” are 3 descriptions from the MS-COCO dataset, and all of them focus on the salient elephant in the image and ignore the other regions in the image. Many real-world scenes are complex, with multiple objects and interactions that are best described using multiple descriptions (Karpathy and Fei-Fei, 2014, Lebret et al., 2015). Our dataset pushes toward a complete understanding of an image by collecting a dataset in which we capture not just scene-level descriptions but also myriad of low-level descriptions, the “grammar” of the scene.",
        "by self-driving cars on the road. It involves classifying an object into a semantic category and localizing the object in the image. Visual Genome uses objects as a core component on which each visual scene is built. Early datasets include the face detectio (Huang et al., 2008) and pedestrian datasets (Dollar et al., 2012). The PASCAL VOC and ILSVRC’s detection dataset (Deng et al., 2009) pushed research in object detection. But the images in these datasets are iconic and do not capture the settings in which these objects usually co-occur. To remedy this problem, MS-COCO (Lin et al., 2014) annotated real-world scenes that capture object contexts. However, MS-COCO was unable to describe all the objects in its images, since they annotated only 91 object categories. In the real world, there are many more objects that the ones captured by existing datasets. Visual Genome aims at collecting annotations for all visual elements that occur in images, increasing the number of semantic categories to over 17,000.",
        "The inclusion of attributes allows us to describe, compare, and more easily categorize objects. Even if we haven’t seen an object before, attributes allow us to infer something about it; for example, “yellow and brown spotted with long neck” likely refers to a giraﬀe. Initial work in this area involved ﬁnding objects with similar features (Malisiewicz et al., 2008) using examplar SVMs. Next, textures were used to study objects (Varma and Zisserman, 2005), while other methods learned to predict colors (Ferrari and Zisserman, 2007). Finally, the study of attributes was explicitly demonstrated to lead to improvements in object classiﬁ- cation (Farhadi et al., 2009). Attributes were deﬁned to be paths (“has legs”), shapes (“spherical”), or materials (“furry”) and could be used to classify new categories of objects. Attributes have also played a large role in improving ﬁne-grained recognition (Goering et al., 2014) on ﬁne-grained attribute datasets like CUB-2011 (Wah et al., 2011). In Visual Genome, we use a generalized formulation (Johnson et al., 2015), but we extend it such that attributes are not image-speciﬁc binaries but rather object-speciﬁc for each object in a real-world scene. We also extend the types of attributes to include size (“small”), pose (“bent”), state (“transparent”), emotion (“happy”), and many more.",
        "Visual question answering (QA) has been recently proposed as a proxy task of evaluating a computer vision system’s ability to understand an image beyond object recognition (Geman et al., 2015, Malinowski and Fritz, 2014). Several visual QA benchmarks have been proposed in the last few months. The DAQUAR (Malinowski and Fritz, 2014) dataset was the ﬁrst toy-sized QA benchmark built upon indoor scene RGB-D images of NYU Depth v2 (Nathan Silberman and Fergus, 2012). Most new datasets (Yu et al., 2015, Ren et al., 2015a, Antol et al., 2015, Gao et al., 2015) have collected QA pairs on MS-COCO images, either generated",
        "In previous datasets, most questions concentrated on simple recognition-based questions about the salient objects, and answers were often extremely short. For instance, 90% of DAQUAR answers (Malinowski and Fritz, 2014) and 87% of VQA answers (Antol et al., 2015) consist of single-word object names, attributes, and quantities. This shortness limits their diversity and fails to capture the long-tail details of the images. Given the availability of new datasets, an array of visual QA models have been proposed to tackle QA tasks. The proposed models range from SVM classiﬁers (Antol et al., 2015) and probabilistic inference (Malinowski and Fritz, 2014) to recurrent neural networks (Gao et al., 2015, Malinowski et al., 2015, Ren et al., 2015a) and convolutional networks (Ma et al., 2015). Visual Genome aims to capture the details of the images with diverse question types and long answers. These questions should cover a wide range of visual tasks from basic perception to complex reasoning. Our QA dataset of 1.7 million QAs is also larger than any currently existing dataset.",
        "Visual Genome was collected and veriﬁed entirely by crowd workers from Amazon Mechanical Turk. In this section, we outline the pipeline employed in creating all the components of the dataset. Each component (region descriptions, objects, attributes, relationships, region graphs, scene graphs, questions and answers) involved multiple task stages. We mention the diﬀerent strategies used to make our data accurate and to enforce diversity in each component. We also provide background information about the workers who helped make Visual Genome possible.",
        "We used Amazon Mechanical Turk (AMT) as our primary source of annotations. Overall, a total of over 33,000 unique workers contributed to the dataset. The dataset was collected over the course of 6 months after 15 months of experimentation and iteration on the data representation. Approximately 800,000 Human Intelligence Tasks (HITs) were launched on AMT, where each HIT involved creating descriptions, questions and answers, or region graphs. Each HIT was designed such that workers manage to earn anywhere between $6-$8 per hour if they work continuously, in line with ethical research standards on Mechanical Turk (Salehi et al., 2015). Visual Genome HITs achieved a 94.1% retention rate, meaning that 94.1% of workers who completed one of our tasks went ahead to do more. Table 2 outlines the percentage distribution of the locations of the workers. 93.02% of workers contributed from the United States.",
        "Visual Genome’s main goal is to enable the study of cognitive computer vision tasks. The next step towards understanding images requires studying relationships between objects in scene graph representations of images. However, we observed that collecting scene graphs directly from an image leads to workers annotating easy, frequently-occurring relationships like wearing(man, shirt) instead of focusing on salient parts of the image. This is evident from previous datasets (Johnson et al., 2015, Lu et al., 2016) that contain a large number of such relationships. After experimentation, we observed that when asked to describe an image using natural language, crowd workers naturally start with the most salient part of the image and then move to describing other parts of the image one by one. Inspired by this ﬁnding, we focused our attention towards collecting a dataset of region descriptions that is diverse in content.",
        "2. Global image descriptions. A list of the top 100 most common written descriptions of all images in the dataset. This prevents very common phrases like “sky is blue” from dominating the set of region descriptions.",
        "All Visual Genome data go through a veriﬁcation stage as soon as they are annotated. This stage helps eliminate incorrectly labeled objects, attributes, and relationships. It also helps remove region descriptions and questions and answers that might be correct but are vague (“This person seems to enjoy the sun.”), subjective (“room looks dirty”), or opinionated (“Being exposed to hot sun like this may cause cancer”).",
        "Veriﬁcation is conducted using two separate strategies: majority voting (Snow et al., 2008) and rapid judgments (Krishna et al., 2016). All components of the dataset except objects are veriﬁed using majority voting. Majority voting (Snow et al., 2008) involves three unique workers looking at each annotation and vot-",
        "ing on whether it is factually correct. An annotation is added to our dataset if at least two (a majority) out of the three workers verify that it is correct.",
        "We only use rapid judgments to speed up the veriﬁcation of the objects in our dataset. Meanwhile, rapid judgments (Krishna et al., 2016) use an interface inspired by rapid serial visual processing that enable veriﬁcation of objects with an order of magnitude increase in speed than majority voting.",
        "All the descriptions and QAs that we collect are freeform worker-generated texts. They are not constrained by any limitations. For example, we do not force workers to refer to a man in the image as a man. We allow them to choose to refer to the man as person, boy, man, etc. This ambiguity makes instances of man from our it diﬃcult to collect all dataset. In order to reduce the ambiguity in the concepts of our dataset and connect it to other resources used by the research community, we map all objects, attributes, relationships, and noun phrases in region descriptions and QAs to synsets in WordNet (Miller, 1995). In the example above, person, boy, and man would map to the synsets: person.n.01 (a human being), male child.n.01 (a youthful male person) and man.n.03 (the generic use of the word to refer to any human being) respectively. Thanks to the WordNet hierarchy it is now possible to fuse those three expressions of into person.n.01 (a human the same concept being) since this is the lowest common ancestor node of all aforementioned synsets.",
        "We use the Stanford NLP tools (Manning et al., 2014) to extract the noun phrases from the region descriptions and QAs. Next, we map them to their most frequent matching synset in WordNet according to WordNet lexeme counts. We then reﬁne this simple heuristic by hand-crafting mapping rules for the 30 most common failure cases. For example according to WordNet’s lexeme counts the most common semantic for “table” is table.n.01 (a set of data arranged in rows and columns). However in our data it is more likely to see pieces of furniture and therefore bias the mapping towards table.n.02 (a piece of furniture having a smooth flat top that is usually supported by one or more vertical legs). The objects in our scene graphs are already noun phrases and are mapped to WordNet in the same way.",
        "Fig. 13: A distribution of the top 25 image synsets in the Visual Genome dataset. A variety of synsets are well represented in the dataset, with the top 25 synsets having at least 800 example images each.",
        "5 Dataset Statistics and Analysis",
        "In this section, we provide statistical insights and analysis for each component of Visual Genome. Speciﬁcally, we examine the distribution of images (Section 5.1) and the collected data for region descriptions (Section 5) and questions and answers (Section 5.7). We analyze region graphs and scene graphs together in one section (Section 5.6), but we also break up these graph structures into their three constituent parts—objects (Section 5.3), attributes (Section 5.4), and relationships (Section 5.5)—and study each part individually. Finally, we describe our canonicalization pipeline and results (Section 5.8).",
        "Fig. 14: (a) An example image from the dataset with its region descriptions. We only display localizations for 6 of the 42 descriptions to avoid clutter; all 50 descriptions do have corresponding bounding boxes. (b) All 42 region bounding boxes visualized on the image.",
        "The Visual Genome dataset consists of all 108,249 images from the intersection of MS-COCO’s (Lin et al., 2014) 328,000 images and YFCC’s (Thomee et al., 2016) 100 million images. These images are real-world, non-iconic images that were uploaded onto Flickr by users. The images range from as small as 72 pixels wide to as large as 1280 pixels wide, with an average width",
        "of 500 pixels. We collected the WordNet synsets into which our 108,249 images can be categorized using the same method as ImageNet (Deng et al., 2009). Visual Genome images cover 972 synsets. Figure 13 shows the top synsets to which our images belong. “ski” is the most common synset, with 2612 images; it is followed by “ballplayer” and “racket,” with all three synsets referring to images of people playing sports. Our dataset is somewhat biased towards images of people, as Figure 13 shows; however, they are quite diverse overall, as the top 25 synsets each have over 800 images, while the top 50 synsets each have over 500 examples.",
        "age of 42 regions with a bounding box and a descriptive phrase. Figure 14 shows an example image from our dataset with its 50 region descriptions. We display bounding boxes for only 6 out of the 50 descriptions in the ﬁgure to avoid clutter. These descriptions tend to be highly diverse and can focus on a single object, like in “A bag,” or on multiple objects, like in “Man taking a photo of the elephants.” They encompass the most salient parts of the image, as in “An elephant taking food from a woman,” while also capturing the background, as in “Small buildings surrounded by trees.”",
        "MS-COCO (Lin et al., 2014) dataset is good at generating variations on a single scene-level descriptor. Consider three sentences from MS-COCO dataset on a similar image: “there is a person petting a very large elephant,” “a person touching an elephant in front of a wall,” and “a man in white shirt petting the cheek of an elephant.” These three sentences are single scenelevel descriptions. In comparison, Visual Genome descriptions emphasize diﬀerent regions in the image and",
        "After examining the distribution of the size of the regions described, it is also valuable to look at the semantic information captured by these descriptions. In Figure 16, we show the distribution of the length (word count) of these region descriptions. The average word count for a description is 5 words, with a minimum of 1 word and a maximum of 12 words. In Figure 18 (a), we plot the most common phrases occurring in our region descriptions, with stop words removed. Common visual elements like “green grass,” “tree [in] distance,” and “blue sky” occur much more often than other, more nuanced elements like “fresh strawberry.” We also study descriptions with ﬁner precision in Figure 18 (b), where we plot the most common words used in descriptions. Again, we eliminate stop words from our study. Colors like “white” and “black” are the most frequently used words to describe visual concepts; we conduct a similar study on other captioning datasets including MSCOCO (Lin et al., 2014) and Flickr 30K (Young et al., 2014) and ﬁnd a similar distribution with colors occur-",
        "Finally, we also compare the descriptions in Visual Genome to the captions in MS-COCO. First we aggregate all Visual Genome and MS-COCO descriptions and remove all stop words. After removing stop words, the descriptions from both datasets are roughly the same length. We conduct a similar study, in which we vectorize the descriptions for each image and calculate each dataset’s cluster diversity per image. We ﬁnd that on average, 2 clusters are represented in the captions for each image in MS-COCO, with very few images in which 5 clusters are represented. Because each image in MS-COCO only contains 5 captions, it is not a fair comparison to compare the number of clusters represented in all the region descriptions in the Visual Genome dataset. We thus randomly sample 5 Visual Genome region descriptions per image and calculate the number of clusters in an image. We ﬁnd that Visual Genome descriptions come from 4 or 5 clusters. We show our comparison results in Figure 19 (c). The diﬀerence between the semantic diversity between the two datasets is statistically signiﬁcant (t = −240, p < 0.01).",
        "Fig. 19: (a) Example illustration showing four clusters of region descriptions and their overall themes. Other clusters not shown due to limited space. (b) Distribution of images over number of clusters represented in each image’s region descriptions. (c) We take Visual Genome with 5 random descriptions taken from each image and MS-COCO dataset with all 5 sentence descriptions per image and compare how many clusters are represented in the descriptions. We show that Visual Genome’s descriptions are more varied for a given image, with an average of 4 clusters per image, while MS-COCO’s images have an average of 3 clusters per image.",
        "In comparison to related datasets, Visual Genome fares well in terms of object density and diversity. Visual Genome contains approximately 21 objects per image, exceeding ImageNet (Deng et al., 2009), PASCAL (Everingham et al., 2010), MS-COCO (Lin et al., 2014), and other datasets by large margins. As shown in Figure 21, there are more object categories represented in Visual Genome than in any other dataset. This comparison is especially pertinent with regards to Microsoft MS-COCO (Lin et al., 2014), which uses the same images as Visual Genome. The lower count of objects per category is a result of our higher number of categories. For a fairer comparison with ILSVRC 2014 Detection (Russakovsky et al., 2015), Visual Genome has about 2239 objects per category when only the top 200 categories are considered, which is comparable to ILSVRC’s 2671.5 objects per category. For a fairer comparison with MS-COCO, Visual Genome has about 3768 objects per category when only the top 91",
        "Fig. 21: Comparison of object diversity between various datasets. Visual Genome far surpasses other datasets in terms of number of object categories.",
        "Objects in Visual Genome come from a variety of categories. As shown in Figure 22 (b), objects related to WordNet categories such as humans, animals, sports, and scenery are most common; this is consistent with the general bias in image subject matter in our dataset. Common objects like man, person, and woman occur especially frequently with occurrences of 24K, 17K, and 11K. Other objects that also occur in MS-COCO (Lin et al., 2014) are also well represented with around 5000 instances on average. Figure 22 (a) shows some examples of objects in images. Objects in Visual Genome span a diverse set of Wordnet categories like food, animals, and man-made structures.",
        "Table 3: Comparison of Visual Genome objects and categories to related datasets.",
        "Fig. 22: (a) Examples of objects in Visual Genome. Each object is localized in its image with a tightly drawn bounding box. (b) Plot of the most frequently occurring objects in images. People are the most frequently occurring objects in our dataset, followed by common objects and visual elements like building, shirt, and sky.",
        "Attributes allow for detailed description and disambiguation of objects in our dataset. About 45% of objects in Visual Genome are annotated with at least one attribute; our dataset contains 1.6 million total attributes with 13,041 unique attributes. Attributes include colors (green), sizes (tall), continuous action verbs (standing), materials (plastic), etc. Each attribute in our scene graphs belongs to one object, while each object can have multiple attributes. We denote attributes as attribute(object).",
        "On average, each image in Visual Genome contains 21 attributes, as shown in Figure 23. Each region contains on average 1 attribute, though about 42% of regions contain no attribute at all; this is primarily because many regions are relationship-focused. Figure 24 (a) shows the distribution of the most common attributes in our dataset. Colors (e.g. white, green) are by far the most frequent attributes. Also common are sizes (e.g. large) and materials (e.g. wooden). Figure 24 (b) shows the distribution of attributes describing people (e.g. man, girls, and person). The most common attributes describing people are intransitive verbs describing their states of motion (e.g.standing and walking). Certain sports (skiing, surfboarding) are overrepresented due to a bias towards these sports in our images.",
        "Attribute Graphs. We also qualitatively analyze the attributes in our dataset by constructing co-occurrence graphs, in which nodes are unique attributes and edges connect those attributes that describe the same object. For example, if an image contained a “large black dog” (large(dog), black(dog)) and another image contained a “large yellow cat” (large(cat), yellow(cat)), its attributes would form an incomplete graph with edges (large, black) and (large, yellow). We create two such graphs: one for both the total set of attributes and a second where we consider only objects that refer to people. A subgraph of the 16 most frequently connected (co-occurring) personrelated attributes is shown in Figure 25 (a).",
        "Cliques in these graphs represent groups of attributes in which at least one co-occurrence exists for each pair of attributes in that group. In the previous example, if a third image contained a “black and yellow taxi” (black(taxi), yellow(taxi)), the resulting third edge would create a clique between the attributes black, large, and yellow. When calculated across the entire Visual Genome dataset, these cliques provide insight into commonly perceived traits of different types of objects. Figure 25 (b) is a selected representation of three example cliques and their overlaps.",
        "Fig. 24: (a) Distribution showing the most common attributes in the dataset. Colors (white, red) and materials (wooden, metal) are the most common. (b) Distribution showing the number of attributes describing people. State-of-motion verbs (standing, walking) are the most common, while certain sports (skiing, surfing) are also highly represented due to an image source bias in our image set.",
        "Top relationship distributions. We display the most frequently occurring relationships in Figure 27 (a). on is the most common relationship in our dataset. This is primarily because of the ﬂexibility of the word on, which can refer to spatial conﬁguration (on top of), attachment (hanging on), etc. Other common relationships involve actions like holding and wearing and spatial conﬁgurations like behind, next to, and under. Figure 27 (b) shows a similar distribution but for relationships involving people. Here we notice more human-centric relationships or actions such as kissing, chatting with, and talking to. The two distributions follow a Zipf distribution.",
        "Related work comparison. It is also worth mentioning in this section some prior work on relationships. The concept of visual relationships has already been explored in Visual Phrases (Sadeghi and Farhadi, 2011), who introduced a dataset of 17 such relationships such as next to(person, bike) and riding(person, horse). However, their dataset is limited to just these 17 relationships. Similarly, the MS-COCO-a dataset (Ruggero Ronchi and Perona, 2015) introduced 140 actions that humans performed in MS-COCO’s dataset (Lin et al., 2014). However, their dataset is limited to just actions, while our relationships are more general and numerous, with over 13K unique relationships. Finally, VisKE (Sadeghi et al., 2015) introduced 6500 relationships, but in a much smaller dataset of images than Visual Genome.",
        "Fig. 27: (a) A sample of the most frequent relationships in our dataset. In general, the most common relationships are spatial (on top of, on side of, etc.). (b) A sample of the most frequent relationships involving humans in our dataset. The relationships involving people tend to be more action oriented (walk, speak, run, etc.).",
        "We introduce in this paper the largest dataset of scene graphs to date. We use these graph representations of images as a deeper understanding of the visual world. In this section, we analyze the properties of these representations, both at the region level through region graphs and at the image level through scene graphs. We also",
        "brieﬂy explore other datasets with scene graphs and provide aggregate statistics on our entire dataset.",
        "Fig. 29: Example QA pairs in the Visual Genome dataset. Our QA pairs cover a spectrum of visual tasks from recognition to high-level reasoning.",
        "Objects, attributes, and relationships occur as a normal distribution in our data. Table 4 shows that in a region graph, there are an average of 0.43 objects, 0.41 attributes, and 0.45 relationships. Each scene graph and consequently each image has average of 21.26 objects, 16.21 attributes, and 18.67 relationships.",
        "We now analyze the diversity and quality of our questions and answers. Our goal is to construct a largescale visual question answering dataset that covers a diverse range of question types, from basic cognition tasks to complex reasoning tasks. We demonstrate the richness and diversity of our QA pairs by examining the distributions of questions and answers in Figure 29.",
        "Question and answer length distributions. We also analyze the question and answer lengths of each 6W category. Figure 31 shows the average question and answer lengths of each category. Overall, the average question and answer lengths are 5.7 and 1.8 words respectively. In contrast to the VQA dataset, where .88%, 8.38%, and 3.25% of the answers consist of one, two, or three words, our answers exhibit a long-tail distribution where 57.3%, 18.1%, and 15.7% of the answers have one, two, or three words respectively. We avoid verbosity by instructing the workers to write answers as concisely as possible. The coverage of long answers means that many answers contain a short description that contains more details than merely an object or an attribute. It shows the richness and complexity of our visual QA tasks beyond object-centric recognition tasks. We foresee that these long-tail questions can mo-",
        "In order to reduce the ambiguity in the concepts of our dataset and connect it to other resources used by the research community, we canonicalize the semantic meanings of all objects, relationships, and attributes in Visual Genome. By “canonicalization,” we refer to word sense disambiguation (WSD) by mapping the components in our dataset to their respective synsets in the WordNet ontology (Miller, 1995). This mapping reduces the noise in the concepts contained in the dataset and also facilitates the linkage between Visual Genome and other data sources such as ImageNet (Deng et al., 2009), which is built on top of the WordNet ontology. Figure 32 shows an example image from the Visual Genome dataset with its components canonicalized. For example, horse is canonicalized as horse.n.01: solid-hoofed herbivorous quadruped domesticated since prehistoric times. Its attribute, clydesdale, is canonicalized as its breed clydesdale.n.01: heavy feathered-legged breed of draft horse originally from Scotland. We also show an example of a QA from",
        "Fig. 32: An example image from the Visual Genome dataset with its region descriptions, QA, objects, attributes, and relationships canonicalized. The large text boxes are WordNet synsets referenced by this image. For example, the carriage is mapped to carriage.n.02: a vehicle with wheels drawn by one or more horses. We do not show the bounding boxes for the objects in order to allow readers to see the image clearly. We also only show a subset of the scene graph for this image to avoid cluttering the ﬁgure.",
        "Related work. Canonicalization, or WSD (Pal and Saha, 2015), has been used in numerous applications, including machine translation, information retrieval, and information extraction (Rothe and Sch¨utze, 2015, Leacock et al., 1998). In English sentences, sentences like “He scored a goal” and “It was his goal in life” carry diﬀerent meanings for the word “goal.” Understanding these diﬀerences is crucial for translating languages and for returning correct results for a query. Similarly, in Visual Genome, we ensure that all our components are canonicalized to understand how different objects are related to each other; for example, “person” is a hypernym of “man” and “woman.” Most past canonicalization models use precision, recall, and F1 score to evaluate on the Semeval dataset (Mihalcea et al., 2004). The current state-of-the-art performance on Semeval is an F1 score of 75.8% (Chen et al., 2014). Since our canonicalization setup is diﬀerent from the Semeval benchmark (we have an open vocabulary and no annotated ground truth for evaluation), our canonicalization method is not directly comparable to these existing methods. We do however, achieve a similar precision and recall score on a held-out test set described below.",
        "address common failure cases in which WordNet’s frequency counts prefer abstract senses of words over the spatial senses present in visual data, e.g. “short.a.01: limited in duration” over short.a.02: lacking in length. For veriﬁcation, we randomly sample 200 attributes, produce ground-truth mappings by hand, and compare them to the results of our algorithm. This resulted in a recall of 95.9% and a mapping accuracy of 83.5%. The most common attribute synsets are shown in Figure 34 (a).",
        "Relationships. As with attributes, we canonicalize the relationships isolated in our scene graphs. We exclude prepositions, which are not recognized in WordNet, leaving a set primarily composed of verb relationships. Since the meanings of verbs are highly dependent upon their morphology and syntactic placement (e.g. passive cases, prepositional phrases), we map the structure of each relationship to the appropriate WordNet sentence frame and only consider those WordNet synsets with matching sentence frames. For each verb-synset pair, we then consider the root hypernym of that synset to reduce potential noise from WordNet’s ﬁne-grained sense distinctions. We also include 20 hand-mapped rules, again to correct for WordNet’s lower representation of concrete or spatial senses; for example, the concrete hold.v.02: have or hold in one’s hand or grip is less frequent in WordNet than the abstract hold.v.01: cause to continue in a certain state. For veriﬁcation, we again randomly sample 200 relationships and compare the results of our canonicalization against ground-truth mappings. This resulted in a recall of 88.5% and a mapping accuracy of 77.6%. While several datasets, such as VerbNet (Schuler, 2005) and FrameNet (Baker et al., 1998), include semantic restrictions or frames to improve classiﬁcation, there is no comprehensive method of mapping to those restrictions or frames. The most common relationship synsets are shown in Figure 34 (b).",
        "Thus far, we have presented the Visual Genome dataset and analyzed its individual components. With such rich information provided, numerous perceptual and cognitive tasks can be tackled. In this section, we aim to provide baseline experimental results using components of Visual Genome that have not been extensively studied. Object detection is already a well-studied problem (Everingham et al., 2010, Girshick et al., 2014, Sermanet et al., 2013, Girshick, 2015, Ren et al., 2015b). Similarly, region graphs and scene graphs have been shown",
        "In Section 6.3 we present results for region captioning. This task is closely related to image captioning (Chen et al., 2015); however, results from the two are not directly comparable, as region descriptions are short, incomplete sentences. We train one of the top 16 state-of-the-art image caption generator (Karpathy and Fei-Fei, 2014) on (1) our dataset to generate region descriptions and on (2) Flickr30K (Young et al., 2014) to generate sentence descriptions. To compare results between the two training approaches, we use simple templates to convert region descriptions into complete sentences. For a more robust evaluation, we validate the descriptions we generate using human judgment.",
        "et al., 2009, Lampert et al., 2009). Visual Genome is the largest dataset of attributes, with 18 attributes per image for a total of 1.8 million attributes.",
        "Setup. For both experiments, we focus on the 100 most common attributes in our dataset. We only use objects that occur at least 100 times and are associated with one of the 100 attributes in at least one image. For both experiments, we follow a similar data pre-processing",
        "for validation and testing. Because each image has about the same number of examples, this results in an approximately 80%-10%-10% split over the attributes themselves. The input data for this experiment is the cropped bounding box of the object associated with each attribute.",
        "While objects are the core building blocks of an image, relationships put them in context. These relationships help distinguish between images that contain the same objects but have diﬀerent holistic interpretations. For example, an image of “a man riding a bike” and “a man falling oﬀ a bike” both contain man and bike, but the relationship (riding vs. falling off) changes how we perceive both situations. Visual Genome is the largest known dataset of relationships, with a total of 1.8 million relationships and an average of 18 relationships per image.",
        "Setup. The setups of both experiments are similar to those of the experiments we performed on attributes. We again focus on the top 100 most frequent relationships. We lowercase, lemmatize, and strip excess whitespace from all relationships. We end up with around 34,000 relationships and 27,000 subject-relationshipobject triples for training, validation, and testing. The input data to the experiment is the image region containing the union of the bounding boxes of the subject and object (essentially, the bounding box containing the two object boxes). We ﬁne-tune a 16-layer VGG network (Simonyan and Zisserman, 2014) with the same learning rates mentioned in Section 6.1.",
        "Generating sentence descriptions of images has gained popularity as a task in computer vision (Kiros et al., 2014, Mao et al., 2014, Karpathy and Fei-Fei, 2014, Vinyals et al., 2014); however, current state-of-the-art models fail to describe all the diﬀerent events captured in an image and instead provide only a high-level summary of the image. In this section, we test how well state-of-the-art models can caption the details of images. For both experiments, we use the NeuralTalk model (Karpathy and Fei-Fei, 2014), since it not only provides state-of-the-art results but also is shown to be robust enough for predicting short descriptions. We train NeuralTalk on the Visual Genome dataset for region descriptions and on Flickr30K (Young et al., 2014) for full sentence descriptions. As a model trained on other datasets would generate complete sentences and would not be comparable (Chen et al., 2015) to our region descriptions, we convert all region descriptions generated by our model into complete sentences using predeﬁned templates (Hou et al., 2002).",
        "Results. Table 8 shows the results for the experiment. We calculate BLEU, CIDEr, and METEOR scores (Chen et al., 2015) between the generated descriptions and their ground-truth descriptions. In all cases, the model trained on VisualGenome performs better. Moreover, we asked crowd workers to evaluate whether a generated description was correct—we got 1.6% and 43.03% for models trained on Flickr30K and on Visual Genome, respectively. The large increase in accuracy when the model trained on our data is due to the speciﬁcity of our dataset. Our region descriptions are shorter and cover a smaller image area. In comparison, the Flickr30K data are generic descriptions of entire images with multiple events happening in diﬀerent regions of the image. The model trained on our data is able to make predictions that are more likely to concentrate on the speciﬁc part of the image it is looking at, instead of generating a summary description. The objectively low accuracy in both cases illustrates that current models are unable to reason about complex images.",
        "Visual Genome is currently the largest dataset of visual question answers with 1.7 million question and answer pairs. Each of our 108,249 images contains an average of 17 question answer pairs. Answering questions requires a deeper understanding of an image than generic image captioning. Question answering can involve ﬁne-grained recognition (e.g. “What is the breed of the dog?”), object detection (e.g. “Where is the kite in the image?”), activity recognition (e.g. “What is this man doing?”), knowledge base reasoning (e.g. “Is this glass full?”), and common-sense reasoning (e.g. “What street will we be on if we turn right?”).",
        "Results. Table 9 shows the performance of the openended visual question answering task. These baseline results imply the long-tail distribution of the answers. Long-tail distribution is common in existing QA datasets as well (Antol et al., 2015, Malinowski and Fritz, 2014). The top 100, 500, and 1000 most frequent answers only cover 41.1%, 57.3%, and 64.1% of the correct answers. In comparison, the corresponding sets of frequent answers in VQA (Antol et al., 2015) cover 63%, 75%, and 80% of the test set answers. The “where” and “why” questions, which tend to involve spatial and common sense reasoning, tend to have more diverse answers and hence perform poorly, with performances of 0.096% and 0.024% top-100 respectively. The top 1000 frequent answers cover only 41.8% and 18.7% of the correct answers from these two question types respectively.",
        "Table 8: Results for the region description generation experiment. Scores in the ﬁrst row are for the region descriptions generated from the NeuralTalk model trained on Flickr8K, and those in the second row are for those generated by the model trained on Visual Genome data. BLEU, CIDEr, and METEOR scores all compare the predicted description to a ground truth in diﬀerent ways.",
        "We have analyzed the individual components of this dataset and presented experiments with baseline results for tasks such as attribute classiﬁcation, relationship classiﬁcation, description generation, and question answering. There are, however, more applications and experiments for which our dataset can be used. In this section, we note a few potential applications that our dataset can enable.",
        "Dense image captioning. We have seen numerous image captioning papers (Kiros et al., 2014, Mao et al., 2014, Karpathy and Fei-Fei, 2014, Vinyals et al., 2014) that attempt to describe an entire image with a single caption. However, these captions do not exhaustively describe every part of the scene. An natural extension to this application, which the Visual Genome dataset enables, is the ability to create dense captioning models that describe parts of the scene.",
        "Visual question answering. While visual question answering has been studied as a standalone task (Yu et al., 2015, Ren et al., 2015a, Antol et al., 2015, Gao et al., 2015), we introduce a dataset that combines all of our question answers with descriptions and scene graphs. Future work can build supervised models that utilize various components of Visual Genome to tackle question answering.",
        "Relationship extraction. Relationship extraction has been extensively studied in information retrieval and natural language processing (Zhou et al., 2007, GuoDong et al., 2005, Culotta and Sorensen, 2004, Socher et al., 2012). Visual Genome is the ﬁrst largescale visual relationship dataset. This dataset can be used to study the extraction of visual relationships (Sadeghi et al., 2015) from images, and its interactions between objects can also be used to study action recognition (Yao and Fei-Fei, 2010, Ramanathan et al., 2015) and spatial orientation between objects (Gupta et al., 2009, Prest et al., 2012).",
        "Visual Genome provides a multi-layered understanding of pictures. It allows for a multi-perspective study of an image, from pixel-level information like objects, to relationships that require further inference, and to even deeper cognitive tasks like question answering. It is a comprehensive dataset for training and benchmarking the next generation of computer vision models. With Visual Genome, we expect these models to develop a broader understanding of our visual world, complementing computers’ capacities to detect objects with abilities to describe those objects and explain their interactions and relationships. Visual Genome is a large formalized knowledge representation for visual understanding and a more complete set of descriptions and question answers that grounds visual concepts to language.",
        "Chen et al., 2015. Chen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. (2015). Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325.",
        "Chen et al., 2013. Chen, X., Shrivastava, A., and Gupta, A. (2013). Neil: Extracting visual knowledge from web data. In Computer Vision (ICCV), 2013 IEEE International Conference on, pages 1409–1416. IEEE.",
        "Deng et al., 2009. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE.",
        "Gao et al., 2015. Gao, H., Mao, J., Zhou, J., Huang, Z., Wang, L., and Xu, W. (2015). Are you talking to a machine? dataset and methods for multilingual image question answering. arXiv preprint arXiv:1505.05612.",
        "(2007). Caltech-256 object category dataset.",
        "Hodosh et al., 2013. Hodosh, M., Young, P., and Hockenmaier, J. (2013). Framing image description as a ranking task: Data, models and evaluation metrics. J. Artif. Int. Res., 47(1):853–899.",
        "Huang et al., 2008. Huang, G. B., Mattar, M., Berg, T., and Learned-Miller, E. (2008). Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. In Workshop on Faces in’Real-Life’Images: Detection, Alignment, and Recognition.",
        "database for english. 38(11):39–41.",
        "Papineni et al., 2002. Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics. Patterson et al., 2014. Patterson, G., Xu, C., Su, H., and Hays, J. (2014). The sun attribute database: Beyond categories for deeper scene understanding. International Journal of Computer Vision, 108(1-2):59–81.",
        "Ren et al., 2015a. Ren, M., Kiros, R., and Zemel, R. (2015a). Image question answering: A visual semantic embedding model and a new dataset. arXiv preprint arXiv:1505.02074. Ren et al., 2015b. Ren, S., He, K., Girshick, R., and Sun, J. (2015b). Faster r-cnn: Towards real-time object detection with region proposal networks. arXiv preprint arXiv:1506.01497.",
        "Russell et al., 2008. Russell, B. C., Torralba, A., Murphy, K. P., and Freeman, W. T. (2008). Labelme: a database and web-based tool for image annotation. International journal of computer vision, 77(1-3):157–173.",
        "Thomee et al., 2016. Thomee, B., Shamma, D. A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth, D., and Li, L.-J. (2016). Yfcc100m: The new data in multimedia research. Commun. ACM, 59(2):64–73.",
        "Torralba et al., 2008. Torralba, A., Fergus, R., and Freeman, W. T. (2008). 80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(11):1958–1970.",
        "Wah et al., 2011. Wah, C., Branson, S., Welinder, P., Perona, P., and Belongie, S. (2011). The caltech-ucsd birds200-2011 dataset.",
        "Xiao et al., 2010. Xiao, J., Hays, J., Ehinger, K., Oliva, A., Torralba, A., et al. (2010). Sun database: Large-scale scene recognition from abbey to zoo. In Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pages 3485–3492. IEEE.",
        "Yao et al., 2007. Yao, B., Yang, X., and Zhu, S.-C. (2007). Introduction to a large-scale general purpose ground truth database: methodology, annotation tool and benchmarks. In Energy Minimization Methods in Computer Vision and Pattern Recognition, pages 169–183. Springer."
    ]
}