{
    "title_author_abstract_introduction": "NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis\nAmir Shahroudy†,‡ amir3@ntu.edu.sg\nJun Liu† jliu029@ntu.edu.sg\nTian-Tsong Ng‡ ttng@i2r.a-star.edu.sg\nGang Wang†,∗ wanggang@ntu.edu.sg\n† School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore ‡ Institute for Infocomm Research, Singapore\nAbstract\nRecent approaches in depth-based human activity analysis achieved outstanding performance and proved the effectiveness of 3D representation for classiﬁcation of action classes. Currently available depth-based and RGB+Dbased action recognition benchmarks have a number of limitations, including the lack of training samples, distinct class labels, camera views and variety of subjects. In this paper we introduce a large-scale dataset for RGB+D human action recognition with more than 56 thousand video samples and 4 million frames, collected from 40 distinct subjects. Our dataset contains 60 different action classes including daily, mutual, and health-related actions. In addition, we propose a new recurrent neural network structure to model the long-term temporal correlation of the features for each body part, and utilize them for better action classiﬁcation. Experimental results show the advantages of applying deep learning methods over state-of-the-art handcrafted features on the suggested cross-subject and crossview evaluation criteria for our dataset. The introduction of this large scale dataset will enable the community to apply, develop and adapt various data-hungry learning techniques for the task of depth-based and RGB+D-based human activity analysis.\n1. Introduction\nRecent development of depth sensors enabled us to obtain effective 3D structures of the scenes and objects [13]. This empowers the vision solutions to move one important step towards 3D vision, e.g. 3D object recognition, 3D scene understanding, and 3D action recognition [1].\nUnlike the RGB-based counterpart, 3D video analysis suffers from the lack of large-sized benchmark datasets. Yet there are no any sources of publicly shared 3D videos such as YouTube to supply “in-the-wild” samples. This limits our ability to build large-sized benchmarks to eval-\n∗Corresponding author\nuate and compare the strengths of different methods, especially the recent data-hungry techniques like deep learning approaches. To the best of our knowledge, all the current 3D action recognition benchmarks have limitations in various aspects.\nFirst is the small number of subjects and very narrow range of performers’ ages, which makes the intra-class variation of the actions very limited. The constitution of human activities depends on the age, gender, culture and even physical conditions of the subjects. Therefore, variation of human subjects is crucial for an action recognition benchmark.\nSecond factor is the number of the action classes. When only a very small number of classes are available, each action class can be easily distinguishable by ﬁnding a simple motion pattern or even the appearance of an interacted object. But when the number of classes grows, the motion patterns and interacting objects will be shared between classes and the classiﬁcation task will be more challenging.\nThird is the highly restricted camera views. For most of the datasets, all the samples are captured from a front view with a ﬁxed camera viewpoint. For some others, views are bounded to ﬁxed front and side views, using multiple cameras at the same time.\nFinally and most importantly, the highly limited number of video samples prevents us from applying the most advanced data-driven learning methods to this problem. Although some attempts have been done [9, 42], they suffered from overﬁtting and had to scale down the size of learning parameters; as a result, they clearly need many more samples to generalize and perform better on testing data.\nTo overcome these limitations, we develop a new largescale benchmark dataset for 3D human activity analysis. The proposed dataset consists of 56,880 RGB+D video samples, captured from 40 different human subjects, using Microsoft Kinect v2. We have collected RGB videos, depth sequences, skeleton data (3D locations of 25 major body joints), and infrared frames. Samples are captured in 80 distinct camera viewpoints. The age range of the subjects in our dataset is from 10 to 35 years which brings more realis-\nDatasets [19] MSR-Action3D [34] CAD-60 RGBD-HuDaAct [23] MSRDailyActivity3D [38] Act42 [6] [18] CAD-120 [25] 3D Action Pairs [43] Multiview 3D Event Online RGB+D Action [46] Northwestern-UCLA [40] [28] UWA3D Multiview [41] Ofﬁce Activity [4] UTD-MHAD UWA3D Multiview II [26] NTU RGB+D\nSamples Classes Subjects Views Sensor\nModalities D+3DJoints RGB+D+3DJoints RGB+D RGB+D+3DJoints RGB+D RGB+D+3DJoints RGB+D+3DJoints RGB+D+3DJoints RGB+D+3DJoints RGB+D+3DJoints RGB+D+3DJoints RGB+D\nYear 2010 N/A 1 2011 Kinect v1 - 2011 Kinect v1 1 2012 Kinect v1 1 2012 Kinect v1 4 2013 Kinect v1 - 2013 Kinect v1 1 2013 Kinect v1 3 2014 Kinect v1 1 2014 Kinect v1 3 2014 Kinect v1 1 2014 Kinect v1 3 Kinect v1+WIS RGB+D+3DJoints+ID 2015 1 5 2015 Kinect v1 RGB+D+IR+3DJoints 2016 80 Kinect v2\nRGB+D+3DJoints\nTable 1. Comparison between NTU RGB+D dataset and some of the other publicly available datasets for 3D action recognition. Our dataset provides many more samples, action classes, human subjects, and camera views in comparison with other available datasets for RGB+D action recogniton.\ntic variation to the quality of actions. Although our dataset is limited to indoor scenes, due to the operational limitation of the acquisition sensor, we provide the ambiance inconstancy by capturing in various background conditions. This large amount of variation in subjects and views makes it possible to have more accurate cross-subject and cross-view evaluations for various 3D-based action analysis methods. The proposed dataset can help the community to move steps forward in 3D human activity analysis and makes it possible to apply data-hungry methods such as deep learning techniques for this task.\nAs another contribution, inspired by the physical characteristics of human body motion, we propose a novel partaware extension of the long short-term memory (LSTM) model [14]. Human actions can be interpreted as interactions of different parts of the body. In this way, the joints of each body part always move together and the combination of their 3D trajectories form more complex motion patterns. By splitting the memory cell of the LSTM into part-based sub-cells, therecurrentnetworkwilllearnthelong-termpatterns speciﬁcally for each body part and the output of the unit will be learned from the combination of all the subcells.\nOur experimental results on the proposed dataset shows the clear advantages of data-driven learning methods over state-of-the-art hand-crafted features.\nThe rest of this paper is organized as follows: Section 2 explores the current 3D-based human action recognition methods and benchmarks. Section 3 introduces the proposed dataset, its structure, and deﬁned evaluation criteria. Section 4 presents our new part-aware long short-term\nmemory network for action analysis in a recurrent neural network fashion. Section 5 shows the experimental evaluations of state-of-the-art hand-crafted features alongside the proposed recurrent learning method on our benchmark, and section 6 concludes the paper.",
    "data_related_paragraphs": [
        "In this section we brieﬂy review publicly available 3D activity analysis benchmark datasets and recent methods in this domain. Here we introduce a limited number of the most famous ones. For a more extensive list of current 3D activity analysis datasets and methods, readers are referred to these survey papers [47, 1, 5, 12, 21, 45, 3].",
        "2.1. 3D activity analysis datasets",
        "After the release of Microsoft Kinect [48], several datasets are collected by different groups to perform research on 3D action recognition and to evaluate different methods in this ﬁeld.",
        "MSR-Action3D dataset [19] was one of the earliest ones which opened up the research in depth-based action analysis. The samples of this dataset were limited to depth sequences of gaming actions e.g. forward punch, side-boxing, forward kick, side kick, tennis swing, tennis serve, golf swing, etc. Later the body joint data was added to the dataset. Joint information includes the 3D locations of 20 different body joints in each frame. A decent number of methods are evaluated on this benchmark and recent ones reported close to saturation accuracies [22, 20, 32].",
        "CAD-60 [34] and CAD-120 [18] contain RGB, depth, and skeleton data of human actions. The special character-",
        "istic of these datasets is the variety of camera views. Unlike most of the other datasets, camera is not bound to frontview or side-views. However, the limited number of video samples (60 and 120) is the downside of them.",
        "RGBD-HuDaAct [23] was one of the largest datasets. It contains RGB and depth sequences of 1189 videos of 12 human daily actions (plus one background class), with high variation in time lengths. The special characteristic of this datasetwasthesyncedandalignedRGBanddepthchannels whichenabledlocalmultimodalanalysisofRBGDsignals1. MSR-DailyActivity [38] was among the most challenging benchmarks in this ﬁeld. It contains 320 samples of 16 dailyactivitieswithhigherintra-classvariation. Smallnumber of samples and the ﬁxed viewpoint of the camera are the limitations of this dataset. Recently reported results on this dataset also achieved very high accuracies [20, 15, 22, 31]. 3D Action Pairs [25] was proposed to provide multiple pairs of action classes. Each pair contains very closely related actions with differences along temporal axis e.g. pick up/put down a box, push/pull a chair, wear/take off a hat, etc. State-of-the-art methods [17, 32, 31] achieved perfect accuracy on this benchmark.",
        "Multiview 3D event [43] and Northwestern-UCLA [40] datasets used more than one Kincect cameras at the same time to collect multi-view representations of the same action, and scale up the number of samples.",
        "It is worth mentioning, there are more than 40 datasets speciﬁcally for 3D human action recognition [47]. Although each of them provided important challenges of human activity analysis, they have limitations in some aspects. Table 1 shows the comparison between some of the current datasets with our large-scale RGB+D action recognition dataset.",
        "To summarize the advantages of our dataset over the existingones, NTURGB+Dhas: 1-manymoreactionclasses, 2- many more samples for each action class, 3- much more intra-class variations (poses, environmental conditions, interacted objects, age of actors, ...), 4- more camera views, 5- more camera-to-subject distances, and 6- used Kinect v.2 which provides more accurate depth-maps and 3D joints, especially in a multi-camera setup compared to the previous version of Kinect.",
        "To have a view-invariant representation of the actions, features can be extracted from the 3D body joint positions which are available for each frame. Evangelidis et al. [10] divided the body into part-based joint quadruples and encodes the conﬁguration of each part with a succinct 6D feature vector, so called skeletal quads. To aggregate the skeletal quads, they applied Fisher vectors and classiﬁed the samples by a linear SVM. In [37] different skeleton conﬁgurations were represented as points on a Lie group. Actions as time-series of skeletal conﬁgurations, were encoded as curves on this manifold. The work of [22] utilized group sparsitybasedclass-speciﬁcdictionarycodingwithgeometric constraints to extract skeleton-based features. Rahmani and Mian [29] introduced a nonlinear knowledge transfer model to transform different views of human actions to a canonical view. To apply ConvNet-based learning to this domain, [30] used synthetically generated data and ﬁtted them to real mocap data. Their learning method was able to recognize actions from novel poses and viewpoints.",
        "In most of 3D action recognition scenarios, there are more than one modality of information and combining them helps to improve the classiﬁcation accuracy. Ohn-Bar and Trivedi [24] combined second order joint-angle similarity representations of skeletons with a modiﬁed two step HOG feature on spatio-temporal depth maps to build global representation of each video sample and utilized a linear SVM to classify the actions. Wang et al. [39], combined Fourier temporal pyramids of skeletal information with local occupancy pattern features extracted from depth maps and applied a data mining framework to discover the most discriminative combinations of body joints. A structured sparsity based multimodal feature fusion technique was introduced by [33] for action recognition in RGB+D domain. In [27] random decision forests were utilized for learning and feature pruning over a combination of depth and skeletonbased features. The work of [32] proposed hierarchical mixed norms to fuse different features and select most informative body parts in a joint learning framework. Hu et al. [15] proposed dynamic skeletons as Fourier temporal pyramids of spline-based interpolated skeleton points and their gradients, and HOG-based dynamic color and depth patterns to be used in a RGB+D joint-learning model for action classiﬁcation.",
        "3. The Dataset",
        "teria of NTU RGB+D action recognition dataset.2",
        "3.1. The RGB+D Action Dataset",
        "Data Modalities: To collect this dataset, we utilized Microsoft Kinect v2 sensors. We collected four major data modalities provided by this sensor: depth maps, 3D joint information, RGB frames, and IR sequences.",
        "Subjects: We invited 40 distinct subjects for our data collection. The ages of the subjects are between 10 and 35. Figure 4 shows the variety of the subjects in age, gender, and height. Each subject is assigned a consistent ID number over the entire dataset.",
        "2http://rose1.ntu.edu.sg/datasets/actionrecognition.asp",
        "Figure 1. Conﬁguration of 25 body joints in our dataset. The labels of the joints are: 1-base of the spine 2-middle of the spine 3-neck 4-head 5-left shoulder 6-left elbow 7-left wrist 8- left hand 9-right shoulder 10-right elbow 11-right wrist 12- right hand 13-left hip 14-left knee 15-left ankle 16-left foot 17- right hip 18-right knee 19-right ankle 20-right foot 21-spine 22- tip of the left hand 23-left thumb 24-tip of the right hand 25- right thumb",
        "Differential RNN [36] added a new gating mechanism to the traditional LSTM to extract the derivatives of internal state (DoS). The derived DoS was fed to the LSTM gates to learn salient dynamic patterns in 3D skeleton data.",
        "Recurrent Neural Networks (RNNs) and Long ShortTerm Memory Networks (LSTMs) [14] have been shown to be among the most successful deep learning models to encode and learn sequential data in various applications [35, 8, 2, 16].",
        "In this section, we introduce a new data-driven learning method to model the human actions using our collected 3D action sequences.",
        "In our experiments, we evaluate state-of-the-art depthbased action recognition methods and compare them with RNN, LSTM, and the proposed P-LSTM based on the evaluation criteria of our dataset.",
        "We use the publicly available implementation of six depth-based action recognition methods and apply them on our new dataset benchmark. Among them, HOG2 [24], Super Normal Vector [44], and HON4D [25] extract features directly from depth maps without using the skeletal information. Lie group [37], Skeletal Quads [10], and FTP Dynamic Skeletons [15] are skeleton-based methods.",
        "Kinect’s body tracker is prone to detecting some objects e.g. seats or tables as bodies. To ﬁlter out these noisy detections, for each tracked skeleton we calculate the spread of the joint locations towards image axis and ﬁltered out the ones whose X spread were more than 0.8 of their Y spread. Forourrecurrent modelevaluation, wereserveabout ﬁve percent of the training data as validation set. The networks are trained on a large number of iterations and we pick the network with the least validation error among all the iterations and report its performance on testing data.",
        "For the baseline methods which use SVM as their classiﬁer, to be able to manage the large scale of the data, we use Libliner SVM toolbox [11].",
        "This shows they have their model was prone to overﬁtting due to the lack of training data and proves the demand for a bigger dataset and approves our motivation for proposing NTU RGB+D dataset.",
        "A large-scale RGB+D action recognition dataset is introduced in this paper. Our dataset includes 56880 video samples collected from 60 action classes in highly variant",
        "Figure 4. Sample frames of the NTU RGB+D dataset. First four rows show the variety in human subjects and camera views. Fifth row depicts the intra-class variation of the performances. The last row illustrates RGB, RGB+joints, depth, depth+joints, and IR modalities of a sample frame.",
        "camera settings. Compared to the current datasets for this task, our dataset is larger in orders and contains much more variety in different aspects.",
        "The large scale of the collected data enables us to apply data-driven learning methods like Long Short-Term Memory networks in this problem and achieve better performance accuracies compared to hand-crafted features.",
        "The provided experimental results show the availability of large-scale data enables the data-driven learning frameworks to outperform hand-crafted features. They also show the effectiveness of the proposed P-LSTM model over traditional recurrent models.",
        "data: A review. PR Letters, 2014.",
        "[2] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene labeling with lstm recurrent neural networks. In CVPR, 2015. [3] Z. Cai, J. Han, L. Liu, and L. Shao. Rgb-d datasets using microsoft kinect or similar sensors: a survey. Multimedia Tools and Applications, 2016.",
        "[4] C. Chen, R. Jafari, and N. Kehtarnavaz. Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor. In ICIP, Sept 2015. [5] L. Chen, H. Wei, and J. Ferryman. A survey of human mo-",
        "[6] Z. Cheng, L. Qin, Y. Ye, Q. Huang, and Q. Tian. Human daily action analysis with multi-view and color-depth data. In ECCV Workshops. 2012.",
        "[12] F. Han, B. Reily, W. Hoff, and H. Zhang. Space-Time Representation of People Based on 3D Skeletal Data: A Review. arXiv, 2016.",
        "[23] B.Ni, G.Wang, andP.Moulin. Rgbd-hudaact: Acolor-depth video database for human daily activity recognition. In ICCV Workshops, 2011.",
        "[45] M. Ye, Q. Zhang, L. Wang, J. Zhu, R. Yang, and J. Gall. A survey on human motion analysis from depth data. In Timeof-Flight and Depth Imaging. Sensors, Algorithms, and Applications. 2013.",
        "[47] J. Zhang, W. Li, P. O. Ogunbona, P. Wang, and C. Tang. Rgbd-based action recognition datasets: A survey. arXiv, 2016. [48] Z. Zhang. Microsoft kinect sensor and its effect. IEEE Mul-"
    ]
}