{
    "title_author_abstract_introduction": "Measuring Mathematical Problem Solving With the MATH Dataset\nDan Hendrycks UC Berkeley\nCollin Burns UC Berkeley\nSaurav Kadavath UC Berkeley\nAkul Arora UC Berkeley\nSteven Basart UChicago\nEric Tang UC Berkeley\nDawn Song UC Berkeley\nJacob Steinhardt UC Berkeley\nAbstract\nMany intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we ﬁnd that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.\nIntroduction\nMathematics is a highly effective tool in many intellectual endeavors. It enables us to count and quantify objects, and it can be relied upon because it is consistent and based on logic. Mathematics pervades the sciences and can be used to model planetary orbits, atomic motion, signal frequencies, and much more. These phenomena can be encoded with mathematics precisely and concisely. This has even led some to describe mathematics as being “unreasonably effective” (Wigner, 1960). These observations speak to the broad reach and domain-generality of mathematics.\nIn machine learning, mathematics is a valuable testbed for problem-solving ability: the ability to analyze a problem, pick out good heuristics from a large set of possibilities, and chain them together to produce an answer. This contrasts with plug-and-chug calculations, a skill which ML models can already exhibit (Henighan et al., 2020). Visual or linguistic reasoning may involve limited problem-solving ability for tasks such as image classiﬁcation, but unlike math this is not the focus of these domains.\nTo measure the problem-solving ability of machine learning models, we introduce the MATH dataset, which consists of 12,500 problems from high school math competitions. Given a problem from MATH, machine learning models generate a sequence, such as $\\frac{2}{3}$, that encodes the ﬁnal answer. These answers are unique after normalization, allowing MATH to be scored with exact match rather than with heuristic metrics such as BLEU. In addition, MATH problems are tagged by difﬁculty from 1 to 5, and span seven subjects including geometry, where diagrams can be speciﬁed in text with the Asymptote language. This enables a ﬁne-grained assessment of\n35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.\nMetamath Theorem Proving",
    "data_related_paragraphs": [
        "(N - 1) e. CC ) ... DeepMind Mathematics Dataset Divide 1136975704 by -142121963. A: -8 Let k(u) = u**2+u-4. Find k(0). A: -4 Sort 2, 4, 0, 6. A: 0, 2, 4, 6 Solve 4 - 4 - 4 = 188*m for m. A: -1/47",
        "MATH Dataset (Ours) Problem: Tom has a red marble, a green marble, a blue marble, and three identical yellow marbles. How many different groups of two marbles can Tom choose? Solution: There are two cases here: either Tom chooses two yellow marbles (1 result), or he chooses two marbles of different colors ((cid:0)4 results). The total number of distinct pairs of marbles Tom can choose is 1 + 6 = 7 . Problem: The equation x2 + 2x = i has two complex solutions. Determine the product of their real parts. Solution: Complete the square by adding 1 to each side. Then (x + 1)2 = 1 + i = e 2, so √ 4 x + 1 = ±e 2. The desired product is then √ (cid:1) 4 (cid:0)−1 + cos(cid:0)π 2(cid:1)(cid:0)−1 − cos(cid:0)π 2(cid:1) = 1 − 8 √ √ 1 − 2 2",
        "Figure 1: Previous work is based on formal theorem provers or straightforward plug-and-chug problems. Our dataset, MATH, has competition mathematics problems with step-by-step solutions written in LATEX and natural language. Models are tasked with generating tokens to construct the ﬁnal (boxed) answer.",
        "The MATH dataset is challenging: large language models achieved accuracies ranging from 3.0% to 6.9%. Despite these low accuracies, models clearly possess some mathematical knowledge: they achieve up to 15% accuracy on the easiest difﬁculty level, and they are able to generate step-by-step solutions that are coherent and on-topic even when incorrect. We also evaluated humans on MATH, and found that a computer science PhD student who does not especially like mathematics attained approximately 40% on MATH, while a three-time IMO gold medalist attained 90%, indicating that MATH can be challenging for humans as well.",
        "While MATH covers advanced problem-solving techniques, models may ﬁrst need to be trained thoroughly on the fundamentals of mathematics. To address this, we create the ﬁrst large-scale mathematics pretraining dataset with hundreds of thousands of step-by-step solutions in natural language and LATEX. We call this dataset the Auxiliary Mathematics Problems and Solutions (AMPS) pretraining corpus, which consists of Khan Academy and Mathematica data. AMPS has over 100,000 Khan Academy problems with step-by-step solutions in LATEX; these exercises are used to teach human students concepts ranging from basic addition to Stokes’ Theorem. It also contains over 5 million problems generated using Mathematica scripts, based on 100 hand-designed modules covering topics such as conic sections, div grad and curl, KL divergence, eigenvalues, polyhedra, and Diophantine equations. In total AMPS contains 23GB of problems and solutions. Pretraining on",
        "Altogether, while large Transformer models (Vaswani et al., 2017) make some progress on the MATH dataset, such as by AMPS pretraining or by training with step-by-step solutions, accuracy nonetheless remains relatively low. While enormous Transformers pretrained on massive datasets can now solve most existing text-based tasks, this low accuracy indicates that our MATH dataset is distinctly harder. Accuracy also increases only modestly with model size: assuming a log-linear scaling trend, models would need around 1035 parameters to achieve 40% accuracy on MATH, which is impractical. Instead, to make large strides on the MATH dataset with a practical amount of resources, we will need new algorithmic advancements from the broader research community.",
        "Rather than prove theorems with standard pretrained Transformers, McAllester (2020) proposes that the community create theorem provers that bootstrap their mathematical capabilities through open-ended self-improvement. For bootstrapping to be feasible, models will also need to understand mathematics as humans write it, as manually converting advanced mathematics to a proof generation language is extremely time-consuming. This is why Szegedy (2020) argues that working on formal theorem provers alone will be an impractical path towards world-class mathematical reasoning. We address Szegedy (2020)’s concern by creating a dataset to test understanding of mathematics written in natural language and commonplace mathematical notation. This also means that the answers in our dataset can be assessed without the need for a cumbersome theorem proving environment, which is another advantage of our evaluation framework.",
        "Neural Calculators. Recent work shows that Transformers can sometimes perform laborious calculations around as well as calculators and computer algebra systems. Lample and Charton (2020) use Transformers to solve algorithmically generated symbolic integration problems and achieve greater than 95% accuracy. Amini et al. (2019); Ling et al. (2017) introduce plug-and-chug multiple choice mathematics problems and focus on sequence-to-program generation. Saxton et al. (2019) introduce the DeepMind Mathematics dataset, which consists of algorithmically generated plug-and-chug problems such as addition, list sorting, and function evaluation, as shown in Figure 1. Recently, Henighan et al. (2020) show that, excluding problems with astronomically large numbers, the vast majority of the problems in the DeepMind Mathematics dataset can be straightforwardly solved with large Transformers.",
        "HOListAuxiliaryHOLStepProofsDeepMindMathSymbolicIntegrationMATH(Ours)020406080100Accuracy (%)State-of-the-Art Accuracy onMathematics Datasets\fAlgebra Calculus Statistics Geometry Linear Algebra Number Theory",
        "Table 1: A subset of the topics covered by our 100 hand-designed Mathematica scripts, which is part of our Auxiliary Mathematics Problems and Solutions (AMPS) pretraining dataset. Of these scripts, 37 also generate step-by-step solutions. We generated around 50,000 exercises with each Mathematica script, or around 5 million problems.",
        "et al. (2020) show that the performance of Transformers predictably increases with an increase in model size and dataset size, raising the question of whether natural language processing can be solved by simply increasing compute and funding. Additionally, Chen et al. (2021); Austin et al. (2021) show that code generation models scale reliably across several orders of magnitude, and, should scaling continue, Chen et al. (2021)’s HumanEval code generation dataset should be solved in a few orders of magnitude. In the Supplementary Materials, we even ﬁnd that large GPT-3 models can perform remarkably well on a sequence completion test similar to an IQ test, the C-Test (Hernández-Orallo, 1998; Legg and Hutter, 2007). Even difﬁcult logical understanding tasks such as LogiQA (Liu et al., 2020) will soon be straightforwardly solved by enormous Transformers should trends continue, which we also show in the Supplementary Materials. Hendrycks et al. (2021) create a multiple-choice benchmark covering 57 subjects. However, unlike our benchmark, which is a text generation task with 12,500 mathematical reasoning questions, their benchmark is a multiple choice task that includes only a few hundred questions about mathematics. In contrast to these benchmarks, we ﬁnd that our MATH benchmark is unusually challenging for current models and, if trends continue, simply using bigger versions of today’s Transformers will not solve our task in the foreseeable future.",
        "3 Datasets",
        "In this section, we introduce two new datasets, one for benchmarking mathematical problem-solving ability (MATH) and one for pretraining (AMPS).",
        "3.1 The MATH Dataset",
        "The MATH dataset consists of problems from mathematics competitions including the AMC 10, AMC 12, AIME, and more. Many of these problems can be collected from aops.com/community/c3158_usa_contests. These competitions span decades and assess the mathematical problem-solving ability of the best young mathematical talent in the United States. Unlike most prior work, most problems in MATH cannot be solved with a straightforward application of standard K-12 mathematics tools. Instead, humans often solve such problem by applying problem solving techniques and “heuristics” (Pólya, 1945).",
        "The Mathematics Aptitude Test of Heuristics dataset, abbreviated MATH, has 12,500 problems (7,500 training and 5,000 test). With this many training problems, models can learn many useful heuristics for problem solving. Each problem has a step-by-step solution and a ﬁnal boxed answer. Example problems with step-by-step solutions are shown in Figure 1.",
        "3.2 AMPS (Khan + Mathematica) Dataset",
        "Since pretraining data can greatly inﬂuence performance (Hernandez et al., 2021; Gururangan et al., 2020) and since mathematics is a small fraction of online text, we introduce a large and diverse mathematics pretraining corpus. Our pretraining dataset, the Auxiliary Mathematics Problems and Solutions (AMPS) dataset, has problems and step-by-step solutions typeset in LATEX. AMPS contains over 100,000 problems pulled from Khan Academy and approximately 5 million problems generated from manually designed Mathematica scripts.",
        "Mathematica. To make AMPS larger, we also contribute our own Mathematica scripts to generate approximately 50× more problems than our Khan Academy dataset. With Mathematica, we designed 100 scripts that test distinct mathematics concepts, 37 of which include full step-by-step LATEX solutions in addition to ﬁnal answers. We generated around 50,000 exercises from each of our scripts, or around 5 million problems in total. This results in over 23 GB of mathematics problems, making it larger than the 16 GB of natural language used to train BERT (Devlin et al., 2019).",
        "In this section, we perform experiments to investigate performance on the MATH dataset. We ﬁnd that accuracy remains low even for the best models. Furthermore, unlike for most other text-based datasets, we ﬁnd that accuracy is increasing very slowly with model size. If trends continue, then we will need algorithmic improvements, rather than just scale, to make substantial progress on MATH. Nevertheless, we show that making progress is also possible today. We ﬁnd that pretraining on AMPS enables a small 0.1B parameter model to perform similarly to a large ﬁne-tuned 13B parameter model.",
        "Before ﬁne-tuning on MATH, models pretrain on AMPS. We pretrain for one epoch, using AdamW (Loshchilov and Hutter, 2019), using a batch size of 128, and using a weight decay of 0.05. We use the standard autoregressive language modeling objective. During pretraining, we upsample Khan Academy data by a factor of 5 and we downsample Mathematica by a factor of 2 to account for the large difference in dataset sizes.",
        "We also evaluate GPT-3 with ﬁne-tuning and also in a few-shot setting using the OpenAI API. We use the ‘Curie’ GPT-3 model which has approximately 13 billion parameters, and the ‘Davinci’ model which has approximately 175 billion parameters. When performing few-shot evaluation, we construct our prompt by prepending 8 problems with correct answers (but not step-by-step solutions due to space). Using temperature 0, models output up to 20 tokens for the ﬁnal answer. The OpenAI API also allows users to ﬁne-tune models up to 13B parameters at the time of writing, but their API does not have the option to pretrain on datasets as large as AMPS.",
        "AMPS Pretraining. As an ablation, we test how models with AMPS pretraining compare with models that were not pretrained on AMPS. Without pretraining on AMPS, a GPT-3 (13B) model ﬁne-tuned on MATH attains 5.2% accuracy. In contrast, a GPT-2 (0.1B) model both pretrained on AMPS and ﬁne-tuned on MATH attains 5.4%. Consequently AMPS increases accuracy about as much as a 130× increase in parameters, demonstrating its value as a pretraining dataset.",
        "We additionally tried pretraining on StackExchange, a real-world but less curated source of mathematics text. A GPT-2 (0.3B) model pretrained on both AMPS and questions and answers from Math StackExchange (∼3 GB) had 6.0% accuracy. This is actually less than the 6.2% accuracy attained by pretraining on AMPS alone. Thus our dataset is more useful for pretraining even than diverse real-world mathematics data.",
        "Figure 4: Additional example problems, generated solutions, and ground truth solutions from our MATH dataset. The ﬁrst problem’s generated solution has the right answer with a correct and simple explanation. The second problem is a combinatorics problem speciﬁed with a ﬁgure, which the model gets wrong.",
        "Scratch Space. Our MATH dataset and AMPS pretraining dataset provide full step-by-step solutions, an important and rare type of side information (Murty et al., 2020) that can in principle teach models how to derive answers and use scratch space. By training a language model on these solutions, we can have models generate full step-by-step solutions. This may be especially useful for difﬁcult problems, for which outputting the correct answer after just a few forward passes may be insufﬁcient. By allowing the model to use several steps of processing before outputting a ﬁnal answer, the model could adaptively use computation and have higher performance, in addition to making its reasoning more interpretable.",
        "A\fWe test this by prompting models with “(cid:104)P(cid:105) Full Solution:” to generate a full solution along with a ﬁnal boxed answer, rather than the boxed answer alone. We evaluated this for GPT2 (1.5B) and found that this actually makes performance worse, dropping accuracy to 5.3%. We hypothesize that the drop in accuracy from using scratch space arises from a snowballing effect, in which partially generated “solutions” with mistakes can derail subsequent generated text. Nevertheless, when generation becomes more reliable and models no longer confuse themselves by their own generations, our dataset’s solutions could in principle teach models to use scratch space and attain higher accuracy.",
        "In this paper, we laid groundwork for future research in machine learning for mathematical problem solving. We introduced the MATH benchmark, which enables the community to measure mathematical problem-solving ability. In addition to having answers, all MATH problems also include answer explanations, which models can learn from to generate their own step-by-step solutions. We also introduce AMPS, a diverse pretraining corpus that can enable future models to learn virtually all of K-12 mathematics. While most other text-based tasks are already nearly solved by enormous Transformers, MATH is notably different. We showed that accuracy is slowly increasing and, if trends continue, the community will need to discover conceptual and algorithmic breakthroughs to attain strong performance on MATH. Given the broad reach and applicability of mathematics, solving the MATH dataset with machine learning would be of profound practical and intellectual signiﬁcance.",
        "problems? large-scale dataset construction and evaluation. In ACL, 2016.",
        "dataset for machine reading comprehension with logical reasoning. In IJCAI, 2020.",
        "In this appendix, we have more comparisons with previous datasets, a discussion of logic and intelligence tests, further AMPS and MATH details, an analysis of model performance as difﬁculty level changes, and results with the BART architecture.",
        "A.1 Expanded Dataset Comparisons",
        "We compared to ten datasets in the main paper, and now we will further describe plug-and-chug datasets. Dolphin18K (Huang et al., 2016) is one of the ﬁrst modern datasets in this space and is based on Yahoo! Answers and includes questions such as “help!!!!!!!(please) i cant ﬁgure this out!? what is the sum of 4 2/5 and 17 3/7 ?”. MathQA (Amini et al., 2019) builds on AQuA-RAT (Ling et al., 2017) and claims AQuA-RATs “rationales are noisy, incomplete and sometimes incorrect.” MathQA then cleans AQuA-RAT, though cleaning led the dataset size to be reduced by half of an order of magnitude. Miao et al. (2020) analyze MathQA and observe “the annotated formulas of 27% of the problems do not match their labeled answers,” and they obtain 86% accuracy on a cleaned version of MATH-QA. In contrast AMPS is large and clean as questions are algorithmically generated, and our MATH dataset is carefully curated by the competition mathematics community and contains competition-level problems that are difﬁcult.",
        "A.3 Further Dataset Information",
        "Rendering Graphics. For the ﬁrst time, our dataset makes it possible for text-based models to process graphical mathematical ﬁgures by expressing ﬁgures in asymptote code. For example, Figure 7 shows asymptote code and the ﬁgure it produces. In short, it is possible to concisely specify many visual mathematics problems with code, sidestepping the complexity of multi-modal models.",
        "MATH problems are created by the Mathematical Association of America (MAA). Although we do not commercialize MATH, we should like to demonstrate that we are far from the boundary for action or infringement. For decades, the MAA has not protected its problem IP even from separate organizations which sell MAA problems, such as AoPS. Courts have ruled that this implies the IP rights are permanently forfeited. We raise this point only to demonstrate the extent to which our reuse for research is within the law, because even commercial reuse of MAA problems is within the law and commonplace. Even so, the MATH dataset is not sold and is likely to have no effect on the value",
        "Figure 12: Khan Academy modules in our AMPS pretraining dataset (Part 1).",
        "Dataset Intended Uses. We document the dataset within the paper and note that the dataset and code for reproducing results is available at https://github.com/hendrycks/apps. We do not intend for this dataset to train models that help students cheat on mathematics exams. We intend for others to use this dataset in order to better forecast reasoning capabilities.",
        "Author Statement and License. We bear all responsibility in case of violation of rights. The MATH data, AMPS data, and our open source code are under an MIT license."
    ]
}