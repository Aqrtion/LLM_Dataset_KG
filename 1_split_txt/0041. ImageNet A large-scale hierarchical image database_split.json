{
    "title_author_abstract_introduction": "ImageNet: A Large-Scale Hierarchical Image Database\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li and Li Fei-Fei Dept. of Computer Science, Princeton University, USA {jiadeng, wdong, rsocher, jial, li, feifeili}@cs.princeton.edu\nAbstract\nThe explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500- 1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.\n1. Introduction\nThe digital era has brought with it an enormous explosion of data. The latest estimations put a number of more than 3 billion photos on Flickr, a similar number of video clips on YouTube and an even larger number for images in the Google Image Search database. More sophisticated and robust models and algorithms can be proposed by exploiting these images, resulting in better applications for users to index, retrieve, organize and interact with these data. But exactly how such data can be utilized and organized is a problem yet to be solved. In this paper, we introduce a new image database called “ImageNet”, a large-scale ontology of images. We believe that a large-scale ontology of images is a critical resource for developing advanced, large-scale\ncontent-based image search and image understanding algorithms, as well as for providing critical training and benchmarking data for such algorithms.\nImageNet uses the hierarchical structure of WordNet [9]. Each meaningful concept in WordNet, possibly described by multiple words or word phrases, is called a “synonym set” or “synset”. There are around 80,000 noun synsets in WordNet. In ImageNet, we aim to provide on average 500-1000 images to illustrate each synset. Images of each concept are quality-controlled and human-annotated as described in Sec. 3.2. ImageNet, therefore, will offer In this paper, tens of millions of cleanly sorted images. we report the current version of ImageNet, consisting of 12 “subtrees”: mammal, bird, ﬁsh, reptile, amphibian, vehicle, furniture, musical instrument, geological formation, tool, ﬂower, fruit. These subtrees contain 5247 synsets and 3.2 million images. Fig. 1 shows a snapshot of two branches of the mammal and vehicle subtrees. The database is publicly available at http://www.image-net.org.\nThe rest of the paper is organized as follows: We ﬁrst show that ImageNet is a large-scale, accurate and diverse image database (Section 2). In Section 4, we present a few simple application examples by exploiting the current ImageNet, mostly the mammal and vehicle subtrees. Our goal is to show that ImageNet can serve as a useful resource for visual recognition applications such as object recognition, image classiﬁcation and object localization. In addition, the construction of such a large-scale and high-quality database can no longer rely on traditional data collection methods. Sec. 3 describes how ImageNet is constructed by leveraging Amazon Mechanical Turk.",
    "data_related_paragraphs": [
        "images spread over 5247 categories (Fig. 2). On average over 600 images are collected for each synset. Fig. 2 shows the distributions of the number of images per synset for the current ImageNet 1. To our knowledge this is already the largest clean image dataset available to the vision research community, in terms of the total number of images, number ofimagespercategoryaswellasthenumberofcategories 2.",
        "Hierarchy ImageNet organizes the different classes of images in a densely populated semantic hierarchy. The main asset of WordNet [9] lies in its semantic structure, i.e. its ontology of concepts. Similarly to WordNet, synsets of images in ImageNet are interlinked by several types of relations, the “IS-A” relation being the most comprehensive and useful. Although one can map any dataset with cate-",
        "gory labels into a semantic hierarchy by using WordNet, the density of ImageNet is unmatched by others. For example, to our knowledge no existing vision dataset offers images of 147 dog categories. Fig. 3 compares the “cat” and “cattle” subtrees of ImageNet and the ESP dataset [25]. We observe that ImageNet offers much denser and larger trees.",
        "Accuracy We would like to offer a clean dataset at all levels of the WordNet hierarchy. Fig. 4 demonstrates the labeling precision on a total of 80 synsets randomly sampled at different tree depths. An average of 99.7% precision is achieved on average. Achieving a high precision for all depths of the ImageNet tree is challenging because the lower in the hierarchy a synset is, the harder it is to classify, e.g. Siamese cat versus Burmese cat.",
        "Table 1: Comparison of some of the properties of ImageNet versus other existing datasets. ImageNet offers disambiguated labels (LabelDisam), clean annotations (Clean), a dense hierarchy (DenseHie), full resolution images (FullRes) and is publicly available (PublicAvail). ImageNet currently does not provide segmentation annotations.",
        "2.1. ImageNet and Related Datasets",
        "We compare ImageNet with other datasets and summa-",
        "Small image datasets A number of well labeled small datasets (Caltech101/256 [8, 12], MSRC [22], PASCAL [7] etc.) have served as training and evaluation benchmarks for most of today’s computer vision algorithms. As computer vision research advances, larger and more challenging",
        "4We focus our comparisons on datasets of generic objects. Special purpose datasets, such as FERET faces [19], Labeled faces in the Wild [13] and the Mammal Benchmark by Fink and Ullman [11] are not included.",
        "datasets are needed for the next generation of algorithms. The current ImageNet offers 20× the number of categories, and 100× the number of total images than these datasets.",
        "TinyImage TinyImage [24] is a dataset of 80 million 32 × 32 low resolution images, collected from the Internet by sending all words in WordNet as queries to image search engines. Each synset in the TinyImage dataset contains an average of 1000 images, among which 10-25% are possibly clean images. Although the TinyImage dataset has had success with certain applications, the high level of noise and low resolution images make it less suitable for general purpose algorithm development, training, and evaluation. Compared to the TinyImage dataset, ImageNet contains high quality synsets (∼ 99% precision) and full resolution images with an average size of around 400 × 350.",
        "ESP dataset The ESP dataset is acquired through an online game [25]. Two players independently propose labels to one image with the goal of matching as many words as possible in a certain time limit. Millions of images are labeled through this game, but its speeded nature also poses a major drawback. Rosch and Lloyd [20] have demonstrated that humans tend to label visual objects at an easily accessible semantic level termed as “basic level” (e.g. bird), as opposed to more speciﬁc level (“sub-ordinate level”, e.g. sparrow), or more general level (“super-ordinate level”, e.g. vertebrate). Labels collected from the ESP game largely concentrate at the “basic level” of the semantic hierarchy as illustrated by the color bars in Fig. 6. ImageNet, however, demonstrates a much more balanced distribution of images across the semantic hierarchy. Another critical difference between ESP and ImageNet is sense disambiguation. When human players input the word “bank”, it is unclear whether it means “a river bank” or a “ﬁnancial institution”. At this large scale, disambiguation becomes a nontrivial task. Without it, the accuracy and usefulness of the ESP data could be affected. ImageNet, on the other hand, does not have this problem by construction. See section 3.2 for more details. Lastly, most of the ESP dataset is not publicly available. Only 60K images and their labels can be accessed [1].",
        "LabelMe and Lotus Hill datasets LabelMe [21] and the LotusHilldataset[27]provide30kand50klabeledandsegmented images, respectively 5. These two datasets provide complementary resources for the vision community compared to ImageNet. Both only have around 200 categories, but the outlines and locations of objects are provided. ImageNet in its current form does not provide detailed object outlines (see potential extensions in Sec. 5.1), but the number of categories and the number of images per category",
        "Lotus Hill dataset also includes 587k video frames.",
        "To collect a highly accurate dataset, we rely on humans to verify each candidate image collected in the previous step for a given synset. This is achieved by using the service of Amazon Mechanical Turk (AMT), an online platform on which one can put up tasks for users to complete and to get paid. AMT has been used for labeling vision data [23]. With a global user base, AMT is particularly suitable for large scale labeling.",
        "Figure 6: Comparison of the distribution of “mammal” labels over tree depth levels between ImageNet and ESP game. The yaxis indicates the percentage of the labels of the corresponding dataset. ImageNet demonstrates a much more balanced distribution, offering substantially more labels at deeper tree depth levels. The actual number of images corresponding to the highest bar is also given for each dataset.",
        "already far exceeds these two datasets. In addition, images in these two datasets are largely uploaded or provided by users or researchers of the dataset, whereas ImageNet contains images crawled from the entire Internet. The Lotus Hill dataset is only available through purchase.",
        "In this section, we show three applications of ImageNet. Theﬁrstsetofexperimentsunderlinetheadvantagesofhaving clean, full resolution images. The second experiment exploits the tree structure of ImageNet, whereas the last experiment outlines a possible extension and gives more insights into the data.",
        "1. NN-voting + noisy ImageNet First we replicate one of the experiments described in [24], which we refer to as “NN-voting” hereafter. To imitate the TinyImage dataset (i.e. images collected from search engines without human cleaning), we use the original candidate images for each synset (Section 3.1) and downsample them to 32 × 32. Given a query image, we retrieve 100 of the nearest neighbor images by SSD pixel distance from the mammal subtree. Then we perform classiﬁcation by aggregating votes (number of nearest neighbors) inside the tree of the target category.",
        "2. NN-voting + clean ImageNet Next we run the same NN-voting experiment described above on the clean ImageNet dataset. This result shows that having more accurate data improves classiﬁcation performance.",
        "4. NBNN-100 Finally, we run the same NBNN experiment, but limit the number of images per category to 100. The result conﬁrms the ﬁndings of [24]. Performance can be signiﬁcantly improved by enlarging the dataset. It is worth noting that NBNN-100 outperforms NN-voting with access to the entire dataset, again demonstrating the beneﬁt of having detailed feature level information by using full resolution images.",
        "Comparedtootheravailabledatasets, ImageNetprovides image data in a densely populated hierarchical structure. Many possible algorithms could be applied to exploit a hierarchical data structure (e.g. [16, 17, 28, 18]).",
        "ImageNet can be extended to provide additional information about each image. One such information is the spatial extent of the objects in each image. Two application areas come to mind. First, for training a robust object detection algorithm one often needs localized objects in different poses and under different viewpoints. Second, having localized objects in cluttered scenes enables users to use ImageNet as a benchmark dataset for object localization algorithms. In this section we present results of localization on 22 categories from different depths of the WordNet hierarchy. Theresultsalsothrowlightonthediversityofimages in each of these categories.",
        "TexaslonghornhumanMinivantigerGolden RetrieverLynxwolfhelicoptertankjetbaby carriagepacecarmopedgreyhoundbovinetuskeryachttricycleArmadillopuppystealthaircraftcameldobbinspaceshuttle00.20.40.60.81PrecisionRecall\fcessible online. We plan to use cloud storage to enable efﬁ- cientdistributionofImageNetdata; (iii)extendImageNetto include more information such as localization as described in Sec. 4.3, segmentation, cross-synset referencing of images, as well as expert annotation for difﬁcult synsets and (iv) foster an ImageNet community and develop an online platform where everyone can contribute to and beneﬁt from ImageNet resources.",
        "A training resource. Most of today’s object recognition algorithms have focused on a small number of common objects, such as pedestrians, cars and faces. This is mainly due to the high availability of images for these categories. Fig. 6 has shown that even the largest datasets today have a strong bias in their coverage of different types of objects. ImageNet, on the other hand, contains a large number of images for nearly all object classes including rare ones. One interesting research direction could be to transfer knowledge of common objects to learn rare object models.",
        "A benchmark dataset. The current benchmark datasets in computer vision such as Caltech101/256 and PASCAL have played a critical role in advancing object recognition and scene classiﬁcation research. We believe that the high quality, diversity and large scale of ImageNet will enable it to become a new and challenging benchmark dataset for future research.",
        "[6] B. Collins, J. Deng, K. Li, and L. Fei-Fei. Towards scalable dataset construction: An active learning approach. In ECCV08, pages I: 86– 98, 2008.",
        "[9] C. Fellbaum. WordNet: An Electronic Lexical Database. Bradford",
        "dataset. Technical Report 7694, Caltech, 2007.",
        "[13] G. Huang, M. Ramesh, T. Berg, and E. Learned Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, UMass, 2007. [14] L.-J. Li, G. Wang, and L. Fei-Fei. OPTIMOL: automatic Online In CVPR07,",
        "[19] P. Phillips, H. Wechsler, J. Huang, and P. Rauss. The feret database IVC, and evaluation procedure for face-recognition algorithms. 16(5):295–306, April 1998.",
        "[21] B. Russell, A. Torralba, K. Murphy, and W. Freeman. Labelme: A database and web-based tool for image annotation. IJCV, 77(1- 3):157–173, May 2008.",
        "[23] A. Sorokin and D. Forsyth. Utility data annotation with amazon me-",
        "[24] A. Torralba, R. Fergus, and W. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. PAMI, 30(11):1958–1970, November 2008.",
        "[26] P. Vossen, K. Hofmann, M. de Rijke, E. Tjong Kim Sang, and K. Deschacht. The Cornetto database: Architecture and user-scenarios. In Proceedings DIR 2007, pages 89–96, 2007.",
        "[27] B. Yao, X. Yang, and S. Zhu. Introduction to a large-scale general purpose ground truth database: Methodology, annotation tool and benchmarks. In EMMCVPR07, pages 169–183, 2007."
    ]
}