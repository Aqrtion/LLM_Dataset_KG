{
    "title_author_abstract_introduction": "MSR-VTT: A Large Video Description Dataset for Bridging Video and Language\nJun Xu , Tao Mei , Ting Yao and Yong Rui Microsoft Research, Beijing, China {v-junfu, tmei, tiyao, yongrui}@microsoft.com\nAbstract\nWhile there has been increasing interest in the task of describing video with natural language, current computer vision algorithms are still severely limited in terms of the variability and complexity of the videos and their associated language that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on speciﬁc ﬁne-grained domains with limited videos and simple descriptions. While researchers have provided several benchmark datasets for image captioning, we are not aware of any large-scale video description dataset with comprehensive categories yet diverse video content.\nIn this paper we present MSR-VTT (standing for “MSRVideo to Text”) which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text. This is achieved by collecting 257 popular queries from a commercial video search enIn its current vergine, with 118 videos for each query. sion, MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering the most comprehensive categories and diverse visual content, and representing the largest dataset in terms of sentence and vocabulary. Each clip is annotated with about 20 natural sentences by 1,327 AMT workers. We present a detailed analysis of MSR-VTT in comparison to a complete set of existing datasets, together with a summarization of different state-of-the-art video-to-text approaches. We also provide an extensive evaluation of these approaches on this dataset, showingthatthehybridRecurrentNeuralNetworkbased approach, which combines single-frame and motion representations with soft-attention pooling strategy, yields the best generalization capability on MSR-VTT.\n1. Introduction\nIt has been a fundamental yet emerging challenge for computer vision to automatically describe visual content with natural language. Especially, thanks to the recent development of Recurrent Neural Networks (RNNs), there has been tremendous interest in the task of image caption-\ning, where each image is described with a single natural sentence [7, 8, 11, 14, 22, 36]. Along with this trend, researchers have provided several benchmark datasets to boost research on image captioning (e.g., Microsoft COCO [21] and Flickr 30K [41]), where tens or hundreds of thousands of images are annotated with natural sentences.\nWhile there has been increasing interest in the task of video to language, existing approaches only achieve severely limited success in terms of the variability and complexity of video contents and their associated language that they can recognize [2, 7, 15, 28, 35, 39, 34]. This is in part due to the simplicity of current benchmarks, which mostly focus on speciﬁc ﬁne-grained domains with limited data scale and simple descriptions (e.g., cooking [5], YouTube [16], and movie [27, 32]). There are currently no large-scale video description benchmarks that match the scale and variety of existing image datasets because videos are signiﬁ- cantly more difﬁcult and expensive to collect, annotate and organize. Furthermore, compared with image captioning, the automatic generation of video descriptions carries additional challenges, such as modeling the spatiotemporal information in video data and pooling strategies.\nMotivated by the above observations, we present in this paper the MSR-VTT dataset (standing for MSR-Video to Text), which is a new large-scale video benchmark for video understanding, especially the emerging task of translating video to text. This is achieved by collecting 257 popular queries from a commercial video search engine, with 118 videos for each query. In its current version, MSR-VTT provides 10K web video clips with 41.2 hours and 200K clip-sentence pairs in total, covering a comprehensive list of 20 categories and a wide variety of video content. Each clip was annotated with about 20 natural sentences.\nFrom a practical standpoint, compared with existing datasets for video to text, such as MSVD [3], YouCook [5], M-VAD [32], TACoS [25, 28], and MPII-MD [27], our MSR-VTT benchmark is characterized by the following major unique properties. First, our dataset has the largest number of clip-sentence pairs, where each video clip is annotated with multiple sentences. This can lead to a better training of RNNs and in consequence the generation of\n1. A black and white horse runs around. 2. A horse galloping through an open field. 3. A horse is running around in green lush grass. 4. There is a horse running on the grassland. 5. A horse is riding in the grass.\n1. A woman giving speech on news channel. 2. Hillary Clinton gives a speech. 3. Hillary Clinton is making a speech at the conference\n1. A child is cooking in the kitchen. 2. A girl is putting her finger into a plastic cup\ncontaining an egg.\nof mayors.\n4. A woman is giving a speech on stage. 5. A lady speak some news on TV.\n3. Children boil water and get egg whites ready. 4. People make food in a kitchen. 5. A group of people are making food in a kitchen.\n1. A man and a woman performing a musical. 2. A teenage couple perform in an amateur musical 3. Dancers are playing a routine. 4. People are dancing in a musical. 5. Some people are acting and singing for performance.\n1. A white car is drifting. 2. Cars racing on a road surrounded by lots of people. 3. Cars are racing down a narrow road. 4. A race car races along a track. 5. A car is drifting in a fast speed.\n1. A player is putting the basketball into the post from\ndistance.",
    "data_related_paragraphs": [
        "Figure 1. Examples of the clips and labeled sentences in our MSR-VTT dataset. We give six samples, with each containing four frames to represent the video clip and ﬁve human-labeled sentences.",
        "more natural and diverse sentences. Second, our dataset contains the most comprehensive yet representative video content, collected by 257 popular video queries in 20 representative categories (including cooking and movie) from a real video search engine. This will beneﬁt the validation of the generalization capability of any approach for video to language. Third, the video content in our dataset is more complex than any existing dataset as those videos are collected from the Web. This plays as a ground challenge for this particular research area. Last, in addition to video content, we keep audio channel for each clip, which leaves a door opened for related areas. Fig. 1 shows some examples of the videos and their annotated sentences. We will make this dataset publically available to the research community to support future work in this area.",
        "In summary, we make the following contributions in this work: 1) we build to-date the largest dataset called MSRVTT for the task of translating video to text, which contains diverse video content corresponding to various categories and diverse textual descriptions, and 2) we summarize existing approaches for translating video to text into a single framework and comprehensively investigate several stateof-the-art approaches on different datasets.",
        "The remaining of this paper are organized as follows. Section 2 reviews related work on vision to text. Section 3 describes the details of MSR-VTT dataset. Section 4 introduces the approaches to video to text. Section 5 presents evaluations, followed by the conclusions in Section 6.",
        "between visual content and sentence. We ﬁrst review the state-of-the-art research along these two dimensions. We then brieﬂy introduce a collection of datasets for videos.",
        "There are several existing datasets for video to text. The YouTube cooking video dataset, named YouCook [5], con-",
        "Figure 2. The distribution of video categories in our MSR-VTT dataset. This distribution well aligns with the real data statistics in a commercial video site.",
        "tains the videos about the scenes where people are cooking various recipes. Each video has a number of humanannotated descriptions about actions and objects. Similarly, TACos [25, 28] and TACos Multi-Level [26] include a set of video descriptions and temporal alignment. MSVD is a collection of 1,970 videos from Youtube with multiple categories [3]. The sentences in this dataset are annotated by AMT workers. On the other hand, M-VAD [32] and MPII-MD [27] provide movie clips with aligned Audio Description (AD) from the Descriptive Video Service (DVS) and scripts rather than human-labeled sentences. We will provide comparison and statistics for all these datasets later. In this work, we build to-date the largest video description dataset with comprehensive video context and well-deﬁned categories. Besides, we summarize the existing approaches for translating video to text into one single framework and conduct a comprehensive evaluation of different methods.",
        "3. The MSR-VTT Datataset",
        "The MSR-VTT dataset is characterized by the unique properties including the large scale clip-sentence pairs, comprehensive video categories, diverse video content and descriptions, as well as multimodal audio and video streams. We next describe how we collect representative videos, select appropriate clips, annotate sentences, and split the dataset. Finally, we would like to summarize the existing video description datasets together and make a comparison.",
        "Current datasets for video to text mostly focus on speciﬁc ﬁne-grained domains. For example, YouCook [5], TACoS [25, 28] and TACoS Multi-level [26] are mainly designed for cooking behavior. MSR-VTT focuses on general videos in our life, while MPII-MD [27] and M-VAD [32] on movie domain. Although MSVD [3] contains general web videos which may cover different categories, the very limited size (1,970) is far from representativeness. To collect representative videos, we obtain the top 257 representative queries from a commercial video search engine, corresponding to 20 categories 1. We then crawl the top 150",
        "Dataset",
        "Table 1. Comparison of video description datasets. Please note that TACos M-L means TACos Multi-Level dataset. Although MSVD dataset has multiple video categories, the category information is not provided. In our MSR-VTT-10K dataset, we provide the category information for each clip. Among all the above datasets, MPII-MD, M-VAD and MSR-VTT contain audio information.",
        "video search results for each query. We remove duplicate and short videos, as well as the videos with bad visual quality, to maintain the data quality. As a result, we have 30,404 representative videos. All the videos were downloaded with high quality and audio channel.",
        "ing searched queries. The clips from the same video or the same queries will not appear solely in the training or testing set to avoid overﬁtting. We split the data according to 65%:30%:5%, corresponding to 6,513, 2,990 and 497 clips in the training, testing and validation sets, respectively.",
        "3.4. Data Statistics",
        "Since our goal is to collect short video clips that each can be described with one single sentence in our current version of MSR-VTT, we adopt color histogram-based approach to segmenting each video (Section 3.1) into shots [23]. As a result, there are 3,590,688 shots detected. As one video clip could have multiple consecutive shots, we asked 15 subjects to watch the videos and select appropriate consecutive shots to form video clips. For each video, at most three clips are selected to ensure the diversity of the dataset. In total there are 30K clips selected, among which we randomly selected 10K clips (originated from 7,180 videos) in the current versionof MSR-VTTandlefttheremaining clipsinoursecond version. The median number of shots for single video clip is 2. The duration of each clip is between 10 and 30 seconds, while the total duration is 41.2 hours.",
        "Although one can leverage Audio Descriptions (AD) to annotate movies [27, 32], it is difﬁcult to obtain quality sentence annotation for web videos. Therefore, we rely on Amazon Mechanical Turk (AMT) workers(1317) to annotate these clips. Each video clip is annotated by multiple workers after being watched. In the post processing, duplicated sentences and too short sentences are removed. As a result, each clip is annotated with 20 sentences by different workers. There are 200K clip-sentence pairs (corresponding to 1.8M words and 29,316 unique words) which represents the datasetwiththelargestnumberofsentencesandvocabulary. Fig. 2 shows the category distribution of these 10K clips.",
        "3.3. Dataset Split",
        "To split the dataset to training, validation and testing sets, we separate the video clips according to the correspond-",
        "Table 1 lists the statistics and comparison among different datasets. We will release more data in the future. In this work, we denote our dataset MSR-VTT-10K as it contains 10, 000 video clips. Our MSR-VTT is the largest dataset in terms of clip-sentence pairs (200K) and word vocabulary (29,316). A major limitation for existing datasets is limited domain and annotated sentences [5, 25, 26, 28]. Although MPII-MD and M-VAD contain a number of clips, both of them are originated from one single domain (i.e., movie). The MSR-VTT is derived from a wide variety of video categories (7,180 videos from 20 general domains/categories), this can beneﬁt the generalization capability of model learning. In addition, compared with the scripts and DVS sentences in the MPII-MD and M-VAD, since MSR-VTT has the largest vocabulary with each clip annotated with 20 different sentences, it can lead to a better training of RNNs and in consequence the generation of more natural and diverse sentences. MSVD [3], which is the most similar dataset to ours, has a small number of clips and sentences. In summary, the MSR-VTT represents the most comprehensive, diverse, and complex dataset for video to language.",
        "We brieﬂy describe different video-to-text approaches that we benchmark on our proposed MSR-VTT dataset. Most of state-of-the-art methods for video to text are based on the Long-Short Term Memory (LSTM), which is a variant of RNN and can capture long-term temporal information by mapping sequences to sequences. As this dimension of research achieves better performance than language model-based approaches, we summarize all the RNN-based approaches in one single framework, as shown in Fig. 3. Speciﬁcally, given an input video, 2-D CNN is utilized to",
        "Among all the RNN-based state-of-the-art methods for translating video to text, we mainly investigate and evaluate in terms of two directions on our MSR-VTT dataset: the mean pooling model proposed in [35] and the soft attention method proposed in [39]. For mean pooling method, we design 7 runs, i.e., MP-LSTM (AlexNet), MP-LSTM (GoogleNet), MP-LSTM (VGG-16), MP-LSTM (VGG19), MP-LSTM (C3D), MP-LSTM (C3D + VGG-16) and MP-LSTM (C3D + VGG-19). The ﬁrst 5 runs employ mean pooling over the frame/clip-level features from AlexNet [17], GoogleNet[31], VGG-16[29], VGG-19[29]andC3D [33] networks, respectively. The last 2 runs feed the concatenations of C3D and VGG-16, C3D and VGG-19 into the LSTM model. Similarly, for soft attention strategy, we also compare 7 runs with different input frame/clip-level",
        "We conducted all the experiments on our newly created MSR-VTT-10K dataset 2 and empirically verify the RNN-based video sentence generation from three aspects: 1) when different video representation is used, 2) when different pooling strategy is exploited, and 3) how the performance is affected when using different size of hidden layer of LSTM.",
        "To describe the visual appearances of frames in video, we adopt the output of 4096-dimensional fc6 layer from AlexNet, VGG-16 and VGG-19 and pool5/7x7s1 layer of GoogleNet which are all pre-trained on ImageNet dataset [6]. C3D architecture, which is pre-trained on Sports-1M video dataset [12] and has been proved to be powerful in action recognition tasks, is utilized to model the temporal information across frames in video. Speciﬁcally, each continuous 16 frames are treated as one short video clip and taken as the inputs of C3D. The 4,096-dimensional outputs of fc6 layer in C3D are regarded as the representations of each video clip. Each sentence is represented as a vector of words, and each word is encoded by one-hot vector. In our experiments, we use about 20,000 most frequent words",
        "2 In addition to MSR-VTT-10K, we will release more data in the future.",
        "The ﬁrst experiment was conducted to examine how different video representations work on sentence generation. Table 3 shows the performances of ﬁve runs averaged over all the test videos in our dataset. It is worth noting that the performances in Table 3 are all with mean pooling. The performance trend is similar with that using soft attention. Overall, the results across BLEU@4 and METEOR consistently indicate that video representations learnt from a temporal clip using C3D leads to a performance boost against frame-based representations. There is a slightly performance difference between VGG-19 and VGG-16. Though both runs utilize VGG network, VGG-19 is deeper than VGG-16 and thus learns a more powerful frame representations. Similar observations are also found when comparing to AlexNet and GoogleNet. The results indicate that improvement can be generally expected when learning frame-based representations by a deeper CNN.",
        "Table 4. Performance comparison on our MSR-VTT dataset of seven video representations with mean pooling and soft attention method, respectively. The number of the hidden layer of LSTM is set to 512 in all the experiments.",
        "these exemplar results, it is easy to see that SA-LSTM (C3D+VGG-19) can generate more accurate sentences. For instance, compared to the sentence “Kids are playing toys” by MP-LSTM (AlexNet) and “People are playing in the room” by SA-LSTM (GoogleNet), the generated sentence “Children are painting in room” by SA-LSTM (C3D + VGG-19) encapsulates the ﬁrst video more clearly. For the last video, the sentences generated by all the methods are not accurate as the video contains multiple diverse scenes. Therefore, there is still much space for researchers in this area to design new algorithms to boost performance on this dataset, especially dealing with complex visual content.",
        "We have extracted the SVO parts from all the sentences and calculated the overlapped percentages of SVO on the 20 annotated sentences to show the human consistency for our dataset. The mean overlapping percentage is 62.7%, which proves the good human consistency for all annotated sentences. Besides, we have conducted human evaluations on different approaches in our dataset in Table 7 in terms of correctness, grammar, and relevance, which shows similar results compared with the above metrics.",
        "We introduced a new dataset for describing video with natural language. Utilizing over 3,400 worker hours, a vast",
        "collection of video-sentence pairs was collected, annotated and organized to drive the advancement of the algorithms for video to text. This dataset contains the most representative videos covering a wide variety of categories and to-date the largest amount of sentences. We comprehensively evaluated RNN-based approaches with variant components on related and our dataset. We found that the temporal representation learned from convolutional 3D networks plays strong complement to the spatial representation, and the soft-attention pooling strategy shows powerful capability to model complex and long video data.",
        "There are several promising directions for future study on our dataset. There remains space to boost the performance on certain categories (corresponding to complex video content) that the approaches introduced in this paper cannot work well. The audio information has not been exploitedforvideodescriptiongeneration. Usingaudioandits AD information may further improve existing performance. The dataset can also be utilized for video summarization if one can build the embedding between video frames and the words. Furthermore, emotion and action recognition could be integrated into existing framework to make the generated language more diverse and natural.",
        "[3] D. L. Chen and W. B. Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of ACL, pages 190–200, 2011. 1, 3, 4",
        "[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A large-scale hierarchical image database. In Proceedings of CVPR, pages 248–255, 2009. 5",
        "[26] A. Rohrbach, M. Rohrbach, W. Qiu, A. Friedrich, M. Pinkal, and B. Schiele. Coherent multi-sentence video description with variable level of detail. Pattern Recognition, pages 184–195, 2014. 3, 4 [27] A. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele. A dataset for",
        "[32] A. Torabi, C. J. Pal, H. Larochelle, and A. C. Courville. Using descriptive video services to create a large data source for video annotation research. arXiv:1503.01070, 2015. 1, 3, 4"
    ]
}