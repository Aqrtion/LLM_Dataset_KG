{
    "title_author_abstract_introduction": "The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes\nGerman Ros†‡, Laura Sellart†, Joanna Materzynska§, David Vazquez†, Antonio M. Lopez†‡\n§ Faculty of Mathematics, ‡Computer Science Dept. †Computer Vision Center University of Vienna Universitat Autonoma de Barcelona Ediﬁci O, Oskar-Morgenstern-Platz Campus UAB Campus UAB Barcelona, Spain Vienna, Austria Barcelona, Spain {gros, laura.sellart, dvazquez, antonio}@cvc.uab.es, a1248555@unet.univie.ac.at\nBuilding\nSidewalk\nVegetation\nMarking\nPedestrian\nCyclist\nFigure. 1. The SYNTHIA Dataset. A sample frame (Left) with its semantic labels (center) and a general view of the city (right).\nVegetation\nPedestrian\nSidewalk\nBuilding\nMarking\nCyclist\nAbstract\n1. Introduction\nVision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classiﬁers to perform such visual tasks. However, DCNNs require learning of many parameters from raw images; thus, having a sufﬁcient amount of diverse images with class annotations is needed. These annotations are obtained via cumbersome, human labour which is particularly challenging for semantic segmentation since pixel-level annotations are required. In this paper, we propose to use a virtual world to automatically generate realistic synthetic images with pixel-level annotations. Then, we address the question of how useful such data can be for semantic segmentation – in particular, when using a DCNN paradigm. In order to answer this questionwe have generateda synthetic collectionof diverse urban images, named SYNTHIA, with automatically generated class annotations. We use SYNTHIA in combination with publicly available real-world urban images with manually provided annotations. Then, we conduct experiments withDCNNs thatshowhowtheinclusionofSYNTHIAinthe trainingstagesigniﬁcantlyimprovesperformanceonthe semantic segmentation task.\nAutonomous driving (AD) will be one of the most revolutionary technologies in the near future in terms of the impact on the lives of citizens of the industrialized countries [43]. Nowadays, advanced driver assistance systems (ADAS) are already improving trafﬁc safety. The computer vision community, among others, is contributing to the development of ADAS and AD due to the rapidly increasing performance of vision-based tools such as object detection, recognition of trafﬁc signs, road segmentation, etc. At the core of such functionality are various types of classiﬁers.\nRoughly until the end of the ﬁrst decade of this century, the design of classiﬁers for recognizing visual phenomena was viewed as a two-fold problem. First, enormous effort was invested in research of discriminative visual descriptors to be fed as features to classiﬁers; as a result, descriptors such as Haar wavelets, SIFT, LBP, or HOG, were born and there use became widespread. Second, many different machine learning methods were developed, with dis-\nAcknowledgements: Authors want to thank Andrew Bagdanov for his help and proofreading and the next funding bodies: the Spanish MEC Project TRA2014-57088-C2-1-R, the Spanish DGT Project SPIP2014- 01352, the People Programme (Marie Curie Actions) FP7/2007-2013 REA grant agreement no. 600388, and by the Agency of Competitiveness for Companies of the Government of Catalonia, ACCIO, the Generalitat de Catalunya Project 2014-SGR-1506 and the NVIDIA Corporation for the generous support in the form of different GPU hardware units.\ncriminative algorithms such as SVM, AdaBoost, or Random Forests usually reporting the best classiﬁcation accuracy due to their inherent focus on searching for reliable class boundaries in feature space. Complementarily, in order to make easier the search for accurate class boundaries, methodsfortransformingfeaturespacewerealsodeveloped (e.g. PCA, BoW encoding, Kernel mappings) as well as more elaborate class models (e.g. DPM, superpixels).\nIn practice, even the best visual descriptors, class models, feature encoding methods and discriminative machine learning techniques are not sufﬁcient to produce reliable classiﬁers if properly annotated datasets with sufﬁcient diversity are not available. Indeed, this is not a minor issue since data annotation remains a cumbersome, human-based labor prone to error; even exploiting crowdsourcing for annotation is a non-trivial task [39]. For instance, for some ADAS and for AD, semantic segmentation is a key issue [33, 18, 29] and it requires pixel-level annotations (i.e. obtained by delineating the silhouette of the different classes in urban scenarios, namely pedestrian, vehicle, road, sidewalk, vegetation, building, etc).\nIn order to ameliorate this problem there are paradigms such as unsupervised learning (no annotations assumed), semi-supervised learning (only a few annotated data), and active learning (to focus on annotating informative data), under the assumption that having annotated data (e.g. images) is problematic but data collection is cheap. However, for ADAS and AD such data collection is also an expensive activity since many kilometers must be traveled to obtain sufﬁcient diversity. Moreover,it is well-known that, in general terms, supervised learning (annotationsassumed) tends to provide the most accurate classiﬁers.\nRecently, the need for large amounts of accurately annotated data has become even more crucial with the massive adoption of deep convolutional neural networks (DCNNs) by the computer vision community. DCNNs have yielded a signiﬁcant performance boost for many visual tasks [17, 10, 40, 35]. Overall, DCNNs are based on highly non-linear, end-to-end training (i.e. from the raw annotated data to the class labels) which implies the learning of millions of parameters and, accordingly, they require a relatively larger amount of annotated data than methods based on hand-crafted visual descriptors.\nAs we will review in section 2, the use of visually realistic synthetic images is gaining attention in recent years (e.g., training in virtual words [21, 34, 1, 26, 12], synthesizingimageswithreal-worldbackgroundsandinsertedvirtual objects [28, 27]) due to the possibility of having diversiﬁed samples with automatically generated annotations. In this spirit, in this paper we address the question of how useful can the use of realistic synthetic images of virtual-world urban scenarios be for the task of semantic segmentation – in particular, when using a DCNN paradigm. To the best of\nour knowledge, this analysis has not been done so far. Note that, in this setting the synthetic training data can not only come with automatically generated class annotations from multiple points of views and simulated lighting conditions (providing diversity), but also with ground truth for depth (simulatingstereorigsandLIDARis possible),opticalﬂow, object tracks, etc.\nMoreover,in the context of ADAS/AD the interest in using virtual scenarios is already increasing for the task of validating functionalities in the Lab, i.e. to perform validation in the real world (which is very expensive) only once after extensive and well-designed simulations are passed. Therefore, these virtual worlds can be used for generating synthetic images to training the classiﬁers involved in environmental perception. In addition, the realism of these virtual worlds is constantly increasing thanks to the continuously growing videogames industry.\nTo address the above mentioned question, we have generated SYNTHIA: a SYNTHetic collection of Imagery and Annotations of urban scenarios.1 SYNTHIA is detailed in section 3 where we highlight its diversity and how we can automatically obtain a large number of images with annotations. On the other hand, it is known that classiﬁers trained only with virtual images may require domain adaptation to work on real images [42, 44, 37, 25, 45]; however, it has been shown that this is just because virtual and real world cameras are different sensors, i.e. domain adaptation is also often required when training images and testing images come from different real-world camera sensors [42, 41].\nAs we will see in section 4, where we explain the DCNN used to perform semantic segmentation, in this work we use a simple domain adaptation strategy which consists of training with the synthetic data and a smaller number of real-world data simultaneously, i.e. in the same spirit than [42] for a HOG-LBP/SVM setting. In our case, the data combination is done in the generation of batches during DCNN training. The experiments conducted in section 5 show how SYNTHIA successfully complements different datasets (Camvid, KITTI, U-LabelMe, CBCL) for the task of semantic segmentation based on DCNNs, i.e. the use of the combined data signiﬁcantly boosts the performance obtained when using the real-world data alone. The future work that we foresee given these results is pointed out in section 6, together with the conclusions of the paper.",
    "data_related_paragraphs": [
        "The generation of semantic segmentation datasets with pixel-level annotations is costly in terms of effort and money,factors that are currentlyslowing down the developmentofnew large-scalecollectionslike ImageNet[15]. Despite these factors, the community has invested great effort",
        "to create datasets such as the NYU-Depth V2 [23] (more than 1,449 images densely labelled), the PASCAL-Context Dataset [22] (10,103 images densely labelled over 540 categories), and MS COCO [19] (more than 300,000 images with annotations for 80 object categories). These datasets have deﬁnitely contributed to boost research on semantic segmentation of indoor scenes and also on common objects, but they are not suitable for more speciﬁc tasks such as those involved in autonomous navigation scenarios.",
        "Whensemanticsegmentationis seeninthecontextofautonomous vehicles, we ﬁnd that the amount and variety of annotatedimages of urbanscenarios is much lower in terms of total number of labeled pixels, number of classes and instances. A good example is the CamVid [4] dataset, which consists of a set of monocular images taken in Cambridge, UK. However, only 701 images contain pixel-level annotations over a total of 32 categories (combining objects and architectural scenes), although usually only the 11 largest categories are used. Similarly, Daimler Urban Segmentation dataset [33] contains 500 fully labelled monochrome frames for 5 categories. The more recent KITTI benchmark suite [9] has provided a large amount of images of urban scenes from Karlsruhe, Germany, with ground truth data for several tasks. However, it only contains a total of 430 labelled images for semantic segmentation.",
        "A common limitation of the aforementioned datasets is thebias introducedbytheacquisitionofimagesin a speciﬁc city. The LabelMe project [32], later reﬁned by [30], correctsthisbyofferingaround1,000fullyannotatedimagesof urban environments around the world and more than 3,000 images with partial (noisy) annotations.",
        "A larger dataset is the CBCL StreetScenes [3], which contains 3,547 images of the streets of Chicago over 9 classes with noisy annotations. This dataset has recently been enhanced in [30], improvingthe quality of the annotations and adding extra classes. To date, the largest dataset for semantic segmentation is the CityScapes dataset [8], which consists of a collection of images acquired in 50",
        "cities around Germany, Switzerland and France in different seasons, and having 5,000 images with ﬁne annotations and 20,000 with coarse annotations over a total of 30 classes. However, the cost of scaling this sort of project would require a prohibitive economic investment in order to capture images from a larger variety of countries, in different seasons and different trafﬁc conditions. For these reasons, a promising alternative proposed in this work is to use synthetic imagery that simulate real urban scenes in a vast variety of conditions and produce the appropriate annotations. The use of synthetic data has increased considerably in recent years within the computervision communityfor several problems. For instance, in [14], the authors used a virtual world to evaluate the performance of image features under certain types of changes. In the area of object detection, similar approaches have been proposedby different groups [27, 37, 21, 12], making use of CAD models, virtual worlds and studying topics such as the impact of a realistic world on the ﬁnal accuracy of detectors and the importance of domain adaptation. Synthetic data has also been used for pose estimation [1, 6] to compensate for the lack of precise pose annotations of objects. The problem of semantic segmentationhas also begunto beneﬁt fromthis trend, with the creationofvirtualscenes to performsegmentationof indoor environments [11, 26].",
        "The present work proposes a novel synthetic dataset of urban scenes, which we call SYNTHIA. This dataset is a large collection of images with high variability due to changes in illumination, textures, pose of dynamic objects and camera view-points. We also explore the beneﬁts of using SYNTHIA in the context of semantic segmentation of urban environments with DCNNs.",
        "3. The SYNTHIA Dataset",
        "Here we describe our synthetic dataset of urban scenes, which we call the SYNTHetic collection of Imagery and Annotations (SYNTHIA). This dataset has been generated with the purpose of aiding semantic segmentation in the",
        "SYNTHIAhas beengeneratedbyrenderinga virtualcity created with the Unity development platform [38]. This city includes the most important elements present on driving environments, such as: street blocks, highways, rural areas, shops, parks and gardens, general vegetation, variety of pavements, lane markings, trafﬁc signs, lamp poles, and people, among others. The virtual environment allows us to freely place any of these elements in the scene and to generate its semantic annotations without additional effort. This enables the creation of new and diverse cities as a simple combination of basic blocks. The basic properties of these blocks, such as textures, colors and shapes can be easily changed to produce new looks and to enhance the visual variety of the data.",
        "We have deﬁned suitable material coefﬁcients for each of the surfaces of the city in order to producephoto realistic outcomes that look as similar as possible to real data. Our virtual world also includes four different seasons with drastic change of appearance, with snow during winter, blooming ﬂowers during spring, etc., (see Fig. 3). Moreover, a dynamic illumination engine serves to produce different illumination conditions, to simulate different moments of the day, including sunny and cloudy days and dusk. Shadows caused by clouds and other objects are dynamically cast on the scene, adding additional realism.",
        "SYNTHIA-Rand consists of 13,400 frames of the city taken from a virtual array of cameras moving randomly through the city, with its height limited to the range [1.5m, 2m] from the ground. In each of the camera poses, several frames are acquired changing the type of dynamic objects present in that part of the scene along with the illumination of the scene and the textures of road and sidewalks. We enforce that the separation between camera positions is at least of 10 meters in order to improve visual variability. This collection is oriented to serve as training data for semantic segmentation methods based on DCNNs.",
        "SYNTHIA-Seqs simulates four video sequences of approximately50,000frameseachoneupto a total of200,000 frames, acquired from a virtual car across different seasons (one sequence per season). The virtual acquisition platform consists of two multi-cameras separated by a baseline B = 0.8m in the x-axis. Each of these multi-cameras consists of four monocular cameras with a common center and orientations varying every 90 degrees, as depicted in Fig. 4. Since all cameras have a ﬁeld of view of 100 degrees the visual overlapping serves to create an omnidirectional view on demand, as shown in Fig. 5. Each of these cameras also has a virtual depth sensor associated, which works in a range from 1.5 to 50 meters and is perfectly aligned with the camera center, resolution and ﬁeld of view (Fig. 5, bottom). The virtual vehicle moves through the city interacting with dynamic objects such as pedestrians and cyclists that present dynamic behaviour. This interaction produces changes in the trajectory and speed of the vehicle and leads to variations of each of the individual video sequences. This collection is oriented to provide data to exploit spatio-temporal constraints of the objects.",
        "We ﬁrst deﬁne a simple but competitive deep Convolutional Neural Network (CNN) for the task of semantic segmentationof urbanscenes, followingthe descriptionof [30] (section 4.1). This architecture, referred as Target-Net (TNet) is more suitable for its application to urban scenes due to its reduced number of parameters. As a reference, we also consider the FCN [20], a state-of-the-art architecture for general semantic segmentation. In 4.2 we describe the strategyusedtodealwiththesyntheticdomain(virtualdata) and the real domain during the training stage.",
        "ance. During training the contraction blocks are initialized using VGG-F [7] for T-Net and VGG-16 [36] for FCN, pretrained on ILSVRC [31]. Kernels are accordingly re-scaled when the original sizes do not match. Expansion blocks are randomly initialized following the method of He et al. [13]. Input data is pre-processed using local contrast normalization of each channel independently to avoid problems with drastic illumination changes.",
        "4.2. Training on Real and Synthetic Data",
        "The aim of this work is to show that the use of synthetic data helps to improve semantic segmentationresults on real imagery. There exist several ways to exploit synthetic data for this purpose. A trivial option would be to use the synthetic data alone for training a model and then apply it on real images. However, due to domain shift [37, 42] this approach does not usually perform well. An alternative is to traina modelon the vast amountof syntheticimagesandafterwards ﬁne-tuning it on a reduced set of real images. This leads to better results, since the statistics of the real domain are considered during the second stage of training [27].",
        "However,here we employthe Balanced GradientContribution (BGC) that was ﬁrst introducedin [30]. It consists of building batches with images from both domains (synthetic and real), given a ﬁxed ratio. Real images dominate the distribution, while synthetic images are used as a sophisticated regularization term. Thus, statistics of both domains are considered during the whole procedure, creating a model which is accurate for both. In section 5 we show that extending real data with synthetic images using this technique leads to a systematic boost in segmentation accuracy.",
        "We present the evaluation of the DCNNs for semantic segmentation described in section 4, training and evaluatingonseveralstate-of-the-artdatasets ofdrivingscenes. We test how the new SYNTHIA dataset can be useful both on its own and along with real images to produce accurate segmentation results. For the following experiments we have made use of the 13,400 images of the SYNTHIA-Rand collection to favour visual variability while using a moderate number of images.",
        "5.1. Validation Datasets",
        "We selectedpubliclyavailableurbandatasetstostudythe beneﬁts of SYNTHIA. Table 1 shows the different datasets used in our experiments,along with a deﬁnitionof the number of images used in our experiments for training (T) and validation (V).",
        "We use weighted cross-entropy as a loss function for both architectures, where the weights are computed as the inverse frequencies of each of the classes for the training data [2]. This helps prevent problems due to class imbal-",
        "Dataset CamVid [4, 5] KITTI [9, 18, 29, 30] Urban LabelMe [32, 30] CBCL StreetScenes [3, 30] SYNTHIA-Rand",
        "It is worth highlighting the differences between these datasets. Each of them has been acquired in a different city or cities. CamVid and KITTI datasets have high quality labels and low complexity in terms of variations and atypical scenes. Urban LabelMe (U-LabelMe) is very challenging, since it contains images from different cities and several view-points. It has been annotated by several users and contains some images with partial and noisy annotations. CBCL imagesarealsochallenging,andcontainmanynoisy, semi-supervised annotations [30]. Each of the individual splitsis designedtoincludealargenumberofvalidationimages, keeping enough images for training on each datasets.",
        "In our ﬁrst experiment we evaluate the capability of SYNTHIA-Rand in terms of the generalizationof the trained models on state-of-the-art datasets. To this end we report in Table 2 the accuracy (%) of T-Net and FCN for each of the 11 classes along with their average per-class and global accuracies for each of the validation sets (V).",
        "Thenetworkstrainedonjust syntheticdataproducegood results recognizingroads, buildings, cars and pedestrians in the presented datasets. Moreover, sidewalks are fairly well recornized in CamVid, probably due to their homogeneity. Thehighaccuracyatsegmentingroads,carsandpedestrians in U-LabelMe–one of the most challenging datasets due to the large variety of view-points–is a proof of the high quality ofSYNTHIA.Notice alsothat FCN performsbetter than T-Net for many of the classes due to the higher capacity of themodel,althoughinpracticeFCNhasthedisadvantageof being too large for embedded context such as autonomous driving. It is worth highlighting that the average per-class accuracy of the models trained with SYNTHIA is close or some times even higher (e.g. T-Net: CamVid, U-LabelMe, CBCL; FCN: CamVid, CBCL) than those models trained on real data (see Table 3).",
        "Our second experiment evaluates the true potential of SYNTHIA to boost DCNN models trained on real data. To this end we perform several tests combining data from SYNTHIA-Rand along with individual real datasets, following the strategy deﬁned in section 4.2. Here, each batch contains 6 images from the real domain and 4 from the syntheticdomain. Theseresultsarecomparedagainstusingjust realdata comingfromeachrespectivetrainingsplit (T).The outcome of this experiment is shown in Table 3. Observe that, for all the datasets and architectures, the inclusion of synthetic data systematically helps to boost the average perclass accuracy. Improvements with respect to the baselines (training only with real data) are highlighted in blue. Notice that for both, T-Net and FCN there are improvements",
        "Table 2. Results of training a T-Net and a FCN on SYNTHIA-Rand and evaluating it on state-of-the-art datasets of driving scenes.",
        "of more than 10 points (up to 18.3 points) in per-class accuracy. We believe that the decrement of global accuracy for FCNmayberelatedtothecombinationofearlyandlatelayers during the upsampling process and the use of BGC, but further investigation is still required. The classes that most beneﬁt from the addition of synthetic data are pedestrian, car and cyclist (dynamic objects), which is due to the lack of enough instances of these classes in the original datasets. On the other hand, signs and poles are very hard to segment as a consequence of the low-resolution images.",
        "Fig. 7 shows qualitative results of the previous experiments. Observe how the training on synthetic data is good enough to recognize pedestrians, roads, cars and some cyclists. Thenthecombinationofrealandsyntheticdata(right column)producessmoothand very accurateresults for both objects and architectural elements, even predicting thin objects like poles. We consider the results of these experi-",
        "ments an important milestone for the use of synthetic data as the main information source for semantic segmentation.",
        "We presented SYNTHIA, a new dataset for semantic segmentationofdrivingsceneswithmorethan213,400synthetic images including both, random snapshots and video sequences in a virtual city. Images are generated simulating different seasons, weather and illumination conditions from multiple view-points. Frames include pixel-level semantic annotations and depth. SYNTHIA was used to train DCNNs for the semantic segmentation of 11 common classes in driving scenes. Our experiments showed that SYNTHIA is good enough to produce good segmentations by itself on real datasets, dramatically boosting accuracy in combination with real data. We believe that SYNTHIA will help to boost semantic segmentation research.",
        "Figure 7. Qualitative results for different testing datasets and architectures (T-Net and FCN). First column shows the RGB testing frame; second column is the ground truth; Training R isthe result of training withthe real dataset; Training V isthe result of training withSYNTHIA; Training R+V is the result of training with the real and SYNTHIA-Rand collection. Including SYNTHIA for training considerably improves the results.",
        "[1] M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic. Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014. [2] V. Badrinarayanan, A. Handa, and R. Cipolla.",
        "[3] S. Bileschi. CBCL StreetScenes challenge framework, 2007. [4] G. J. Brostow, J. Fauqueur, and R. Cipolla. Semantic object classes in video: A high-deﬁnition ground truth database. Pattern Recognition Letters, 2009.",
        "[6] P. P. Busto, J. Liebelt, and J. Gall. Adaptation of synthetic data for coarse-to-ﬁne viewpoint reﬁnement. In British Machine Vision Conf. (BMVC), 2015.",
        "[8] M. Cordts, M. Omran, S. Ramos, T. Scharw¨achter, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset. In CVPR, Workshop, 2015.",
        "[9] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets Robotics: The KITTI Dataset. Intl. J. of Robotics Research, 2013.",
        "[11] A. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and R. Cipolla. Synthcam3d: Semantic understanding with synthetic indoor scenes. arXiv preprint abs/1505.00171, 2015. [12] H. Hattori, V. Naresh Boddeti, K. M. Kitani, and T. Kanade. Learning scene-speciﬁc pedestrian detectors without real data. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2015.",
        "[25] P. Panareda, J. Liebelt, and J. Gall. Adaptation of synthetic data for coarse-to-ﬁne viewpoint reﬁnement. In British Machine Vision Conf. (BMVC), 2015.",
        "[30] G.Ros, S.Stent, P.F.Alcantarilla,and T.Watanabe. Training constrained deconvolutional networks for road scene semantic segmentation. arXiv preprint abs/1604.01545, 2016. [31] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet large scale visual recognition challenge. Intl. J. of Computer Vision, 2015. [32] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. LabelMe: a database and web-based tool for image annotation. Intl. J. of Computer Vision, 2008.",
        "[38] U. Technologies. Unity Development Platform. [39] T.L Berg, A. Sorokin, G. Wang, D.A. Forsyth, D. Hoeiem, I. Endres, and A. Farhadi. It’sall about the data. Proceedings of the IEEE, 2010.",
        "[41] A. Torralba and A. Efros. Unbiased look at dataset bias. In IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2011."
    ]
}