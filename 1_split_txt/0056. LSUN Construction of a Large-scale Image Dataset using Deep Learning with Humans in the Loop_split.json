{
    "title_author_abstract_introduction": "LSUN: Construction of a Large-Scale Image Dataset using Deep Learning with Humans in the Loop\nFisher Yu\nAri Seff\nYinda Zhang\nShuran Song\nThomas Funkhouser\nJianxiong Xiao\nPrinceton University\nAbstract\nWhile there has been remarkable progress in the performance of visual recognition algorithms, the state-of-the-art models tend to be exceptionally data-hungry. Large labeled training datasets, expensive and tedious to produce, are required to optimize millions of parameters in deep network models. Lagging behind the growth in model capacity, the available datasets are quickly becoming outdated in terms of size and density. To circumvent this bottleneck, we propose to amplify human effort through a partially automated labeling scheme, leveraging deep learning with humans in the loop. Starting from a large set of candidate images for each category, we iteratively sample a subset, ask people to label them, classify the others with a trained model, split the set into positives, negatives, and unlabeled based on the classiﬁcation conﬁdence, and then iterate with the unlabeled set. To assess the effectiveness of this cascading procedure and enable further progress in visual recognition research, we construct a new image dataset, LSUN. It contains around one million labeled images for each of 10 scene categories and 20 object categories. We experiment with training popular convolutional networks and ﬁnd that they achieve substantial performance gains when trained on this dataset.\nIntroduction\nHigh capacity supervised learning algorithms, such as deep convolutional neural networks [10, 9], have led to a discontinuity in visual recognition over the past four years and continue to push the state-of-the-art performance (e.g. [7, 22, 8]). These models usually have millions of parameters, resulting in two consequences. On the one hand, the many degrees of freedom of the models allow for extremely impressive description power; they can learn to represent complex functions and transformations automatically from the data. On the other hand, to search for the optimal settings for a large number of parameters, these data-hungry algorithms require a massive amount of training data with humainitiallced labels [4, 6, 23, 24].\nAlthough there has been remarkable progress in improving deep learning algorithms (e.g. [7, 16]) and developing high performance training systems (e.g. [22]), advancements are lacking in dataset construction. The ImageNet dataset [4], used by most of these algorithms, is 7 years old and has been heavily over-ﬁtted [1]. The recently released Places [24] dataset is not much larger. The number of parameters in many deep models now exceeds the number of images in these datasets. While the models are getting deeper (e.g. [15, 16]) and the accessible computation power is increasing, the size of the datasets for training and evaluation is not increasing by much, but rather lagging behind and hindering further progress in large-scale visual recognition.\nMoreover, the density of examples in current datasets is quite low. Although they have several million images, they are spread across many categories, and so there are not very many images in each category (e.g., 1000 in ImageNet). As a result, deep networks trained on them often learn features that are noisy and/or unstable [17, 11]. To address this problem, researchers have used techniques\nthat augment the training sets with perturbations of the original images [22]. Those methods add to the stability and generalizability to trained models, but without increasing the actual density of novel examples. We propose an alternative: an image dataset with high density. We aim for a dataset with ∼ 106 images per category, which is around 10 times denser than PLACES [24] and 100 times denser than ImageNet [4].\nAlthough the amount of available image data on the Internet is increasing constantly, it is nontrivial to build a supervised dataset that dense because of the high costs of manual labeling. Clever methods have been proposed to improve the efﬁciency of human-in-the-loop annotation (e.g., [2, 5, 13, 20, 21]). However, manual effort is still the bottleneck – the construction of ImageNet and Places both required more than a year of human effort via Amazon Mechanical Turk (AMT), the largest crowd-sourcing platform available on the Internet. If we desire a N-times bigger/denser dataset, it will require more than N years of human annotation. This will not scale quickly enough to support advancements in deep visual learning. Clearly, we must introduce a certain amount of automation into this process in order to let data annotation maintain pace with the growth of deep models.\nIn this paper, we introduce an integrated framework using deep learning with humans in the loop to annotate a large-scale image dataset. The key new idea is a labeling propagation system that automatically ampliﬁes manual human effort. We study strategies for selecting images for people to label, interfaces for acquiring labels quickly, procedures for verifying the labels acquired, and methods for amplifying the labels by propagating them to other images in the dataset. We have used the system to construct a large scale image database, “LSUN”, with 10 million labeled images in 10 scene categories and 59 million labeled images in 20 object categories.1.\nOne of the main challenges for a semi-automatic approach is achieving high precision in the ﬁnal labeled dataset. Our procedure uses statistical tests to ensure labeling quality, providing more than 90% precision on average according to veriﬁcation tests. This is slightly lower than manual annotation, but still good enough to train high performance classiﬁcation models. During experiments with popular deep learning models, we observe substantial classiﬁer performance gains when using our larger training set. Although the LSUN training labels may contain some noise, it seems that training on a larger, noisy dataset produces better models than training on smaller, noise-free datasets.",
    "data_related_paragraphs": [
        "Our iterative method learns a cascade of over-ﬁt classiﬁers, each of which splits a set of images into positive, negative, and unlabeled subsets. The positive examples are added to our dataset; the negative examples are discarded; and the unlabeled subset is passed to the next iteration for further processing. The cascade terminates when the unlabeled subset is small enough that all images can be labeled manually. If the unlabeled subset shrinks by a signiﬁcant factor at each iteration, the process can label a very large data set with a practical amount of human effort.",
        "The following sections describe the main components of our system, with detailed descriptions of how we collect data, learn a deep network classiﬁer for each iteration, design an interface for manual labeling, and control for label quality. These descriptions are followed by results of experiments designed to test whether the collected dataset is helpful for image classiﬁcation.",
        "1We are continuing to collect and annotate data through our platform at a rate of 5 million images and 2",
        "3 Data Collection",
        "The ﬁrst step in constructing a large-scale image database is collecting the images themselves. As in previous work, we leverage existing image search engines to gather image URLs. Here, we use Google Images search. By querying with appropriate adjectives and synonyms, we can bypass the usual limit on search results and obtain nearly 100 million relevant URLs for each category.",
        "Similar to SUN and MS COCO, our target database will contain images from both scene and object categories. To generate queries for scene categories, 696 common adjectives relevant to scenes (messy, spare, sunny, desolate, etc.), manually selected from a list of popular adjectives in English (obtained by [24]), are combined with each scene category name. Adding adjectives to the queries allows us to both download a larger number of images than what is available in previous datasets and increase the diversity of visual appearances. To further increase the number of search results per category, we set the time span for each query to three days and query all three-day time spans since 2009. We ﬁnd that a shorter time span seldom returns more useful results. We then remove duplicate URLs and download all accessible images (some URLs may be invalid or the images not properly encoded). These same methods are used to collect image URLs for object categories, except the queries use a different set of object-relevant adjectives. As an example of our querying results, we obtain more than 111 million unique URLs for images relevant to “dog”.",
        "To date, more than 1 billion images have been downloaded. An initial quality check is performed before labeling these images. Only images with smaller dimension greater than 256 are kept. After the check, around 60% of the images are kept. Unlike previous systems [23, 24], we don’t remove duplicates (in terms of image content) in this initial pool of images as it is very expensive to remove duplicates while avoiding overkill on tens of millions of non-duplicate images. In addition, our pipeline requires only a small subset of this pool (with rare duplicates) to be manually labeled. Therefore leaving duplicates in the original image pool does not increase the cost of manual labeling. The duplicate images usually exhibit various compression qualities, sizes, and even croppings. They augment our dataset naturally, and it will be up to algorithm designers how to utilize the duplication. All images from this initial pool will be released for unsupervised image analysis.",
        "To gather effective examples for each category, we ﬁrst address obvious corner cases. For example, for the “car” category, we would like trucks and buses to be excluded as these will later form separate categories. Therefore, we specify that trucks and buses should be marked as negative on the “car” instruction page. In addition, for each object category, we decide which parts of the object are signiﬁcant enough that when visible, they make the object’s identity clear. For example, when only one wheel is visible in an image, there is not enough information to conﬁdently assert the presence of a bicycle. While we acknowledge that these decisions are ultimately subjective, by keeping these instructions consistent for all the workers we can build a dataset that cleanly abides by these class deﬁnitions.",
        "In addition to category-speciﬁc examples, we also provide the workers with examples of general types of images we do not want to include in our dataset. For example, as we wish to build a natural image dataset, we advise the workers to mark as negative any computer-generated or cartoon imagery.",
        "We are running this labeling pipeline continuously and have now collected enough data to evaluate how well it is performing and study its potential impact on visual recognition.",
        "Dataset Statistics: So far, we have collected 59 million images for 20 object categories selected from PASCAL VOC 2012 and 10 million images for 10 scene categories selected from the SUN database. Figure 3 shows the statistics of object categories. The number of images used in ImageNet classiﬁcation challenge is about 1.4 million, which is used to train recent convolutional networks. Most of our object categories have more images than the whole challenge. Even if we only consider the basic level categories in ImageNet such as putting all the dogs together, our dataset is still much denser.",
        "Label Precision: We have run a series of experiments to test the labeling precision of this dataset. We sampled 2000 images from 11 object categories and label them with completely manual pipeline using trained experts (AMT labeling is not reliable enough for this test). The ﬁnal precision of those categories is shown in Figure 4aa. We observe that normally, the precision is around 90%, but it varies across different categories. The major mistakes in our labels are caused by toys, computer rendered images, photo collages, and human edited photos. The inclusion of computer-rendered images is the most serious for the person category. Although we have tests for classiﬁer conﬁdence, there are lots of confusing cases for human labelers on these images, which subsequently confuse the statistical tests.",
        "Figure 3: Number of images in our object categories. Compared to ImageNet dataset, we have more images in each category, even comparing only basic level categories.",
        "Impact on Model Performance: We next investigate whether the new dataset can help current models achieve better classiﬁcation performance. As a ﬁrst test, we use the standard AlexNet [9] trained on PLACES and compare the model ﬁne-tuned by the PLACES data [24] with the same model ﬁne-tuned by both PLACES and LSUN, in both cases ﬁne-tuning on only the 10 categories in LSUN. As a simple measure to balance the dataset, we only take at most 200 thousand images from each LSUN category. Then we test the classiﬁcation results on this 10 categories in PLACES testing set. The error percentage comparison is shown in Figure 5. The overall detection error percentage is 28.6% with only PLACES and 22.2% with both PLACES and LSUN, which yields a 22.37% improvement on testing error. Although the testing is unfavorable to LSUN due to potential dataset bias issues [19], we can see that the additional of more data can improve the classiﬁcation accuracy on PLACES testing set for 8 of the 10 categories.",
        "0.00E+01.00E+62.00E+63.00E+64.00E+65.00E+66.00E+618.9E+680%85%90%95%100%010203040506070bedroomkitchenliving roomdining roombridgetowerrestaurantconference roomclassroomchurch outdoor0%10%20%30%40%Error PercentagePLACES+LSUNPLACES\fWe then use the pre-trained model to ﬁne-tune on PASCAL VOC 2012 classiﬁcation training images with Hinge loss and evaluate the learned representation. We use two pre-trained models for comparison. One is AlexNet trained by our object data from scratch. The other is VGG trained by our object data but initialized by ImageNet. We sample 300 thousand images from each of the 20 object categories and use them as training data from LSUN. To get classiﬁcation score for testing images, we sample 10 crops from each testing image at center and four corners with mirroring and take the average. The results are evaluated on the validation set for each ablation study. As a comparison, we go through the same procedure with model pre-trained on ImageNet. The results are shown in Table 1. The table shows that the model pre-trained on our data performs signiﬁcantly better. It indicates that it is better to have more images in relevant categories than more categories.",
        "Together with the results on scene categories, it is interesting to observe that although AlexNet is small compared to other image classiﬁcation models developed in recent years, even it can beneﬁt from more training data.",
        "Table 1: Comparison of pre-training on ImageNet and LSUN. We pre-train AlexNet and VGGnet with ImageNet and our data to compare the learned representation. We ﬁne tune the networks after removing the last layer on PASCAL VOC 2012 images and compare the results. The table shows that pre-training with more images in related categories is better than that with more categories.",
        "Learned Image Representation: As a ﬁnal test, we study the image representation learned by our object categories and compare it to ImageNet to understand the tradeoff between number of images and categories. We compare AlexNet trained by ImageNet and our training data used in the last section. Then we check the response in the ﬁrst layer. As shown in Figure 6, we ﬁnd that the ﬁlters in the ﬁrst level present cleaner patterns than those learned on ImageNet data.",
        "Figure 6: Comparison of learned ﬁlters. We train AlexNet with our object images and compare the ﬁrst level ﬁlter with ImageNet. We observe that the ﬁlter pattern learned with our data is cleaner, while the ﬁlters from ImageNet are noisier, as highlighted in (b).",
        "This paper proposes a working pipeline to acquire large image datasets with category labels using deep learning with humans in the loop. We have constructed an initial version of “LSUN” database, a database with around a million labeled images in each scene category and more than a million in each of 20 object categories. Experiments with this dataset have already demonstrated the great potential of denser training data for visual recognition. We will continue to grow the dataset indeﬁnitely, making it freely available to the community for further experimentation.",
        "[3] B. Collins, J. Deng, K. Li, and L. Fei-Fei. Towards scalable dataset construction: An active learning",
        "database. In CVPR, pages 248–255. IEEE, 2009.",
        "[19] A. Torralba and A. A. Efros. Unbiased look at dataset bias. In CVPR, 2011. [20] S. Vijayanarasimhan and K. Grauman. Multi-level active prediction of useful image annotations for In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural",
        "[23] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition",
        "using places database. In NIPS, 2014."
    ]
}