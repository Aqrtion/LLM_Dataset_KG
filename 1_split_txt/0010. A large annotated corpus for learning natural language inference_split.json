{
    "title_author_abstract_introduction": "A large annotated corpus for learning natural language inference\nSamuel R. Bowman∗† sbowman@stanford.edu\nGabor Angeli†‡ angeli@stanford.edu\nChristopher Potts∗ cgpotts@stanford.edu\nChristopher D. Manning∗†‡ manning@stanford.edu\n∗Stanford Linguistics\n†Stanford NLP Group ‡Stanford Computer Science\nAbstract\nUnderstanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classiﬁers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the ﬁrst time.\nIntroduction\nThe semantic concepts of entailment and contradiction are central to all aspects of natural language meaning (Katz, 1972; van Benthem, 2008), from the lexicon to the content of entire texts. Thus, naturallanguageinference(NLI)—characterizing and using these relations in computational systems (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009) — is essential in tasks ranging from information retrieval to semantic parsing to commonsense reasoning.\nNLI has been addressed using a variety of techniques, including those based on symbolic logic, knowledge bases, and neural networks. In recent years, it has become an important testing ground\nfor approaches employing distributed word and phrase representations. Distributed representations excel at capturing relations based in similarity, and have proven effective at modeling simple dimensions of meaning like evaluative sentiment (e.g., Socher et al. 2013), but it is less clear that they can be trained to support the full range of logical and commonsense inferences required for NLI (Bowman et al., 2015; Weston et al., 2015b; In a SemEval 2014 task Weston et al., 2015a). aimed at evaluating distributed representations for NLI, the best-performing systems relied heavily on additional features and reasoning capabilities (Marelli et al., 2014a).\nOur ultimate objective is to provide an empirical evaluation of learning-centered approaches to NLI, advancing the case for NLI as a tool for the evaluation of domain-general approaches to semantic representation. However, in our view, existing NLI corpora do not permit such an assessment. They are generally too small for training modern data-intensive, wide-coverage models, many contain sentences that were algorithmically generated, and they are often beset with indeterminacies of event and entity coreference that signiﬁcantly impact annotation quality.\nTo address this, this paper introduces the Stanford Natural Language Inference (SNLI) corpus, a collection of sentence pairs labeled for entailment, contradiction, and semantic independence. At 570,152 sentence pairs, SNLI is two orders of magnitude larger than all other resources of its type. And, in contrast to many such resources, all of its sentences and labels were written by humansinagrounded, naturalisticcontext. Inaseparate validation phase, we collected four additional judgments for each label for 56,941 of the examples. Of these, 98% of cases emerge with a threeannotator consensus, and 58% see a unanimous consensus from all ﬁve annotators.\nIn this paper, we use this corpus to evaluate\nAmaninspectstheuniformofaﬁgureinsomeEast Asian country.\ncontradiction C C C C C\nThe man is sleeping\nAn older and younger man smiling.\nneutral N N E N N\nTwo men are smiling and laughing at the cats playing on the ﬂoor.\nA black race car starts up in front of a crowd of people.\ncontradiction C C C C C\nA man is driving down a lonely road.\nA soccer game with multiple males playing.\nentailment E E E E E\nSome men are playing a sport.\nA smiling costumed woman is holding an umbrella.\nneutral N N E C N\nA happy woman in a fairy costume holds an umbrella.\nTable 1: Randomly chosen examples from the development section of our new corpus, shown with both the selected gold labels and the full set of labels (abbreviated) from the individual annotators, including (in the ﬁrst position) the label used by the initial author of the pair.\na variety of models for natural language inference, including rule-based systems, simple linear classiﬁers, and neural network-based models. We ﬁnd that two models achieve comparable performance: a feature-rich classiﬁer model and a neural network model centered around a Long Short-Term Memory network (LSTM; Hochreiter and Schmidhuber 1997). We further evaluate the LSTMmodelbytakingadvantageofitsreadysupport for transfer learning, and show that it can be adaptedtoanexistingNLIchallengetask, yielding the best reported performance by a neural network model and approaching the overall state of the art.",
    "data_related_paragraphs": [
        "To date, the primary sources of annotated NLI corpora have been the Recognizing Textual Entailment (RTE) challenge tasks.1 These are generally high-quality, hand-labeled data sets, and they have stimulated innovative logical and statistical models of natural language reasoning, but their small size (fewer than a thousand examples each) limits their utility as a testbed for learned distributed representations. The data for the SemEval 2014 task called Sentences Involving Compositional Knowledge (SICK) is a step up in terms of size, but only to 4,500 training examples, and its partly automatic construction introduced some spurious patterns into the data (Marelli et al. 2014a, §6). The Denotation Graph entailment set (Young et al., 2014) contains millions of examples of entailments between sentences and artiﬁcially constructed short phrases, but it was labeled using fully automatic methods, and is noisy enough that it is probably suitable only as a source of sup-",
        "plementary training data. Outside the domain of sentence-level entailment, Levy et al. (2014) introduce a large corpus of semi-automatically annotated entailment examples between subject–verb– object relation triples, and the second release of the Paraphrase Database (Pavlick et al., 2015) includes automatically generated entailment annotations over a large corpus of pairs of words and short phrases.",
        "With SNLI, we sought to address the issues of size, quality, and indeterminacy. To do this, we employed a crowdsourcing framework with the following crucial innovations. First, the examples were grounded in speciﬁc scenarios, and the premise and hypothesis sentences in each examplewereconstrainedtodescribethatscenariofrom the same perspective, which helps greatly in controlling event and entity coreference.2 Second, the prompt gave participants the freedom to produce entirely novel sentences within the task setting, which led to richer examples than we see with the more proscribed string-editing techniques of earlier approaches, without sacriﬁcing consistency. Third, a subset of the resulting sentences were sent to a validation task aimed at providing a highly reliable set of annotations over the same data, and at identifying areas of inferential uncertainty.",
        "2.1 Data collection",
        "We used Amazon Mechanical Turk for data collection. In each individual task (each HIT), a worker was presented with premise scene descriptions from a pre-existing corpus, and asked to supply hypotheses for each of our three labels— entailment, neutral, and contradiction—forcing the data to be balanced among these classes.",
        "The instructions that we provided to the workers are shown in Figure 1. Below the instructions were three ﬁelds for each of three requested sentences, corresponding to our entailment, neutral, and contradiction labels, a fourth ﬁeld (marked optional) for reporting problems, and a link to an FAQ page. That FAQ grew over the course of data collection. It warned about disallowed techniques (e.g., reusing the same sentence for many different prompts, which we saw in a few cases), provided guidance concerning sentence length and",
        "Figure 1: The instructions used on Mechanical Turk for data collection.",
        "Data set sizes: Training pairs Development pairs Test pairs",
        "2.2 Data validation",
        "In order to measure the quality of our corpus, and in order to construct maximally useful testing and development sets, we performed an additional round of validation for about 10% of our data. This validation phase followed the same basic form as the Mechanical Turk labeling task used to label the SICK entailment data: we presented workers with pairs of sentences in batches of ﬁve, and asked them to choose a single label for each pair. We supplied each pair to four annotators, yielding ﬁve labels per pair including the label used by the original author. The instructions were similar to the instructions for initial data collection shown in Figure 1, and linked to a similar FAQ. Though we initially used a very restrictive qualiﬁcation (based on past approval rate) to select workers for the validation task, we nonetheless discovered (and deleted) some instances of random guessing in an early batch of work, and subsequently instituted a fully closed qualiﬁcation restricted to about 30 trusted workers.",
        "The results of this validation process are summarized in Table 3. Nearly all of the examples received a majority label, indicating broad consensus about the nature of the data and categories. The gold-labeled examples are very nearly evenly distributed across the three labels. The Fleiss κ scores (computed over every example with a full ﬁve annotations) are likely to be conservative given our large and unevenly distributed pool of annotators, but they still provide insights about the levels of disagreement across the three semantic classes. This disagreement likely reﬂects not just the limitations of large crowdsourcing efforts but also the uncertainty inherent in naturalistic NLI. Regardless, the overall rate of agreement is extremely high, suggesting that the corpus is sufﬁ- ciently high quality to pose a challenging but realistic machine learning task.",
        "Table 1 shows a set of randomly chosen validated examples from the development set with their labels. Qualitatively, we ﬁnd the data that we collected draws fairly extensively on commonsense knowledge, and that hypothesis and premise sentences often differ structurally in signiﬁcant ways, suggesting that there is room for improvement beyond superﬁcial word alignment models. We also ﬁnd the sentences that we collected to be largely",
        "3 Our data as a platform for evaluation",
        "Our initial goal was to better understand the difﬁculty of the task of classifying SNLI corpus inferences, rather than necessarily the performance of a state-of-the-art RTE system. We approached this by running the same system on several data sets: our own test set, the SICK test data, and the standard RTE-3 test set (Giampiccolo et al., 2007). We report results in Table 4. Each of the models",
        "Unlike the RTE datasets, SNLI’s size supports approaches which make use of rich lexicalized features. We evaluate a simple lexicalized classiﬁer to explore the ability of non-specialized models to exploit these features in lieu of more involved language understanding. Our classiﬁer implements 6 feature types; 3 unlexicalized and 3 lexicalized:",
        "Table 5: 3-class accuracy, training on either our data or SICK, including models lacking crossbigram features (Feature 6), and lacking all lexical features (Features 4–6). We report results both on the test set and the training set to judge overﬁtting.",
        "It is surprising that the classiﬁer performs as well as it does without any notion of alignment or tree transformations. Although we expect that richer models would perform better, the results suggestthatgivenenough data, crossbigramswith the noisy part-of-speech overlap constraint can produce an effective model.",
        "Figure 4 shows a learning curve for the LSTM and the lexicalized and unlexicalized feature-based models. It shows that the large size of the corpus is crucial to both the LSTM and the lexicalized model, and suggests that additional data would yield still better performance for both. In addition, though the LSTM and the lexicalized model showsimilarperformancewhentrainedonthecurrent full corpus, the somewhat steeper slope for the LSTM hints that its ability to learn arbitrarily structured representations of sentence meaning may give it an advantage over the more constrained lexicalized model on still larger datasets. We were struck by the speed with which the lexicalized classiﬁer outperforms its unlexicalized",
        "Figure 4: A learning curve showing how the baseline classiﬁers and the LSTM perform when trained to convergence on varied amounts of training data. The y-axis starts near a random-chance accuracy of 33%. The minibatch size of 64 that we used to tune the LSTM sets a lower bound on data for that model.",
        "Our data only SICK only Our data and SICK (transfer)",
        "The results are shown in Table 7. Training on SICK alone yields poor performance, and the model trained on SNLI fails when tested on SICK data, labelingmoreneutralexamplesascontradictions than correctly, possibly as a result of subtle differences in how the labeling task was presented. In contrast, transferring SNLI representations to SICK yields the best performance yet reported for an unaugmented neural network model, surpasses the available EOP models, and approaches both the overall state of the art at 84.6% (Lai and Hockenmaier, 2014) and the 84% level of interannotator agreement, which likely represents an approximate performance ceiling. This suggests that the introduction of a large high-quality corpus makes it possible to train representation-learning models for sentence meaning that are competitive with the best hand-engineered models on inference tasks.",
        "We attempted to apply this same transfer evaluation technique to the RTE-3 challenge, but found that the small training set (800 examples) did not allow the model to adapt to the unfamiliar genre of text used in that corpus, such that no training conﬁguration yielded competitive performance. Further research on effective transfer learning on small data sets with neural models might facilitate improvements here.",
        "Natural languages are powerful vehicles for reasoning, and nearly all questions about meaningfulness in language can be reduced to questions of entailment and contradiction in context. This suggests that NLI is an ideal testing ground for theories of semantic representation, and that training for NLI tasks can provide rich domain-general semantic representations. To date, however, it has not been possible to fully realize this potential due to the limited nature of existing NLI resources. Thispapersoughttoremedythiswithanew, largescale, naturalistic corpus of sentence pairs labeled for entailment, contradiction, and independence. We used this corpus to evaluate a range of models, and found that both simple lexicalized models and neural network models perform well, and that the representations learned by a neural network model on our corpus can be used to dramatically improve performance on a standard challenge dataset. We hope that SNLI presents valuable training data and a challenging testbed for the continued application of machine learning to semantic representation.",
        "a lexical database for english. Communications of the ACM, 38(11):39–41."
    ]
}