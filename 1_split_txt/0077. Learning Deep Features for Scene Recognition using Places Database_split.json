{
    "title_author_abstract_introduction": "Learning Deep Features for Scene Recognition using Places Database\nBolei Zhou1, Agata Lapedriza1,3, Jianxiong Xiao2, Antonio Torralba1, and Aude Oliva1\n1Massachusetts Institute of Technology 2Princeton University 3Universitat Oberta de Catalunya\nAbstract\nScene recognition is one of the hallmark tasks of computer vision, allowing deﬁ- nition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet andtheriseofConvolutionalNeuralNetworks(CNNs)forlearninghigh-levelfeatures, performance at scene recognition has not attained the same level of success. ThismaybebecausecurrentdeepfeaturestrainedfromImageNetarenotcompetitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers’ responses allows us to show differences in the internal representations of object-centric and scene-centric networks.\nIntroduction\nUnderstanding the world in a single glance is one of the most accomplished feats of the human brain: it takes only a few tens of milliseconds to recognize the category of an object or environment, emphasizing an important role of feedforward processing in visual recognition. One of the mechanisms subtending efﬁcient human visual recognition is our capacity to learn and remember a diverse set of places and exemplars [11]; by sampling the world several times per second, our neural architecture constantly registers new inputs even for a very short time, reaching an exposure to millions of natural images within just a year. How much would an artiﬁcial system have to learn before reaching the scene recognition abilities of a human being?\nBesides the exposure to a dense and rich variety of natural images, one important property of the primate brain is its hierarchical organization in layers of increasing processing complexity, an architecture that has inspired Convolutional Neural Networks or CNNs [2, 14]. These architectures together with recent large databases (e.g., ImageNet [3]) have obtained astonishing performance on object classiﬁcation tasks [12, 5, 20]. However, the baseline performance reached by these networks on scene classiﬁcation tasks is within the range of performance based on hand-designed features and sophisticated classiﬁers [24, 21, 4]. Here, we show that one of the reasons for this discrepancy is that the higher-level features learned by object-centric versus scene-centric CNNs are different: iconic images of objects do not contain the richness and diversity of visual information that pictures of scenes and environments provide for learning to recognize them.\nHere we introduce Places, a scene-centric image dataset 60 times larger than the SUN database [24]. With this database and a standard CNN architecture, we establish new baselines of accuracies on\nvariousscenedatasets(Scene15[17,13], MITIndoor67[19], SUNdatabase[24], andSUNAttribute Database [18]), signiﬁcantly outperforming the results obtained by the deep features from the same network architecture trained with ImageNet1.\nThe paper is organized as follows: in Section 2 we introduce the Places database and describe the collection procedure. In Section 3 we compare Places with the other two large image datasets: SUN [24] and ImageNet [3]. We perform experiments on Amazon Mechanical Turk (AMT) to compare these 3 datasets in terms of density and diversity. In Section 4 we show new scene classiﬁcation performance when training deep features from millions of labeled scene images. Finally, we visualize the units’ responses at different layers of the CNNs, demonstrating that an object-centric network (using ImageNet [12]) and a scene-centric network (using Places) learn different features.",
    "data_related_paragraphs": [
        "2 Places Database",
        "The ﬁrst benchmark for scene classiﬁcation was the Scene15 database [13] based on [17]. This dataset contains only 15 scene categories with a few hundred images per class, where current classiﬁers are saturating this dataset nearing human performance at 95%. The MIT Indoor67 database [19] has 67 categories on indoor places. The SUN database [24] was introduced to provide a wide coverage of scene categories. It is composed of 397 categories containing more than 100 images per category.",
        "Despite those efforts, all these scene-centric datasets are small in comparison with current object datasets such as ImageNet (note that ImageNet also contains scene categories but in a very small proportion as is shown in Fig. 2). Complementary to ImageNet (mostly object-centric), we present here a scene-centric database, that we term the Places database. As now, Places contain more than 7 million images from 476 place categories, making it the largest image database of scenes and places so far and the ﬁrst scene-centric database competitive enough to train algorithms that require huge amounts of data, such as CNNs.",
        "2.1 Building the Places Database",
        "Since the SUN database [24] has a rich scene taxonomy, the Places database has inherited the same list of scene categories. To generate the query of image URL, 696 common adjectives (messy, spare, sunny, desolate, etc), manually selected from a list of popular adjectives in English, are combined with each scene category name and are sent to three image search engines (Google Images, Bing Images, and Flickr). Adding adjectives to the queries allows us to download a larger number of images than what is available in ImageNet and to increase the diversity of visual appearances. We then remove duplicated URLs and download the raw images with unique URLs. To date, more than 40 million images have been downloaded. Only color images of 200×200 pixels or larger are kept. PCA-based duplicate removal is conducted within each scene category in the Places database and across the same scene category in the SUN database, which ensures that Places and the SUN do not contain the same images, allowing us to combine the two datasets.",
        "The images that survive this initial selection are sent to Amazon Mechanical Turk for two rounds of individual image annotation. For a given category name, its deﬁnition as in [24], is shown at the top of a screen, with a question like is this a living room scene? A single image at a time is shown centered in a large window, and workers are asked to press a Yes or No key. For the ﬁrst round of labeling, the default answer is set to No, requiring the worker to actively pick up the positive images. The positive images resulting from the ﬁrst round annotation are further sent for a second round annotation, in which the default answer is set to Yes (to pick up the remaining negative images). In each HIT(one assignment for each worker), 750 downloaded images are included for annotation, and an additional 30 positive samples and 30 negative samples with ground truth from the SUN database are also randomly injected as control. Valid HITs kept for further analyses require anaccuracyof90%orhigheronthesecontrolimages. Afterthetworoundsofannotation, andasthis paper is published, 7,076,580 images from 476 scene categories are included in the Places database. Fig. 1 shows image samples obtained with some of the adjectives used in the queries.",
        "1The database and pre-trained networks are available at http://places.csail.mit.edu",
        "Figure 2: Comparison of the number of images per scene category in three databases.",
        "3 Comparing Scene-centric Databases",
        "Despite the importance of benchmarks and training datasets in computer vision, comparing datasets is still an open problem. Even datasets covering the same visual classes have notable differences providing different generalization performance when used to train a classiﬁer [23]. Beyond the number of images and categories, there are aspects that are important but difﬁcult to quantify, like the variability in camera poses, in decoration styles or in the objects that appear in the scene.",
        "Although the quality of a database will be task dependent, it is reasonable to assume that a good database should be dense (with a high degree of data concentration), and diverse (it should include a high variability of appearances and viewpoints). Both quantities, density and diversity, are hard to estimate in image sets, as they assume some notion of similarity between images which, in general, is not well deﬁned. Two images of scenes can be considered similar if they contain similar objects, and the objects are in similar spatial conﬁgurations and pose, and have similar decoration styles. However, this notion is loose and subjective so it is hard to answer the question are these two images similar? For this reason, we deﬁne relative measures for comparing datasets in terms of density and diversity that only require ranking similarities. In this section we will compare the densities and diversities of SUN, ImageNet and Places using these relative measures.",
        "Density is a measure of data concentration. We assume that, in an image set, high density is equivalent to the fact that images have, in general, similar neighbors. Given two databases A and B, relative density aims to measure which one of the two sets has the most similar nearest neighbors. Let a1 be a random image from set A and b1 from set B and let us take their respective nearest neighbors in each set, a2 from A and b2 from B. If A is denser than B, then it would be more likely that a1 and a2 are closer to each other than b1 and b2. From this idea we deﬁne the relative density as DenB(A) = p(d(a1,a2) < d(b1,b2)), where d(a1,a2) is a distance measure between two images (small distance implies high similarity). With this deﬁnition of relative density we have that A is denser than B if, and only if, DenB(A) > DenA(B). This deﬁnition can be extended to an arbitrary number of datasets, A1,...,AN:",
        "The quality of a dataset can not be measured just by its density. Imagine, for instance, a dataset composed of 100,000 images all taken within the same bedroom. This dataset would have a very high density but a very low diversity as all the images would look very similar. An ideal dataset, expected to generalize well, should have high diversity as well.",
        "There are several measures of diversity, most of them frequently used in biology to characterize the richness of an ecosystem (see [9] for a review). In this section, we will use a measure inspired by Simpson index of diversity [22]. Simpson index measures the probability that two random individuals from an ecosystem belong to the same species. It is a measure of how well distributed are the individuals across different species in an ecosystem and it is related to the entropy of the distribution. Extending this measure for evaluating the diversity of images within a category is non-trivial if there are no annotations of sub-categories. For this reason, we propose to measure relative diversity of image datasets A and B based on this idea: if set A is more diverse than set B, then two random images from set B are more likely to be visually similar than two random samples from A. Then, the diversity of A with respect to B can be deﬁned as DivB(A) = 1 − p(d(a1,a2) < d(b1,b2)), where a1,a2 ∈ A and b1,b2 ∈ B are randomly selected. With this deﬁnition of relative diversity we have that A is more diverse than B if, and only if, DivB(A) > DivA(B). For an arbitrary number of datasets, A1,...,AN:",
        "In these experiments, the only difference when estimating density and diversity is how the pairs are generated. For the diversity experiment, the pairs are randomly sampled from each database. Each trial is composed of 4 pairs from each database, giving a total of 12 pairs to chose from. We used 4 pairs per database to increase the chances of ﬁnding a similar pair and avoiding users having to skip trials. AMT workers had to select the most similar pair on each trial. We ran 40 trials per category and two observers per trial, for the 88 categories in common between ImageNet, SUN and Places databases. Fig. 3a shows some examples of pairs from one of the density experiments.The pair selected by AMT workers as being more similar is highlighted.",
        "Figure 3: a) Examples of pairs for the diversity experiment. b) Examples of pairs for the density experiment. c) Scatter plot of relative diversity vs. relative density per each category and dataset.",
        "Figure 4: Cross dataset generalization of training on the 88 common scenes between Places, SUN and ImageNet then testing on the 88 common scenes from: a) SUN, b) ImageNet and c) Places database.",
        "Fig. 3c shows a scatter plot of relative diversity vs. relative density for all the 88 categories and the three databases. The point of crossing between the two black lines indicates the point where all the results should fall if all the datasets were identical in terms of diversity and density. The ﬁgure also shows the average of the density and diversity over all categories for each dataset.",
        "In terms of density, the three datasets are, on average, very similar. However, there is a larger variation in terms of diversity, showing Places to be the most diverse of the three datasets. The average relative diversity on each dataset is 0.83 for Places, 0.67 for ImageNet and 0.50 for SUN. In the experiment, users selected pairs from the SUN database to be the closest to each other 50% of the time, while the pairs from the Places database were judged to be the most similar only on 17% of the trials. The categories with the largest variation in diversity across the three datasets are playground, veranda and waiting room.",
        "3.3 Cross Dataset Generalization",
        "As discussed in [23], training and testing across different datasets generally results in a drop of performance due to the dataset bias problem. In this case, the bias between datasets is due, among other factors, to the differences in the density and diversity between the three datasets. Fig. 4 shows the classiﬁcation results obtained from the training and testing on different permutations of the 3 datasets. For these results we use the features extracted from a pre-trained ImageNet-CNN and a linear SVM. In all three cases training and testing on the same dataset provides the best performance for a ﬁxed number of training examples. As the Places database is very large, it achieves the best performance on two of the test sets when all the training data is used. In the next section we will show that a CNN network trained using the Places database achieves a signiﬁcant improvement over scene-centered benchmarks in comparison with a network trained using ImageNet.",
        "Through the visualization of the responses of the units for various levels of network layers, we can have a better understanding of the differences between the ImageNet-CNN and Places-CNN given that they share the same architecture. Fig.5 visualizes the learned representation of the units at the Conv 1, Pool 2, Pool 5, and FC 7 layers of the two networks. Whereas Conv 1 units can be directly visualized (they capture the oriented edges and opponent colors from both networks), we use the mean image method to visualize the units of the higher layers: we ﬁrst combine the test set of ImageNet LSVRC2012 (100,000 images) and SUN397 (108,754 images) as the input for both networks; then we sort all these images based on the activation response of each unit at each layer; ﬁnally we average the top 100 images with the largest responses for each unit as a kind of receptive ﬁeld (RF) visualization of each unit. To compare the units from the two networks, Fig. 5 displays mean images sorted by their ﬁrst principal component. Despite the simplicity of the method, the units in both networks exhibit many differences starting from Pool 2. From Pool 2 to Pool 5 and FC 7, gradually the units in ImageNet-CNN have RFs that look like object-blobs, while units in Places-CNN have more RFs that look like landscapes with more spatial structures. These learned unit structures are closely relevant to the differences of the training data. In future work, it will be fascinating to relate the similarity and differences of the RF at different layers of the object-centric network and scene-centric network with the known object-centered and scenecentered neural cortical pathways identiﬁed in the human brain (for a review, [16]). In the next section we will show that these two networks (only differing in the training sets) yield very different performances on a variety of recognition benchmarks.",
        "We use the responses from the trained CNN as generic features for visual recognition tasks. Responses from the higher-level layers of CNN have proven to be effective generic features with stateof-the-art performance on various image datasets [5, 20]. Thus we evaluate performance of the",
        "Table 2: Classiﬁcation accuracy/precision on scene-centric databases and object-centric databases for the Places-CNN feature and ImageNet-CNN feature. The classiﬁer in all the experiments is a linear SVM with the same parameters for the two features.",
        "As a comparison, we evaluate the deep feature’s performance from the ImageNet-CNN on those same benchmarks. Places-CNN and ImageNet-CNN have exactly the same network architecture, but they are trained on scene-centric data and object-centric data respectively. We use the deep features from the response of the Fully Connected Layer (FC) 7 of the CNNs, which is the ﬁnal fully connected layer before producing the class predictions. There is only a minor difference between the feature of FC 7 and the feature of FC 6 layer [5]. The deep feature for each image is a 4096- dimensional vector.",
        "Table 2 summarizes the classiﬁcation accuracy on various datasets for the ImageNet-CNN feature and the Places-CNN feature. Fig.6 plots the classiﬁcation accuracy for different visual features on SUN397 database and SUN Attribute dataset. The classiﬁer is a linear SVM with the same default parameters for the two deep features (C=1) [6]. The Places-CNN feature shows impressive performance on scene classiﬁcation benchmarks, outperforming the current state-of-the-art methods for SUN397 (47.20% [21]) and for MIT Indoor67 (66.87% [4]). On the other hand, the ImageNetCNN feature shows better performance on object-related databases. Importantly, our comparison",
        "Conv 1Pool 2Pool 5 FC 7 NNC-teNegamINNC-secalP\fFigure 6: Classiﬁcation accuracy on the SUN397 Dataset and average precision on the SUN Attribute Dataset with increasing size of training samples for the ImageNet-CNN feature and the Places-CNN feature. Results of other hand-designed features/kernels are fetched from [24] and [18] respectively.",
        "Table 3: Classiﬁcation accuracy/precision on various databases for Hybrid-CNN feature. The numbers in bold indicate the results outperform the ImageNet-CNN feature or Places-CNN feature.",
        "shows that Places-CNN and ImageNet-CNN have complementary strengths on scene-centric tasks and object-centric tasks, as expected from the benchmark datasets used to train these networks.",
        "Additionally, we train a Hybrid-CNN, by combining the training set of Places-CNN and training set of ImageNet-CNN. We remove the overlapping scene categories from the training set of ImageNet, and then the training set of Hybrid-CNN has 3.5 million images from 1183 categories. HybridCNN is trained over 700,000 iterations, under the same network architecture of Places-CNN and ImageNet-CNN. The accuracy on the validation set is 52.3%. We evaluate the deep feature (FC 7) from Hybrid-CNN on benchmarks shown in Table 3. Combining the two datasets yields an additional increase in performance for a few benchmarks.",
        "Deep convolutional neural networks are designed to beneﬁt and learn from massive amounts of data. We introduce a new benchmark with millions of labeled images, the Places database, designed to represent places and scenes found in the real world. We introduce a novel measure of density and diversity, and show the usefulness of these quantitative measures for estimating dataset biases and comparing different datasets. We demonstrate that object-centric and scene-centric neural networks differ in their internal representations, by introducing a simple visualization of the receptive ﬁelds of CNN units. Finally, we provide the state-of-the-art performance using our deep features on all the current scene benchmarks.",
        "Acknowledgement. Thanks to Aditya Khosla for valuable discussions. This work is supported by the National Science Foundation under Grant No. 1016862 to A.O, ONR MURI N000141010933 to A.T, as well as MIT Big Data Initiative at CSAIL, Google and Xerox Awards, a hardware donation from NVIDIA Corporation, to A.O and A.T., Intel and Google awards to J.X, and grant TIN2012-38187-C03-02 to A.L. This work is also supported by the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Research Laboratory, contract FA8650-12-C-7211 to A.T. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofﬁcial policies or endorsements, either expressed or implied, of IARPA, AFRL, or the U.S. Government.",
        "1 5 102050010203040506070Number of training samples per categoryClassification accuracy  1/15/520/2050/50150/1500.550.60.650.70.750.80.850.9Number of training samples per attribute (positive/negative)Average Precision  Places−CNN [0.912]ImageNet−CNN [0.898]Combined kernel [0.879]HoG2x2 [0.848]Self−similarity [0.820]Geometric Color Hist [0.783]Gist [0.799]Combined kernel [37.5]HoG2x2 [26.3] DenseSIFT [23.5] Texton [21.6] Gist [16.3] LBP [14.7] ImageNet−CNN [42.6]Places−CNN [54.3]Benchmark on SUN397 DatasetBenchmark on SUN Attribute Dataset\fReferences",
        "database. In Proc. CVPR, 2009.",
        "[8] G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 object category dataset. 2007. [9] C. Heip, P. Herman, and K. Soetaert. Indices of diversity and evenness. Oceanis, 1998. [10] Y. Jia. Caffe: An open source convolutional architecture for fast feature embedding. http://caffe.",
        "[18] G. Patterson and J. Hays. Sun attribute database: Discovering, annotating, and recognizing scene at-",
        "and practice. Int’l Journal of Computer Vision, 2013. [22] E. H. Simpson. Measurement of diversity. Nature, 1949. [23] A. Torralba and A. A. Efros. Unbiased look at dataset bias. In Proc. CVPR, 2011. [24] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition"
    ]
}