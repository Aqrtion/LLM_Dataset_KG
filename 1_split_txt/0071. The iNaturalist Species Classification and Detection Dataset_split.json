{
    "title_author_abstract_introduction": "The iNaturalist Species Classiﬁcation and Detection Dataset\nGrant Van Horn1 Oisin Mac Aodha1 Yang Song2 Yin Cui3 Chen Sun2\nAlex Shepard4 Hartwig Adam2\nPietro Perona1\nSerge Belongie3\n1Caltech\n2Google\n3Cornell Tech\n4iNaturalist\nAbstract\nExisting image classiﬁcation datasets used in computer vision tend to have a uniform distribution of images across object categories. In contrast, the natural world is heavily imbalanced, as some species are more abundant and easier tophotographthanothers. Toencouragefurtherprogressin challenging real world conditions we present the iNaturalist species classiﬁcation and detection dataset, consisting of 859,000 images from over 5,000 different species of plants and animals. It features visually similar species, captured in a wide variety of situations, from all over the world. Images were collected with different camera types, have varying image quality, feature a large class imbalance, and have been veriﬁed by multiple citizen scientists. We discuss the collection of the dataset and present extensive baseline experiments using state-of-the-art computer vision classiﬁcation and detection models. Results show that current nonensemble based methods achieve only 67% top one classiﬁcation accuracy, illustrating the difﬁculty of the dataset. Speciﬁcally, we observe poor results for classes with small numbers of training examples suggesting more attention is needed in low-shot learning.\n1. Introduction\nPerformance on existing image classiﬁcation benchmarks such as [32] is close to being saturated by the current generation of classiﬁcation algorithms [9, 37, 35, 46]. However, the number of training images is crucial. If one reduces the number of training images per category, typically performance suffers. It may be tempting to try and acquire more training data for the classes with few images but this is often impractical, or even impossible, in many application domains. We argue that class imbalance is a property of the realworld andcomputer vision models should be able to deal with it. Motivated by this problem, we introduce the iNaturalist Classiﬁcation and Detection Dataset (iNat2017). Just like the real world, it exhibits a large class imbalance, as some species are much more likely to be observed.\nFigure 1. Two visually similar species from the iNat2017 dataset. Through close inspection, we can see that the ladybug on the left has two spots while the one on the right has seven.\nItisestimatedthatthenaturalworldcontainsseveralmillion species with around 1.2 million of these having already been formally described [26]. For some species, it may only be possible to determine the species via genetics or by dissection. For the rest, visual identiﬁcation in the wild, while possible, can be extremely challenging. This can be due to the sheer number of visually similar categories that an individual would be required to remember along with the challenging inter-class similarity; see Fig. 1. As a result, there is a critical need for robust and accurate automated tools to scale up biodiversity monitoring on a global scale [4].\nThe iNat2017 dataset is comprised of images and labels from the citizen science website iNaturalist1. The site allows naturalists to map and share photographic observations of biodiversity across the globe. Each observation consists of a date, location, images, and labels containing the name of the species present in the image. As of November 2017, iNaturalist has collected over 6.6 million observations from 127,000 species. From this, there are close to 12,000 species that have been observed by at least twenty\n1www.inaturalist.org\npeople and have had their species ID conﬁrmed by multiple annotators.\nThe goal of iNat2017 is to push the state-of-the-art in image classiﬁcation and detection for ‘in the wild’ data featuring large numbers of imbalanced, ﬁne-grained, categories. iNat2017 contains over 5,000 species, with a combined training and validation set of 675,000 images, 183,000 test images, and over 560,000 manually created bounding boxes. It is free from one of the main selection biases that are encountered in many existing computer vision datasets - as opposed to being scraped from the web all images have been collected and then veriﬁed by multiple citizen scientists. It features many visually similar species, captured in a wide variety of situations, from all over the world. We outline how the dataset was collected and report extensive baseline performance for state-of-the-art classiﬁ- cation and detection algorithms. Our results indicate that iNat2017 is challenging for current models due to its imbalanced nature and will serve as a good experimental platform for future advances in our ﬁeld.",
    "data_related_paragraphs": [
        "2. Related Datasets",
        "In this section we review existing image classiﬁcation datasets commonly used in computer vision. Our focus is on large scale, ﬁne-grained, object categories as opposed to datasets that feature common everyday objects, e.g. [6, 5, 21]. Fine-grained classiﬁcation problems typically exhibit two distinguishing differences from their coarse grained counter parts. First, there tends to be only a small number of domain experts that are capable of making the classiﬁcations. Second, as we move down the spectrum of granularity, the number of instances in each class becomes smaller. This motivates the need for automated systems that are capable of discriminating between large numbers of potentially visually similar categories with small numbers of training examples for some categories. In the extreme, face identiﬁcation can be viewed as an instance of ﬁne-grained classiﬁcation and many existing benchmark datasets with long tail distributions exist e.g. [13, 28, 8, 3]. However, due to the underlying geometric similarity between faces, current state-of-the-art approaches for face identiﬁcation tend to perform a large amount of face speciﬁc pre-processing [38, 33, 28].",
        "The vision community has released many ﬁne-grained datasets covering several domains such as birds [44, 42, 2, 40, 18], dogs [16, 29, 23], airplanes [24, 41], ﬂowers [27], leaves [20], food [10], trees [43], and cars [19, 22, 48, 7]. ImageNet [32] is not typically advertised as a ﬁne-grained dataset, yet contains several groups of ﬁne-grained classes, including about 60 bird species and about 120 dog breeds. In Table 1 we summarize the statistics of some of the most commondatasets. Withtheexceptionofasmallnumbere.g. [18, 7], many of these datasets were typically constructed",
        "Dataset Name Flowers 102 [27] Aircraft [24] Oxford Pets [29] DogSnap [23] CUB 200-2011 [42] Stanford Cars [19] Stanford Dogs [16] Urban Trees [43] NABirds [40] LeafSnap∗ [20] CompCars∗ [48] VegFru∗ [10] Census Cars [7] ILSVRC2012 [32] iNat2017",
        "Table 1. Summary of popular general and ﬁne-grained computer vision classiﬁcation datasets. ‘Imbalance’ represents the number of images in the largest class divided by the number of images in the smallest. While susceptible to outliers, it gives an indication of the imbalance found in many common datasets. ∗Total number of train, validation, and test images.",
        "to have an approximately uniform distribution of images across the different categories. In addition, many of these datasets were created by searching the internet with automated web crawlers and as a result can contain a large proportion of incorrect images e.g. [18]. Even manually vetted datasets such as ImageNet [32] have been reported to contain up to 4% error for some ﬁne-grained categories [40]. While current deep models are robust to label noise at training time, it is still very important to have clean validation and test sets to be able to quantify performance [40, 31].",
        "Unlike web scraped datasets [18, 17, 45, 10], the annotations in iNat2017 represent the consensus of informed enthusiasts. Images of natural species tend to be challenging as individuals from the same species can differ in appearance due to sex and age, and may also appear in different environments. Depending on the particular species, they can also be very challenging to photograph in the wild. In contrast, mass-produced, man-made object categories are typically identical up to nuisance factors, i.e. they only differ in terms of pose, lighting, color, but not necessarily in their underlying object shape or appearance [49, 7, 50].",
        "3. Dataset Overview",
        "In this section we describe the details of the dataset, including how we collected the image data (Section 3.1), how we constructed the train, validation and test splits (Section 3.2), how we vetted the test split (Section 3.2.1) and how we collected bounding boxes (Section 3.3). Future researchers may ﬁnd our experience useful when constructing their own datasets.",
        "3.1. Dataset Collection",
        "iNat2017 was collected in collaboration with iNaturalist, a citizen science effort that allows naturalists to map and share observations of biodiversity across the globe through a custom made web portal and mobile apps. Observations, submitted by observers, consist of images, descriptions, location and time data, and community identiﬁcations. If the community reaches a consensus on the taxa in the observation, then a “research-grade” label is applied to the observation. iNaturalist makes an archive of researchgrade observation data available to the environmental science community via the Global Biodiversity Information Facility (GBIF) [39]. Only research-grade labels at genus, species or lower are included in this archive. These archives contain the necessary information to reconstruct which photographs belong to each observation, which observations belong to each observer, as well as the taxonomic hierarchy relating the taxa. These archives are refreshed on a rolling basis and the iNat2017 dataset was created by processing the archive from October 3rd, 2016.",
        "3.2. Dataset Construction",
        "The next step was to partition the images from these taxa into the train, validation, and test splits. For each of the selected taxa, we sorted the observers by their number of observations (fewest ﬁrst) and selected the ﬁrst 40% of observers to be in the test split, and the remaining 60% to be in the “train-val” split. By partitioning the observers in this way, and subsequently placing all of their photographs into one split or the other, we ensure that the behavior of a particular user (e.g. camera equipment, location, background, etc.) is contained within a single split, and not available as a useful source of information for classiﬁcation on the other split for a speciﬁc taxa. Note that a particular observer may be put in the test split for one taxa, but the “train-val” split for another taxa. By ﬁrst sorting the observers by their number of observations we ensure that the test split contains a high number of unique observers and therefore a high degree of variability. To be concrete, at this point, for a taxa that has exactly 20 unique observers (the minimum allowed), 8 observers would be placed in the the test split and the remaining 12 observers would be placed in the “trainval” split. Rather than release all test images, we randomly sampled ∼183,000 to be included in the ﬁnal dataset. The remaining test images were held in reserve in case we encountered unforeseen problems with the dataset.",
        "At this point we have the ﬁnal image splits, with a total of 579,184 training images, 95,986 validation images and 182,707 test images. All images were resized to have a max dimension of 800px. Sample images from the dataset can be viewed in Fig. 8. The iNat2017 dataset is available from our project website2.",
        "Under the above guidelines, 561,767 bounding boxes were obtained from 449,313 images in the training and validation sets. Following the size conventions of COCO [21], the iNat2017 dataset is composed of 5.7% small instances (area < 322), 23.6% medium instances (322 ≤ area ≤ 962) and 70.7% large instances (area > 962), with area computed as 50% of the annotated bounding box area (since segmentation masks were not collected). Fig. 4 shows the distribution of relative bounding box sizes, indicating that a majority of instances are relatively small and medium sized.",
        "Overall, the Inception ResNetV2 SE was the best performing model. As a comparison, this model achieves a single crop top-1 and top-5 accuracy of 80.2% and 95.21% respectively on the ILSVRC 2012 [32] validation set [35], as opposed to 67.74% and 87.89% on iNat2017, highlighting the comparative difﬁculty of the iNat2017 dataset. A more detailed super-class level breakdown is available in Table 4 for the Inception ResNetV2 SE model. We can see that the Reptiliasuper-class(with289classes)wasthemostdifﬁcult with an average top-1 accuracy of 45.87%, while the Protozoa super-class (with 4 classes) had the highest accuracy at 89.19%. Viewed as a collection of ﬁne-grained datasets (one for each super-class) we can see that the iNat2017 dataset exhibits highly variable classiﬁcation difﬁculty.",
        "In Fig. 5 we plot the top one public test set accuracy against the number of training images for each class from the Inception ResNet V2 SE model. We see that as the number of training images per class increases, so does the test accuracy. However, we still observe a large variance in accuracy for classes with a similar amount of training data, revealing opportunities for algorithmic improvements in both the low data and high data regimes.",
        "Table3.ClassiﬁcationresultsforvariousCNNstrainedononlythe training set, using a single center crop at test time. Unlike some current datasets where performance is near saturation, iNat2017 still poses a challenge for state-of-the-art classiﬁers.",
        "To characterize the detection difﬁculty of iNat2017, we adoptFaster-RCNN[30]foritsstate-of-the-artperformance as an object detection setup (which jointly predicts object bounding boxes along with class labels). We use a TensorFlow [1] implementation of Faster-RCNN with default hyper-parameters [14]. Each model is trained with 0.9 momentum, andasynchronouslyoptimizedon9GPUstoexpedite experiments. We use an Inception V3 network, initialized from ImageNet, as the backbone for our Faster-RCNN models. Finally, each input image is resized to have 600 pixels as the short edge while maintaining the aspect ratio. As discussed in Section 3.3, we collected bounding boxes on 9 of the 13 super-classes, translating to a total of 2,854 classes with bounding boxes. In the following experiments we only consider performance on this subset of classes. Additionally, we report performance on the the validation set in place of the test set and we only evaluate on images that contained a single instance. Images that contained only evidence of the species’ presence and images that contained multiple instances were excluded. We evaluate the models usingthe detectionmetrics from COCO[21]. We ﬁrst study the performance of ﬁne-grained localization and classiﬁcation by training the Faster-RCNN model on the 2,854 class subset. Fig. 7 shows some sample detection results. Table 5 provides the break down in performance for each super-class, where super-class performance is computed by taking an average across all classes within the super-class. The precision-recall curves (again at the super-class level) for 0.5 IoU are displayed in Fig. 6. Across all super-classes we achieve a comprehensive average precision (AP) of 43.5. Again the Reptilia super-class proved to be the most difﬁcult, with an AP of 21.3 and an AUC of 0.315. At the other end of the spectrum we achieved an AP of 49.4 for Insecta and an AUC of 0.677. Similar to the classiﬁcation results, when viewed as a a collection of datasets (one for each super-class) we see that iNat2017 exhibits highly variable detection difﬁculty, posing a challenge to researchers to build improved detectors that work across a broad group of ﬁne-grained classes.",
        "We present the iNat2017 dataset, in contrast to many existing computer vision datasets it is: 1) unbiased, in that it was collected by non-computer vision researchers for a well deﬁned purpose, 2) more representative of real-world challenges than previous datasets, 3) represents a long-tail classiﬁcation problem, and 4) is useful in conservation and ﬁeld biology. The introduction of iNat2017 enables us to study two important questions in a real world setting: 1) do long-tailed datasets present intrinsic challenges? and 2) do our computer vision systems exhibit transfer learning fromthe well-representedcategories totheleast represented ones? Whileourbaselineclassiﬁcationanddetectionresults are encouraging, from our experiments we see that state-ofthe-art computer vision models have room to improve when applied to large imbalanced datasets. Small efﬁcient models designed for mobile applications and embedded devices have even more room for improvement [11].",
        "researcher-collected datasets,",
        "the iNat2017 dataset has the opportunity to grow with the iNaturalist community. Currently, every 1.7 hours another species passes the 20 unique observer threshold, making it",
        "available for inclusion in the dataset (already up to 12k as of November 2017, up from 5k when we started work on the dataset). Thus, the current challenges of the dataset (long tail with sparse data) will only become more relevant.",
        "Inthefutureweplantoinvestigateadditionalannotations such as sex and life stage attributes, habitat tags, and pixel level labels for the four super-classes that were challenging to annotate. We also plan to explore the “open-world problem” where the test set contains classes that were never seen during training. This direction would encourage new error measures that incorporate taxonomic rank [25, 47]. Finally, we expect this dataset to be useful in studying how to teach ﬁne-grained visual categories to humans [34, 15], and plan to experiment with models of human learning.",
        "Acknowledgments. This work was supported by a Google Focused Research Award. We would like to thank: Scott Loarie and Ken-ichi Ueda from iNaturalist; Steve Branson, David Rolnick, Weijun Wang, and Nathan Frey for their helpwiththedataset; WendyKanandMaggieDemkinfrom the iNat2017 competitors, and the FGVC2017 Kaggle; workshop organizers. We also thank NVIDIA and Amazon Web Services for their donations.",
        "[3] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. Vggface2: A dataset for recognising faces across pose and age. arXiv preprint arXiv:1710.08092, 2017. 2",
        "[8] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In ECCV, 2016. 2",
        "[10] S. Hou, Y. Feng, and Z. Wang. Vegfru: A domain-speciﬁc datasetforﬁne-grainedvisualcategorization. InICCV,2017. 2",
        "[13] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognitioninunconstrainedenvironments. Technicalreport, University of Massachusetts, Amherst, 2007. 2",
        "[16] A. Khosla, N. Jayadevaprakash, B. Yao, and L. Fei-Fei. Novel dataset for ﬁne-grained image categorization. In FGVC Workshop at CVPR, 2011. 2",
        "Openimages: A public dataset for large-scale multi-label and multiclass image classiﬁcation. Dataset available from https://github. com/openimages, 2016. 2",
        "[18] J. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev, T. Duerig, J. Philbin, and L. Fei-Fei. The unreasonable effectiveness of noisy data for ﬁne-grained recognition. In ECCV, 2016. 2",
        "iNaturalist Research-grade ObserDataset. Occurrence",
        "[40] G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. Belongie. Building a bird recognitionappandlargescaledatasetwithcitizenscientists: The ﬁne print in ﬁne-grained dataset collection. In CVPR, 2015. 2",
        "The caltech-ucsd birds-200-2011 dataset. 2011. 2",
        "[44] P. Welinder, S. Branson, T. Mita, C. Wah, F. Schroff, S. Belongie, and P. Perona. Caltech-ucsd birds 200. 2010. 2 [45] M. J. Wilber, C. Fang, H. Jin, A. Hertzmann, J. Collomosse, and S. Belongie. Bam! the behance artistic media dataset for recognition beyond photography. ICCV, 2017. 2",
        "[48] L. Yang, P. Luo, C. Change Loy, and X. Tang. A large-scale car dataset for ﬁne-grained categorization and veriﬁcation. In CVPR, 2015. 2",
        "[50] X. Zhang, Y. Cui, Y. Song, H. Adam, and S. Belongie. The iMaterialist Challenge 2017 Dataset. FGVC Workshop at CVPR, 2017. 2"
    ]
}