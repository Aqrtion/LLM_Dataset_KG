{
    "title_author_abstract_introduction": "Pointer Sentinel Mixture Models\nStephen Merity Caiming Xiong James Bradbury Richard Socher MetaMind - A Salesforce Company, Palo Alto, CA, USA\nSMERITY@SALESFORCE.COM CXIONG@SALESFORCE.COM JAMES.BRADBURY@SALESFORCE.COM RSOCHER@SALESFORCE.COM\nAbstract Recent neural network sequence models with softmax classiﬁers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standardsoftmaxclassiﬁer. OurpointersentinelLSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.1\n1. Introduction\nA major difﬁculty in language modeling is learning when to predict speciﬁc words from the immediate context. For instance, imagine a new person is introduced and two paragraphs later the context would allow one to very accurately predict this person’s name as the next word. For standard neural sequence models to predict this name, they would have to encode the name, store it for many time steps in their hidden state, and then decode it when appropriate. As the hidden state is limited in capacity and the optimization of such models suffer from the vanishing gradient problem, this is a lossy operation when performed over many timesteps. This is especially true for rare words.\nModels with soft attention or memory components have been proposed to help deal with this challenge, aiming to allow for the retrieval and use of relevant previous hidden\n1Available for download at the WikiText dataset site\nFigure 1. Illustration of the pointer sentinel-RNN mixture model. g is the mixture gate which uses the sentinel to dictate how much probability mass to give to the vocabulary.\nstates, in effect increasing hidden state capacity and providing a path for gradients not tied to timesteps. Even with attention, the standard softmax classiﬁer that is being used in these models often struggles to correctly predict rare or previously unknown words.\nPointer networks (Vinyals et al., 2015) provide one potential solution for rare and out of vocabulary (OoV) words as a pointer network uses attention to select an element from the input as output. This allows it to produce previously unseen input tokens. While pointer networks improve performance on rare words and long-term dependencies they are unable to select words that do not exist in the input.\nWe introduce a mixture model, illustrated in Fig. 1, that combines the advantages of standard softmax classiﬁers with those of a pointer component for effective and efﬁ- cient language modeling. Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of G¨ulc¸ehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel. The model improves the state of the art perplexity on the Penn Treebank. Since this commonly used dataset is small and no other freely available alternative exists that allows for learning long range dependencies, we also introduce a new benchmark dataset for language modeling called WikiText.\np(Yellen)=gpvocab(Yellen)+(1\u0000g)pptr(Yellen)p(Yellen)=gpvocab(Yellen)+(1\u0000g)pptr(Yellen)zebraChairJanetYellen…raisedrates.Ms.???Fed…YellenRosenthalBernankeaardvark……Sentinel…PointerSoftmaxRNNpvocab(Yellen)pvocab(Yellen)ggpptr(Yellen)pptr(Yellen)\nPointer Sentinel Mixture Models\nFigure 2. Visualization of the pointer sentinel-RNN mixture model. The query, produced from applying an MLP to the last output of the RNN, is used by the pointer network to identify likely matching words from the past. The (cid:12) nodes are inner products between the query and the RNN hidden states. If the pointer component is not conﬁdent, probability mass can be directed to the RNN by increasing the value of the mixture gate g via the sentinel, seen in grey. If g = 1 then only the RNN is used. If g = 0 then only the pointer is used.",
    "data_related_paragraphs": [
        "Given the length of the documents used in language modeling, it may not be feasible for the pointer network to evaluate an attention score for all the words back to the beginningofthedataset. Instead, wemayelecttomaintainonlya window of the L most recent words for the pointer to match against. The length L of the window is a hyperparameter that can be tuned on a held out dataset or by empirically analyzing how frequently a word at position t appears within the last L words.",
        "Our mixture model has two base distributions: the softmax vocabulary of the RNN output and the positional vocabulary of the pointer model. We refer to these as the RNN component and the pointer component respectively. To combine the two base distributions, we use a gating funcxi) where zi is the latent variable stating tion g = p(zi = k which base distribution the data point belongs to. As we only have two base distributions, g can produce a scalar in the range [0,1]. A value of 0 implies that only the pointer",
        "Table 1. Statistics of the Penn Treebank, WikiText-2, and WikiText-103. The out of vocabulary (OoV) rate notes what percentage of tokens have been replaced by an (cid:104)unk(cid:105) token. The token count includes newlines which add to the structure of the WikiText datasets.",
        "Weﬁrstdescribethemostcommonlyusedlanguagemodeling dataset and its pre-processing in order to then motivate the need for a new benchmark dataset.",
        "In order to compare our model to the many recent neural language models, we conduct word-level prediction experiments on the Penn Treebank (PTB) dataset (Marcus et al., 1993), pre-processed by Mikolov et al. (2010). The dataset consists of 929k training words, 73k validation words, and 82k test words. As part of the pre-processing performed by Mikolov et al. (2010), words were lower-cased, numbers were replaced with N, newlines were replaced with , (cid:105) and all other punctuation was removed. The vocabulary is the most frequent 10k words with the rest of the tokens be-",
        "4.2. Reasons for a New Dataset",
        "Other larger scale language modeling datasets exist. Unfortunately, they either have restrictive licensing which prevents widespread use or have randomized sentence ordering (Chelba et al., 2013) which is unrealistic for most language use and prevents the effective learning and evaluation of longer term dependencies. Hence, we constructed a language modeling dataset using text extracted from Wikipedia and will make this available to the community.",
        "To ensure the dataset is immediately usable by existing language modeling tools, we have provided the dataset in the",
        "same format and following the same conventions as that of the PTB dataset above.",
        "The full WikiText dataset is over 103 million words in size, a hundred times larger than the PTB. It is also a tenth the size of the One Billion Word Benchmark (Chelba et al., 2013), one of the largest publicly available language modeling benchmarks, whilst consisting of articles that allow for the capture and usage of longer term dependencies as might be found in many real world tasks.",
        "The dataset is available in two different sizes: WikiText-2 and WikiText-103. Both feature punctuation, original casing, a larger vocabulary, and numbers. WikiText-2 is two times the size of the Penn Treebank dataset. WikiText-103 features all extracted articles. Both datasets use the same articles for validation and testing with the only difference being the vocabularies. For full statistics, refer to Table 1.",
        "Figure 3. Zipﬁan plot over the training partition in Penn Treebank and WikiText-2 datasets. Notice the severe drop on the Penn Treebank when the vocabulary hits 104. Two thirds of the vocabulary in WikiText-2 are past the vocabulary cut-off of the Penn Treebank.",
        "language modeling training schemes, k1 = k2, meaning that every k timesteps truncated BPTT is performed for the k previous timesteps. This results in only a single RNN output receiving backpropagation for k timesteps, with the other extreme being that the ﬁrst token receives backpropagation for 0 timesteps. This issue is compounded by the fact that most language modeling code split the data temporally such that the boundaries are always the same. As such, most words in the training data will never experience a full backpropagation for k timesteps.",
        "Table 2 compares the pointer sentinel-LSTM to a variety of other models on the Penn Treebank dataset. The pointer sentinel-LSTM achieves the lowest perplexity, followed by the recent Recurrent Highway Networks (Zilly et al., 2016). The medium pointer sentinel-LSTM model also achieves lower perplexity than the large LSTM models. Note that the best performing large variational LSTM model uses computationally intensive Monte Carlo (MC) dropout averaging. Monte Carlo dropout averaging is a general improvement for any sequence model that uses dropout but comes at a greatly increased test time cost. In Gal (2015) it requires rerunning the test model with 1000 different dropout masks. The pointer sentinel-LSTM is able to achieve these results with far fewer parameters than other models with comparable performance, speciﬁ- cally with less than a third the parameters used in the large variational LSTM models.",
        "As WikiText-2 is being introduced in this dataset, there are noexistingbaselines. Weprovidetwobaselines tocompare the pointer sentinel-LSTM against: our variational LSTM using zoneout and the medium variational LSTM used in Gal (2015).4 Attempts to run the Gal (2015) large model variant, a two layer LSTM with hidden size 1500, resulted in out of memory errors on a 12GB K80 GPU, likely due to the increased vocabulary size. We chose the best hyperparameters from PTB experiments for all models.",
        "We introduced the pointer sentinel mixture model and the WikiText language modeling dataset. This model achieves state of the art results in language modeling over the Penn Treebank while using few additional parameters and little additional computational complexity at prediction time.",
        "We have also motivated the need to move from Penn Treebank to a new language modeling dataset for long range dependencies, providing WikiText-2 and WikiText-103 as potential options. We hope this new dataset can serve as a platform to improve handling of rare words and the usage of long term dependencies in language modeling.",
        "Figure 13. Zipﬁan plot over the training partition in the WikiText-103 dataset. With the dataset containing over 100 million tokens, there is reasonable coverage of the long tail of the vocabulary."
    ]
}