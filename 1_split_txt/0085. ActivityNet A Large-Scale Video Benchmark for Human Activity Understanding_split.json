{
    "title_author_abstract_introduction": "ActivityNet: A Large-Scale Video Benchmark for Human Activity Understanding Fabian Caba Heilbron1,2, Victor Escorcia1,2, Bernard Ghanem2 and Juan Carlos Niebles1\n1Universidad del Norte, Colombia 2King Abdullah University of Science and Technology (KAUST), Saudi Arabia\nAbstract\nIn spite of many dataset efforts for human action recognition, current computer vision algorithms are still severely limited in terms of the variability and complexity of the actions that they can recognize. This is in part due to the simplicity of current benchmarks, which mostly focus on simple actions and movements occurring on manually trimmed videos. In this paper we introduce ActivityNet, a new largescale video benchmark for human activity understanding. Our benchmark aims at covering a wide range of complex human activities that are of interest to people in their daily living. In its current version, ActivityNet provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours. We illustrate three scenarios in which ActivityNet can be used to compare algorithms for human activity understanding: untrimmed video classiﬁcation, trimmed activity classiﬁcation and activity detection.\n1. Introduction\nWith the growth of online media, surveillance and mobile cameras, the amount and size of video databases are increasing at an incredible pace. For example, YouTube reported that over 300 hours of video are uploaded every minute to their servers [43]. Arguably, people are the most important and interesting subjects of such videos. The computer vision community has embraced this observation to validate the crucial role that human activity/action recognition plays in building smarter surveillance systems, semantically aware video indexes, and more natural humancomputer interfaces. However, despite the explosion of video data, the ability to automatically recognize and understand human activities is still rather limited. This is primarily due to impeding challenges inherent to the task, namely the large variability in execution styles, complexity of the visual stimuli in terms of camera motion, background clutter and viewpoint changes, as well as, the level of detail and number of activities that can be recognized. An important limitation that hinders the performance of current\ntechniques is the state of existing video datasets and benchmarks available to action/activity recognition researchers.\nFor example, note that the range of activities performed by one person in a day varies from making the bed after waking up to brushing teeth before going to sleep. Between these moments, he/she performs many activities relevant to his/her daily life. The American Time Use Survey reports that Americans spent an average 1.7 hours in household activities against only 18 minutes participating in sports, exercise or recreation per day [37]. In spite of this fact, most computer vision algorithms for human activity understanding are benchmarked on datasets that cover a limited number of activity types. In fact, existing databases tend to be speciﬁc and focus on certain types of activities i.e. sports, cooking or simple actions. Typically, these datasets have a small number of categories (around 100), a small number of samples (short clips) per category (around 100), and limited category diversity.\nIn this paper, we address these dataset limitations by using a ﬂexible framework that allows continuous acquisition, crowdsourced annotation, and segmentation of online videos, thus, culminating in a large-scale (large in the number of categories and number of samples per category), rich (diverse taxonomy), and easy-to-use (annotations, baseline classiﬁcation models will be available online) activity dataset, known as ActivityNet. One of the most important aspects of ActivityNet is that it is structured around a semantic ontology which organizes activities according to social interactions and where they usually take place. It provides a rich activity hierarchy with at least four levels of depth. For example, the activity Filing nails falls under the third tier category Washing, dressing and grooming, which belongs to the second tier Grooming and ﬁnally the major category Personal care. Figure 1 illustrates other examples of this organization. To the best of our knowledge, ActivityNet is the ﬁrst database for human activity recognition organized under a rich semantic taxonomy.\nWe organize the paper as follows: we ﬁrst review and summarize existing benchmarks for human activity understanding. Then, we present the details of our dataset collection and annotation framework and provide a summary of the properties of ActivityNet. We illustrate three\nCleaning windowsInterior cleaningHouseworkHousehold activitiesBrushing teethGrooming oneselfGroomingPersonal careTop levelSecond tierThird tierActivityFigure1.ActivityNetorganizesalargenumberofdiversevideosthatcontainhumanactivitiesintoasemantictaxonomy.Top-rowshowstheroot-leafpathfortheactivityCleaningwindows.Bottom-rowshowstheroot-leafpathfortheactivityBrushingteeth.Eachboxillustratesexamplevideosthatliewithinthecorrespondingtaxonomynode.Greenintervalsindicatethetemporalextentoftheactivity.Allﬁguresarebestviewedincolor.benchmarkingscenariosforevaluatingtheperformanceofstate-of-the-artalgorithms:untrimmedvideoclassiﬁcation,trimmedactivityclassiﬁcationandactivitydetection.2.RelatedWorkThechallengesofbuildingsystemsthatunderstandandrecognizecomplexactivitiesinrealenvironmentsandcon-ditions,haspromptedtheconstructionofstandardizeddatasetsforalgorithmtrainingandevaluation.However,currentbenchmarksareratherlimitedinatleastoneoftheseaspects:numberofcategories,samplespercategory,tem-porallengthofeachsample,diversityofvideocapturingconditionsorenvironments,andthediversityofcategorytaxonomy.Furthermore,extendingmostofthesedatasetsinvolvesextremelycostlymanuallabor.Webrieﬂyreviewsomeofthemostinﬂuentialactiondatasetsavailable.TheHollywooddataset[20]containsvideostakenfromHollywoodmovies.Twelveactioncat-egoriesareperformedbyprofessionalactors,whichresultsinmorenaturalscenesthanearliersimpleactiondatasets[33,9].Similarly,otherdatasetsalsorelaxtheenviron-mentassumptionsleadingtochallengingrecognitiontaskswithdifﬁcultbackgroundandcameraangles.Forexample,UCFSports[30]andOlympicSports[24]increasetheac-tioncomplexitybyfocusingonhighlyarticulatedsportingactivities.However,thesmallnumberofcategorieskeepsthescopeoftheactivitiesnarrow,andcannotbeconsideredarepresentativesampleofactivitiesinthereal-world.An-otherdimensionofcomplexityisaddressedbydatasetsthatfocusoncomposable[21]andconcurrent[41]activities,buttheseareconstrainedwithrespecttothesceneandenviron-mentassumptions.NextintermsofsamplesizearetheUCF101[17]-Thumos’14[35]andtheHMDB51[19]datasets,compiledfromYouTubevideosandwithmorethan50actioncate-gories.Theresultingvideosamplesareshortandonlycon-veysimplisticshort-termactionsorevents.Thesevideoswerecollectedthroughamanualandcostlyprocess,whichisdifﬁculttoscaleifthesizeofthedatasetistobeextended.Intermsofsemanticorganization,HMDB51groupsactivi-tiesinto5majortypes:general-facial,facialwithobjectma-nipulation,generalbodymovement,bodymovementswithobjectinteractionandbodymovementsforhumaninterac-tion.Ontheotherhand,UCF101groupscategoriesinto5types:human-objectinteraction,bodymotiononly,playingmusicalinstruments,sports.Unfortunately,thesearesim-pletaxonomieswithonlytwolevelsofresolution,anddonotprovideadetailedorganizationofactivities.TheMPIIHumanPoseDataset[2]focusesonhumanposeestimation,andwasrecentlyappliedtoactionrecog-nition[29].Itprovidesshortclips(41framesorlonger)thatdepicthumanactions.Unfortunately,thedistributionofvideosamplespercategoryisnon-uniformandbiasedtowardssomeactioncategories.Currently,thelargestvideodatasetavailableistheSports-1Mdataset[16],withabout500sports-relatedcat-egories,annotatedbyanautomatictaggingalgorithm.De-spiteitssheersize,thisdatasetisstructuredusingasome-whatlimitedactivitytaxonomy,asitonlyfocusesonsportsactions.Furthermore,theautomaticcollectionprocessin-troducesanundisclosedamountoflabelnoise.Alsorelatedtoourworkaretheeffortstoconstructlarge-scalebenchmarksforobjectrecognitioninstaticimages.ImagebenchmarkssuchasImageNet[5],SUN[42]andTinyImages[36]havespawnedsigniﬁcantadvancesforcom-putervisionalgorithmsintherelatedtasks.AnexampleistheLargeScaleVisualRecognitionChallenge(ILSVRC)[32],fromwhichtheAlexNetarchitecture[18]gainsitspop-ularityduetoanoutstandingperformanceinthechallenge.ActivityNetattemptstoﬁllthegapinthefollowingas-pects:alarge-scaledatasetthatcoversactivitiesthataremostrelevanttohowhumansspendtheirtimeintheirdailyliving;aqualitativejumpintermsofnumberandlength\fof each video (instead of short clips), diversity of activity taxonomy and number of classes; a human-in-the-loop annotation process that can provide higher label accuracy as compared to fully automatic annotation algorithms; and a framework for continuous dataset expansion at low cost.\n3. Building ActivityNet\nActivityNet aims at providing a semantic organization In this section, we of videos depicting human activities. introduce the activity lexicon and hierarchy that serves as a backbone for ActivityNet. Another important goal is to provide a large set of diverse video samples for each activity In this section, we also describe our scalable of interest. data collection and video annotation scheme. Finally, we summarize some interesting properties of ActivityNet.\n3.1. Deﬁning the Activity lexicon\nOur goal is to build ActivityNet upon a rich semantic taxonomy. In contrast to the object domain, it is difﬁ- cult to deﬁne an explicit semantic organization of activities. Beyond the shallow hierarchies that organize current benchmarks, some attempts have been made at providing a structured organization of activities within the computer vision community. Aloimonos et al. [10, 26] propose a twolevel organization of activities into 6 groups: ground, general object, general person, speciﬁc object, speciﬁc person, group; which connects to verbs in WordNet. Unfortunately, verbs are more difﬁcult to use directly, because unlike objects in ImageNet [5], there is more ambiguity and polysemism between verbs and activities, than between objects and synsets. This may be partly explained by the fact that our spoken language for activities needs more complicated constructions compared to what is needed for objects.\nOutside the vision community, there are efforts that organize general knowledge into structured repositories, such as Freebase[8], FrameNet[7], among others. Since none of them are speciﬁc to activities, their richness and depth are limited. On the other hand, there are also efforts more speciﬁc to activities. In the medical community, Ainsworth et al. [1] organizes a small number of physical human activities into a two level taxonomy.\nSince we aim at a large scale benchmark with high activity diversity, we propose the use of the activity taxonomy built by the Department of Labor for conducting the American Time Use Survey [37]. The ATUS taxonomy organizes more than 2000 activities according to two key dimensions: a) social interactions and b) where the activity usually takes place. The ATUS coding lexicon contains a large variety of daily human activities organized under 18 top level categories such as Personal Care, Work-Related, Education and Household activities. In addition, there are two more levels of granularity under these top level categories. For example, the activity Polishing shoes, appears in the hierarchy\nFigure 3. Visualization of the sub-tree of the top level category Household activities. Full taxonomy is available in the supplementary material.\nas a leaf node under the third category, Sewing, repairing and maintaining textiles, which is part of the second tier category, Housework, which falls under the Household activities top level category.\nFor the ﬁrst release of ActivityNet, we have manually selected a subset of 203 activity categories, out of the more than two thousand activity examples provided by the ATUS activity hierarchy. The activity classes belong to 7 different top level categories: Personal Care, Eating and Drinking, Household, Caring and Helping, Working, Socializing and Leisure and Sports and Exercises. Figure 3 illustrates the sub-tree for the top-level category Household activities. The rich taxonomy in ActivityNet, which has four levels of granularity, constitutes a semantic organization backbone that may be useful in algorithms that are able to exploit the hierarchy during model training.\n3.2. Collecting and annotating human activities\nBuilding benchmark datasets for visual recognition has been traditionally a difﬁcult and time consuming task. The goal of ActivityNet is to provide a large-scale dataset of activities that can be expanded and annotated continously at a reasonably low cost. Traditional data collection practices that require many expert researcher hours are prohibitive. On the other hand, fully automatic methods introduce label noise that is difﬁcult to erradicate.\nWe now describe the collection and annotation process forobtainingActivityNet. Inspiredby[5,11,38], wefollow a semi-automatic crowdsourcing strategy to collect and annotate videos (Figure 2). We ﬁrst search the web for potential videos depicting a particular human activity. Then, we\nHouseholdActivitiesHouseworkInteriorcleaningVacuuming floorCleaning toiletMaking the bedCleaning windowsCleaning out closetPolishing furnitureRecyclingLaundryHand washing clothesIroning clothesSorting laundrySewing,Repair.Polishing shoesCleaning shoesKnitting sweatersInteriorMaint.Interiorarrangem.Changing light bulbsPaintingFixing leaksFumigating houseInstalling carpetPut. Christmas lightsBuild andRep. Furnit.Assembling furniturePainting furnitureHeating& coolingChopping woodSetting the fireExteriorMaint.ExteriorcleaningShoveling snowCleaning chimneyCleaning garageSweeping sidewalkRepair,improv.Fixing broken windowsFixing mailboxLawn,GardenPlantcareGardeningMowing the lawnWatering gardenApplying pesticidesCutting the grassPicking fruitsAnimalsand PetsCare foranimalsBathing dogChanging dog waterExercisingwith animalsWalking the dogVehiclesVehiclerepairChanging oilChanging wheelAssembling bicycleInstalling car stereoPut. air in tires\fa) Unlabeled Videosb) Untrimmed Videosc) Trimmed Activity InstancesFigure2.Videocollectionandannotationprocess.(a)Westartwithalargenumberofcandidatevideos,forwhichthelabelsarepartiallyunknown.(b)AMTworkersverifyifanactivityofinterestispresentineachvideo,sothatwecandiscardfalsepositivevideos(inred).Thisresultsinasetofuntrimmedvideosthatcontaintheactivity(ingreen).(c)Finally,weobtaintemporalboundariesforactivityinstances(ingreen)withthehelpofAMTworkers.relyonAmazonMechanicalTurk(AMT)workerstoverifythepresenceoftheactivityineachvideo.Finally,multipleworkersannotateeachvideowiththetemporalboundariesassociatedtotheactivity.SearchtheWeb:Atthisstage,wehaveatextuallistofhumanactivityclassesandourgoalistosearchthewebtoretrievevideosrelatedtoeachactivity.ExploitingthelargeamountofvideodataononlinerepositoriessuchasYouTube,wesearchvideosusingtextbasedqueries.ThesequeriesareexpandedwithWordNet[23]usinghyponyms,hypernymsandsynonymsinordertoincreasethenumberofretrievedvideosandcontentvariety.LabelingUntrimmedVideos:Weverifyallvideosre-trievedandremovethosenotrelatedwiththeactivityathand.WeemployAMTworkers(turkers)torevieweachvideoanddetermineifitcontainsanintendedactivityclass.Inordertokeeptheannotationqualityhigh,weinsertver-iﬁablelabelingquestionsandonlyemploymultipleexpertturkers.Duetotheinaccuracyoftext-basedqueries,weusuallydiscardmanyvideosthatarenotrelatedwithanyoftheintendedactivityclasses.Attheendofthisprocess,wehaveasetofveriﬁeduntrimmedvideosthatareassociatedtoatleastonegroundtruthactivitylabel.AnnotatingtheActivityInstances:Mostcurrentac-tivityclassiﬁcationsystemsrequiretrainingvideostobetrimmedtoonlycontaintheintendedactivity.Neverthe-less,itishardtoﬁndwebvideoscontainingonlyinforma-tionwithaspeciﬁcactivity.Forexample,whensearchingYouTubewiththequery“Preparingpasta”,resultsincludevideoscontainingcontextualinformationaboutthechef.Inthisdirection,weaimtomanuallyannotatethetemporalboundarieswhereanactivityisperformedinavideo.Totacklethismanualprocess,werelyonAMTworkerstotemporallyannotatealltheactivityinstancespresentinavideo.Inordertoensurequality,thetemporalextentofeachactivityinstanceislabelledbymultipleexpertturkers.Then,weclustertheirannotationstoobtainrobustannota-tionagreements.Thisstageproducesacuratedsetofactiv-ityinstances,eachofthemassociatedtoexactlyonegroundtruthactivitylabel.Moreover,itisimportanttonotethatwithinoneuntrimmedvideo,theremaybemorethanoneactivityinstancefrommorethanoneactivityclass.3.3.ActivityNetataGlanceWenowlookintosomeofthepropertiesofthevideosinActivityNet.Weﬁrstreportstatisticsrelatedtothevideodata.Second,wecompareActivityNettoseveralexistingdatasetsforthebenchmarkingofhumanactivities.VideoPropertiesAllActivityNetvideosareobtainedfromonlinevideosharingsites.Wedownloadtheoriginalvideosatthebestqualityavailable.Inordertolimitthetotalstoragerequirement,weprioritizethesearchtowardvideoslessthan20minuteslong.Inpractice,alargeproportionofvideoshaveadurationbetween5and10minutes.Around50%ofthevideosareinHDresolution(1280⇥720),whilethemajorityhaveaframerateof30FPS.CollectionandAnnotationSummaryFigure4-(toprows)showsthenumberofuntrimmedvideosandtrimmedactivityinstancesperclassinthecurrentversionofActiv-ityNet.Thedistributionisclosetouniform,whichhelpstoavoiddataunbalancewhentrainingclassiﬁers.Alsonotethatthereisafactorof1.41trimmedinstancesperuntrimmedvideoinaverage.Finally,ourcollectionpro-cesswillalloweasyexpansionofActivityNetintermsofnumberofsamplespercategoryandnumberofcategories.ComparisonwithexistingdatasetsWecompareActiv-ityNetwithseveralactiondatasets[17,19,20,24,35,16,31]intermsof:1)varietyintermsofthetypeofactivities,and2)numberofactivityclassesandsamplesperclass.Tocomparethevarietyonactivitytypes,wemanuallyanno-tatealltheactionsineachdatasetwithaparenttoplevelcategoryfromtheActivityNethierarchy.Forexample,theactionPushupsfromUCF101isannotatedunderSportsandexercising.InFigure4(bottom-left),weplotastackedhistogramfortheactionsassignedtoeachtoplevelcate-gory.Itillustratesthelackofvarietyonactivitytypesforallexistingdatasets.Incontrast,ActivityNetstrivesforinclud-ingactivitiesintoplevelcategoriesthatarerarelyconsid-eredincurrentbenchmarks:Householdactivities,Personalcare,EducationandWorkingactivities.ToanalyzethescaleofActivityNetcomparedtotheexistingactiondatasets,weplotinFigure4(bottom-right)thenumberofinstancesperclassvsthenumberofactivity/actionclasses.ThecurrentversionofActivityNetrankssecondlargestactivityanalysisdatasetbutitisthemostvariedintermsofactivitytypes.\fl\nActivityNet UCF101 HMDB51 Hollywood Thumos’14 Sports-1M Olympic MPII\n0.7 0.5 Ratio of  top level categories\nPersonal care Work-related\nEating & drinking Sports & exercises\nHousehold activities Socializing & leisure\nCaring & helping Simple actions\nUntrimmed\nTrimmed - Untrimmed\nMPII HPD\nActivityNet\nSports-1M\nUCF101\nThumos’14 HMDB51\nOlympic\nHollywood",
    "data_related_paragraphs": [
        "Figure 4. Summary of ActivityNet statistics. First two rows: Red bars indicate the number of untrimmed instances per activity category; and the top of the gray bars indicate the number of trimmed instances in each class. Bottom left compares the distribution of the activity classes in different datasets with the top levels of our hierarchy. Bottom right compares the scale in terms of both number of samples per category and number of categories between different datasets.",
        "Deep Features (DF): These features aim to encode information about the objects in the scene. In many activities involving object interactions, this is an important cue for disambiguation [6]. In practice, we adopt features derived from convolutional networks that have been trained for the task of object recognition. This is motivated by the versatility of these features, which have been successfully applied to many visual recognition tasks. For the network implementation, we adopt the AlexNet [18] architecture trained on ILSVRC-2012 [32] as provided by Caffe [15]. We retain activations of the network associated with the top-3 fullyconnected layers (fc-6, fc-7, and fc-8). We encode temporal information in the activity by averaging activations across several frames. In practice, we compute these deep features every ten frames for all the videos in our dataset.",
        "We deﬁne three different application scenarios in which ActivityNet can be used for benchmarking. First, we investigate the performance of an activity recognition algorithm on the task of Untrimmed video classiﬁcation. For the second task, we use the manually annotated trimmed video instances to construct the largest dataset for Trimmed activity classiﬁcation. Finally, we benchmark Activity detection on all the untrimmed videos in ActivityNet.",
        "Dataset: Using the labeled untrimmed videos from ActivityNet, we deﬁne a dataset for benchmarking untrimmed video classiﬁcation. The dataset consists of 27801 videos that belong to 203 activity classes. We randomly split the data into three different subsets: train, validation and test, where 50% is used for training, and 25% for validation and testing.",
        "Dataset: We deﬁne a dataset for benchmarking human activity classiﬁcation algorithms. The dataset includes 203 activity classes with 193 samples per category on average. These samples correspond to trimmed activity instances in ActivityNet. When generating the training, validation, and test subsets, we constrain the instances from a single video to be in the same subset so as to avoid data contamination. Classiﬁers: As compared to untrimmed video classiﬁ- cation, we build classiﬁers here with features that are only extracted from the trimmed activity itself. We learn a linear SVMclassiﬁerforeachfeaturetype. Whencombiningmultiplefeatures, we simplysumthekernelsbefore thelearning procedure. To enable multi-class classiﬁcation, we utilize a one-vs-all learning approach. Given a test video clip, we select the class with highest score.",
        "In this task, the goal is to ﬁnd and recognize all activity instances within an untrimmed test video sequence. Activity detection algorithms should provide start and end frames, designating the duration of each activity present in the video. To evaluate the different classiﬁcation models, we exploit ActivityNet annotations for the evaluation, thus, forming the largest and most diverse activity detection dataset in the literature.",
        "Dataset: To the best of our knowledge, the ActivityNetbased detection dataset we use here is the largest existing datasetforthistask. Itcontainsatotalof849hoursofvideo, where 68.8 hours of video contain 203 human-centric activities. Here, we split the dataset in three different subsets as in the video classiﬁcation tasks above.",
        "Comparing performance with existing datasets: To emphasize the difﬁculty of ActivityNet, we compare results for several datasets in Table 4. We consistently observe that ActivityNet constitutes a signiﬁcant challenge to state-ofthe-art recognition methods and is substantially more difﬁ- cult than existing activity benchmarks. We attribute this to the following: a) ActivityNet increases the number of categories by a factor of two, and b) the variety in the video data represents a real world challenge for existing algorithms.",
        "Dataset",
        "notation effort that is easily scalable to larger numbers of activities and larger samples per activity, at a reasonably low cost. We compare ActivityNet with existing datasets for action/activity recognition. We show that ActivityNet presents more variety in terms of activity diversity and richness of taxonomy. It also contains more categories and samples per category than traditional action datasets. We also introduce three possible applications for using ActivityNet: untrimmed video classiﬁcation, trimmed activity classiﬁcation, and activity detection. The results obtained in these tasks reveal that ActivityNet unveils new challenges in understanding and recognizing human activities.",
        "Table 4. Cross-dataset performance comparison. State-of-the-art results are reported for each dataset. Reported results for the activity detection task corresponds to the performance obtained with ↵ = 0.2",
        "[4] I. Atmosukarto, B. Ghanem, and N. Ahuja. Action recognition using discriminative structured trajectory groups. 2015. [5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009.",
        "[8] Freebase: A community-curated database of well-known people, places, and things. https://www.freebase. com.",
        "[17] A. R. Z. Khurram Soomro and M. Shah. A dataset of 101 human action classes from videos in the wild. Technical report, University of Central Florida, 2012.",
        "[19] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: A large video database for human motion recognition. In ICCV, 2011.",
        "[23] G. A. Miller. WordNet: A Lexical Database for English.",
        "[31] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A database for ﬁne grained activity detection of cooking activities. In CVPR, 2012.",
        "[36] A. Torralba, R. Fergus, and W. T. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1958–1970, 2008.",
        "[42] J. Xiao, K. A. Ehinger, J. Hays, A. Torralba, and A. Oliva. SUN Database: Exploring a large collection of scene categories. International Journal of Computer Vision, 2014. [43] YouTube statistics. http://www.youtube.com/yt/"
    ]
}