{
    "title_author_abstract_introduction": "Argoverse: 3D Tracking and Forecasting with Rich Maps\nMing-Fang Chang∗ 1,2, John Lambert∗3, Patsorn Sangkloy∗1,3, Jagjeet Singh∗1, Sławomir B ˛ak , Andrew Hartnett1, De Wang1, Peter Carr1, Simon Lucey1,2, Deva Ramanan1,2, and James Hays1,3\n1Argo AI, 2Carnegie Mellon University, 3Georgia Institute of Technology\nFigure 1: We introduce datasets for 3D tracking and motion forecasting with rich maps for autonomous driving. Our 3D tracking dataset contains sequences of LiDAR measurements, 360◦ RGB video, front-facing stereo (middle-right), and 6-dof localization. All sequences are aligned with maps containing lane center lines (magenta), driveable region (orange), and ground height. Sequences are annotated with 3D cuboid tracks (green). A wider map view is shown in the bottom-right.\nAbstract\nWe present Argoverse – two datasets designed to support autonomous vehicle machine learning tasks such as 3D tracking and motion forecasting. Argoverse was collected by a ﬂeet of autonomous vehicles in Pittsburgh and Miami. The Argoverse 3D Tracking dataset includes 360◦ images from 7 cameras with overlapping ﬁelds of view, 3D point clouds from long range LiDAR, 6-DOF pose, and 3D track annotations. Notably, it is the only modern AV dataset that provides forward-facing stereo imagery. The Argoverse Motion Forecasting dataset includes more than 300,000 5 second tracked scenarios with a particular vehicle identiﬁed for trajectory forecasting. Argoverse is the ﬁrst autonomous vehicle dataset to include “HD maps” with 290 km of mapped lanes with geometric and semantic metadata. All data is released under a Creative Commons license at\n*Equal contribution\nwww.argoverse.org. In our baseline experiments, we illustrate how detailed map information such as lane direction, driveable area, and ground height improves the accuracy of 3D object tracking and motion forecasting. Our tracking and forecasting experiments represent only an initial exploration of the use of rich maps in robotic perception. We hope that Argoverse will enable the research community to explore these problems in greater depth.\n1. Introduction\nDatasets and benchmarks for a variety of perception tasks in autonomous driving have been hugely inﬂuential to the computer vision community over the last few years. We are particularly inspired by the impact of KITTI [14], which opened and connected a plethora of new research directions. However, publicly available datasets for autonomous driving rarely include map data, even though detailed maps are\ncritical to the development of real world autonomous systems. Publicly available maps, e.g. OpenStreetMap, can be useful, but have limited detail and accuracy.\nIntuitively, 3D scene understanding would be easier if maps directly told us which 3D points belong to the road, which belong to static buildings, which lane a tracked object is in, how far it is to the next intersection, etc. But since publicly available datasets do not contain richly-mapped attributes, how to represent and utilize such features is an open research question. Argoverse is the ﬁrst large-scale autonomous driving dataset with such detailed maps. We investigate the potential utility of these new map features on two tasks – 3D tracking and motion forecasting, and we offer a signiﬁcant amount of real-world, annotated data to enable new benchmarks for these problems. Our contributions in this paper include:\n• We release a large scale 3D tracking dataset with synchronized data from LiDAR, 360◦ and stereo cameras sampled across two cities in varied conditions. Unlike other recent datasets, our 360◦ is captured at 30fps.\n• We provide ground truth 3D track annotations across 15 object classes, with ﬁve times as many tracked objects as the KITTI [14] tracking benchmark.\n• We create a large-scale forecasting dataset consisting of trajectory data for interesting scenarios such as turns at intersections, high trafﬁc clutter, and lane changes.\n• We release map data and an API which can be used to develop map-based perception and forecasting algorithms. We are the ﬁrst self-driving vehicle dataset with a semantic vector map of road infrastructure and trafﬁc rules. The inclusion of “HD” map information also means our dataset is the ﬁrst large-scale benchmark for automatic map creation, often known as map automation.\n• We are the ﬁrst to examine the inﬂuence of HD map context for 3D tracking and motion forecasting. In the case of 3D tracking, we measure the inﬂuence of mapbased ground point removal and orientation snapping to lanes. In the case of motion forecasting, we experiment with the creation of diverse predictions from the lane graph and the pruning of predictions by the driveable area map. In both cases, we see higher accuracy with the use of a map.",
    "data_related_paragraphs": [
        "Autonomous Driving Datasets with Map Information. Until recently, it was rare to ﬁnd datasets that provide detailed map information associated with annotated data. The prohibitive cost of annotating and constructing such maps has spurred interest in the growing ﬁeld of map automation [35, 25, 4]. Prior to Argoverse’s release, no public dataset",
        "included 3D vector map information, thus preventing the development of common benchmark for map automation. TorontoCity [58] also focuses on map construction tasks but without 3D annotation for dynamic objects. The nuScenes dataset [6] originally contained maps in the form of binary, rasterized, top-down indicators of region of interest (where region of interest is the union of driveable area and sidewalk). This map information is provided for 1000 annotated vehicle log segments (or “scenes”) in Singapore and Boston. Subsequent to Argoverse release, nuScenes has released labels for 2D semantic map regions, without a lane or graph structure. Like nuScenes, we include maps of driveablearea, butalso includegroundheightanda“vectormap” of lane centerlines and their connectivity. Autonomous Driving Datasets with 3D Track Annotations. Many existing datasets for object tracking focus on pedestrian tracking from image/video sequences [16, 48, 43, 2]. Several datasets provide raw data from self-driving vehicle sensors, but without any object annotations [42, 45, 49]. The ApolloCar3D dataset [55] is oriented towards 3D semantic object keypoint detection instead of tracking. KITTI [14] and H3D [47] offer 3D bounding boxes and track annotations but do not provide a map. The camera ﬁeld of view is frontal, rather than 360◦. VIPER [52] provides data from a simulated world with 3D track annotations. nuScenes [6] currently provides 360◦ data and a benchmark for 3D object detection, with tracking annotation also available. The Argoverse 3D Tracking dataset contains 360◦ track annotations in 3D space aligned with detailed map information. See Table 1 for a comparison between 3D autonomous vehicle datasets. Autonomous Driving Datasets with Trajectory Data. ApolloScape [26] also uses sensor-equipped vehicles to observe driving trajectories in the wild and presents a forecasting benchmark [41] from a subset of the ApolloScape 3D tracking annotations. This dataset consists of 155 minutes of observations compared to 320 hours of observations in the Argoverse Forecasting dataset. IntentNet [7] mines roof-mountedLiDARdatafor54millionobject trajectories, but the data is not publicly available. Using Maps for Self-driving Tasks. While high deﬁnition (HD) maps are widely used by motion planning systems, few works explore the use of this strong prior in perception systems [60] despite the fact that the three winning entries of the 2007 DARPA Urban Challenge relied on a DARPA-supplied map – the Route Network Deﬁnition File (RNDF) [44, 57, 3]. Hecker et al. [20] show that end-toend route planning can be improved by processing rasterized maps from OpenStreetMap and TomTom. Liang et al. [36] demonstrate that using road centerlines and intersection polygons from OpenStreetMap can help infer crosswalk location and direction. Yang et al. [60] show that incorporating ground height and bird’s eye view (BEV) road",
        "3D Object Tracking. In traditional approaches for point cloud tracking, segments of points can be accumulated using clustering algorithms such as DBSCAN [13, 33] or connected components of an occupancy grid [34, 24], and then associated based on some distance function using the Hungarian algorithm. Held et al. utilize probabilistic approaches to point cloud segmentation and tracking [21, 23, 22]. Recent work demonstrates how 3D instance segmentation and 3D motion (in the form of 3D scene ﬂow, or per-point velocity vectors) can be estimated directly on point cloud input with deep networks [59, 38]. Our dataset enables 3D tracking with sensor fusion in a 360◦ frame. Trajectory Forecasting. Spatial context and social interactions can inﬂuence the future path of pedestrians and cars. Social-LSTM[1] proposes a novel pooling layer to capture social interaction of pedestrians. Social-GAN [17] attempts to model the multimodal nature of the predictions. However, both have only been tested on pedestrian trajectories, with no use of static context (e.g. a map). Deo et al. [11] propose a convolutional social pooling approach wherein they ﬁrst predict the maneuver and then the trajectory conditioned on that maneuver. In the self-driving domain, the use of spatial context is of utmost importance and it can be efﬁciently leveraged from the maps. Chen et al. [9] use a feature-driven approach for social and spatial context by mapping the input image to a small number affordances of a road/trafﬁc state. However, they limit their experiments to a simulation environment. IntentNet [7] extends the joint detection and prediction approach of Luo et al. [39] by discretizing the prediction space and attempting to predict one of eight common driving maneuvers. DESIRE [32] demonstrates a forecasting model capturing both social interaction and spatial context. The authors note that the beneﬁts from these two additional components are small on the KITTI dataset, attributing this to the minimal inter-vehicle interactions in the data. Another challenging problem in the trajectory forecasting domain is to predict diverse trajectories which can address multimodal nature of the problem. R2P2 [50] address the diversity-precision trade-off of generative forecasting models and formulate a symmetric cross-entropy training objective to address it. It is then followed by PRECOG [51] wherein they present the ﬁrst generative multi-agent forecasting method to condition on agent intent. They achieve state-of-the-art results for forecasting methods in real (nuScenes [6]) and simulated",
        "(CARLA [12]) datasets.",
        "3. The Argoverse Dataset",
        "Our sensor data, maps, and annotations are the primary contribution of this work. We also provide an API which connects the map data with sensor information e.g. ground point removal, nearest centerline queries, and lane graph connectivity; see the Appendix for more details. The data is available at www.argoverse.org under a Creative Commons license. The API, tutorials, and code for baseline algorithms are available at github.com/argoai/ argoverse-api under an MIT license. The statistics and experiments in this document are based on Argoverse v1.1 released in October 2019.",
        "We collected raw data from a ﬂeet of autonomous vehicles (AVs) in Pittsburgh, Pennsylvania, and Miami, Florida, both in the USA. These cities have distinct climate, architecture, infrastructure, and behavioral patterns. The captured data spans different seasons, weather conditions, and times of the day. The data used in our dataset traverses nearly 300 km of mapped road lanes and comes from a subset of our ﬂeet operating area. Sensors. Ourvehiclesareequippedwithtworoof-mounted, rotating 32 beam LiDAR sensors. Each LiDAR has a 40◦ vertical ﬁeld of view, with 30◦ overlapping ﬁeld of view and 50◦ total ﬁeld of view with both LiDAR. LiDAR range is up to 200 meters, roughly twice the range as the sensors used in nuScenes and KITTI. On average, our LiDAR sensors produce a point cloud at each sweep with three times the density of the LiDAR sweeps in the nuScenes [6] dataset (ours ∼ 107,000 points vs. nuScenes ∼ 35,000 points). The two LiDAR sensors rotate at 10 Hz and are out of phase, i.e. rotating in the same direction and speed but with an offset to avoid interference. Each 3D point is motion-compensated to account for ego-vehicle motion throughout the duration of the sweep capture. The vehicles have 7 high-resolution ring cameras (1920 × 1200) recording at 30 Hz with overlapping ﬁelds of view, providing 360◦",
        "DATASET NAME",
        "KITTI [14] Oxford RobotCar [42] H3D [47] Lyft Dataset [29] 1 nuScenes v1.0 [6] ApolloScape Tracking [41] Waymo Open Dataset",
        "Table 1: Public self-driving datasets. We compare recent, publicly available self-driving datasets with 3D object annotations for tracking (top) and trajectories for forecasting (bottom). Coverage area for nuScenes is based on its road and sidewalk raster map. Argoverse coverage area is based on our driveable area raster map. Statistics updated September 2019.",
        "coverage. In addition, there are 2 front-facing stereo cameras (2056 × 2464 with a 0.2986 m baseline) sampled at 5 Hz. Faces and license plates are procedurally blurred in camera data to maintain privacy. Finally, 6-DOF localization for each timestamp comes from a combination of GPSbased and sensor-based localization. Vehicle localization and maps use a city-speciﬁc coordinate system described in more detail in the Appendix. Sensor measurements for particular driving sessions are stored in “logs”, and we provide intrinsic and extrinsic calibration data for the LiDAR sensors and all 9 cameras for each log. Figure 2 visualizes our sensor data in 3D. Similar to [49], we place the origin of the ego-vehicle coordinate system at the center of the rear axle. All LiDAR data is provided in the ego-vehicle coordinate system, rather than in the respective LiDAR sensor coordinate frames. All sensors are roof-mounted, with a LiDAR sensor surrounded by 7 “ring” cameras (clockwise: facing front center, front right, side right, rear right, rear left, side left, and front left) and 2 stereo cameras. Figure 3 visualizes the geometric arrangement of our sensors.",
        "Argoverse contains three distinct map components – (1) a vector map of lane centerlines and their attributes; (2) a rasterized map of ground height, and (3) a rasterized map of driveable area and region of interest (ROI). Vector Map of Lane Geometry. Our vector map consists ofsemanticroaddatarepresentedasalocalizedgraphrather than rasterized into discrete samples. The vector map we release is a simpliﬁcation of the map used in ﬂeet operations. In our vector map, we offer lane centerlines, split into lane segments. We observe that vehicle trajectories generally follow the center of a lane so this is a useful prior for tracking and forecasting.",
        "The Argoverse Tracking Dataset contains 113 vehicle log segments with human-annotated 3D tracks. These 113 segments vary in length from 15 to 30 seconds and collectively contain 11,052 tracked objects. We compared these with other datasets in Table 1. For each log segment, we annotated all objects of interest (both dynamic and static) with bounding cuboids which follow the 3D LiDAR returns associated with each object over time. We only annotated objectswithin5mofthedriveableareaasdeﬁnedbyourmap. For objects that are not visible for the entire segment duration, tracks are instantiated as soon as the object becomes visible in the LiDAR point cloud and tracks are terminated when the object ceases to be visible. The same object ID is used for the same object, even if temporarily occluded. Each object is labeled with one of 15 categories, including ON_ROAD_OBSTACLE and OTHER_MOVER for static and dynamic objects that do not ﬁt into other predeﬁned categories. More than 70% of tracked objects are vehicles, but we also observe pedestrians, bicycles, mopeds, and more. Figure 5 shows the distribution of classes for annotated objects. All track labels pass through a manual quality assurance review process. Figures 1 and 2 show qualitative examples of our human annotated labels. We divide our annotated tracking data into 65 training, 24 validation, and 24 testing sequences.",
        "Wearealsointerestedinstudyingthetaskofmotionforecasting in which we predict the location of a tracked object some time in the future. Motion forecasts can be critical to safe autonomous vehicle motion planning. While our human-annotated 3D tracks are suitable training and test data for motion forecasting, the motion of many vehicles is relatively uninteresting – in a given frame, most cars are either parked or traveling at nearly constant velocity. Such tracks are hardly a representation of real forecasting challenges. We would like a benchmark with more diverse scenarios e.g. managing an intersection, slowing for a merging vehicle, accelerating after a turn, stopping for a pedestrian on the road, etc. To sample enough of these interesting scenarios, we track objects from 1006 driving hours across both Miami and Pittsburgh and ﬁnd vehicles with interesting behavior in 320 of those hours. In particular, we mine for vehicles that are either (1) at intersections, (2) taking left or right turns, (3) changing to adjacent lanes, or (4) in dense trafﬁc. In total, we collect 324,557 ﬁve second sequences and use them in the forecasting benchmark. Figure 6 shows the geographic distribution of these sequences. Each sequence contains the 2D, bird’s eye view centroid of each tracked object sampled at 10 Hz. The “focal” object in each sequence is always a vehicle, but the other tracked objects can be vehicles, pedestrians, or bicycles. Their trajectories areavailableascontextfor“social”forecastingmodels. The 324,557 sequences are split into 205,942 train, 39,472 validation, and 78,143 test sequences. Each sequence has one challenging trajectory which is the focus of our forecasting benchmark. The train, validation, and test sequences are taken from disjoint parts of our cities, i.e. roughly one eighth and one quarter of each city is set aside as validation and test data, respectively. This dataset is far larger than what could be mined from publicly available autonomous driving datasets. While data of this scale is appealing because it allows us to see rare behaviors and train complex models, it is too large to exhaustively verify the accuracy of the mined trajectories and, thus, there is some noise and error inherent in the data.",
        "Figure 4: Uneven ground scene in Argoverse dataset. Some Argoverse scenes contain uneven ground, which is challenging to remove with simple heuristics (e.g. assuming that ground is planar). Above, we show a LiDAR slice with a slope on the right side and corresponding front-right camera image.",
        "Lane Direction. Determining the vehicle orientation from LiDAR alone is a challenging task even for humans due to LiDAR sparsity and partial views. We observe that vehicle orientation rarely violates lane direction, especially so outside of intersections. Fortunately, such information is availableinourdataset, soweadjustvehicleorientationbasedon lane direction whenever the vehicle is not at the intersection and contains too few LiDAR points.",
        "Figure 5: Distribution of object classes. This plot shows, in log scale, the number of 3D object tracks annotated for each class in the 113 log segments in the Argoverse 3D Tracking dataset.",
        "We have employed relatively simple baselines to track objects in 3D. We believe that our data enables new approaches to map-based and multimodal tracking research.",
        "The coordinate system we used for trajectory forecasting is a top-down, bird’s eye view (BEV). There are three reference coordinate frames of interest to forecasting: (1) The raw trajectory data is stored and evaluated in the city coordinate system (See Section C of the Appendix). (2) For models using lane centerlines as a reference path, we deﬁned a 2D curvilinear coordinate system with axes tangential and perpendicular to the lane centerline. (3) For models without the reference path (without a map), we normalize trajectories such that the observed portion of the trajectory starts at the origin and ends somewhere on the positive x axis. If (xt i) represent coordinates of trajectory Vi at timestep t, then this normalization makes sure that yTobs = 0, where Tobs is last observed timestep of the trajectory (Section 5.1). We ﬁnd this normalization works better than leaving trajectories in absolute map coordinates or absolute orientations. 2. Feature Engineering: We deﬁne additional features to capture social or spatial context. For social context, we use the minimum distance to the objects in front, in back, and the number of neighbors. Such heuristics are meant to capture the social interaction between vehicles. For spatial context, we use the map as a prior by computing features in the lane segment coordinate system. We compute the lane centerline corresponding to each trajectory and then map (xt i) coordinates to the distance along the centerline (at i) and offset from the centerline (ot i). In the subsequent sections, we denote social features and map features for trajeci and mt tory Vi at timestep t by st",
        "racy. We maintain a public leaderboard for 3D object tracking and motion forecasting. The sensor data, map data, annotations, and code which make up Argoverse are available at our website Argoverse.org.",
        "Argoverse represents two large-scale datasets for autonomous driving research. The Argoverse datasets are the ﬁrst such datasets with rich map information such as lane centerlines, ground height, and driveable area. We examinebaselinemethodsfor3Dtrackingwithmap-derivedcontext. We also mine one thousand hours of ﬂeet logs to ﬁnd diverse, real-world object trajectories which constitute our motion forecasting benchmark. We examine baseline forecasting methods and verify that map data can improve accu-",
        "Giancarlo Baldan, and Oscar Beijbom. nuscenes: A mularXiv preprint timodal dataset for autonomous driving. arXiv:1903.11027, 2019.",
        "[7] Sergio Casas, Wenjie Luo, and Raquel Urtasun. Intentnet: Learning to predict intention from raw sensor data. In Aude Billard, Anca Dragan, Jan Peters, and Jun Morimoto, editors, Proceedings of The 2nd Conference on Robot Learning, volume 87 of Proceedings of Machine Learning Research, pages 947–956. PMLR, 29–31 Oct 2018.",
        "[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes In Proc. dataset for semantic urban scene understanding. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.",
        "[13] Martin Ester, Hans peter Kriegel, Jörg Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In KDD Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, pages 226–231. AAAI Press, 1996.",
        "[14] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research, 32(11):1231– 1237, 2013.",
        "[16] Junyao Guo, Unmesh Kurup, and Mohak Shah. Is it safe to drive? an overview of factors, challenges, and datasets for driveability assessment in autonomous driving. IEEE Transactions on Intelligent Transportation Systems, 2019. [17] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajecIn The IEEE tories with generative adversarial networks. Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.",
        "[22] David Held, Jesse Levinson, and Sebastian Thrun. Precision tracking with sparse 3d and dense color 2d data. In ICRA, 2013.",
        "[26] Xinyu Huang, Xinjing Cheng, Qichuan Geng, Binbin Cao, Dingfu Zhou, Peng Wang, Yuanqing Lin, and Ruigang Yang. The apolloscape dataset for autonomous driving. In arXiv:1803.06184, 2018.",
        "[29] R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni, A. Ferreira, M. Yuan, B. Low, A. Jain, P. Ondruska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova, C. Tao, L. Platinsky, W. Jiang, and V. Shet. Lyft level 5 av dataset 2019. https://level5.lyft.com/dataset/, 2019.",
        "[42] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The Oxford Robotcar dataset. The International Journal of Robotics Research, 36(1):3–15, 2017.",
        "Ford campus vision and lidar data set. 30(13):1543–1552, Nov. 2011.",
        "[47] Abhishek Patil, Srikanth Malla, Haiming Gang, and Yi-Ting Chen. The h3d dataset for full-surround 3d multi-object deIn Internatection and tracking in crowded urban scenes. tional Conference on Robotics and Automation, 2019. [48] Luis Patino, Tom Cane, Alain Vallee, and James Ferryman. In Proceedings of the Pets 2016: Dataset and challenge. IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1–8, 2016.",
        "The model of the world that we subscribe to within our map and dataset is a local tangent plane centered at a central point located within each city. This model has a ﬂat earth assumption which is approximately correct at the scale of a city. Thus, we provide map object pose values in city coordinates. City coordinates can be converted to the UTM (Universal Transverse Mercator) coordinate system by simply adding the city’s origin in UTM coordinates to the object’s city coordinate pose. The UTM model divides the earth into 60 ﬂattened, narrow zones, each of width 6 degrees of longitude. Each zone is segmented into 20 latitude bands. In Pittsburgh, our city origin lies at 583710.0070 Easting, 4477259.9999 Northing in UTM Zone 17. In Miami, our city origin lies at 580560.0088 Easting, 2850959.9999 Northing in UTM Zone 17.",
        "We provide ground-truth object pose data in the ego-vehicle frame, meaningasingleSE(3)transformisrequiredtobringpoints into the city frame for alignment with the map:",
        "The dataset’s rich maps are a novelty for autonomous driving datasets and we aim to make it easy to develop computer vision tools that leverage the map. Figure 13 outlines several functions which we hope will make it easier for researchers to access the map. Our API is provided in Python. For example, our API can provide rasterized bird’s eye view (BEV) images of the map around the egovehicle, extending up to 100 m in all directions. It can also provide a dense 1 meter resolution grid of the ground surface, especially useful for ground classiﬁcation when globally planar ground surface assumptions are violated (see Figure 14).",
        "These dense, pixel-level map renderings, similar to visualizations of instance-level or semantic segmentation [10], have recently been demonstrated to improve 3D perception and are relatively easy to use as an input to a convolutional network [60, 7]. We provide our vector map data in a modiﬁed OpenStreetMap (OSM) format, i.e. consisting of “Nodes” (waypoints) composed into “Ways” (polylines) so that the community can take advantage of open source mapping tools built to handle OSM formats. The data we provide is richer than existing OSM data which does not contain per-lane or elevation information.",
        "Figure 10: (a) Lane centerlines and hallucinated area are shown in red and yellow, respectively. We provide lane centerlines in our dataset because simple road centerline representations cannot handle the highly complicated nature of real world mapping, as shown above with divided roads. (b) We show lane segments within intersections in pink, and all other lane segments in yellow. Black shows lane centerlines. (c) Example of a speciﬁc lane centerline’s successors and predecessors. Red shows the predecessor, green shows the successor, and black indicates the centerline segment of interest.",
        "data mining and baselines.",
        "C.1. Motion Forecasting Data Mining Details",
        "Here we describe our approach for mining data for trajectory forecasting. The scenarios that are challenging for a forecasting task are rare, but with a vector map they are easy to identify. We focus on some speciﬁc behavioral scenarios from over 1006 driving hours. For every 5 second sequence, we assign an interesting",
        "well, especially with map information available (e.g. driveable area, ground height, and lane information). More recent attempts in 3D tracking and detection include the baseline introduced in H3D [47], with VoxelNet [62] detection and an Unscented Kalman Filter [28, 27] for tracking. The NuScenes dataset [6] uses PointPillars [31] for a 3D bounding box detection baseline. ApolloCar3D [55] implements 2D to 3D car pose estimation baselines based on 3D-RCNN[30] and DeepMANTA [8]. However, we do not compare our baseline tracker against these methods.",
        "Our tracker tracks the position and velocity of surrounding vehicles from LiDAR data. The tracking pipeline has the following stages:",
        "Others have proposed compensating for point cloud undersegmentation and oversegmentation scenarios by conditioning on the data association and then jointly track and perform probabilistic segmentation [21]. We can avoid many such segmentation failures",
        "Figure 17: Above: Sample LiDAR sweeps in the egovehicle frame, with marked x and y axes, with x ∈ [−200,200] and y ∈ [−160,160] for all plots. The Argoverse LiDAR has up to twice the range of the sensors used to collect the KITTI or nuScenes datasets, allowing us to observe more objects in each scene."
    ]
}