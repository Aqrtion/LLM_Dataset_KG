{
    "title_author_abstract_introduction": "2D Human Pose Estimation: New Benchmark and State of the Art Analysis\nMykhaylo Andriluka1,3, Leonid Pishchulin1, Peter Gehler2, and Bernt Schiele1\n1Max Planck Institute for Informatics, Germany 2Max Planck Institute for Intelligent Systems, Germany 3Stanford University, USA\nAbstract\nHuman pose estimation has made signiﬁcant progress during the last years. However current datasets are limited in their coverage of the overall pose estimation challenges. Still these serve as the common sources to evaluate, train and compare different models on. In this paper we introduce a novel benchmark “MPII Human Pose”1 that makes a signiﬁcant advance in terms of diversity and difﬁculty, a contribution that we feel is required for future developments in human body models. This comprehensive dataset was collected using an established taxonomy of over 800 human activities [1]. The collected images cover a wider variety of human activities than previous datasets including various recreational, occupational and householding activities, and capture people from a wider range of viewpoints. We provide a rich set of labels including positions of body joints, full 3D torso and head orientation, occlusion labels for joints and body parts, and activity labels. For each image we provide adjacent video frames to facilitate the use of motion information. Given these rich annotations we perform a detailed analysis of leading human pose estimation approaches and gaining insights for the success and failures of these methods.\n1. Introduction\nRecent pose estimation methods employ complex appearance models [2, 9, 15] and rely on learning algorithms to estimate model parameters from the training data. The performance of these approaches crucially depends on the availability of the annotated training images that are representative for the appearance of people clothing, strong articulation, partial (self-)occlusions and truncation at image borders. Although there exists training sets for special scenarios such as sport scenes [12, 13] and upright people [17, 2], these benchmarks are still limited in their scope and variability of represented activities. Sport scene datasets\n1Available at human-pose.mpi-inf.mpg.de.\ntypically include highly articulated poses, but are limited with respect to variability of appearance since people are typically wearing tight sports outﬁts. In turn, datasets such as “FashionPose” [2] and “Armlets” [9] aim to collect images of people wearing a variety of different clothing types, and include occlusions and truncation but are dominated by people in simple upright standing poses.\nTo the best of our knowledge no attempt has been made to establish a more representative benchmark aiming to cover a wide pallet of challenges for human pose estimation. We believe that this hinders further development on this topic and propose a new benchmark “MPII Human Pose”. Our benchmark signiﬁcantly advances state of the art in terms of appearance variability and complexity, and includes more than 40,000 images of people. We used YouTube as a data source and collected images and image sequences using queries based on the descriptions of more than 800 activities. This results in a diverse set of images covering not only different activities, but indoor and outdoor scenes, a variety of imaging conditions, as well as both amateur and professional recordings (c.f. Fig. 1). This allows us to study existing body pose estimation techniques and identify their individual failure modes.\nRelated work The commonly used publicly available datasets for evaluation of 2D human pose estimation are summarized in Tab. 1 according to the year of the corresponding publication. Both full body and upper body datasets are included.\nExisting benchmarks cover aspects of the human pose estimation task such as sport scenes [12, 21], frontal-facing people [8, 3, 17], people interacting with objects [23], pose estimation in group photos [5] and pose estimation of people performing synchronized activities [4].\nEarlier datasets such as “Parse” [16] and “Buffy” [8] are still commonly found in evaluations [22, 15]. However the small training sets included in these datasets make them unsuitablefortrainingmodelswithcomplexappearancerepresentations and multiple components [13, 17, 2], which have been shown to perform best.\nbicycling bicycling, BMX\nconditioning exercise ski machine\ndancing ballroom\nﬁshing and hunting ﬁsh. from river bank\nhome activities tanning hides\nhome repair carpentry\ninactivity quiet sitting quietly\nlawn and garden driving tractor\nmiscellaneous standing\nmusic playing violin, sitting\noccupation horse grooming\nreligious activities sit., playing instrum.\nrunning running, stairs, up\nself care taking medication\nsports soccer\ntransportation riding in a bus\nwalking bird watching\nwater activities snorkeling\nvolunteer activities playing with children Figure 1. Randomly chosen images from each of 20 activity categories of the proposed “MPII Human Pose” dataset. Image captionsindicateactivitycategory(1strow)andactivity(2ndrow). To view the full dataset visit human-pose.mpi-inf.mpg.de.\nwinter activities skating, ice dancing\nSome efforts have been made to collect larger sets of images. For example [13] extends the LSP dataset to 10,000 images of people performing gymnastics, athletics [2] proposes a large “FashionPose” dataset and parkour. collected from fashion blogs. This dataset aims to cover a wide variety in people clothing. The LSP and FashionPose datasets are complementary and focus on two different challenges for human pose estimation: pose variability and variability of people appearance. However since they are collected with a speciﬁc focus in mind, these datasets do not cover real-life challenges such as truncation, occlusions by scene objects and variability of imaging conditions.\nThe works of [6] and [9] propose a challenging dataset building on the PASCAL VOC image collection. Results reported in [9] indicate that the best performing approaches for pose estimation of people in the presence of occlusion and complex appearance are under-performing on sportoriented datasets such as LSP [12] and vice versa. There are qualitative differences between methods that work well for LSP and “Armlets” datasets. On LSP the best performing methods are typically based on ﬂexible part-based models that are well suited for capturing pose variability. In contrary on the “Armlets” dataset the best performing approach [9] uses a set of rigid detectors for groups of parts, that are more robust to the variability in appearance.\nOur dataset is complementary to the J-HMDB dataset [11] and provides more images and a wider coverage of ac-\nDataset\n#training\nimg. type\nFull body pose datasets Parse [16] LSP [12] PASCAL Person Layout [6] Sport [21] UIUC people [21] LSP extended [13] FashionPose [2] J-HMDB [11]\nUpper body pose datasets Buffy Stickmen [8] ETHZ PASCAL Stickmen [3] Human Obj. Int. (HOI) [23] We Are Family [5] Video Pose 2 [18] FLIC [17] Sync. Activities [4] Armlets [9]\ndiverse sports (8 types) everyday sports sports (2 types) sports (3 types) fashion blogs diverse (21 act.)\n472 - 180 350 imgs. 766 6,543 - 9,593",
    "data_related_paragraphs": [
        "Table 1. Overview of the publicly available datasets for articulated human pose estimation. For each dataset we report the number of annotatedpeopleintrainingandtestsetsandthetypeofimagesthe set include. The numbers indicate the number of unique annotated people without mirroring.",
        "tivities (491 in our dataset vs. 21 in J-HMDB), whereas J-HMDB provides densely annotated image sequences and larger number of videos for each activity. Our dataset also addresses a different set of challenges compared to datasets such as “HumanEva” [19] and “Human3.6M” [10] that includes images and 3D poses of people but are captured in the controlled indoor environments, whereas our dataset includes real-world images but provides 2D poses only.",
        "2. Dataset",
        "In this paper we introduce a large dataset of images that covers a wide variety of human poses and clothing types and includes people interacting with various objects and environments. The key rationale behind our data collection strategy is that we want to represent both common and rare human poses that might be missed when simply collecting moreimageswithoutaimingforgoodcoverage. Tothisend, we use a two-level hierarchy of human activities proposed in [1] to guide the collection process. This hierarchy was developed for the assignment of standardized energy levels during physical activity surveys and includes 823 activities in total of 21 different activity categories. The activities at the ﬁrst level of the hierarchy correspond to thematically related groups of activities such as “Home Activities”, “Lawn and Garden” or “Sports”. The activities at the second level then correspond to individual activities such as “Washing windows”, “Picking fruit” or “Rock climbing”. Note that using the activity hierarchy for collection has an additional advantage that all images have an associated activity label. As a result one can assess and analyze any performance measure also on subsets of activities or activity categories.",
        "Figure 2. Visualization of upper body pose variability. From left to right we show, (a) color coding of the body parts (b) annotations of the “Armlets” dataset [9], and (c) annotations of this dataset.",
        "Due to the coverage of the hierarchy the images in this dataset are representative of the diversity of human poses, overcoming one of the main limitations of previous collections. In Fig. 2 we visualize this diversity by comparing upper body annotations of the “Armlets” dataset Fig. 2(b) and the proposed dataset (c). Note that although “Armlets” contain about 13,500 images, the annotations resemble a person with arms down along the torso (distribution of red, cyan, green, and blue sticks).",
        "We collect images from YouTube using queries based on the activity descriptions. Using YouTube allows us to access a rich collection of videos originating from various sources, including amateur and professional recordings and capturing a variety of public events and performances. In Fig. 2 (c) we show the distribution of upper body poses on our dataset. Note the variability in the location of hands and the absence of distinctive peaks for the upper and lower arms that are present in the case of the “Armlets” dataset.",
        "Data collection. As a ﬁrst step of the data collection we manually query YouTube using descriptions of activities from [1]. We select up to 10 videos for each activity ﬁltering out videos of low quality and those that do not include people. This resulted in 3,913 videos spanning 491 different activities. Note that we merged a number of the original 823 activities due to high similarity between them, such as cycling at different speeds. In the second step we manually pick several frames with people from each video. As the focus of our benchmark is pose estimation we do not include video frames in which people are severely truncated or in which pose is not recognizable due to poor image quality or small scale. We aim to select frames that either depict different people present in the video or the same person in a substantially different pose. In addition we restrict the selected frames to be at least 5 seconds apart. This step resulted to a total of 24,920 extracted frames from all collected videos. Next, we annotate all people present in the collected images, but ignore dense people crowds in which signiﬁcant number of people are almost fully occluded. Following this procedure we collect images of 40,522 people. We allocate roughly tree quaters of the collected images for training and use the rest for testing. Images from the same video are either all in the training or all in the test set. This",
        "results in a training/test set split of 28,821 to 11,701. Dataannotation. Weproviderichannotationsforthecollected images, an example can be seen in Fig. 3. Annotated are the body joints, 3D viewpoint of the head and torso, and position of the eyes and nose. Additionally for all body joints and parts visibility is annotated. Following [13, 9] we annotate joints in a “person centric” way, meaning that the left/right joints refer to the left/right limbs of the person. At test time this requires pose estimation with both a correct localization of the limbs of a person along with the correct match to the left/right limb. The annotations are performed by in-house workers and via Amazon Mechanical Turk (AMT). In our annotation process we build and extend the annotation tools described in [14]. Similarly to [13, 20] we found that effective use of AMT requires careful selection of qualiﬁed workforce. We pre-select AMT workers based on a qualiﬁcation task, and then maintain data quality by manually inspecting the annotated data. Experimental protocol and evaluation metrics. We deﬁne the baseline evaluation protocol on our dataset following the current practices in the literature [13, 9, 15]. We assume that at test time the rough location and scale of a person are known, and we exclude the cases with multiple people in close proximity to each other from the evaluation. We feel that these simpliﬁcations are necessary for the rapid adoption of the dataset as the majority of the current approaches does not address multiple people pose estimation and does not search over people positions and scales.",
        "Table 2. Pose estimation results (PCPm) on the proposed dataset without and with using rough body location (“+ loc” in the table).",
        "In our analysis we consider two full body and two upper body pose estimation approaches. The full body approaches are the version 1.3 of the ﬂexible mixture of parts (FMP) approach of Yang and Ramanan [22] and the pictorial structures (PS) approach of Pishchulin et al. [15]. The upper body pose estimation approaches are the multimodal decomposable models (MODEC) approach of Sapp et al. [17] and the Armlets approach of Gkioxari et al. [9]. In case of FMP and MODEC we use publicly available code and pre-trained models. The PS model used here corresponds to our best model published in [15]. In case of the Armlets model, the code and pre-trained model provided by the authors correspond to the version from [9] that includes the HOG features only. The performance of our version of Armlets on the “Armlets” dataset is 3.3 PCP lower than the version based on combination of all features.2",
        "The PS approach achieves the best results to date on LSP that is focused on the strongly articulated people [15]. The Armlets approach is best on the “Armlets” dataset [9] that includes large number of truncation and occlusions, and MODEC is the best on the recent upper body pose estimation dataset “FLIC” [17]. We include the FMP approach thatiswidelyusedintheliteratureandtypicallyshowscompetitive performance for a variety of settings. In the following experiments we use “PCPm” as our working metric, while also providing results for “PCP” and “PCKh” in the supplementary material. While we observe little performance differences when using each metric, all conclusions obtained during “PCPm”-based evaluation are valid for “PCP” and “PCKh”-based evaluations as well. Overall performance evaluation. We begin our analysis by reporting the overall pose estimation performance of each approach and summarize the results in Tab. 2. We include both upper- and full body results to enable comparison across different models. The PS approach achieves the best result of 42.3% PCPm, followed by the FMP approach with 38.3% PCPm. On the upper body evaluation, PS performs best with 39.1%, while both MODEC (27.8% PCPm) and Armlets (26.4% PCPm) perform signiﬁcantly worse.",
        "The interesting outcome of this comparison is that both upper body approaches MODEC and Armlets are outperformed by the full body approaches PS and FMP evaluated on upper body only. This is interesting because signiﬁcant portion of the dataset (15 %) includes people that have only upper body visible. It appears that the PS and FMP approaches are sufﬁciently robust to missing parts to produce reliable estimates even in the case of lower body occlusion. Lower part of Tab. 2 shows the results when using provided rough location of person during test time inference. We observe, that while the performance increases for all methods, upper body approaches proﬁt at most, as they heavily depend on correct torso localization. For the sake of fair comparison among the methods, we do not use the rough location in the following experiments. Another interesting outcome is that the achieved performance is substantially lower than current best results on the sports-centric LSP dataset, but comparable to results on the “Armlets” dataset (42.2 PCP on our benchmark (see supplemental) vs. 69.2 on LSP [15] vs. 36.2 PCP on “Armlets”). This suggests that sport activities are not necessary the most difﬁ- cult cases for pose estimation; challenges such as appearance variability, occlusion and truncation apparently deserve more attention in the future.",
        "We deﬁne the following complexity measures. Pose complexity is measured as the deviation from the mean pose on the entire dataset. We deﬁne mpose(L) = (cid:81) (i,j)∈E pps(li|lj), where E is a set of body joints and pps(li|lj) is a Gaussian distribution measuring relative position of the two adjacent body parts using the transformed state-space representation introduced in [7]. Note that mpose(L) corresponds to the likelihood of the pose under the tree structured pictorial structures model [7]. The amount of foreshortening is measured by mf(L) = (cid:80)N i=1 |d(li)−mi|/mi, where d(li) is the length of the body part i, and mi is the mean length over the entire dataset. The viewpoint complexity is measured by the deviation from the frontal viewpoint: mv(L) = (cid:80)3 i=1 αi. Finally, the amount of occlusion and truncation correspond to the number of occluded and truncated body parts: mocc = (cid:80)N i=1 ρi, and mt = (cid:80)N",
        "Body pose performance. As stated above the complexity of the pose is a dominating factor for the performance of all considered approaches. For example the PS approach achieves 72.8% PCPm on the 1000 images with lowest pose complexity, compared to 42.3% for the entire dataset. The same is true for the FMP model, 63.4% PCPm on 1000 least pose complex images vs. 38.3% overall.",
        "As mentioned above, truncation showed the least inﬂuence overall among the discussed factors. There are at least two reasons. First, the number of images with truncation is limited in our dataset (about 30% of the test data contain truncated people). Second, and more importantly, for truncation one cannot annotate positions of body parts outside oftheimage. Thereforethestandardprocedureistoexclude truncated body parts from the evaluation. In that sense approaches that wrongly estimate the position of a truncated body part are not punished for that. This limitation could be addressed by requiring that models have to also report which parts of the body are truncated.",
        "1234567891011121314150102030405060Viewpoint clustersPCPm, %  Pishchulin et al.Pishchulin et al. OCYang&RamananYang&Ramanan OCSapp&TaskarSapp&Taskar OCGkioxari et al.Gkioxari et al. OC20253035404550556065activity categoriesPCPm, %  sportsoccupationconditioning exercisedancingwater activitieshome repairhome activitieslawn and gardenfishing and huntingmusic playingwinter activitieswalkingmiscellaneousbicyclingrunninginactivity quiet/lighttransportationself carereligious activitiesvolunteer activitiesPishchulin et al.Yang&RamananSapp&TaskarGkioxari et al.1234567891011121314150204060Viewpoint clustersPCPm, %203040506070activity categoriesPCPm, %  sportsoccupationconditioning exercisedancingwater activitieshome repairhome activitieslawn and gardenfishing and huntingmusic playingwinter activitieswalkingmiscellaneousbicyclingrunninginactivity quiet/lighttransportationself carereligious activitiesvolunteer activitiesPishchulin et al.Pishchulin et al. retrainedYang&RamananYang&Ramanan retrained\fRetrained models. To showcase the usefulness of the benchmark as an analysis tool we retrain the PS and FMP models on the training set from our benchmark. To speed up training we consider a subset of 4000 images, which is 4 times as many images as in the LSP and 40 times as many as in the PARSE datasets used by the publicly available PS and FMP models, respectively. The results are shown in Tab. 3. FMP signiﬁcantly beneﬁts from retraining (44.7 PCPm for retrained vs. 38.3 for original). PS achieves slightly better result, although overall improvement due to retraining is smaller (46.1 PCPm for retrained vs. 42.3 PCPm the original).",
        "Although performances for FMP and PS are close overall, we observe interesting differences when examining performance at the level of individual activities and viewpoints (thereby exploiting the rich annotations of our benchmark). Results are shown in Fig. 8. We observe that our publicly available PS model is winning by a large margin on the highly articulated categories, such as “Dancing” and “Running”. Retraining the model boosts performance on activities with less articulation but more complex appearance (e.g. “Home Activities”, “Lawn and Garden”, “Bicycling”, and “Occupation”). Our results show that training on the larger amount of more variable data signiﬁcantly improved robustness of FMP to viewpoint changes. Performance of FMP improves on the difﬁcult viewpoints by a large margin (e.g. for viewpoint cluster 10 improvement is from 17 to 31% PCPm). Retraining improves the performance of PS model on difﬁcult viewpoints as well, although not as dramatically as for FMP, likely because PS already models in-plane rotations explicitly.",
        "In this work we advance the state of the art in human pose estimation by establishing new qualitatively higher standards for evaluation and analysis of pose estimation methods and demonstrate the most promising research directions for the next years. To that end we propose a novel “MPII Human Pose” benchmark that we collected by leveraging a taxonomy of activities established in the literature. Compared to current datasets our benchmark covers significantly wider range of human poses spanning from householding to recreational activities and sports. Rich labeling of the collected data and a set of developed evaluation tools enable comprehesive analysis which we perform to demonstrate the strengths and weaknesses of the current methods. Our ﬁndings indicate that current methods are signiﬁ- cantly challenged by cases outside their comfort zone, such as large torso rotation and loose clothing. From all other factors, pose complexity has the most profound effect on the pose estimation performance. Current methods perform best on activities with simple tight clothing (e.g. in sport scenes), and are challenged by images with complex cloth-",
        "We will make the data, rich annotations for training images and evaluation tools publicly available in order to enable detailed analysis of future pose estimation methods. To prevent accidentally tuning on the test set, the annotations for the test images will be withheld and made accessible through an online evaluation tool. In the future we plan to extend our benchmark to joint pose estimation of multiple people and pose estimation in image sequences. Acknowledgements. The authors are thankful to Steve Hillyer and numerous anonymous workers on Amazon Mechanical Turk for the help with preparation of the dataset.",
        "[10] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments. PAMI’13.",
        "[19] L. Sigal, A. Balan, and M. J. Black. Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. International Journal of Computer Vision, 87, 2010."
    ]
}