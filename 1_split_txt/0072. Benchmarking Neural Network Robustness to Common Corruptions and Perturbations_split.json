{
    "title_author_abstract_introduction": "Published as a conference paper at ICLR 2019\nBENCHMARKING NEURAL NETWORK ROBUSTNESS TO COMMON CORRUPTIONS AND PERTURBATIONS\nDan Hendrycks University of California, Berkeley hendrycks@berkeley.edu\nThomas Dietterich Oregon State University tgd@oregonstate.edu\nABSTRACT\nIn this paper we establish rigorous benchmarks for image classiﬁer robustness. Our ﬁrst benchmark, IMAGENET-C, standardizes and expands the corruption robustness topic, while showing which classiﬁers are preferable in safety-critical applications. Then we propose a new dataset called IMAGENET-P which enables researchers to benchmark a classiﬁer’s robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We ﬁnd that there are negligible changes in relative corruption robustness from AlexNet classiﬁers to ResNet classiﬁers. Afterward we discover ways to enhance corruption and perturbation robustness. We even ﬁnd that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.\nINTRODUCTION\nThe human vision system is robust in ways that existing computer vision systems are not (Recht et al., 2018; Azulay & Weiss, 2018). Unlike current deep learning classiﬁers (Krizhevsky et al., 2012; He et al., 2015; Xie et al., 2016), the human vision system is not fooled by small changes in query images. Humans are also not confused by many forms of corruption such as snow, blur, pixelation, and novel combinations of these. Humans can even deal with abstract changes in structure and style. Achieving these kinds of robustness is an important goal for computer vision and machine learning. It is also essential for creating deep learning systems that can be deployed in safety-critical applications.\nMost work on robustness in deep learning methods for vision has focused on the important challenges of robustness to adversarial examples (Szegedy et al., 2014; Carlini & Wagner, 2017; 2016), unknown unknowns (Hendrycks et al., 2019; Hendrycks & Gimpel, 2017b; Liu et al., 2018), and model or data poisoning (Steinhardt et al., 2017; Hendrycks et al., 2018). In contrast, we develop and validate datasets for two other forms of robustness. Speciﬁcally, we introduce the IMAGETNET-C dataset for input corruption robustness and the IMAGENET-P dataset for input perturbation robustness.\nTo create IMAGENET-C, we introduce a set of 75 common visual corruptions and apply them to the ImageNet object recognition challenge (Deng et al., 2009). We hope that this will serve as a general dataset for benchmarking robustness to image corruptions and prevent methodological problems such as moving goal posts and result cherry picking. We evaluate the performance of current deep learning systems and show that there is wide room for improvement on IMAGENET-C. We also introduce a total of three methods and architectures that improve corruption robustness without losing accuracy.\nTo create IMAGENET-P, we introduce a set of perturbed or subtly differing ImageNet images. Using metrics we propose, we measure the stability of the network’s predictions on these perturbed images. Although these perturbations are not chosen by an adversary, currently existing networks exhibit surprising instability on common perturbations. Then we then demonstrate that approaches which enhance corruption robustness can also improve perturbation robustness. For example, some recent architectures can greatly improve both types of robustness. More, we show that the Adversarial Logit Pairing (cid:96)∞ adversarial example defense can yield substantial robustness gains on diverse and common perturbations. By deﬁning and benchmarking perturbation and corruption robustness, we facilitate research that can be overcome by future networks which do not rely on spurious correlations or cues inessential to the object’s class.\nPublished as a conference paper at ICLR 2019",
    "data_related_paragraphs": [
        "Adversarial Examples. An adversarial image is a clean image perturbed by a small distortion carefully crafted to confuse a classiﬁer. These deceptive distortions can occasionally fool black-box classiﬁers (Kurakin et al., 2017). Algorithms have been developed that search for the smallest additive distortions in RGB space that are sufﬁcient to confuse a classiﬁer (Carlini et al., 2017). Thus adversarial distortions serve as type of worst-case analysis for network robustness. Its popularity has often led “adversarial robustness” to become interchangeable with “robustness” in the literature (Bastani et al., 2016; Rauber et al., 2017). In the literature, new defenses (Lu et al., 2017; Papernot et al., 2017; Metzen et al., 2017; Hendrycks & Gimpel, 2017a) often quickly succumb to new attacks (Evtimov et al., 2017; Carlini & Wagner, 2017; 2016), with some exceptions for perturbations on small images (Schott et al., 2018; Madry et al., 2018). For some simple datasets, the existence of any classiﬁcation error ensures the existence of adversarial perturbations of size O(d−1/2), d the input dimensionality (Gilmer et al., 2018b). For some simple models, adversarial robustness requires an increase in the training set size that is polynomial in d (Schmidt et al., 2018). Gilmer et al. (2018a) suggest modifying the problem of adversarial robustness itself for increased real-world applicability.",
        "Robustness in Speech. Speech recognition research emphasizes robustness to common corruptions rather than worst-case, adversarial corruptions (Li et al., 2014; Mitra et al., 2017). Common acoustic corruptions (e.g., street noise, background chatter, wind) receive greater focus than adversarial audio, because common corruptions are ever-present and unsolved. There are several popular datasets containing noisy test audio (Hirsch & Pearce, 2000; Hirsch, 2007). Robustness in noisy environments requires robust architectures, and some research ﬁnds convolutional networks more robust than fully connected networks (Abdel-Hamid et al., 2013). Additional robustness has been achieved through pre-processing techniques such as standardizing the statistics of the input (Liu et al., 1993; Torre et al., 2005; Harvilla & Stern, 2012; Kim & Stern, 2016).",
        "ConvNet Fragility Studies. Several studies demonstrate the fragility of convolutional networks on simple corruptions. For example, Hosseini et al. (2017) apply impulse noise to break Google’s Cloud Vision API. Using Gaussian noise and blur, Dodge & Karam (2017b) demonstrate the superior robustness of human vision to convolutional networks, even after networks are ﬁne-tuned on Gaussian noise or blur. Geirhos et al. (2017) compare networks to humans on noisy and elastically deformed images. They ﬁnd that ﬁne-tuning on speciﬁc corruptions does not generalize and that classiﬁcation error patterns underlying network and human predictions are not similar. Temel et al. (2017; 2018); Temel & AlRegib (2018) propose different corrupted datasets for object and trafﬁc sign recognition.",
        "Figure 1: Our IMAGENET-C dataset consists of 15 types of algorithmically generated corruptions from noise, blur, weather, and digital categories. Each type of corruption has ﬁve levels of severity, resulting in 75 distinct corruptions. See different severity levels in Appendix B.",
        "4.1 THE DATA OF IMAGENET-C AND IMAGENET-P",
        "IMAGENET-C Design. The IMAGENET-C benchmark consists of 15 diverse corruption types applied to validation images of ImageNet. The corruptions are drawn from four main categories— noise, blur, weather, and digital—as shown in Figure 1. Research that improves performance on this benchmark should indicate general robustness gains, as the corruptions are diverse and numerous. Each corruption type has ﬁve levels of severity since corruptions can manifest themselves at varying intensities. Appendix A gives an example of the ﬁve different severity levels for impulse noise. Real-world corruptions also have variation even at a ﬁxed intensity. To simulate these, we introduce variation for each corruption when possible. For example, each fog cloud is unique to each image. These algorithmically generated corruptions are applied to the ImageNet (Deng et al., 2009) validation images to produce our corruption robustness dataset IMAGENET-C. The dataset can be downloaded or re-created by visiting https://github.com/hendrycks/robustness. IMAGENET-C images are saved as lightly compressed JPEGs; this implies an image corrupted by Gaussian noise is also slightly corrupted by JPEG compression. Our benchmark tests networks with IMAGENET-C images, but networks should not be trained on these images. Networks should be trained on datasets such as ImageNet and not be trained on IMAGENET-C corruptions. To enable further experimentation, we designed an extra corruption type for each corruption category (Appendix B), and we provide CIFAR-10-C, TINY IMAGENET-C, IMAGENET 64 × 64-C, and Inception-sized editions. Overall, the IMAGENET-C dataset consists of 75 corruptions, all applied to ImageNet validation images for testing a pre-existing network.",
        "IMAGENET-P Design. The second benchmark that we propose tests the classiﬁer’s perturbation robustness. Models lacking in perturbation robustness produce erratic predictions which undermines user trust. When perturbations have a high propensity to change the model’s response, then perturbations could also misdirect or destabilize iterative image optimization procedures appearing in style transfer (Gatys et al., 2016), decision explanations (Fong & Vedaldi, 2017), feature visualization (Olah et al., 2017), and so on. Like IMAGENET-C, IMAGENETP consists of noise, blur, weather, and digital distortions. Also as before, the dataset has validation perturbations; has difﬁculty levels; has CIFAR-10, Tiny ImageNet, ImageNet 64 × 64, standard, and Inception-sized editions; and has been designed for benchmarking not IMAGENET-P departs from IMAGENET-C by having perturbation sequences generated from each ImageNet validation image; examples are in Figure 2. Each sequence contains more than 30 frames, so we counteract an increase in dataset size and evaluation time by using only 10 common perturbations.",
        "IMAGENET-C Metrics. Common corruptions such as Gaussian noise can be benign or destructive depending on their severity. In order to comprehensively evaluate a classiﬁer’s robustness to a given type of corruption, we score the classiﬁer’s performance across ﬁve corruption severity levels and aggregate these scores. The ﬁrst evaluation step is to take a trained classiﬁer f, which has not been trained on IMAGENET-C, and compute the clean dataset top-1 error rate. Denote this error rate Ef clean. The second step is to test the classiﬁer on each corruption type c at each level of severity s (1 ≤ s ≤ 5). This top-1 error is written Ef s,c. Before we aggregate the classiﬁer’s performance across severities and corruption types, we will make error rates more comparable since different corruptions pose different levels of difﬁculty. For example, fog corruptions often obscure an object’s class more than brightness corruptions. We adjust for the varying difﬁculties by dividing by AlexNet’s errors,",
        "Wenowintroduceamorenuancedcorruptionrobustnessmeasure. Consideraclassiﬁerthatwithstands most corruptions, so that the gap between the mCE and the clean data error is minuscule. Contrast this with a classiﬁer with a low clean error rate which has its error rate spike in the presence of corruptions; this corresponds to a large gap between the mCE and clean data error. It is possible that the former classiﬁer has a larger mCE than the latter, despite the former degrading more gracefully in the presence of corruptions. The amount that the classiﬁer declines on corrupted inputs is given by (cid:1). Averaging these the formula Relative CEf clean 15 Relative Corruption Errors results in the Relative mCE. This measures the relative robustness or the performance degradation when encountering corruptions. IMAGENET-P Metrics. AstraightforwardapproachtoestimateEε∼E[P(x,y)∼D(f(ε(x)) (cid:54)= f(x))] falls into place when using IMAGENET-P perturbation sequences. Let us denote m perturbation sequences with S = (cid:8)(cid:0)x(i) i=1 where each sequence is made with perturbation p. The “Flip Probability” of network f : X → {1,2,...,1000} on perturbation sequences S is",
        "counts of the scenery before them. Hence, we propose the following protocol. The image recognition network should be trained on the ImageNet training set and on whatever other training sets the investigator wishes to include. Researchers should clearly state whether they trained on these corruptions or perturbations; however, this training strategy is discouraged (see Section 2). We allow training with other distortions (e.g., uniform noise) and standard data augmentation (i.e., cropping, mirroring), even though cropping overlaps with translations. Then the resulting trained model should be evaluated on IMAGENET-C or IMAGENET-P using the above metrics. Optionally, researchers can test with the separate set of validation corruptions and perturbations we provide for IMAGENET-C and IMAGENET-P.",
        "Histogram Equalization. Histogram equalization successfully standardizes speech data for robust speech recognition (Torre et al., 2005; Harvilla & Stern, 2012). For images, we ﬁnd that preprocessing with Contrast Limited Adaptive Histogram Equalization (Pizer et al., 1987) is quite effective. Unlike our image denoising attempt (Appendix F), CLAHE reduces the effect of some corruptions while not worsening performance on most others, thereby improving the mCE. We demonstrate CLAHE’s net improvement by taking a pre-trained ResNet-50 and ﬁne-tuning the whole model for ﬁve epochs on images processed with CLAHE. The ResNet-50 has a 23.87% error rate, but ResNet-50 with CLAHE has an error rate of 23.55%. On nearly all corruptions, CLAHE slightly decreases the Corruption Error. The ResNet-50 without CLAHE preprocessing has an mCE of 76.7%, while with CLAHE the ResNet-50’s mCE decreases to 74.5%.",
        "Feature Aggregating and Larger Networks. Some recent models enhance the ResNet architecture by increasing what is called feature aggregation. Of these, DenseNets and ResNeXts (Xie et al., 2016) are most prominent. Each purports to have stronger representations than ResNets, and the evidence is largely a hard-won ImageNet error-rate downtick. Interestingly, the IMAGENET-C mCE clearly indicates that DenseNets and ResNeXts have superior representations. Accordingly, a switch from a ResNet-50 (23.9% top-1 error) to a DenseNet-121 (25.6% error) decreases the mCE from 76.7% to 73.4% (and the relative mCE from 105.0% to 92.8%). More starkly, switching from a ResNet-50 to a ResNeXt-50 (22.9% top-1) drops the mCE from 76.7% to 68.2% (relative mCE decreases from 105.0% to 88.6%). Corruption robustness results are summarized in Figure 5. This shows that corruption robustness may be a better way to measure future progress in representation learning than the clean dataset top-1 error rate.",
        "Stylized ImageNet. Geirhos et al. (2019) propose a novel data augmentation scheme where ImageNet images are stylized with style transfer. The intent is that classiﬁers trained on stylized images will rely less on textural cues for classiﬁcation. When a ResNet-50 is trained on typical ImageNet images and stylized ImageNet images, the resulting model has an mCE of 69.3%, down from 76.7%.",
        "In this paper, we introduced what are to our knowledge the ﬁrst comprehensive benchmarks for corruption and perturbation robustness. This was made possible by introducing two new datasets, IMAGENET-C and IMAGENET-P. The ﬁrst of which showed that many years of architectural advancements corresponded to minuscule changes in relative corruption robustness. Therefore benchmarking and improving robustness deserves attention, especially as top-1 clean ImageNet accuracy nears its ceiling. We also saw that classiﬁers exhibit unexpected instability on simple perturbations. Thereafter we found that methods such as histogram equalization, multiscale architectures, and larger featureaggregating models improve corruption robustness. These larger models also improve perturbation robustness. However, we found that even greater perturbation robustness can come from an adversarial defense designed for adversarial (cid:96)∞ perturbations, indicating a surprising interaction between adversarial and common perturbation robustness. In this work, we found several methods to increase robustness, introduced novel experiments and metrics, and created new datasets for the rigorous study of model robustness, a pressing necessity as models are unleashed into safety-critical real-world settings.",
        "hierarchical image database. CVPR, 2009.",
        "Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train",
        "sarially robust generalization requires more data. arXiv preprint, 2018.",
        "Jacob Steinhardt, Pang Wei Koh, and Percy Liang. Certiﬁed defenses for data poisoning attacks.",
        "Anothergoalformachinelearningistolearnthefundamentalstructureofcategories. Broadcategories, such as “bird,” have many subtypes, such as “cardinal” or “bluejay.” Humans can observe previously unseen bird species yet still know that they are birds. A test of learned fundamental structure beyond superﬁcial features is subtype robustness. In subtype robustness we test generalization to unseen subtypes which share share essential characteristics of a broader type. We repurpose the ImageNet-22K dataset for a closer investigation into subtype robustness.",
        "SubtypeRobustness. Anaturalimagedataset with a hierarchical taxonomy and numerous types and subtypes is ImageNet-22K, an ImageNet-1K superset. In this subtype robustness experiment, we manually select 25 broad types from ImageNet-22K, listed in the next paragraph. Each broad type has many subtypes. We call a subtype “seen” if and only if it is in ImageNet-1K and a subtype of one of the 25 broad types. The subtype is “unseen” if and only if it is a subtype of the 25 broad types and is from ImageNet-22K but not ImageNet-1K. In this experiment, the correct classiﬁcation decision for an image of a subtype is the broad type label. We take pre-trained ImageNet-1K classiﬁers which have not trained on unseen subtypes. Next we ﬁne-tune the last layer of these pre-trained ImageNet-1K classiﬁers on seen subtypes so that they predict one of 25 broad types. Then, we test the accuracy on images of seen subtypes and on images of unseen subtypes. Accuracy on unseen subtypes is our measure of subtype robustness. Seen and unseen accuracies are shown in Figure 9, while the ImageNet-1K classiﬁcation accuracy before ﬁne-tuning is on the horizontal axis. Despite only having 25 classes and having trained on millions of images, these classiﬁers demonstrate a subtype robustness performance gap that should be far less pronounced. We also observe that the architectures proposed so far hardly deviate from the trendline."
    ]
}