{
    "title_author_abstract_introduction": "ConceptNet 5.5: An Open Multilingual Graph of General Knowledge\nRobyn Speer Luminoso Technologies, Inc. 675 Massachusetts Avenue Cambridge, MA 02139\nJoshua Chin Union College 807 Union St. Schenectady, NY 12308\nCatherine Havasi Luminoso Technologies, Inc. 675 Massachusetts Avenue Cambridge, MA 02139\nAbstract Machine learning about language can be improved by supplying it with speciﬁc knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings.\nConceptNetisaknowledgegraphthatconnectswordsand phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expertcreated resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use.\nWhen ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.\nIntroduction ConceptNet is a knowledge graph that connects words and phrases of natural language (terms) with labeled, weighted edges (assertions). The original release of ConceptNet (Liu and Singh 2004) was intended as a parsed representation of Open Mind Common Sense (Singh 2002), a crowd-sourced knowledge project. This paper describes the release of ConceptNet 5.5, which has expanded to include lexical and world knowledge from many different sources in many languages.\nConceptNet represents relations between words such as:\n• A net is used for catching ﬁsh. • “Leaves” is a form of the word “leaf”. • The word cold in English is studen´y in Czech. • O alimento ´e usado para comer [Food is used for eating].\nIn this paper, we will concisely represent assertions such as the above as triples of their start node, relation label, and end node: the assertion that “a dog has a tail” can be represented as (dog, HasA, tail).\nConceptNet also represents links between knowledge resources. In addition to its own knowledge about the English term astronomy, for example, ConceptNet contains links to URLs that deﬁne astronomy in WordNet, Wiktionary, OpenCyc, and DBPedia.\nThe graph-structured knowledge in ConceptNet can be particularly useful to NLP learning algorithms, particularly those based on word embeddings, such as (Mikolov et al. 2013). We can use ConceptNet to build semantic spaces that are more effective than distributional semantics alone.\nThe most effective semantic space is one that learns from both distributional semantics and ConceptNet, using a generalization of the “retroﬁtting” method (Faruqui et al. 2015). We call this hybrid semantic space “ConceptNet Numberbatch”, to clarify that it is a separate artifact from ConceptNet itself.\nConceptNet Numberbatch performs signiﬁcantly better than other systems across many evaluations of word relatedness, and this increase in performance translates to improvements on downstream tasks such as analogies. On a corpus of SAT-style analogy questions (Turney 2006), its accuracy of 56.1% outperforms other systems based on word embeddings and ties the previous best overall system, Turney’s LRA. This level of accuracy is only slightly lower than the performance of the average human test-taker.\nBuilding word embeddings is not the only application of ConceptNet, but it is a way to apply ConceptNet that achieves clear beneﬁts and is compatible with ongoing research in distributional semantics.\nAfter introducing related work, we will begin by describing ConceptNet 5.5 and its features, show how to use ConceptNet alone as a semantic space and a measure of word relatedness, and then proceed to describe and evaluate the hybrid system ConceptNet Numberbatch on these various semantic tasks.\nRelated Work\nCopyright c(cid:13) 2017, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.\nConceptNet is the knowledge graph version of the Open MindCommonSenseproject(Singh2002),acommonsense\nknowledge base of the most basic things a person knows. It was last published as version 5.2 (Speer and Havasi 2013). Many projects strive to create lexical resources of general knowledge. Cyc (Lenat and Guha 1989) has built an ontology of common-sense knowledge in predicate logic form over the decades. DBPedia (Auer et al. 2007) extracts knowledge from Wikipedia infoboxes, providing a large number of facts, largely focused on named entities that have Wikipedia articles. The Google Knowledge Graph (Singhal 2012) is perhaps the largest and most general knowledge graph, though its content is not freely available. It focuses largely on named entities that can be disambiguated, with a motto of “things, not strings”.\nConceptNet’s role compared to these other resources is to provide a sufﬁciently large, free knowledge graph that focuses on the common-sense meanings of words (not named entities) as they are used in natural language. This focus on words makes it particularly compatible with the idea of representing word meanings as vectors.\nWord embeddings represent words as dense unit vectors of real numbers, where vectors that are close together are semantically related. This representation is appealing because it represents meaning as a continuous space, where similarity and relatedness can be treated as a metric. Word embeddings are often produced as a side-effect of a machine learning task, such as predicting a word in a sentence from its neighbors. This approach to machine learning about semantics is sometimes referred to as distributional semantics or distributed word representations, and it contrasts with the knowledge-driven approach of semantic networks or knowledge graphs.\nTwo prominent matrices of embeddings are the word2vec embeddings trained on 100 billion words of Google News using skip-grams with negative sampling (Mikolov et al. 2013), and the GloVe 1.2 embeddings trained on 840 billion wordsoftheCommonCrawl(Pennington,Socher,andManning 2014). These matrices are downloadable, and we will be using them both as a point of comparison and as inputs to an ensemble. Levy, Goldberg, and Dagan (2015) evaluated multiple embedding techniques and the effects of various explicit and implicit hyperparameters, produced their own performant word embeddings using a truncated SVD of words and their contexts, and provided recommendations for the engineering of word embeddings.\nHolographic embeddings (Nickel, Rosasco, and Poggio 2016) are embeddings learned from a labeled knowledge graph,undertheconstraintthatacircularcorrelationofthese embeddings gives a vector representing a relation. This representation seems extremely relevant to ConceptNet. In our attempt to implement it on ConceptNet so far, it has converged too slowly to experiment with, but this could be overcome eventually with some optimization and additional computing power.\nStructure of ConceptNet\nKnowledge Sources\nConceptNet 5.5 is built from the following sources:\nFigure 1: ConceptNet’s browsable interface (conceptnet.io) shows facts about the English word “bicycle”.\n• Facts acquired from Open Mind Common Sense (OMCS) (Singh 2002) and sister projects in other languages (Anacleto et al. 2006)\n• Information extracted from parsing Wiktionary, in multi-\nple languages, with a custom parser (“Wikiparsec”)\n• “Games with a purpose” designed to collect common knowledge (von Ahn, Kedia, and Blum 2006) (Nakahara and Yamada 2011) (Kuo et al. 2009)\n• Open Multilingual WordNet (Bond and Foster 2013), a linked-data representation of WordNet (Miller et al. 1998) and its parallel projects in multiple languages\n• JMDict (Breen 2004), a Japanese-multilingual dictionary • OpenCyc, a hierarchy of hypernyms provided by Cyc (Lenat and Guha 1989), a system that represents common sense knowledge in predicate logic\n• A subset of DBPedia (Auer et al. 2007), a network of facts\nextracted from Wikipedia infoboxes\nWith the combination of these sources, ConceptNet contains over 21 million edges and over 8 million nodes. Its English vocabulary contains approximately 1,500,000 nodes, and there are 83 languages in which it contains at least 10,000 nodes.\nThe largest source of input for ConceptNet is Wiktionary, which provides 18.1 million edges and is mostly responsible for its large multilingual vocabulary. However, much of the character of ConceptNet comes from OMCS and the various games with a purpose, which express many different kinds of relations between terms, such as PartOf (“a wheel is part of a car”) and UsedFor (“a car is used for driving”).\nRelations ConceptNet uses a closed class of selected relations such as IsA, UsedFor, and CapableOf, intended to represent a relationship independently of the language or the source of the terms it connects.\nConceptNet 5.5 aims to align its knowledge resources on its core set of 36 relations. These generalized relations are similar in purpose to WordNet’s relations such as hyponym and meronym, as well as to the qualia of the Generative Lexicon theory (Pustejovsky 1991). ConceptNet’s edges are directed, but as a new feature in ConceptNet 5.5, some relations are designated as being symmetric, such as SimilarTo. The directionality of these edges is unimportant.\nThe core relations are:\n• Symmetricrelations:Antonym,DistinctFrom,EtymologicallyRelatedTo, LocatedNear, RelatedTo, SimilarTo, and Synonym\n• Asymmetric relations: AtLocation, CapableOf, Causes, CausesDesire, CreatedBy, DeﬁnedAs, DerivedFrom, Desires, Entails, ExternalURL, FormOf, HasA, HasContext, HasFirstSubevent, HasLastSubevent, HasPrerequisite, HasProperty, InstanceOf, IsA, MadeOf, MannerOf, MotivatedByGoal, ObstructedBy, PartOf, ReceivesAction, SenseOf, SymbolOf, and UsedFor\nDeﬁnitions and examples of these relations appear in a\npage of the ConceptNet 5.5 documentation1.\nRelations with speciﬁc semantics, such as UsedFor and HasPrerequisite, tend to connect common words and phrases, while rarer words are connected by more general relations such as Synonym and RelatedTo.\nAn example of edges in ConceptNet, in a browsable interface that groups them by their relation expressed in natural English, appears in Figure 1.\nTerm Representation\nConceptNet represents terms in a standardized form. The text is Unicode-normalized in NFKC form2 using Python’s unicodedata implementation, lowercased, and split into non-punctuation tokens using the tokenizer in the Python package wordfreq (Speer et al. 2016), which builds on the standard Unicode word segmentation algorithm. The tokens are joined with underscores, and this text is prepended with the URI /c/lang, where lang is the BCP 47 language code3 for the language the term is in. As an example, the English term “United States” becomes /c/en/united states.\nRelations have a separate namespace of URIs preﬁxed with /r, such as /r/PartOf. These relations are given artiﬁcial names in English, but apply to all languages. The statement that was obtained in Portuguese as “O alimento ´e usado para comer” is still represented with the relation /r/UsedFor.\nThe most signiﬁcant change from ConceptNet 5.4 and earlier is in the representation of terms. ConceptNet 5.4 required terms in English to be in lemmatized form, so that, for example, “United States” had to be represented as /c/en/unite state. In this representation, “drive” and “driving” were the same term, allowing the assertions (car, UsedFor, driving) and (drive, HasPrerequisite, have license) to be connected. ConceptNet 5.5 removes the lemmatizer, and instead relates inﬂections of words using the FormOf relation. The two assertions above are now linked by the third assertion (driving, FormOf, drive), and both “driving” and “drive” can be looked up in ConceptNet.\n1https://github.com/commonsense/\nconceptnet5/wiki/Relations",
    "data_related_paragraphs": [
        "ConceptNet’s representation allows for more speciﬁc, disambiguated versions of a term. The URI /c/en/lead/n refers to noun senses of the word “lead”, and is effectively included within /c/en/lead when searching or traversing ConceptNet, and linked to it with the implicit relation SenseOf. Many data sources provide information about parts of speech, allowing us to use this as a common representation that provides a small amount of disambiguation. Further disambiguation is allowed by the URI structure, but not currently used.",
        "Linked Data ConceptNet imports knowledge from some other systems, such as WordNet, into its own representation. These other systems have their own target vocabularies that need to be aligned with ConceptNet, which is usually an underspeciﬁed, many-to-many alignment.",
        "A term that is imported from another knowledge graph will be connected to ConceptNet nodes via the relation ExternalURL, pointing to an absolute URL that represents that term in that external resource. This newly-introduced relation preserves the provenance of the data and enables looking up what the untransformed data was. ConceptNet terms can also be represented as absolute URLs, so this allows ConceptNettoconnectbidirectionallytothebroaderecosystem of Linked Open Data.",
        "Combining Multiple Sources of Embeddings Retroﬁtting can be applied to any existing matrix of word embeddings, without needing access to the data that was",
        "used to train them. This is particularly useful because it allows building on publicly-released matrices of embeddings whose input data is unavailable or difﬁcult to acquire.",
        "To avoid manually overﬁtting by designing our semantic space around a particular evaluation, we experimented using smaller development sets, holding out some test data until it was time to include results in this paper:",
        "Much of the groundwork for evaluating systems’ ability to solve proportional analogies was laid by Peter Turney, including his method of Latent Relational Analysis (Turney 2006), which was quite effective at solving proportional analogies by repeatedly searching the Web for the words involved in them. A newer method called SuperSim (Turney 2013) does not require Web searching. These methods are evaluated on a dataset of 374 SAT questions that Turney and his collaborators have collected.",
        "Thescoresofoursystemonalltheseevaluationsappearin Table 1, including a development/test breakdown that shows no apparent overﬁtting. The “Final” column is meant for comparisons to other papers and used in the graph. It uses the standard test set that other publications use, if it exists (which is the case for MEN-3000 and Story Cloze), or all of the data otherwise.",
        "On all of the four word-relatedness evaluations, ConceptNet Numberbatch 16.09 (the complete system described in this paper) is state of the art, performing better than all other systems evaluated to an extent that exceeds the conﬁdence intervalofthechoiceofquestions.Itshighscoresonboththe Rare Words dataset and the crowd-sourced MEN-3000 and MTurk-771 datasets, exceeding the performance of other embeddings with high conﬁdence, shows both the breadth and the depth of its understanding of words.",
        "Table 1: The Spearman correlation (ρ) or accuracy (acc) of ConceptNet Numberbatch 16.09, our hybrid system, on data used in development and data held out for testing.",
        "MEN-3000Rare WordsMTurk-771WS353Story ClozeSAT analogies0.00.10.20.30.40.50.60.70.80.91.0Evaluation scoreword2vec Google NewsGloVe 1.2 840BLexVec: enWP + NewsCrawlConceptNet-PPMIConceptNet Numberbatch\fconﬁdence intervals, indicating that the ranking of the results could easily change with a different selection of questions. Our score of 56.1% is also within the conﬁdence interval of the performance of the average human college applicant on these questions, said to be 57.0% (Turney 2006). We have shown that knowledge-informed word embeddings are up to the challenge of real SAT analogies; they perform the same as or slightly better than non-wordembedding systems on the same evaluation, when other word embeddings perform worse. In practice, recent word embeddings have instead been evaluated on simpler, synthetic analogy data sets (Mikolov et al. 2013), and have not usually been compared to existing non-embedding-based methods of solving analogies.",
        "Availability of the Code and Data The code and documentation of ConceptNet 5.5 can found on GitHub at https://github.com/ be commonsense/conceptnet5, and the knowledge graph can be browsed at http://conceptnet.io. The full build process, as well as the evaluation graph, can be reproduced using the instructions included in the README ﬁle for using Snakemake, a build system for data science (K¨oster and Rahmann 2012), and optionally using Docker Compose to reproduce the system environment. The version of the repository as of the submission of this paper has been tagged as aaai2017.",
        "References [Anacleto et al. 2006] Anacleto, J.; Lieberman, H.; Tsutsumi, M.; Neris, V.; Carvalho, A.; Espinosa, J.; Godoi, M.; and Zem-Mascarenhas, S. 2006. Can common sense uncover cultural differences in computer applications? In Artiﬁcial intelligence in theory and practice. Springer. 1–10. [Auer et al. 2007] Auer, S.; Bizer, C.; Kobilarov, G.; Lehmann, J.; Cyganiak, R.; and Ives, Z. 2007. DBpedia: A nucleus for a web of open data. Springer. [Bond and Foster 2013] Bond, F., and Foster, R. 2013. Linking and Extending an Open Multilingual Wordnet. In 51st Annual Meeting of the Association for Computational Linguistics: ACL-2013, 1352–1362. [Bonett and Wright 2000] Bonett, D. G., and Wright, T. A. Sample size requirements for estimating Pear2000. son, Kendall and Spearman correlations. Psychometrika 65(1):23–28. [Breen 2004] Breen, J. JMDict: a Japanese2004. multilingual dictionary. In Proceedings of the Workshop on Multilingual Linguistic Resources, 71–79. Association for Computational Linguistics.",
        "[Bruni, Tran, and Baroni 2014] Bruni, E.; Tran, N.-K.; and Baroni, M. 2014. Multimodal distributional semantics. J. Artif. Intell. Res. (JAIR) 49:1–47. [Faruqui et al. 2015] Faruqui, M.; Dodge, J.; Jauhar, S. K.; Dyer,C.;Hovy,E.;andSmith,N.A. 2015. Retroﬁttingword vectors to semantic lexicons. In Proceedings of NAACL. [Finkelstein et al. 2001] Finkelstein, L.; Gabrilovich, E.; Matias, Y.; Rivlin, E.; Solan, Z.; Wolfman, G.; and Ruppin, E. 2001. Placing search in context: The concept revisited. In Proceedings of the 10th international conference on World Wide Web, 406–414. ACM. [Halawi et al. 2012] Halawi, G.; Dror, G.; Gabrilovich, E.; and Koren, Y. 2012. Large-scale learning of word relatIn Proceedings of the 18th ACM edness with constraints. SIGKDD international conference on Knowledge discovery and data mining, 1406–1414. ACM. [Hassan and Mihalcea 2009] Hassan, S., and Mihalcea, R. 2009. Cross-lingual semantic relatedness using encyclopedic knowledge. In Proceedings of the conference on Empirical Methods in Natural Language Processing. [Herdaˇgdelen and Baroni 2009] Herdaˇgdelen, A., and Baroni, M. 2009. Bagpack: A general framework to represent semantic relations. In Proceedings of the Workshop on Geometrical Models of Natural Language Semantics, GEMS ’09, 33–40. Stroudsburg, PA, USA: Association for Computational Linguistics. [K¨oster and Rahmann 2012] K¨oster, J., and Rahmann, S. 2012. Snakemake—a scalable bioinformatics workﬂow engine. Bioinformatics 28(19):2520–2522. [Kuo et al. 2009] Kuo, Y.-L.; Lee, J.-C.; Chiang, K.-Y.; Wang, R.; Shen, E.; Chan, C.-W.; and Hsu, J. Y.-J. 2009. Community-based game design: experiments on social games for commonsense data collection. In Proceedings of the ACM SIGKDD Workshop on Human Computation, 15– 22. ACM. [Lenat and Guha 1989] Lenat, D. B., and Guha, R. V. 1989. Building large knowledge-based systems: representation and inference in the Cyc project. Addison-Wesley Longman. [Levy, Goldberg, and Dagan 2015] Levy, O.; Goldberg, Y.; and Dagan, I. 2015. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics 3:211–225. [Liu and Singh 2004] Liu, H., and Singh, P. 2004. ConceptNet – a practical commonsense reasoning tool-kit. BT Technology Journal 22(4):211–226. [Luong, Socher, and Manning 2013] Luong, M.-T.; Socher, R.; and Manning, C. D. 2013. Better word representations with recursive neural networks for morphology. CoNLL2013 104. [Mikolov et al. 2013] Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efﬁcient estimation of word representations in vector space. CoRR abs/1301.3781. [Miller et al. 1998] Miller, G.; Fellbaum, C.; Tengi, R.; Wakeﬁeld, P.; Langone, H.; and Haskell, B. 1998. WordNet. MIT Press Cambridge."
    ]
}