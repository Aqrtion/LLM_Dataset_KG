{
    "title_author_abstract_introduction": "Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering\nTodor Mihaylov‡ and Peter Clark† and Tushar Khot† and Ashish Sabharwal†\n† Allen Institute for Artiﬁcial Intelligence, Seattle, WA, U.S.A. ‡ Research Training Group AIPHES & Heidelberg University, Heidelberg, Germany {peterc,tushark,ashishs}@allenai.org, mihaylov@cl.uni-heidelberg.de\nAbstract\nWe present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1326 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge(e.g., asuitofarmorismadeofmetal)obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic—in the context of common knowledge—and the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.\nIntroduction\nOpen book exams are a common mechanism for assessing human understanding of a subject, where test takers are allowed free access to a relevant book, study guide, or class notes when answering questions. In this context, the goal is not to evaluate memorization but a deeper understanding of the material and its application to new situations (Jenkins, 1995; Landsberger, 1996). The application, in turn, often requires combining a fact in the book (e.g., metals conduct electricity) with additional common knowledge the test taker is ex-\nQuestion: Which of these would let the most heat travel through? A) a new pair of jeans. B) a steel spoon in a cafeteria. C) a cotton candy at a store. D) a calvin klein cotton hat.\nScience Fact: Metal is a thermal conductor.\nCommon Knowledge: Steel is made of metal. Heat travels through a thermal conductor.\nFigure 1: An example for a question with a given set of choices and supporting facts.\npected to have acquired by this stage (e.g., a suit of armor is made of metal).\nMotivated by this setting, we present a new kind of question answering dataset, OpenBookQA,1 that consists of two parts: Q, a set of 5957 multiple-choice questions, and F, a set of 1326 diverse facts about elementary level science. F has three key characteristics of an ‘open book’: (a) it forms the basis for generating Q; (b) it has been deemed central to scientiﬁc explanations (Jansen et al., 2018); and (c) by itself, F is generally insufﬁcient to answer questions in Q. Faced with a question q ∈ Q, a student or system S is expected retrieve a relevant fact f ∈ F, and appeal to their own common knowledge, KS, when applying f to answer q.\nFigure 1 provides an example. Here, metals are thermal conductors is a core scientiﬁc fact available in F. One way to apply this fact to decide whether a steel spoon would let the most heat travel through is to appeal to common knowledge that steel is metallic and heat travels through thermal conductors. In general, the expected common knowledge is relatively simple (taxonomic facts,\n1The dataset and the code for the models are available at\nhttp://data.allenai.org/OpenBookQA.\ndeﬁnitions, object properties, etc.); the difﬁculty lies in identifying it and meaningfully combining it with a core fact from F to answer the question. OpenBookQA questions are challenging as they require multi-hop reasoning with partial context provided by F. Speciﬁcally, unlike existing datasets for reading comprehension (RC), answering questions on the back of a textbook (TQA),2 as well as question answering over structured knowledge-bases (KBQA), the open book F that comes with OpenBookQA is not self-contained. A successful system must therefore go beyond the typical challenges such as paraphrase matching and coreference resolution, without beneﬁting from the canonicalized and complete information in KBQA.\nGenerating interesting open book questions is a difﬁcult task. We used a multi-stage process starting with F, using crowd-sourcing to generate (noisy) questions based on F that probe novel situations, using an automatic ﬁlter to ensure hardness for retrieval and association based systems, using a crowd ﬁlter to ensure answerability by a lay person, and further using an expert ﬁlter to ensure higher quality in Dev and Test sets.\nWe evaluate a number of existing QA systems for science (without retraining) on OpenBookQA, ﬁnding that they perform surprisingly close to the random guessing baseline of 25%. Human performance, on the other hand, is close to 92%.3\nMotivated by recent ﬁndings of gameability of NLP datasets (Gururangan et al., 2018), we also develop and evaluate simple, attention-based, neural baselines including a plausible answer detector (which ignores the question text completely) and an odd-one-out solver. These highlight inevitable human bias in any crowdsourced dataset, increasing performance on OpenBookQA to 48%.\nBuilding upon a recent neural model for incorporating external knowledge in the story cloze setting (Mihaylov and Frank, 2018), we propose a knowledge-aware neural baseline that can utilize both the open book F and common knowledge retrieved from sources such as ConceptNet (Speer et al., 2017). While retrieving the most useful pieces of knowledge remains an open challenge, our‘oracle’experimentswiththefactf usedwhile generating a question q and an interpretation (by",
    "data_related_paragraphs": [
        "the question author) of the additional knowledge k needed for q, provides valuable insight into the nature of this dataset: Facts from the open book F are valuable (5% improvement) but not sufﬁcient. Using both f and k increases the accuracy to 76%, but is still far from human level performance, suggesting the need for non-trivial reasoning to combine these facts.",
        "Reading Comprehension (RC) datasets have been proposed as benchmarks to evaluate the ability of systems to understand a document by answering factoid-style questions over this document. These datasets have taken various forms: multiple-choice (Richardson et al., 2013), clozestyle (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2016), and span prediction (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017) However, analysis (Chen et al., 2016; Sugawara et al., 2017) of these datasets has shown that many of the questions can be solved with context token matching (Chen et al., 2017a; Weissenborn et al., 2017) or relatively simple paraphrasing.",
        "To focus on the more challenging problem of reasoning across sentences, new datasets have been proposed for multi-step RC. QAngaroo (Welbl et al., 2018) have used a knowledgebase to identify entity pairs (s, o) with a known relation, r, which is also supported by a multihop path in a set of documents. They use structured tuple queries (s, r, ?) and use all the documents along the path as the input passage. NarrativeQA (Kocisk´y et al., 2017) is an RC dataset that has been shown to require an iterative reasoning about the narrative of a story. Similar to OpenBookQA, the questions were generated to ensure that the answer is not a direct match or paraphrase",
        "that can be retrieved with an IR approach. Most recently, Khashabi et al. (2018) proposed MultiRC, a multiple-choice RC dataset that is designedtorequiremulti-sentencereasoningandcan have multiple correct answers. Again, like most RC datasets, it is self-contained.",
        "Tasks with external knowledge. While many of the RC datasets could beneﬁt from commonsenseorbackgroundknowledge, theyaredesigned to be self-contained, i.e., solvableby the document context alone. Datasets such as the Story Cloze Test (Mostafazadeh et al., 2016), MCScript,4 and ProPara (Mishra et al., 2018) do require additional domain knowledge about everyday events, scripts, and processes, respectively. However, these datasets need domain-speciﬁc modeling of events, whereas OpenBookQA appeals to broad common knowledge cutting across a variety of types and topics.",
        "Lastly, many Science Question Answering datasets (e.g. Clark et al., 2016, 2018) have been released that need broad external knowledge to answer the questions. However, these questions are not associated with a core set of facts, i.e., an “open book” used to deﬁne these questions. As a result, the questions vary widely in style and complexity (Clark et al., 2018). In contrast, OpenBookQA focuses on a more well-deﬁned subset of science QA, appealing to one core fact from the open book and one (or few) relatively simple commonly known supporting facts.",
        "3 OpenBookQA Dataset",
        "The OpenBookQA dataset consists of about 6,000 4-way multiple-choice questions, each associated with one core fact from a “book” F of 1326 such facts, and an auxiliary set K of about 6000 additional facts. The questions were created via a multi-stage crowdsourcing and partial expert ﬁltering process, discussed in Section 3.1.",
        "OpenBookQA additionally requires broad common knowledge, which is expected to come from large corpora, such as ConceptNet, Wikipedia, or a corpus with 14M science-related sentences used by some existing baselines. The crowdsourcing process below also asks workers to mark a second fact, k, needed for each question q, in addition to f. These second facts, unfortunately, were often incomplete, over-complete, or only distantly related to q. We thus include in OpenBookQA the set K of such second facts only as auxiliary data for optional use. We emphasize that K should not be viewed as ‘gold’ additional facts, or as a substitute for broad common knowledge.",
        "To assess human accuracy on this dataset, we consider the following model: Each question q ∈ Q has some (unknown) human accuracy pq, deﬁned as the probability that a random human subject, chosen uniformly from a large pool H, would answer q correctly. Thus, we can think of this as deﬁning a Bernoulli random variable, Xq ∼ B(pq), whose mean is (unknown) pq. The average human accuracy on Q under this model is:",
        "process is equivalent to obtaining 5 independent samples, Xq,i,i ∈ I,|I| = 5, from B(pq). We must, however, be careful when using this data to estimate pq, as the same 5 samples were used to decide whether q makes it into the question set Q or not. For instance, if we had kept only those questions that all 5 workers answered correctly, it would clearly be inaccurate to claim that the human accuracy on Q is 100%. Nevertheless, it is possible to re-use the judgments from Step 6 to approximate H(Q) with high conﬁdence, without posing the questions to new workers.",
        "Table 1: Statistics for full OpenBookQA dataset. Parenthetical numbers next to each average are the max.",
        "OpenBookQA consists of 5957 questions, with 4957/500/500 in the Train/Dev/Test splits.9 Table 1 summarizes some statistics about the full dataset. Each question has exactly four answer choices and one associated fact used in the creation process. We report the average length of questions, candidate choices, and associated facts, as well as how often is the longest/shortest choice the correct one.",
        "empirically, most baseline approaches achieve a relatively low score on this dataset (even when the core fact is provided). We claim that this is due to the fact that the reasoning needed to answer these questions is non-trivial. Table 3 shows few questions with the associated facts and high-level reasoning needed to answer these questions. Assuming a model can extract the described relations (e.g. defn, contains), the QA system still needs to be able to chain these facts together, identify the resulting relation and verify its expression for each choice. In the extreme case (as shown in the last example), even though only one additional fact is needed to answer the question, it needs a system to apply the core “general” science fact to a “speciﬁc” situation.",
        "Lastly, we implement a two stage model for incorporating external common knowledge, K. The ﬁrst module performs information retrieval on K to select a ﬁxed size subset of potentially relevant facts KQ,C for each instance in the dataset (see Appendix A). The second module is a neural network that takes (Q, C, KQ,C) as input to predict the answer aq,c to a question Q from the set of choices C.",
        "The fourth group demonstrates that carefully designed trainable neural models—even if simplistic and knowledge-free—can be surprisingly powerful. For example, the “plausible answer detector” can predict the correct answer with 49.6% accuracy without even looking at the question. The “odd-one-out” solver, by considering other answer choices, raises this to 50.2%. The “question match” solver, which simply compares the BiLSTM max-out encoding of the question with that of various answer choices, also achieves 50.2%.13 Similar ﬁndings have been reported for several recent datasets (Gururangan et al., 2018), making it imperative to perform such tests early.",
        "We present a new dataset, OpenBookQA, of about 6000 questions for open book question answering. The task focuses on the challenge of combining a corpus of provided science facts (open book) with external broad common knowledge. We show that this dataset requires simple common knowledge beyond the provided core facts, as well as multihop reasoning combining the two. While simple neural methods are able to achieve an accuracy of about 50%, this is still far from the human performance of 92% on this task. We leave closing this gap for future research, and illustrate, via oraclestyle experiments, the potential of better retrieval and reasoning on this task.",
        "13This model also achieves the current best score, 33.87%, on the ARC Reasoning Challenge (Clark et al., 2018). When adapted for the textual entailment task by comparing BiLSTM max-out encodings of premise and hypothesis, it achieves 85% on the SciTail dataset (Khot et al., 2018).",
        "A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In EMNLP, pages 670–680.",
        "S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. R. Bowman, and N. A. Smith. 2018. Annotation artifacts in natural language inference data. In NAACL.",
        "M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised chalIn ACL, lenge dataset for reading comprehension. pages 1601–1611.",
        "T. Khot, A. Sabharwal, and P. Clark. 2018. SciTail: A textual entailment dataset from science question answering. In AAAI.",
        "T. Mihaylov and A. Frank. 2017. Story Cloze Ending Selection Baselines and Data Examination. In LSDSem Shared Task.",
        "G. A. Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39– 41.",
        "G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller. 1990. Introduction to WordNet: An online lexical database. International Journal of Lexicography, 3(4):235–244.",
        "B. D. Mishra, L. Huang, N. Tandon, W. tau Yih, and P. Clark. 2018. Tracking state changes in procedural text: A challenge dataset and models for process paragraph comprehension. In NAACL.",
        "S. Sugawara, H. Yokono, and A. Aizawa. 2017. Prerequisite skills for reading comprehension: Multiperspective analysis of mctest datasets and systems. In AAAI, pages 3089–3096.",
        "A. Trischler, T. Wang, X. Yuan, J. Harris, A. Sordoni, P. Bachman, and K. Suleman. 2017. NewsQA: A machine comprehension dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 191–200.",
        "J. Welbl, P. Stenetorp, and S. Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. TACL.",
        "T. Onishi, H. Wang, M. Bansal, K. Gimpel, and D. McAllester. 2016. Who did what: A large-scale In EMNLP, pages person-centered cloze dataset. 2230–2235, Austin, Texas.",
        "M. Richardson, C. J. Burges, and E. Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of text. In EMNLP, pages 193–203.",
        "This module is the ﬁrst part of a two stage model for incorporating knowledge from an external source K. For each instance (q,C) in the dataset, where q is a question and C = {c1,...,c4} a set of answer choices, it performs information retrieval (IR) on K to select a ﬁxed size subset Kq,C of potentially relevant facts. The second module is a neural network that takes (q,C,Kq,C) as input, and predicts the answer aq,C.",
        "The code for the models and the conﬁguration ﬁles required for reproducing the results are available at http://data.allenai.org/OpenBookQA.",
        "We perform textual entailment experiments on the Science enTailment dataset SciTail (Khot et al., 2018). We change the Question Match model to a classic BiLSTM Max-Out (Conneau et al., 2017) for textual entailment, by replacing the question q and a choice ci with the premise p and the hypothesis h, resp., and perform binary classiﬁ- cation on the entailment labels (Entail, Neural). We run experiments with BiLSTM encoders with LSTM hidden size of 384 and share the encoder parameters between the premise and the hypothesis. Without additional hyper-parameter tuning, this yields entailment accuracy scores of 87.9% and 85.4% on the Dev and Test sets, respectively.",
        "In the shown examples, the ﬁrst question falls outside the domain of Science where most of the core facts come from. The scientiﬁc fact “(f) An example of collecting data is measuring” is transformed into a question related to the law and judicial domain of collecting data for a (court) case. This is an indication that the model trained on the Train set does not perform well on distant domains, even if the core facts are provided.",
        "An example of data collection is: (A - 0.9977) Deleting case ﬁles on the computer, (B - 0.0000) Touching evidence without gloves, (C - 0.0004) speaking with a witness, (D - 0.0019) Throwing documents in the trash. Oracle facts: (f) An example of collecting data is measuring. (k) Interviews are used to collect data. If a farmland up the hill gets rainfall, what could happen to lower lands? (A - 0.0005) all of these, (B - 0.0245) they could get fertilizer washed to them, (C - 0.9542) they could experience unfavorable chemical change in their lands, (D - 0.0208) they could have their lands poisoned. Oracle facts: (f) runoff contains fertilizer from cropland. (k) fertilizers for certain crops could poison other crops or soil types. Layers of the earth include all but: (A - 0.0429) mantle, (B - 0.0059) center, (C - 0.0334) crust, (D - 0.9177) inner core. Oracle facts: (f) the crust is a layer of the Earth. (k) the last layer is the outer core."
    ]
}