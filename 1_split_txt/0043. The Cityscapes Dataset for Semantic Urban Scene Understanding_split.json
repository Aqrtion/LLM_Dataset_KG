{
    "title_author_abstract_introduction": "The Cityscapes Dataset for Semantic Urban Scene Understanding\nMarius Cordts1,2\nMohamed Omran3\nSebastian Ramos1,4\nTimo Rehfeld1,2\nMarkus Enzweiler1\nRodrigo Benenson3\nUwe Franke1\nStefan Roth2\nBernt Schiele3\n1Daimler AG R&D, 2TU Darmstadt, 3MPI Informatics, 4TU Dresden www.cityscapes-dataset.net\ntrain/val – ﬁne annotation – 3475 images\ntrain – coarse annotation – 20000 images\ntest – ﬁne annotation – 1525 images\nAbstract\nVisual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has beneﬁted enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark.\n1. Introduction\nVisual scene understanding has moved from an elusive goal to a focus of much recent research in computer vision [27]. Semantic reasoning about the contents of a scene is thereby done on several levels of abstraction. Scene recognition aims to determine the overall scene category by putting emphasis on understanding its global properties, e.g. [46, 82]. Scene labeling methods, on the other hand, seek to identify the individual constituent parts of a whole scene as well as their interrelations on a more local pixel-\nand instance-level, e.g. [41,71]. Specialized object-centric methods fall somewhere in between by focusing on detecting a certain subset of (mostly dynamic) scene constituents, e.g.[6,12,13,15]. Despitesigniﬁcantadvances, visualscene understanding remains challenging, particularly when taking human performance as a reference.\nThe resurrection of deep learning [34] has had a major impact on the current state-of-the-art in machine learning and computer vision. Many top-performing methods in a variety of applications are nowadays built around deep neural networks [30, 41, 66]. A major contributing factor to their success is the availability of large-scale, publicly available datasets such as ImageNet [59], PASCAL VOC [14], PASCAL-Context [45], and Microsoft COCO [38] that allow deep neural networks to develop their full potential.\nDespite the existing gap to human performance, scene understanding approaches have started to become essential components of advanced real-world systems. A particularly popular and challenging application involves selfdriving cars, which make extreme demands on system performance and reliability. Consequently, signiﬁcant research efforts have gone into new vision technologies for understanding complex trafﬁc scenes and driving scenarios [4,16–18,58,62]. Also in this area, research progress can be heavily linked to the existence of datasets such as theKITTIVisionBenchmarkSuite[19], CamVid [7], Leuven [35], and Daimler Urban Segmentation [61] datasets. These urban scene datasets are often much smaller than datasets addressing more general settings. Moreover, we argue that they do not fully capture the variability and complexity of real-world inner-city trafﬁc scenes. Both shortcomings currently inhibit further progress in visual understanding of street scenes. To this end, we propose the Cityscapes benchmark suite and a corresponding dataset, speciﬁcally\n1 instance-level annotations are available 2 ignored for evaluation\nconstruction\nnature\nvehicle\nobject\nFigure 1. Number of ﬁnely annotated pixels (y-axis) per class and their associated categories (x-axis).\ntailored for autonomous driving in an urban environment and involving a much wider range of highly complex innercity street scenes that were recorded in 50 different cities. Cityscapes signiﬁcantly exceeds previous efforts in terms of size, annotation richness, and, more importantly, regarding scene complexity and variability. We go beyond pixel-level semantic labeling by also considering instance-level semantic labeling in both our annotations and evaluation metrics. To facilitate research on 3D scene understanding, we also provide depth information through stereo vision.\nVery recently, [75] announced a new semantic scene labeling dataset for suburban trafﬁc scenes. It provides temporally consistent 3D semantic instance annotations with 2D annotations obtained through back-projection. We consider our efforts to be complementary given the differences in the way that semantic annotations are obtained, and in the type of scenes considered, i.e. suburban vs. inner-city trafﬁc. To maximize synergies between both datasets, a common label deﬁnition that allows for cross-dataset evaluation has been mutually agreed upon and implemented.",
    "data_related_paragraphs": [
        "2. Dataset",
        "Designing a large-scale dataset requires a multitude of decisions, e.g. on the modalities of data recording, data preparation, and the annotation protocol. Our choices were guided by the ultimate goal of enabling signiﬁcant progress in the ﬁeld of semantic urban scene understanding.",
        "2.1. Data speciﬁcations",
        "Our data recording and annotation methodology was carefully designed to capture the high variability of outdoor street scenes. Several hundreds of thousands of frames were acquired from a moving vehicle during the span of several months, covering spring, summer, and fall in 50 cities, primarily in Germany but also in neighboring countries. We deliberately did not record in adverse weather conditions, such as heavy rain or snow, as we believe such conditions to require specialized techniques and datasets [51].",
        "For comparability and compatibility with existing datasets we also provide low dynamic-range (LDR) 8bit RGB images that are obtained by applying a logarithmic compression curve. Such tone mappings are common in automotive vision, since they can be computed efﬁciently and independently for each pixel. To facilitate highest annotationquality, weapplied aseparatetone mappingtoeach image. The resulting images are less realistic, but visually more pleasing and proved easier to annotate. 5000 images were manually selected from 27 cities for dense pixel-level annotation, aiming for high diversity of foreground objects, background, and overall scene layout. The annotations (see Sec. 2.2) were done on the 20th frame of a 30-frame video snippet, which we provide in full to supply context information. For the remaining 23 cities, a single image every 20s or 20m driving distance (whatever comes ﬁrst) was selected for coarse annotation, yielding 20000 images in total.",
        "In addition to the rectiﬁed 16bit HDR and 8bit LDR stereo image pairs and corresponding annotations, our dataset includes vehicle odometry obtained from in-vehicle sensors, outside temperature, and GPS tracks.",
        "Our dataset DUS",
        "In two experiments we assessed the quality of our labeling. First, 30 images were ﬁnely annotated twice by difIt ferent annotators and passed the same quality control. turned out that 96% of all pixels were assigned to the same label. Since our annotators were instructed to choose a void label if unclear (such that the region is ignored in training and evaluation), we exclude pixels having at least one void label and recount, yielding 98% agreement. Second, all our ﬁne annotations were additionally coarsely annotated such that we can enable research on densifying coarse labels. We found that 97% of all labeled pixels in the coarse annotationswereassignedthesameclassasintheﬁneannotations. We deﬁned 30 visual classes for annotation, which are grouped into eight categories: ﬂat, construction, nature, vehicle, sky, object, human, and void. Classes were selected based on their frequency, relevance from an application standpoint, practical considerations regarding the annotation effort, as well as to facilitate compatibility with existing datasets, e.g. [7,19,75]. Classes that are too rare are excluded from our benchmark, leaving 19 classes for evaluation, see Fig. 1 for details. We plan to release our annotation tool upon publication of the dataset.",
        "2.3. Dataset splits",
        "We split our densely annotated images into separate training, validation, and test sets. The coarsely annotated images serve as additional training data only. We chose not to split the data randomly, but rather in a way that ensures each split to be representative of the variability of different street scene scenarios. The underlying split criteria involve a balanced distribution of geographic location and population size of the individual cities, as well as regarding the time of year when recordings took place. Speciﬁcally, each of the three split sets is comprised of data recorded with the",
        "following properties in equal shares: (i) in large, medium, andsmallcities; (ii)in thegeographicwest, center, andeast; (iii)inthegeographicnorth, center, andsouth; (iv)atthebeginning, middle, and end of the year. Note that the data is split at the city level, i.e. a city is completely within a single split. Following this scheme, we arrive at a unique split consisting of 2975 training and 500 validation images with publicly available annotations, as well as 1525 test images with annotations withheld for benchmarking purposes.",
        "In order to assess how uniform (representative) the splits are regarding the four split characteristics, we trained a fully convolutional network [41] on the 500 images in our validation set. This model was then evaluated on the whole test set, as well as eight subsets thereof that reﬂect the extreme values of the four characteristics. With the exception of the time of year, the performance is very homogeneous, varying less than 1.5% points (often much less). Interestingly, the performance on the end of the year subset is 3.8% points better than on the whole test set. We hypothesize that this is due to softer lighting conditions in the frequently cloudy fall. To verify this hypothesis, we additionally tested on images taken in low- or high-temperature conditions, ﬁnding a 4.5% point increase in low temperatures (cloudy) and a 0.9% point decrease in warm (sunny) weather. Moreover, speciﬁcally training for either condition leads to an improvement on the respective test set, but not on the balanced set. These ﬁndings support our hypothesis and underline the importance of a dataset covering a wide range of conditions encountered in the real world in a balanced way.",
        "We compare Cityscapes to other datasets in terms of (i) annotation volume and density, (ii) the distribution of visual",
        "Table 2. Absolute and average number of instances for Cityscapes, KITTI, and Caltech (1 via interpolation) on the respective training and validation datasets.",
        "classes, and (iii) scene complexity. Regarding the ﬁrst two aspects, we compare Cityscapes to other datasets with semantic pixel-wise annotations, i.e. CamVid [7], DUS [62], and KITTI [19]. Note that there are many other datasets with dense semantic annotations, e.g. [2, 56, 65, 69, 70]. However, we restrict this part of the analysis to those with a focus on autonomous driving.",
        "CamVid consists of ten minutes of video footage with pixel-wise annotations for over 700 frames. DUS consists of a video sequence of 5000 images from which 500 have been annotated. KITTI addresses several different tasks including semantic labeling and object detection. As no ofﬁcial pixel-wise annotations exist for KITTI, several independent groups have annotated approximately 700 frames [22,29,32,33,58,64,77,80]. We map those labels to our high-level categories and analyze this consolidated set. In comparison, Cityscapes provides signiﬁcantly more annotated images, i.e. 5000 ﬁne and 20000 coarse annotations. Moreover, the annotation quality and richness is notably better. As Cityscapes provides recordings from 50 different cities, it also covers a signiﬁcantly larger area than previous datasets that contain images from a single city only, e.g. Cambridge (CamVid), Heidelberg (DUS), and Karlsruhe (KITTI). In terms of absolute and relative numbers of semantically annotated pixels (training, validation, and test data), Cityscapes compares favorably to CamVid, DUS, and KITTI with up to two orders of magnitude more annotated pixels, c.f. Tab. 1. The majority of all annotated pixels in Cityscapes belong to the coarse annotations, providing many individual (but correlated) training samples, but missing information close to object boundaries.",
        "Figures 1 and 2 compare the distribution of annotations across individual classes and their associated higher-level categories. Notable differences stem from the inherently different conﬁgurations of the datasets. Cityscapes involves dense inner-city trafﬁc with wide roads and large intersections, whereas KITTI is composed of less busy suburban trafﬁc scenes. As a result, KITTI exhibits signiﬁcantly fewer ﬂat ground structures, fewer humans, and more nature. In terms of overall composition, DUS and CamVid seem more aligned with Cityscapes. Exceptions are an abundance of sky pixels in CamVid due to cameras with a comparably large vertical ﬁeld-of-view and the absence of certain categories in DUS, i.e. nature and object.",
        "Figure 3. Dataset statistics regarding scene complexity. Only MS COCO and Cityscapes provide instance segmentation masks.",
        "Our dataset KITTI",
        "Finally, we assess scene complexity, where density and scale of trafﬁc participants (humans and vehicles) serve as proxy measures. Out of the previously discussed datasets, only Cityscapes and KITTI provide instance-level annotations for humans and vehicles. We additionally compare to the Caltech Pedestrian Dataset [12], which only contains annotationsforhumans, butnoneforvehicles. Furthermore, KITTI and Caltech only provide instance-level annotations in terms ofaxis-aligned boundingboxes. We usethe respective training and validation splits for our analysis, since test set annotations are not publicly available for all datasets. In absolute terms, Cityscapes contains signiﬁcantly more object instance annotations than KITTI, see Tab. 2. Being a specialized benchmark, the Caltech dataset provides the most annotations for humans by a margin. The major share of those labels was obtained, however, by interpolation between a sparse set of manual annotations resulting in signiﬁcantly degraded label quality. The relative statistics emphasize the much higher complexity of Cityscapes, as the average numbers of object instances per image notably exceed those of KITTI and Caltech. We extend our analysis to MS COCO [38] and PASCAL VOC [14] that also contain street scenes while not being speciﬁc for them. We analyze the frequency of scenes with a certain number of trafﬁc participant instances, see Fig. 3. We ﬁnd our dataset to cover a greatervarietyofscenecomplexityandtohaveahigherportion of highly complex scenes than previous datasets. Using stereo data, we analyze the distribution of vehicle distances to the camera. From Fig. 4 we observe, that in comparison to KITTI, Cityscapes covers a larger distance range. We attribute this to both our higher-resolution imagery and the careful annotation procedure. As a consequence, algorithms need to take a larger range of scales and object sizes into account to score well in our benchmark.",
        "We conduct several control experiments to put our baseline results below into perspective. First, we count the relative frequency of every class label at each pixel location of the ﬁne (coarse) training annotations. Using the most frequentlabelateachpixelasaconstantpredictionirrespective of the test image (called static ﬁne, SF, and static coarse, SC) results in roughly 10% IoUclass, as shown in Tab. 3. These low scores emphasize the high diversity of our data. SC and SF having similar performance indicates the value of our additional coarse annotations. Even if the ground truth (GT) segments are re-classiﬁed using the most frequent training label (SF or SC) within each segment mask, the performance does not notably increase.",
        "Lastly, we employ 128-times subsampled annotations and retrieve the nearest training annotation in terms of the Hamming distance. The full resolution version of this training annotation is then used as prediction, resulting in 21% IoUclass. While outperforming the static predictions, the poor result demonstrates the high variability of our dataset and its demand for approaches that generalize well.",
        "Drawing on the success of deep learning algorithms, a number of semantic labeling approaches have shown very promising results and signiﬁcantly advanced the state of the art. These new approaches take enormous advantage from recently introduced large-scale datasets, e.g. PASCALContext [45] and Microsoft COCO [38]. Cityscapes aims to complement these, particularly in the context of understanding complex urban scenarios, in order to enable further research in this area.",
        "Table 4. Quantitative results of baselines for semantic labeling using the metrics presented in Sec. 3.1. The ﬁrst block lists results from our own experiments, the second from those provided by 3rd parties. All numbers are given in percent and we indicate the used training data for each method, i.e. train ﬁne, val ﬁne, coarse extra as well as a potential downscaling factor (sub) of the input image.",
        "To obtain further baseline results, we asked selected groups that have proposed state-of-the-art semantic labeling approaches to optimize their methods on our dataset and evaluated their predictions on our test set. The resulting scores are given in Tab. 4 (bottom) and qualitative examples of three selected methods are shown in Fig. 5. Interestingly enough, the performance ranking in terms of the main IoUclass score on Cityscapes is highly different from PASCAL VOC [14]. While DPN [40] is the 2nd best method on PASCAL, it is only the 6th best on Cityscapes. FCN8s [41] is last on PASCAL, but 3rd best on Cityscapes. Adelaide [37] performs consistently well on both datasets with rank 1 on PASCAL and 2 on Cityscapes.",
        "From studying these results, we draw several conclusions: (1) The amount of downscaling applied during training and testing has a strong and consistent negative inﬂuence on performance (c.f. FCN-8s vs. FCN-8s at half resolution, as well as the 2nd half of the table). The ranking according to IoUclass is strictly consistent with the degree of downscaling. We attribute this to the large scale variation present in our dataset, c.f. Fig. 4. This observation clearly indicates the demand for additional research in the direction of memory and computationally efﬁcient CNNs when facing such a large-scale dataset with high-resolution images. (2) Our novel iIoU metric treats instances of any size equally and is therefore more sensitive to errors in predicting small objects compared to the IoU. Methods that leverage a CRF for regularization [9, 40, 48, 81] tend to over smooth small objects, c.f. Fig. 5, hence show a larger drop from IoU to iIoU than [4] or FCN-8s [41]. [37] is the only exception; its speciﬁc FCN-derived pairwise terms apparently allow for a more selective regularization. (3) When considering IoUcategory, Dilated10 [79] and FCN8s [41] perform particularly well, indicating that these approaches produce comparatively many confusions between the classes within the same category, c.f. the buses in Fig. 5 (top). (4) Training FCN-8s [41] with 500 densely annotated",
        "Dataset",
        "images (750h of annotation) yields comparable IoU performance to a model trained on 20000 weakly annotated images (1300h annot.), c.f. rows 5 & 6 in Tab. 4. However, in both cases the performance is signiﬁcantly lower than FCN8s trained on all 3475 densely annotated images. Many ﬁne labels are thus important for training standard methods as well as for testing, but the performance using coarse annotationsonlydoesnotcollapseandpresentsaviableoption. (5) Since the coarse annotations do not include small or distant instances, their iIoU performance is worse. (6) Coarse labels can complement the dense labels if applying appropriate methods as evidenced by [48] outperforming [9], which it extends by exploiting both dense and weak annotations (e.g. bounding boxes). Our dataset will hopefully stimulate research on exploiting the coarse labels further, especially given the interest in this area, e.g. [25,43,47].",
        "Overall, we believe that the unique characteristics of our dataset (e.g. scale variation, amount of small objects, focus on urban street scenes) allow for more such novel insights.",
        "3.5. Cross-dataset evaluation",
        "In order to show the compatibility and complementarity ofCityscapesregardingrelateddatasets, weappliedanFCN model trained on our data to Camvid [7] and two subsets of KITTI [58,64]. We use the half-resolution model (c.f. 4th row in Tab. 4) to better match the target datasets, but we do not apply any speciﬁc training or ﬁne-tuning. In all cases, we follow the evaluation of the respective dataset to be able to compare to previously reported results [4,73]. The obtained results in Tab. 5 show that our large-scale dataset enables us to train models that are on a par with or even outperforming methods that are speciﬁcally trained on another benchmark and specialized for its test data. Further, our analysis shows that our new dataset integrates well with existing ones and allows for cross-dataset research.",
        "(i.e. using the test time ground truth). For our experiments, we rely on publicly available implementations. We train a Fast-R-CNN (FRCN) detector [20] on our training data in order to score MCG object proposals [1]. Then, we use either its output bounding boxes as (rectangular) segmentations, the associated region proposal, or its convex hull as a per-instance segmentation. The best main score AP is 4.6%, is obtained with convex hull proposals, and becomes larger when restricting the evaluation to 50% overlap or close instances. We contribute these rather low scores to our challenging dataset, biased towards busy and cluttered scenes, where many, often highly occluded, objects occur at various scales, c.f. Sec. 2. Further, the MCG bottom-up proposals seem to be unsuited for such street scenes and cause extremely low scores when requiring large overlaps. We conﬁrm this interpretation with oracle experiments, where we replace the proposals at test-time with ground truth segments or replace the FRCN classiﬁer with an oracle. In doing so, the task of object localization is decoupled from the classiﬁcation task. The results in Tab. 6 show that when bound to MCG proposals, the oracle classiﬁer is onlyslightlybetterthanFRCN.Ontheotherhand, whenthe proposals are perfect, FRCN achieves decent results. Overall, these observations unveil that the instance-level performance of our baseline is bound by the region proposals.",
        "In this work, we presented Cityscapes, a comprehensive benchmark suite that has been carefully designed to spark progress in semantic urban scene understanding by: (i) creating the largest and most diverse dataset of street scenes with high-quality and coarse annotations to date; (ii) developing a sound evaluation methodology for pixel-level and instance-level semantic labeling; (iii) providing an in-depth analysis of the characteristics of our dataset; (iv) evaluating several state-of-the-art approaches on our benchmark. To keep pace with the rapid progress in scene understanding, we plan to adapt Cityscapes to future needs over time.",
        "The signiﬁcance of Cityscapes is all the more apparent from three observations. First, the relative order of performance for state-of-the-art methods on our dataset is notably different than on more generic datasets such as PASCAL VOC. Our conclusion is that serious progress in urban scene understanding may not be achievable through such generic datasets. Second, the current state-of-the-art in semantic labeling on KITTI and CamVid is easily reached and to some extentevenoutperformedbyapplyinganoff-the-shelffullyconvolutional network [41] trained on Cityscapes only, as demonstrated in Sec. 3.5. This underlines the compatibility and unique beneﬁt of our dataset. Third, Cityscapes will pose a signiﬁcant new challenge for our ﬁeld given that it is currently far from being solved. The best performing baseline for pixel-level semantic segmentation obtains an IoU score of 67.1%, whereas the best current methods on PASCAL VOC and KITTI reach IoU levels of 77.9% [3] and 72.5% [73], respectively. In addition, the instance-level task is particularly challenging with an AP score of 4.6%.",
        "[7] G. J. Brostow, J. Fauqueur, and R. Cipolla. Semantic object classes in video: A high-deﬁnition ground truth database. Pattern Recognition Letters, 30(2):88–97, 2009. 1, 3, 4, 7, ii [8] W. Byeon, T. M. Breuel, F. Raue, and M. Liwicki. Scene Labeling with LSTM Recurrent Neural Networks. In CVPR, 2015. 6",
        "[19] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The KITTI dataset. IJRR, 32(11), 2013. 1, 3, 4, ii",
        "[25] H. Hattori, V. N. Boddeti, K. M. Kitani, and T. Kanade. Learning scene-speciﬁc pedestrian detectors without real data. In CVPR, 2015. 7",
        "[60] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. LabelMe: A database and web-based tool for image annotation. IJCV, 77(1-3):157–173, 2008. 2, i",
        "[82] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In NIPS, 2014. 1",
        "A. Related Datasets",
        "In Tab. 7 we provide a comparison to other related datasets in terms of the type of annotations, the meta information provided, the camera perspective, the type of scenes, and their size. The selected datasets are either of large scale or focus on street scenes.",
        "Figure 7 presents several examples of annotated frames from our dataset that exemplify its diversity and difﬁculty. All examples are taken from the train and val splits and were chosen by searching for the extremes in terms of the number of trafﬁc participant instances in the scene; see Fig. 7 for details.",
        "Dataset",
        "Table 7. Comparison to related datasets. We list the type of labels provided, i.e. object bounding boxes (B), dense pixel-level semantic labels (D), coarse labels (C) that do not aim to label the whole image. Further, we mark if color, video, and depth information are available. We list the camera perspective, the scene type, the number of images, and the number of semantic classes.",
        "Since a part of the vehicle from which our data was recorded is visible in all frames, it is assigned to this special label. This label is also available at test time.",
        "Table 11. Detailed results of our baseline experiments for the pixel-level semantic labeling task in terms of the IoU score on the class level. All numbers are given in percent and we indicate the used training data for each method, i.e. train ﬁne, val ﬁne, coarse extra, as well as a potential downscaling factor (sub) of the input image. See the main paper and Sec. D.1 for details on the listed methods.",
        "Table 12. Detailed results of our baseline experiments for the pixel-level semantic labeling task in terms of the instance-normalized iIoU score on the class level. All numbers are given in percent and we indicate the used training data for each method, i.e. train ﬁne, val ﬁne, coarse extra, as well as a potential downscaling factor (sub) of the input image. See the main paper and Sec. D.1 for details on the listed methods."
    ]
}