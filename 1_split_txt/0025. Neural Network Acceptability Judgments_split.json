{
    "title_author_abstract_introduction": "Neural Network Acceptability Judgments\nAlex Warstadt New York University warstadt@nyu.edu\nAmanpreet Singh New York University Facebook AI Research∗ amanpreet@nyu.edu\nSamuel R. Bowman New York University bowman@nyu.edu\nAbstract\nThis paper investigates the ability of artiﬁ- cial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence. We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. As baselines, we train several recurrent neural network models on acceptability classiﬁ- cation, and ﬁnd that our models outperform unsupervised models by Lau et al. (2016) on CoLA. Error-analysis on speciﬁc grammatical phenomena reveals that both Lau et al.’s models and ours learn systematic generalizations like subject-verb-object order. However, all models we test perform far below human level on a wide range of grammatical constructions.\nIntroduction\nArtiﬁcial neural networks (ANNs) achieve a high degree of competence on many applied natural language understanding (NLU) tasks, but this does not entail that they have knowledge of grammar. A key property of a human’s linguistic competence is the ability to identify in one’s native language, without formal training in grammar, a contrast in acceptability1 between pairs of sentences like those in (1). Acceptability judgments like these are the primary behavioral measure that generative linguists use to observe humans’ grammatical knowledge (Chomsky, 1957; Schütze, 1996).\n(1) a. What did Betsy paint a picture of?\nb. *What was a picture of painted by Betsy?\n∗Current afﬁliation. This work was completed when the\nauthor was at New York University.\n1Following terminological conventions in linguistics, a sentence’s grammaticality is determined by a grammatical formalism, while its acceptability is determined by introspective judgments of native speakers (Schütze, 1996).\nWe train neural networks to perform acceptability judgments—following work by Lawrence et al. (2000), Lau et al. (2016), and others—in order to evaluate their acquisition of the kinds of grammatical concepts linguists identify as central to human linguistic competence. This contributes to a growing effort to test ANNs’ ability to make ﬁnegrained grammatical distinctions (Linzen et al., 2016; Adi et al., 2017; Conneau et al., 2018; Ettingeretal.,2018;MarvinandLinzen,2018). This research program seeks to provide new informative ways to evaluate ANN models popular with engineers. Furthermore, it has the potential to address foundational questions in theoretical linguistics by investigating how well unbiased learners can acquire grammatical knowledge.\nIn this paper we make four concrete contributions: (i) We introduce the Corpus of Linguistic Acceptability (CoLA), a collection of sentences from the linguistics literature with expert acceptability labels which, at over 10k examples, is by far the largest of its kind. (ii) We train several semi-supervised neural sequence models to do acceptability classiﬁcation on CoLA and compare their performance with unsupervised models from Lau et al. (2016). Our best model outperforms unsupervised baselines, but falls short of human performance on CoLA by a wide margin. (iii) We analyze the impact of supervised training on acceptability classiﬁers by varying the domain and quantity of training data. (iv) We assess our models’ performance on acceptability classiﬁcation of speciﬁc linguistic phenomena. These experiments illustrate how acceptability classiﬁcation and CoLA can give detailed insights into what grammatical knowledge typical neural network models can acquire. We ﬁnd that our models do not show evidence of learning non-local dependencies related to agreement and questions, but do appear to acquire knowledge about basic subject-verb-object word order and verbal argument structure.\nIncluded\nExcluded\nMorphological Violation Syntactic Violation Semantic Violation\nPragmatical Anomalies Unavailable Meanings Prescriptive Rules Nonce Words\n(a) (b) (c)\n(d) (e) (f) (g)\n*Maryann should leaving. *What did Bill buy potatoes and ? *Kim persuaded it to rain.\n*Bill fell off the ladder in an hour. *Hei loves Johni. (intended: John loves himself.) Prepositions are good to end sentences with. *This train is arrivable.\nTable 1: Our informal classiﬁcation of unnacceptable sentences, shown with their presence or absence in CoLA.\nResources CoLA can be downloaded from the corpus website.2 The code for training our baselines is available as well.3 There are also two competition sites for evaluating acceptability classiﬁers on CoLA’s in-domain4 and out-of-domain5 test sets (unlabeled). Finally, CoLA is included in the GLUE benchmark6 (Wang et al., 2018), which also hosts CoLA training data, (unlabeled) test data, and a leaderboard.",
    "data_related_paragraphs": [
        "Datasets for acceptability classiﬁcation require a source of unacceptable sentences, which are not generally found in naturalistic speech or writing bynativespeakers. ThesentencesinCoLAconsist entirely of examples from the linguistics literature. Lawrence et al. (2000) and Lau et al. (2016) build datasets similar in this respect. However, at over 10k sentences, CoLA is by far the largest dataset of this kind, and represents the widest range of sources. Prior work in this area also obtains unacceptable sentences by programmatically generating fake sentences that are unlikely to be acceptable. Wagner et al. (2009) distort real sentences by, for example, deleting words, inserting words, or altering verbal inﬂection. Lau et al. (2016) use round-trip machine-translation from English into various languages and back. We also generate fake sentences to pre-train our baselines before further training on CoLA.",
        "We leave a comparison of this methodology with our own for future work. We settle on the single-sentence judgment task because it is directly comparable with methodology in generative linguistics. While some work in theoretical linguists presents acceptability judgments as a ranking of two or more sentences (Schütze, 1996, pp. 77-81), Boolean judgments are still the norm, and the dominant current theories still make Boolean predictions about whether a sentence is or is not grammatical (Chomsky, 1995, pp. 12- 16). Accordingly, CoLA, but not datasets based solely on preferences between minimal pairs, may be used to evaluate models’ ability to make judgments that align with both native speaker judgments and the predictions of generative theories.",
        "ThispaperintroducestheCorpusofLinguisticAcceptability (CoLA), a set of example sentences from the linguistics literature labeled for acceptability. CoLA is available online, alongside source code for our baseline models, and a leaderboard showing model performance on test data using privately-held labels (see footnotes 2-6 for links).",
        "Preparing the Data The corpus includes all usable examples from each source. We manually remove unacceptable examples falling into any of the excluded categories described in Section 2.4. The labels in the corpus are the original authors’ acceptability judgments whenever possible. When examples appear with non-Boolean judgments (this occurs in less than 3% of cases), we either exclude them (for labels ‘?’ or ‘#’), or label them unacceptable (‘??’ and ‘*?’). We also expand examples with optional or alternate phrases into multiple data points, e.g. Betsy buttered (*at) the toast becomes Betsy buttered the toast and *Betsy buttered at the toast.",
        "Splitting the Data the train/development/test split used to control overﬁtting in standard benchmark datasets, CoLA is further divided into an in-domain set and an out-of-domain set, as speciﬁed in Table 2. The is constructed to be about out-of-domain set 10% the size of CoLA and to include sources of varying sizes, degrees of domain speciﬁcity, and time period.7 The in-domain set is split three ways into training (8551 examples), development (527), and test sets (530), all drawn from the same 17 sources. The out-of-domain set is split into development (516) and a test sets (533), drawn from another 6 sources. We split CoLA in this way in order to monitor two types of overﬁtting during training: overﬁtting to the speciﬁc sentences in the training set (in-domain), and overﬁtting to the speciﬁc sources and phenomena represented in the training set (out-of-domain).",
        "7InSection6weconsiderseveralalternatesplitsofCoLA. 8The annotated data also includes 63 ﬁne-grained features. The annotated data is available for download on the CoLA website alongside detailed annotation guidelines.",
        "upon set of key phenomena in linguistics and any attempt to create one is likely to be controversial and overly simplistic. Furthermore, if such a set of phenomena did exist, the builders of a balanced dataset must decide whether it should be balanced equally across phenomena, or weighted by either the frequency in broad coverage corpora of English or the number of distinguishing syntactic contrasts associated with each phenomenon. We assume that CoLA skews towards the latter, as a major goal of linguistics articles is to document key unique facts about some phenomenon without excessive repetition.",
        "Figure 1 shows the frequency of these 8 features in the development set. Argument alternations are the best represented phenomenon and appear in over 40% of sentences in this sample. This isduebothtothehighfrequencyoftheseconstructions as well as the inclusion of several sources directly addressing this topic (Levin, 1993; Collins, 2005; Rappaport Hovav and Levin, 2008). Most other constructions appear in about 10-20% of sentences, indicating that CoLA is fairly balanced according to this annotation scheme. There are likely biases in CoLA that other annotation schemes could detect. However, it is open to debate what a balanced dataset for acceptability judgments should look like. There is no agreed",
        "We compare our models to a continuous bag of words (CBOW) baseline, the unsupervised models proposed by (Lau et al., 2016), and human performance. To make these comparisons more meaningful, we avoid giving our models distinct advantages over human learners by limiting the training data in two ways: (i) Aside from acceptability labels, our training has no grammatical annotation. (ii) Our large sentence encoders are limited to 100-200 million tokens of training data, which is within a factor of ten of the number of tokens human learners are exposed to during language",
        "acquisition (Hart and Risley, 1992).11 We avoid trainingmodelsonsigniﬁcantlymoredatabecause such models have a distinct advantage over the human learners we aim to match.",
        "Language Model We use an LSTM language model (LSTM LM) at various stages in our experiments: (i) Several of our models use word embeddings or hidden states from the LM as input. (ii) The LM generates fake data for the real/fake task. (iii) The LM is an integral part of our implementation of the method proposed by (Lau et al., 2016). We train the LM on the 100 million-token BritishNationalCorpus(BNC).Itlearnswordembeddings from scratch for the 100k most frequent words in the BNC (with out of vocabulary words replaced by <unk>). We lowercase and tokenize theBNCdatausingNLTK(BirdandLoper,2004). The LM achieves a word-level perplexity of 56.1 on the BNC.",
        "Real/Fake Auxiliary Task We train sentence encoders on a real/fake task in which the objective is to distinguish real sentences from the BNC and “fake” English sentences automatically generated by two strategies: (i) We sample strings, e.g. (2-a), from the LSTM LM. (ii) We manipulate sentences of the BNC, e.g. (2-b), by randomly permuting a subset of the words, keeping the other words in situ. Training data includes the entire BNC and an equal amount of fake data. We lowercase and",
        "Lau et al. (2016) We compare our models to those of Lau et al. (2016). Their models obtain an acceptability prediction from unsupervised LMs by normalizing the LM output using one of several metrics. Following their recommendation, we use the Word LogProb Min-1 metric.13 Since this metric produces unbounded scalar scores rather than probabilities or Boolean judgments, we ﬁt a threshold to the outputs in order to use these models as acceptability classiﬁers. This is done with 10-fold cross-validation on the CoLA test set: We repeatedly ﬁnd the optimum threshold for 90% of the model outputs and evaluate the remaining 10% with that threshold, until all the data have been evaluated. Following their methods, we train ngram models on the BNC using their published code.14 In place of their RNN LM, we use the same LSTM LM that we use to generate sentences",
        "tokenize all real/fake data and replace out of vocabulary words as in LM training.",
        "We train 20 pooling classiﬁers end-to-end on real/fake data with BNC embeddings, 20 with GloVe, and 20 with ELMo-style embeddings for up to 7 days or until completing 4 epochs without improving in development MCC. We train 20 pooling classiﬁers end-to-end on CoLA using ELMo-style embeddings. Hyperparameters are chosen at random in these ranges: embedding size",
        "fundamental ingredients of grammaticality from unlabeled data.",
        "The results in the previous section highlight the effects of pretraining, but give little insight into how the labeled training data in CoLA impacts classiﬁer performance. To quantify the impact of CoLA training, we conduct two additional experiments: First, we measure how the amount of training data impactsmodelperformanceontheCoLAdevelopment set. Second, we investigate how the speciﬁc contents of the in-domain and out-of-domain sets impact model generalization.",
        "Training Set Size In this experiment, we vary the amount of training data seen by our acceptability classiﬁers. We construct alternate training sets of sizes 100, 300, 1000, and 3000 by randomly downsampling the 8551-example CoLA training set. Then, for each training set we train classiﬁers with 20 restarts using the best performing ELMo-style real/fake encoder, and evaluate on the entire development set. Figure 3 plots the results. As training data increases from 100 to 8551 sentences, we see approximately log-linear improvements in performance. The small decrease in performance between 1000 and 3000 sentences is likely an artifact of the random downsampling. From these results we draw two main conclusions: First, it appears that increasing the amount of training data in CoLA by an order of magnitude may signiﬁcantly beneﬁt our models. Second, much of what our models learn from CoLA can be learned from as few as 300 training examples. This suggests that CoLA training is not teaching our models speciﬁc facts about acceptability as much as teaching them to use existing grammatical knowledge from the sentence encoders.",
        "Splitting CoLA Our results in Table 4 show that our models’ performance drops noticeably when tested on out-of-domain sentences from publications not represented in the training data. In this experiment, we investigate different splits of CoLA into in-domain and out-of-domain to test the degree to which the decrease in performance on out-of-domain sentences is a stable property of these models, or simply an artifact of the particular publications represented in the out-of-domain set (as described in section 3).",
        "ing sources from the 23 sources from CoLA to hold out until the sum of their sizes exceeds 750. This gives out-of-domain set sizes ranging from 789 to 1539, consisting of 2 to 6 sources. CoLA’s original out-of-domain set contains 1049 examples and 6 sources. Development and test sets are constructed by randomly splitting the out-ofdomain data in half, and randomly selecting an approximately equal number of in-domain sentences. For each training set we train classiﬁers with 20 restarts using the encoder from the best performing ELMo-style real/fake classiﬁer.",
        "Wh-Extraction This test set consists of 260 pairs of contrasting examples, as in (4). This is to test (i) whether a model has learned that a whwordmustcorrespondtoagapinthesentence, and (ii) whether the model can identify non-local dependencies up to three words away. The data contain 10 ﬁrst names as subjects and 8 sets of verbs and related objects (5). Every compatible verbobject pair appears with every subject.",
        "Here, we run additional evaluations to probe whether our models can reliably classify sets of sentences which target a single grammatical contrast. This kind of evaluation can give insight into what kinds of grammatical features our models do and do not acquire easily. Using data generation techniques inspired by Ettinger et al. (2016), we build ﬁve auxiliary datasets (described below) using simple rewrite grammars which target speciﬁc grammatical contrasts.",
        "Reﬂexive-Antecedent Agreement This test set probes whether a model has learned that every reﬂexive pronouns must agree with an antecedent noun phrase in person, number, and gender. The dataset consists of a set of 4 verbs crossed with 6 subject pronouns and 6 reﬂexive pronouns, giving 144 sentences, only 1 out of 6 acceptable.",
        "In the setting of machine learning, the APS leaner trained with predicts that any artiﬁcial no prior knowledge of the principles of syntax and no more data than a human child sees must fail to make acceptability judgments with human-level accuracy (Clark and Lappin, 2011). If linguistically-uninformed neural network models achieve human-level performance on speciﬁc phenomena or on a domain-general dataset like CoLA, this would be clear evidence limiting the scope of phenomena for which the APS can hold. However, acceptability classiﬁcation alone can-",
        "Still, we maintain that our approach is a valuable step in the direction of evaluating the APS. Our results strongly suggest that grammatically unbiased sentence embeddings and contextualized word embeddings have non-trivial implicit knowledge of grammar before supervised training on CoLA. As our experiments in Section 6 show, a signiﬁcant portion of what these models learn from CoLA can be learned from relatively little acceptability judgment data (as few as 300 sentences, of which fewer than 100 are unacceptable). Furthermore, the real/fake encoders and ELMo-style embeddings are trained on a quantity of data comparable to what human learners are exposed to. Given the rapid pace of development of new robust sentence embeddings, we expect to see increasingly human-like acceptability judgments from powerful neural networks in coming years, though with an eye towards evaluating the APS, future work should continue to investigate acceptability classiﬁers with unsupervised methods and restricted training resources.",
        "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680.",
        "Steve Lawrence, C. Lee Giles, and Sandiway Fong. 2000. Natural language grammatical inIEEE ference with recurrent neural networks. Transactions on Knowledge and Data Engineering, 12(1):126–140.",
        "Jon Sprouse and Diogo Almeida. 2012. Assessing the reliability of textbook data in syntax: Adger’s Core Syntax. Journal of Linguistics, 48(3):609–652."
    ]
}