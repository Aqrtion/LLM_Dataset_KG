{
    "title_author_abstract_introduction": "Evaluating Large Language Models Trained on Code\nMark Chen * 1 Jerry Tworek * 1 Heewoo Jun * 1 Qiming Yuan * 1 Henrique Ponde de Oliveira Pinto * 1 Jared Kaplan * 2 Harri Edwards 1 Yuri Burda 1 Nicholas Joseph 2 Greg Brockman 1 Alex Ray 1 Raul Puri 1 Gretchen Krueger 1 Michael Petrov 1 Heidy Khlaaf 3 Girish Sastry 1 Pamela Mishkin 1 Brooke Chan 1 Scott Gray 1 Nick Ryder 1 Mikhail Pavlov 1 Alethea Power 1 Lukasz Kaiser 1 Mohammad Bavarian 1 Clemens Winter 1 Philippe Tillet 1 Felipe Petroski Such 1 Dave Cummings 1 Matthias Plappert 1 Fotios Chantzis 1 Elizabeth Barnes 1 Ariel Herbert-Voss 1 William Hebgen Guss 1 Alex Nichol 1 Alex Paino 1 Nikolas Tezak 1 Jie Tang 1 Igor Babuschkin 1 Suchir Balaji 1 Shantanu Jain 1 William Saunders 1 Christopher Hesse 1 Andrew N. Carr 1 Jan Leike 1 Josh Achiam 1 Vedant Misra 1 Evan Morikawa 1 Alec Radford 1 Matthew Knight 1 Miles Brundage 1 Mira Murati 1 Katie Mayer 1 Peter Welinder 1 Bob McGrew 1 Dario Amodei 2 Sam McCandlish 2 Ilya Sutskever 1 Wojciech Zaremba 1\nAbstract\n1. Introduction\nWe introduce Codex, a GPT language model ﬁnetuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we ﬁnd that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difﬁcult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difﬁculty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.\n*Equal contribution 1OpenAI, San Francisco, California, USA. 2Anthropic AI, San Francisco, California, USA. Work per-\nformed while at OpenAI.\n3Zipline, South San Francisco, California, USA. Work per-\nformed while at OpenAI.\nCorrespondence to: Mark Chen <mark@openai.com>, Jun <hee-\nJerry Tworek <jt@openai.com>, Heewoo woo@openai.com>, Qiming Yuan <qiming@openai.com>.\nScalable sequence prediction models (Graves, 2014; Vaswani et al., 2017; Child et al., 2019) have become a general-purpose method for generation and representation learning in many domains, including natural language processing (Mikolov et al., 2013; Sutskever et al., 2014; Dai & Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin etal.,2018), computervision(VanOordetal.,2016;Menick & Kalchbrenner, 2018; Chen et al., 2020; Bao et al., 2021), audio and speech processing (Oord et al., 2016; 2018; Dhariwal et al., 2020; Baevski et al., 2020), biology (Alley et al., 2019; Rives et al., 2021), and even across multiple modalities (Das et al., 2017; Lu et al., 2019; Ramesh et al., 2021; Zellers et al., 2021). More recently, language models have also fueled progress towards the longstanding challenge of program synthesis (Simon, 1963; Manna & Waldinger, 1971), spurred by the presence of code in large datasets (Husain et al., 2019; Gao et al., 2020) and the resulting programming capabilities of language models trained on these datasets (Wang & Komatsuzaki, 2021). Popular language modelingobjectiveslikemaskedlanguagemodeling(Devlin et al., 2018) and span prediction (Raffel et al., 2020) have also been adapted to train their programming counterparts CodeBERT (Feng et al., 2020) and PyMT5 (Clement et al., 2020).\nSimilarly, our early investigation of GPT-3 (Brown et al., 2020) revealed that it could generate simple programs from Python docstrings. While rudimentary, this capability was exciting because GPT-3 was not explicitly trained for code generation. Given the considerable success of large language models in other modalities and the abundance of publicly available code, we hypothesized that a specialized GPT model, called Codex, could excel at a variety of coding tasks. This paper describes several early Codex models, whose descendants power GitHub Copilot and the Codex models in the OpenAI API.\nEvaluating Large Language Models Trained on Code\ngenerate at least one correct function for 77.5% of the problems. This result suggests that accurate code samples can be selected via heuristic ranking instead of fully evaluating each sample, the latter of which may not be possible or practical in deployment. Indeed, we ﬁnd that the sample with highest mean log-probability passes unit tests for 44.5% of the problems.\nWe conclude by discussing the limitations and potential broader impacts of these Codex models and of increasingly powerful code generating models more generally.",
    "data_related_paragraphs": [
        "In this section, we discuss the details of our evaluation framework. We begin by deﬁning the pass@k metric, and explain its advantages over standard match-based metrics. Next, we describe the dataset of hand-written problems, called “HumanEval,” which we created in order to benchmark our models. Finally, we discuss the sandbox environment we used to safely execute model-generated code.",
        "Figure 1. Pass rates of our models on the HumanEval dataset as a function of model size. When a single sample is generated for each problem, GPT-12B solves no problems, but Codex (ﬁne-tuned on code) solves 28.8% of the problems, and Codex-S (further ﬁne-tuned on correctly implemented standalone functions) solves 37.7% of the problems. From here, further gains can be realized by generating 100 samples per problem and selecting the sample with the highest mean log-probability (44.5% solved) or by selecting the sample that passes the unit tests (77.5% solved). All samples are generated with temperature 0.8.",
        "In this work, we focus on the task of generating standalone Python functions from docstrings, and evaluate the correctness of code samples automatically through unit tests. This is in contrast to natural language generation, where samples are typically evaluated by heuristics or by human evaluators. To accurately benchmark our model, we create a dataset of 164 original programming problems with unit tests. These problems assess language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions. We release this data along with an evaluation framework at https://www.github.com/openai/human-eval.",
        "To solve a problem in our test set, we generate multiple samples from the models, and check if any of them pass the unit tests. With just a single sample, a 12B parameter Codex solves 28.8% of these problems, and a 300M parameter Codex solves 13.2% of these problems. In contrast, the 6B parameter GPT-J (Wang & Komatsuzaki, 2021) achieves 11.4% on the same dataset, while all GPT models achieve near 0%. To improve our model’s performance at the task of function synthesis from docstrings, we ﬁne-tune Codex on standalone, correctly implemented functions. The resulting model, Codex-S, solves 37.7% of problems with a single sample. Figure 2 showcases problems of varying difﬁculty in our dataset, along with correct model generated solutions.",
        "Figure 2. Three example problems from the HumanEval dataset, where the probabilities that a single sample from Codex-12B passes unit tests are 0.9, 0.17, and 0.005. The prompt provided to the model is shown with a white background, and a successful model-generated completion is shown in a yellow background. Though not a guarantee for problem novelty, all problems were hand-written and not programmatically copied from existing sources. Random problems and samples can be found in Appendix B.",
        "We evaluate functional correctness on a set of 164 handwritten programming problems, which we call the HumanEval dataset. Each problem includes a function signature, docstring, body, and several unit tests, with an average of 7.7 tests per problem. It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources. For example, there are more than ten public repositories containing solutions to Codeforces problems, which make up part of the recently proposed APPS dataset (Hendrycks et al., 2021).",
        "Programming tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms, and simple mathematics. We release the HumanEval dataset so that others can evaluate functional correctness and measure the problem-solving capabilities of their models. The dataset can be found at https://www.github.com/openai/human-eval.",
        "Therefore, we developed a sandbox environment to safely run untrusted programs against unit tests. Our goals were to prevent these programs from modifying, gaining persistence on, accessingsensitiveresourceson, orexﬁltratingdatafrom a host or network. Since OpenAI’s training infrastructure is built on Kubernetes and cloud services, we designed our sandbox to address the limitations of these environments while remaining idiomatic with their patterns of use.",
        "We ﬁne-tune GPT models containing up to 12B parameters on code to produce Codex. In contrast with GPT, Codex displays non-trivial performance on the HumanEval dataset. In fact, Codex is able to solve the majority of the problems in HumanEval if we generate and evaluate 100 samples per",
        "3.1. Data Collection",
        "Our training dataset was collected in May 2020 from 54 million public software repositories hosted on GitHub, containing 179 GB of unique Python ﬁles under 1 MB. We ﬁltered out ﬁles which were likely auto-generated, had average line length greater than 100, had maximum line length greater than 1000, or contained a small percentage of alphanumeric characters. After ﬁltering, our ﬁnal dataset totaled 159 GB.",
        "Since Codex is evaluated on natural language prompts, we hypothesized that it would be beneﬁcial to ﬁne-tune from the GPT-3 (Brown et al., 2020) model family, which already contains strong natural language representations. Surprisingly, we did not observe improvements when starting from a pre-trained language model, possibly because the ﬁnetuning dataset is so large. Nevertheless, models ﬁne-tuned from GPT converge more quickly, so we apply this strategy for all subsequent experiments.",
        "3.5. Results on the APPS Dataset",
        "Two recent works similar in spirit to Codex are GPT-Neo (Black et al., 2021) and GPT-J (Wang & Komatsuzaki, 2021), which are trained on The Pile (Gao et al., 2020), a dataset containing text from a variety of sources as well as 8% GitHub code. The broader research community has found that these models outperform existing GPT systems in qualitative programming evaluations (Woolf, 2021).",
        "We conﬁrm these ﬁndings using the HumanEval dataset, showing that GPT-Neo achieves 6.4% pass@1 and 21.3% pass@100, while GPT models of comparable sizes achieve near 0% on both metrics. We see a remarkable progression in capabilities, with GPT-Neo-2.7B roughly equivalent to Codex-85M (30× fewer parameters). Similarly, GPT-J-6B achieves 11.6% pass@1 and 27.7% pass@100, which is roughly equivalent to Codex-300M (20× fewer parameters). Pass rates are obtained by taking the best result from eval-",
        "Recently, Hendrycks et al. (2021) introduced the APPS dataset to measure the coding challenge competence of language models. The APPS dataset consists of 5000 training and 5000 test examples of coding problems, each with a set of unit tests and, for the training data, a set of correct solutions. Most of the APPS tests problems are not formulated as single-function synthesis tasks, but rather as full-program synthesis, reading input from stdin and printing output to stdout, in contrast to the main Codex training data.",
        "• In coding competitions and in the APPS datasets, tasks are provided with 3 input/output examples included in the task description. We utilize this by sampling 1000 solutions from the model and ﬁltering out only those that pass these 3 unit tests (if such solutions exist). We then calculate pass rates in this ﬁltered set, and call it ﬁltered pass@k. Results without ﬁltering are presented as raw pass@k.",
        "In addition to standalone functions, Python code found on GitHub contains class implementations, conﬁguration ﬁles, scripts, and even ﬁles used to store data. This code is seemingly unrelated to synthesizing functions from docstrings, and we hypothesize that the distribution mismatch reduces HumanEval performance.",
        "Next, we curated programming problems from open source projects. Taking advantage of sys.setprofile, we were able to trace and collect inputs and outputs for all functions called during integration tests. This data could then be used to create unit tests for the functions.",
        "Since our tracing methodology produced inputs and outputs for all invoked functions, even builtin and library calls imported by the project were turned into problems. For this reason, functions from tracing tended to be the building blocks of command-line utilities. To excel at these tasks, the model does not need to know advanced algorithms and data structures. Rather, it needs to be able to follow instructions to implement the functionality speciﬁed in the docstring. Thus, tracing complements the puzzle nature of coding competition problems and broadens the distribution of tasks.",
        "Generating code from docstrings is possible with Codex because code typically follows after a docstring, but it is not easy to induce Codex to generate docstrings from code. Nevertheless, we are motivated to produce a docstring writing model for safety reasons, as such a model can be used to describe the intent behind generated code. Using the training problems described in the previous section, we can easily create a training dataset for code-conditional docstring generation.",
        "When we benchmark our code generation models, we measure pass@k on the HumanEval dataset, where correctness is deﬁned by passing a set of unit tests. However, there is no similar way to evaluate docstring samples automatically. Therefore, we grade sample docstrings by hand, considering a docstring correct if it uniquely and accurately speciﬁes the code body. Due to the time consuming nature of this process, we only grade 10 samples per problem, for a total of 1640 problems, from Codex-D-12B at temperature 0.8.",
        "As shown in Table 3, pass rates for Codex-D are lower but comparable to the corresponding pass rates for Codex-S at the same temperature. We do not have a strong hypothesis for which direction should yield higher pass rates. While generating docstrings may be more forgiving because natural language syntax is less strict than code syntax, docstrings in our dataset may be lower quality because developers tend to devote less time to writing docstrings. Indeed, our model produces docstrings like “I just found this function online” and “This test is not correctly written and it’s not my solution.”",
        "First, Codex is not sample efﬁcient to train. Our training dataset comprises a signiﬁcant fraction of publicly available Python code on GitHub, totaling hundreds of millions of lines of code. Even seasoned developers do not encounter anywhere near this amount of code over their careers. Indeed, a strong student who completes an introductory computer science course is expected to be able to solve a larger fraction of problems than Codex-12B.",
        "To concretely illustrate model performance degradation as docstring length increases, we create a dataset of synthetic problems assembled from 13 basic building blocks, each of which modiﬁes an input string in a deterministic way. Example building blocks are “convert the string to lowercase” or “remove every third character from the string” (the full",
        "It is important to study misalignment because it is a problem that is likely to become worse, not better, as the capabilities of our systems increase. For example, the model size scaling trend for the example in Figure 12 indicates that misalignment would likely persist and even get worse if data, parameters, and training time were scaled up.",
        "Mirroring what has been found in the case of other language models trained on Internet data (Bender et al., 2021; Blodgett et al., 2020; Abid et al., 2021; Brown et al., 2020), we found that Codex can be prompted in ways that generate racist, denigratory, and otherwise harmful outputs as code comments, meriting interventions such as those discussed in the subsection on risk mitigation below. We also found that code generation models raise further bias and representation issues beyond problematic natural language: Codex can generate code with structure that reﬂects stereotypes about gender, race, emotion, class, the structure of names, and other characteristics. Particularly in the context of users who might over-rely on Codex or use it without ﬁrst thinking through project design, this issue could have signiﬁcant safety implications, giving further motivation to discourage over-reliance. We discuss bias and representation issues further in Appendix F. Filtration or modulation of generated outputs, documentation, and other interventions may help to mitigate these risks.",
        "Similar to large language models, Codex models can learn patterns present in their training data (Carlini et al., 2021). Sensitive data present in source code are liable to be predicted by the model. Because Codex is trained on public repositories, we consider any sensitive data present in the training data to have already been compromised. Similarly, the public data should generally be treated as untrusted, as previous work (Goldblum et al., 2021; Schuster et al., 2020) has found that attackers may be able to corrupt training data to trigger speciﬁc model behaviors at runtime. We further discuss security implications in Appendix G.",
        "There are several legal considerations related to generated code. To begin with, the training of AI systems on Internet data, such as public GitHub repositories, has previously been identiﬁed as an instance of “fair use” (O’Keefe et al., 2019).",
        "OurpreliminaryresearchalsoﬁndsthatCodexmodelsrarely generate code that is identical to the contents of training data. Such occurrences were < 0.1% in a study examining the frequency of code generations that appear to match code snippets in the training data (Ziegler, 2021). In these rare instances, the generated code consisted of common expressions or conventions within the programming language that appeared over and over again in the training data. We ﬁnd that, to the extent the generated code appears identical to the training data, it is due to the predictive weightings in the model rather than retention and copying of speciﬁc code.",
        "7Microsoft made a commitment in 2020 to shift to 100 percent renewable energy supply in its buildings and data centers by 2025. https://blogs.microsoft.com/blog/2020/01/16/microsoftwill-be-carbon-negative-by-2030/ A full assessment of the environmental impact of compute use is impossible to conduct without grounding in context and making comparison to the counterfactual impacts of competing products or services. Such analysis is out of scope for this paper.",
        "8While data center energy usage has become much more efﬁ- cient in recent years (Masanet et al., 2020), the production, use, and disposal of semiconductors still imposes environmental and human costs. See, e.g., (Crawford, 2021)",
        "Two early domain-speciﬁc datasets used to benchmark neu-",
        "ral programming systems were FlashFill (Gulwani, 2011; Gulwani et al., 2012) and Hearthstone (Ling et al., 2016), though the community has trended towards broader and more difﬁcult datasets. Barone & Sennrich (2017) proposed a large training and evaluation dataset consisting of Python declarations, docstrings, and bodies scraped from GitHub. The CodeSearchNet challenge (Husain et al., 2019) built an even larger corpus from GitHub with data from multiple popular programming languages. Recently, CodeXGLUE (Lu et al., 2021) aggregated several programming benchmarks, making use of the recently proposed CodeBLEU metric (Ren et al., 2020). Most relevant to our evaluation work is the APPS (Hendrycks et al., 2021) benchmark for measuring functional correctness based on problems from the competitive programming website Codeforces.",
        "We investigated whether it was possible to train large language models to produce functionally correct code bodies from natural language docstrings. By ﬁne-tuning GPT on code from GitHub, we found that our models displayed strong performance on a dataset of human-written problems with difﬁculty level comparable to easy interview problems. Model performance could be improved by training on a distribution more similar to the evaluation set, and also by producing multiple samples from a model. We also found that it was simple to train a model to complete the reverse",
        "Cwe-327: Use of a broken or risky cryptographic algorithm, 2006. URL https://cwe.mitre.org/data/definitions/ 327.html.",
        "Cwe-780: Use of rsa algorithm without oaep, 2009. URL https: //cwe.mitre.org/data/definitions/780.html.",
        "Agrawal, H., Horgan, J. R., London, S., and Wong, W. E. Fault localization using execution slices and dataﬂow tests. Proceedings of Sixth International Symposium on Software Reliability Engineering. ISSRE’95, pp. 143–151, 1995.",
        "Aye, G. A., Kim, S., and Li, H. Learning autocompletion from realworld datasets. 2021 IEEE/ACM 43rd International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), pp. 131–139, 2021.",
        "Carlini, N., Tram`er, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel, C. Extracting training data from In 30th USENIX Security Sympolarge language models. sium (USENIX Security 21). USENIX Association, August 2021. URL https://www.usenix.org/conference/",
        "Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C. The pile: An 800gb dataset of diverse text for language modeling. 2020.",
        "Goldblum, M., Tsipras, D., Xie, C., Chen, X., Schwarzschild, A., Song, D., Madry, A., Li, B., and Goldstein, T. Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses, 2021.",
        "Gulwani, S., Harris, W. R., and Singh, R. Spreadsheet data manipulation using examples. Commun. ACM, 55:97–105, 2012.",
        "Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., Gong, M., Zhou, M., Duan, N., Sundaresan, N., Deng, S. K., Fu, S., and Liu, S. Codexglue: A machine learning benchmark dataset for code understanding and generation. ArXiv, abs/2102.04664, 2021.",
        "Masanet, E., Shehabi, A., Lei, N., Smith, S., and Koomey, J. Recalibrating global data center energy-use estimates. Science, 367(6481):984–986, 2020.",
        "– Weak Fairness: every process that is almost always enabled should be executed inﬁnitely often – Mutual exclusion, atomicity, and synchronization – Freedom from race conditions and data races",
        "There are several subtleties here; probably the most important one is distinguishing our observations from a robustness failure. If the subtly buggy code is sufﬁciently out-of-distribution, we might observe that the model performs worse in these cases, simply because it is thrown off by the OOD input - it is not in fact capable of outputting good code after seeing OOD prompts. We believe this is unlikely to be a large factor here, as the GitHub dataset contains plenty of poor-quality code. The bugs are designed to be of the sort we’d expect to appear commonly in the dataset; code that compiles and often runs without errors but gives an incorrect answer. Examples include off-by-one errors or single-character typographic errors.",
        "We hope that measuring (and improving) alignment will become standard practice for research on powerful ML models. The datasets used for these evaluations are available at https://github.com/openai/code-align-evals-data.",
        "Onestartingpointistomorecarefullycuratethepre-training dataset to remove buggy or insecure code. Another possibility is to label the pre-training data based on code quality, then condition the model on the ’high quality’ label at deployment time (Keskar et al., 2019).",
        "A common approach to adjusting the behavior of Transformers is to ﬁne-tune large pre-trained models with curated or human-generated datasets of the desired behavior (e.g., Raffel et al. (2020); He et al. (2020)). In this case we might want to ﬁne-tune on a dataset of high-quality, bug-free code. However, it is notoriously difﬁcult for most humans to write bug-free code, so rather than acquiring this dataset through labeling it might need to be obtained by ﬁltering input datasets using formal analysis or other metrics of code quality.",
        "• It can be made to perform task X by prompt engineering, by ﬁne-tuning on a much smaller quantity of data than used in pre-training, by model surgery, or some other technique which harnesses capabilities latent in the model rather than adding new capabilities; or",
        "In the context of code models, this would involve collecting data from human labelers on whether generations were correct and helpful. Assisting human labelers with existing automated testing and formal veriﬁcation tools, or even tools built with the code-generating models themselves, may be useful for providing a correct reward signal for RL or expert iteration.",
        "The alignment evaluations are based on the HumanEval dataset described earlier in the paper: 158 problems with a docstring describing the task, reference solution, and tests. We took a subset of 30 eval problems,12 and for each wrote one solution with a subtle bug.",
        "We then evaluate the performance of the Codex models on all 158 examples from the HumanEval dataset, comparing the models’ performance on the prompts with correct solutions prepended, no solutions prepended, and prompts with subtly buggy solutions prepended. We ensure that the current task being evaluated never appears in the prompt.",
        "The datasets are available at https://github.com/openai/codealign-evals-data.",
        "These representations are learned not just from training data that encodes social biases but also code written to process",
        "and analyze datasets that encode classes in potentially harmful ways.",
        "In addition to generating semantically meaningful source code, Codex can also be used to produce text, e.g. in the form of comments or docstrings. Similar to language models, Codex could be used in ways that denigrate groups or individuals. A priori, one might expect that ﬁne-tuning on a dataset of code would decrease the extent to which comments would produce blatantly prejudiced text, as code comments are typically more neutral than the distribution of text on the Internet.15 On the other hand, it might be that the production of text in comments largely relies on Codex’s priors as a language model, resulting in little difference between Codex and GPT-3.",
        "15To conﬁrm this intuition, we ran our co-occurrence evaluations on the comments in our ﬁne-tuning GitHub dataset and found that negative, occupation-related, and profane words did not preferentially occur in the presence of group words (race, gender, religion).",
        "Because of the training process of pre-training and ﬁnetuning on public data, there is a natural trust boundary",
        "present in the training data, wherein an attacker could insert adversarial inputs that cause models to suggest vulnerable, malicious, or misaligned code. The pre-training and ﬁnetuning processes should generally be thought of as untrusted. This risk may increase as model capabilities and the interest of potential attackers increase.",
        "Finally, the Codex model itself may suggest insecure or otherwise bad code. Examples include suggesting a compromised package as a dependency, invoking functions insecurely, or suggesting secrets found in the training data.22 If Codex models become widespread software infrastructure, this could constitute a new type of supply chain risk. We discuss this more in the next section.",
        "Similar to the alignment problems in Appendix E, a securityrelevant subclass of behaviors is the generation of insecure code. A priori, we might expect that Codex will sometimes produce insecure code because the pre-training and ﬁnetuning paradigm involves training on large quantities of untrusted data, which is known to contain insecure code. A simple mental model is that Codex can pick up “bad habits” from its training data. But what does this look like",
        "to extract training data from large language models.",
        "Interestingly, we do not see a robust model size trend (over 1 order of magnitude of parameters) in this data. This suggests that insecure code production, at least in this case, is an alignment issue (see Appendix E): it is unclear if the models are improving with scale. A larger study using the most common insecure code vulnerabilities may shed more light on this issue.",
        "23Previous work (Schuster et al., 2020) has found that it is possible to poison training data for code autocompleters and trigger them at runtime to make insecure suggestions such as improper cryptographic function usage.",
        "25We used 5 prompts across different libraries for RSA and AES based on Sonar Source’s Python vulnerability database, and generated ˜30k samples total. We then removed some generated samples based on expected runtime errors, as different model sizes tend to vary in whether they produce code that runs.",
        "26There is unfortunately only limited research on the demographic distribution of Python users. Understanding this better could shed light on how the beneﬁts and risks associated with Codex might be distributed across society. A 2020 survey of StackOverﬂow users (Stack Overﬂow, 2020) suggests that women are comparatively more represented in data science and analysis roles than in DevOps specialist, system administrator, and site reliability",
        "Within a code ﬁle, one often imports packages or programs written by third parties. Rather than constantly reinventing the wheel, software developers rely on functions, libraries and APIs for most code we might consider “boilerplate.” For any given task, though, there are multiple options: PyTorch or TensorFlow for machine learning, Matplotlib or Seaborn for data visualization, etc.",
        "Codex imports substitutable packages at different rates based on patterns in its training data, which can have various",
        "engineer roles while a 2020 survey of Python developers (Python Software Foundation and JetBrains, 2020) suggests that those data science and analysis roles are some of the most common Python use cases. Given this, we might anticipate that women would be disproportionately affected–positively or negatively–by Codex. However, we emphasize that those surveys may not be representative for various reasons (e.g. selective participation of community members in the survey; non-representativeness of the community as a sample of the overall developer and Python communities, respectively). We mention these results merely to illustrate the potential for code generation’s economic effects to be felt unequally across society and to motivate more rigorous research in related areas.",
        "trust Codex to do the rest. Users might be more inclined to accept the Codex answer under the assumption that the package it suggests is the one with which Codex will be more helpful. As a result, certain players might become more entrenched in the package market and Codex might not be aware of new packages developed after the training data was originally gathered. Further, for already existing packages, the model may make suggestions for deprecated methods. This could increase open-source developers’ incentive to maintain backward compatibility, which could pose challenges given that open-source projects are often under-resourced (Eghbal, 2020; Trinkenreich et al., 2021).",
        "More work is needed to compare the prevalence of different packages in Codex outputs with the input data to understand how or if these biases are concentrated by training, as well as to understand the direct and indirect impacts of these biases."
    ]
}