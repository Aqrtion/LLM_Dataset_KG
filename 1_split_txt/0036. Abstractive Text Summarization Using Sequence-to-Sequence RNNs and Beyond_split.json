{
    "title_author_abstract_introduction": "Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond\nRamesh Nallapati IBM Watson nallapati@us.ibm.com\nBowen Zhou IBM Watson zhou@us.ibm.com\nCicero dos Santos IBM Watson cicerons@us.ibm.com\nÇa˘glar G˙ulçehre Université de Montréal gulcehrc@iro.umontreal.ca\nBing Xiang IBM Watson bingxia@us.ibm.com\nAbstract\nIn this work, we model abstractive text summarization using Attentional EncoderDecoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-toword structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.\nIntroduction\nAbstractive text summarization is the task of generating a headline or a short summary consisting of a few sentences that captures the salient ideas of an article or a passage. We use the adjective ‘abstractive’ to denote a summary that is not a mere selection of a few existing passages or sentences extracted from the source, but a compressed paraphrasing of the main contents of the document, potentially using vocabulary unseen in the source document.\nThis task can also be naturally cast as mapping an input sequence of words in a source document to a target sequence of words called summary. In the recent past, deep-learning based models that map an input sequence into another output sequence, called sequence-to-sequence models, have been successful in many problems such as machine translation (Bahdanau et al., 2014),\nspeech recognition (Bahdanau et al., 2015) and video captioning (Venugopalan et al., 2015). In the framework of sequence-to-sequence models, a very relevant model to our task is the attentional Recurrent Neural Network (RNN) encoderdecoder model proposed in Bahdanau et al. (2014), which has produced state-of-the-art performance in machine translation (MT), which is also a natural language task.\nDespite the similarities, abstractive summarization is a very different problem from MT. Unlike in MT, the target (summary) is typically very short and does not depend very much on the length of the source (document) in summarization. Additionally, akeychallengeinsummarizationistooptimally compress the original document in a lossy manner such that the key concepts in the original document are preserved, whereas in MT, the translation is expected to be loss-less. In translation, there is a strong notion of almost one-to-one wordlevel alignment between source and target, but in summarization, it is less obvious.\nWe make the following main contributions in this work: (i) We apply the off-the-shelf attentional encoder-decoder RNN that was originally developed for machine translation to summarization, and show that it already outperforms stateof-the-art systems on two different English corpora. (ii) Motivated by concrete problems in summarization that are not sufﬁciently addressed by the machine translation based model, we propose novel models and show that they provide additional improvement in performance. (iii) We propose a new dataset for the task of abstractive summarization of a document into multiple sentences and establish benchmarks.\nThe rest of the paper is organized as follows. In Section 2, we describe each speciﬁc problem in abstractive summarization that we aim to solve, and present a novel model that addresses it. Sec-\ntion 3 contextualizes our models with respect to closelyrelatedworkonthetopicofabstractivetext summarization. We present the results of our experiments on three different data sets in Section 4. We also present some qualitative analysis of the output from our models in Section 5 before concluding the paper with remarks on our future direction in Section 6.",
    "data_related_paragraphs": [
        "Often-times in summarization, the keywords or named-entities in a test document that are central to the summary may actually be unseen or rare with respect to training data. Since the vocabulary of the decoder is ﬁxed at training time, it cannot emit these unseen words. Instead, a most common way of handling these out-of-vocabulary (OOV) words is to emit an ‘UNK’ token as a placeholder. However this does not result in legible summaries. In summarization, an intuitive way to handle such OOV words is to simply point to their location in the source document instead. We model this no-",
        "In datasets where the source document is very long, in addition to identifying the keywords in the document, it is also important to identify the key sentences from which the summary can be drawn. This model aims to capture this notion of two levels of importance using two bi-directional",
        "Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topicswithmultiplereferencesummariesperstory generated by humans. The best performing system on the DUC-2004 task, called TOPIARY (Zajic et al., 2004), used a combination of linguistically motivated compression techniques, and an unsupervised topic detection algorithm that appends keywords extracted from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches (Banko et al., 2000), compression using weighted tree-transformation rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010).",
        "With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alterIn Rush et native to abstractive summarization. al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets.",
        "In another paper that is closely related to our work, Hu et al. (2015) introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese dataset using an encoder-decoder RNN, but do not report experiments on English corpora.",
        "Our work starts with the same framework as (Hu et al., 2015), where we use RNNs for both source and target, but we go beyond the standard architecture and propose novel models that address critical problems in summarization. We also note that this work is an extended version of Nallapati et al. (2016). In addition to performing more extensive experiments compared to that work, we also propose a novel dataset for document summarization on which we establish benchmark numbers too.",
        "4.1 Gigaword Corpus In this series of experiments3, we used the annotated Gigaword corpus as described in Rush et al. (2015). We used the scripts made available by the authors of this work4 to preprocess the data, which resulted in about 3.8M training examples. The script also produces about 400K validation and test examples, but we created a randomly sampled subset of 2000 examples each for validation and testing purposes, on which we report our performance. Further, we also acquired the exact test sample used in Rush et al. (2015) to make precise comparison of our models with theirs. We also made small modiﬁcations to the script to extract not only the tokenized words, but also systemgenerated parts-of-speech and named-entity tags. Training: Forallthemodelswediscussbelow, we used 200 dimensional word2vec vectors (Mikolov et al., 2013) trained on the same corpus to initialize the model embeddings, but we allowed them to be updated during training. The hidden state dimension of the encoder and decoder was ﬁxed at 400 in all our experiments. When we used only the ﬁrst sentence of the document as the source, as done in Rush et al. (2015), the encoder vocabularysizewas119,505andthatofthedecoderstood at 68,885. We used Adadelta (Zeiler, 2012) for training, with an initial learning rate of 0.001. We used a batch-size of 50 and randomly shufﬂed the training data at every epoch, while sorting every 10 batches according to their lengths to speed up training. We did not use any dropout or regularization, but applied gradient clipping. We used early stopping based on the validation set and used the best model on the validation set to report all test performance numbers. For all our models, we employ the large-vocabulary trick, where we restrict the decoder vocabulary size to 2,0005, because it cuts down the training time per epoch by nearly three times, and helps this and all subse-",
        "The existing abstractive text summarization corpora including Gigaword and DUC consist of only one sentence in each summary. In this section, we present a new corpus that comprises multisentence summaries. To produce this corpus, we modify an existing corpus that has been used for the task of passage-based question answering In this work, the au(Hermann et al., 2015). thors used the human generated abstractive summary bullets from new-stories in CNN and Daily Mail websites as questions (with one of the entities hidden), and stories as the corresponding passages from which the system is expected to answer the ﬁll-in-the-blank question. The authors released the scripts that crawl, extract and generate pairs of passages and questions from these websites. With a simple modiﬁcation of the script, we restored all the summary bullets of each story in the original order to obtain a multi-sentence summary, where each bullet is treated as a sentence. In all, this corpus has 286,817 training pairs, 13,368 validation pairs and 11,487 test pairs, as deﬁned bytheirscripts. Thesourcedocumentsinthetraining set have 766 words spanning 29.74 sentences on an average while the summaries consist of 53 words and 3.72 sentences. The unique characteristics of this dataset such as long documents, and ordered multi-sentence summaries present interesting challenges, and we hope will attract future",
        "Table 1: Performance comparison of various models. ’*’ indicates statistical signiﬁcance of the corresponding model with respect to the baseline model on its dataset as given by the 95% conﬁdence interval in the ofﬁcial Rouge script. We report",
        "statistical signiﬁcance only for the best performing models. ’src. copy rate’ for the reference data on our validation sample is",
        "The dataset is released in two versions: one consisting of actual entity names, and the other, in which entity occurrences are replaced with document-speciﬁc integer-ids beginning from 0. in the Since the vocabulary size is smaller anonymized version, we used it in all our experiments below. We limited the source vocabulary size to 150K, and the target vocabulary to 60K, the source and target lengths to at most 800 and 100 words respectively. We used 100-dimensional word2vec embeddings trained on this dataset as input, and we ﬁxed the model hidden state size at 200. We also created explicit pointers in the training data by matching only the anonymized entityids between source and target on similar lines as we did for the OOV words in Gigaword corpus. Computational costs: We used a single Tesla K40 GPU to train our models on this dataset as well. While the ﬂat models (words-lvt2k and wordslvt2k-ptr) took under 5 hours per epoch, the hierarchical attention model was very expensive, consuming nearly 12.5 hours per epoch. Convergence of all models is also slower on this dataset compared to Gigaword, taking nearly 35 epochs for all models. Thus, the wall-clock time for training until convergence is about 7 days for the ﬂat models, but nearly 18 days for the hierarchical attention model. Decoding is also slower as well, with a throughput of 2 examples per second for",
        "Results: Results from the basic attention encoderdecoder as well as the hierarchical attention model are displayed in Table 3. Although this dataset is smaller and more complex than the Gigaword corpus, it is interesting to note that the Rouge numbers are in the same range. However, the hierarchical attention model described in Sec. 2.4 outperforms the baseline attentional decoder only marginally.",
        "Upon visual inspection of the system output, we noticed that on this dataset, both these models produced summaries that contain repetitive phrases or even repetitive sentences at times. Since the summaries in this dataset involve multiple sentences, it is likely that the decoder ‘forgets’ what part of the document was used in producing earlier highlights. To overcome this problem, we used the Temporal Attention model of Sankaran et al. (2016) that keeps track of past attentional weights of the decoder and expliticly discourages it from attending to the same parts of the document in future time steps. The model works as shown by the",
        "8On this dataset, we used the pyrouge script (https:// pypi.python.org/pypi/pyrouge/0.1.0) that allows evaluation of each sentence as a separate unit. Additional pre-processing involves assigning each highlight to its own \"<a>\" tag in the system and gold xml ﬁles that go as input to the Rouge evaluation script. Similar evaluation was also done by (Cheng and Lapata, 2016).",
        "On CNN/Daily Mail data, although our models are able to produce good quality multi-sentence summaries, we notice that the same sentence or",
        "and Yasemin Altun. Overcoming the lack In Proof parallel data in sentence compression. the 2013 Conference on Empirical ceedings of Methods in Natural Language Processing, pages 1481–1491.",
        "In this work, we apply the attentional encoderdecoder for the task of abstractive summarization with very promising results, outperforming stateof-the-art results signiﬁcantly on two different datasets. Each of our proposed novel models addresses a speciﬁc problem in abstractive summarization, yielding further improvement in performance. We also propose a new dataset for multisentence summarization and establish benchmark numbers on it. As part of our future work, we plan to focus our efforts on this data and build more robust models for summaries consisting of multiple sentences.",
        "[Hu et al.2015] Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. Lcsts: A large scale chinese short text summarization dataset. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1967–1972, Lisbon, Portugal, September. Association for Computational Linguistics."
    ]
}