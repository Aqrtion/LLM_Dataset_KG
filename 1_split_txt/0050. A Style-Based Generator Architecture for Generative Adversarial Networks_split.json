{
    "title_author_abstract_introduction": "A Style-Based Generator Architecture for Generative Adversarial Networks\nTero Karras NVIDIA tkarras@nvidia.com\nSamuli Laine NVIDIA slaine@nvidia.com\nTimo Aila NVIDIA taila@nvidia.com\nAbstract\nWe propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-speciﬁc control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.\n1. Introduction\nThe resolution and quality of images produced by generative methods—especially generative adversarial networks (GAN) [22]—have seen rapid improvement recently [30, 45, 5]. Yet the generators continue to operate as black boxes, and despite recent efforts [3], the understanding of various aspects of the image synthesis process, e.g., the origin of stochastic features, is still lacking. The properties of the latent space are also poorly understood, and the commonly demonstrated latent space interpolations [13, 52, 37] provide no quantitative way to compare different generators against each other.\nMotivated by style transfer literature [27], we re-design the generator architecture in a way that exposes novel ways to control the image synthesis process. Our generator starts from a learned constant input and adjusts the “style” of the image at each convolution layer based on the latent code, therefore directly controlling the strength of image features at different scales. Combined with noise injected directly into the network, this architectural change leads to automatic, unsupervised separation of high-level attributes\n(e.g., pose, identity) from stochastic variation (e.g., freckles, hair) in the generated images, and enables intuitive scale-speciﬁc mixing and interpolation operations. We do not modify the discriminator or the loss function in any way, and our work is thus orthogonal to the ongoing discussion about GAN loss functions, regularization, and hyperparameters [24, 45, 5, 40, 44, 36].\nOur generator embeds the input latent code into an intermediate latent space, which has a profound effect on how the factors of variation are represented in the network. The input latent space must follow the probability density of the training data, and we argue that this leads to some degree of unavoidable entanglement. Our intermediate latent space is free from that restriction and is therefore allowed to be disentangled. As previous methods for estimating the degree of latent space disentanglement are not directly applicableinourcase, weproposetwonewautomatedmetrics— perceptual path length and linear separability—for quantifyingtheseaspectsofthegenerator. Usingthesemetrics, we show that compared to a traditional generator architecture, our generator admits a more linear, less entangled representation of different factors of variation.\nFinally, we present a new dataset of human faces (Flickr-Faces-HQ, FFHQ) that offers much higher quality and covers considerably wider variation than existing high-resolution datasets (Appendix A). We have made this dataset publicly available, along with our source code and pre-trained networks.1 The accompanying video can be found under the same link.",
    "data_related_paragraphs": [
        "Before studying the properties of our generator, we demonstrate experimentally that the redesign does not compromise image quality but, in fact, improves it considerably. Table 1 gives Fr´echet inception distances (FID) [25] for various generator architectures in CELEBA-HQ [30] and our new FFHQ dataset (Appendix A). Results for other datasets are given in Appendix E. Our baseline conﬁguration (A) is the Progressive GAN setup of Karras et al. [30], from which we inherit the networks and all hyperparameters except where stated otherwise. We ﬁrst switch to an improved baseline (B) by using bilinear up/downsampling operations [64], longer training, and tuned hyperparameters. A detailed description of training setups and hyperparameters is included in Appendix C. We then improve this new baseline further by adding the mapping network and AdaIN operations (C), and make a surprising observation that the networknolongerbeneﬁtsfromfeedingthelatentcodeintothe ﬁrst convolution layer. We therefore simplify the architecture by removing the traditional input layer and starting the image synthesis from a learned 4 × 4 × 512 constant tensor (D). We ﬁnd it quite remarkable that the synthesis network is abletoproducemeaningfulresultseven thoughit receives input only through the styles that control the AdaIN operations.",
        "Figure 2. Uncurated set of images produced by our style-based generator (conﬁg F) with the FFHQ dataset. Here we used a variation of the truncation trick [42, 5, 34] with ψ = 0.7 for resolutions 42 − 322. Please see the accompanying video for more results.",
        "while FFHQ uses WGAN-GP for conﬁguration A and nonsaturating loss [22] with R1 regularization [44, 51, 14] for conﬁgurations B–F. We found these choices to give the best results. Our contributions do not modify the loss function. We observe that the style-based generator (E) improves FIDs quite signiﬁcantly over the traditional generator (B), almost 20%, corroborating the large-scale ImageNet measurements made in parallel work [6, 5]. Figure 2 shows an uncurated set of novel images generated from the FFHQ dataset using our generator. As conﬁrmed by the FIDs, the average quality is high, and even accessories such as eyeglasses and hats get successfully synthesized. For this ﬁgure, we avoided sampling from the extreme regions of W using the so-called truncation trick [42, 5, 34]— Appendix B details how the trick can be performed in W instead of Z. Note that our generator allows applying the truncation selectively to low resolutions only, so that highresolution details are not affected.",
        "There are various deﬁnitions for disentanglement [54, 50, 2, 7, 19], but a common goal is a latent space that consists of linear subspaces, each of which controls one factor of variation. However, the sampling probability of each combination of factors in Z needs to match the corresponding density in the training data. As illustrated in Figure 6, this precludes the factors from being fully disentangled with typical datasets and input latent distributions.2",
        "pling according to any ﬁxed distribution; its sampling density is induced by the learned piecewise continuous mapping f(z). This mapping can be adapted to “unwarp” W so that the factors of variation become more linear. We posit that there is pressure for the generator to do so, as it should be easier to generate realistic images based on a disentangled representation than based on an entangled representation. As such, we expect the training to yield a less entangled W in an unsupervised setting, i.e., when the factors of variation are not known in advance [10, 35, 49, 8, 26, 32, 7]. Unfortunately the metrics recently proposed for quantifying disentanglement [26, 32, 7, 19] require an encoder network that maps input images to latent codes. These metrics are ill-suited for our purposes since our baseline GAN lacks such an encoder. While it is possible to add an extra network for this purpose [8, 12, 15], we want to avoid investingeffortintoacomponentthatisnotapartoftheactual solution. To this end, we describe two new ways of quantifying disentanglement, neitherof which requiresan encoder or known factors of variation, and are therefore computable for any image dataset and generator.",
        "2The few artiﬁcial datasets designed for disentanglement studies (e.g., [43, 19]) tabulate all combinations of predetermined factors of variation with uniform frequency, thus hiding the problem.",
        "In order to label the generated images, we train auxiliary classiﬁcation networks for a number of binary attributes, e.g., to distinguish male and female faces. In our tests, the classiﬁers had the same architecture as the discriminator we use (i.e., same as in [30]), and were trained using the CELEBA-HQ dataset that retains the 40 attributes available in the original CelebA dataset. To measure the separability ofoneattribute, wegenerate200,000imageswithz ∼ P(z) and classify them using the auxiliary classiﬁcation network. We then sort the samples according to classiﬁer conﬁdence and remove the least conﬁdent half, yielding 100,000 labeled latent-space vectors.",
        "Figure 7. The FFHQ dataset offers a lot of variety in terms of age, ethnicity, viewpoint, lighting, and image background.",
        "Furthermore, increasing the depth of the mapping network improves both image quality and separability in W, which is in line with the hypothesis that the synthesis network inherently favors a disentangled input representation. Interestingly, adding a mapping network in front of a traditional generator results in severe loss of separability in Z but improves the situation in the intermediate latent space W, and the FID improves as well. This shows that even the traditional generator architecture performs better when we introduce an intermediate latent space that does not have to follow the distribution of the training data.",
        "A. The FFHQ dataset",
        "We have collected a new dataset of human faces, FlickrFaces-HQ (FFHQ), consisting of 70,000 high-quality images at 10242 resolution (Figure 7). The dataset includes vastly more variation than CELEBA-HQ [30] in terms of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr",
        "(thus inheriting all the biases of that website) and automatically aligned [31] and cropped. Only images under permissive licenses were collected. Various automatic ﬁlters were used to prune the set, and ﬁnally Mechanical Turk allowed us to remove the occasional statues, paintings, or photos of photos. We have made the dataset publicly available at https://github.com/NVlabs/ffhq-dataset",
        "If we consider the distribution of training data, it is clear that areas of low density are poorly represented and thus likely to be difﬁcult for the generator to learn. This is a signiﬁcant open problem in all generative modeling techniques. However, it is known that drawing latent vectors from a truncated [42, 5] or otherwise shrunk [34] sampling space tends to improve average image quality, although some amount of variation is lost.",
        "For our improved baseline (B in Table 1), we make several modiﬁcations to improve the overall result quality. We replace the nearest-neighbor up/downsampling in both networks with bilinear sampling, which we implement by lowpass ﬁltering the activations with a separable 2nd order binomial ﬁlter after each upsampling layer and before each downsampling layer [64]. We implement progressive growing the same way as Karras et al. [30], but we start from 82 images instead of 42. For the FFHQ dataset, we switch from WGAN-GP to the non-saturating loss [22] with R1 regularization [44] using γ = 10. With R1 we found that the FID scores keep decreasing for considerably longer than with WGAN-GP, and we thus increase the training time from 12M to 25M images. We use the same learning rates as Karras et al. [30] for FFHQ, but we found that setting the learning rate to 0.002 instead of 0.003 for 5122 and 10242 leads to better stability with CelebA-HQ.",
        "Figure 9. FID and perceptual path length metrics over the course of training in our conﬁgurations B and F using the FFHQ dataset. Horizontal axis denotes the number of training images seen by the discriminator. The dashed vertical line at 8.4M images marks the point when training has progressed to full 10242 resolution. On the right, we show only one curve for the traditional generator’s path length measurements, because there is no discernible difference between full-path and endpoint sampling in Z.",
        "Figure 9 shows how the FID and perceptual path length metrics evolve during the training of our conﬁgurations B and F with the FFHQ dataset. With R1 regularization active in both conﬁgurations, FID continues to slowly decrease as the training progresses, motivating our choice to increase the training time from 12M images to 25M images. Even when the training has reached the full 10242 resolution, the slowly rising path lengths indicate that the improvements in FID come at the cost of a more entangled representation. Considering future work, it is an interesting question whether this is unavoidable, or if it were possible to encourage shorter path lengths without compromising the convergence of FID.",
        "E. Other datasets",
        "Style-based (F), fullStyle-based (F), endFIDPath length1098765405M10M15M20M25M05M10M15M20M25M5004003002001000resolutionFullresolutionFullTraditional (B)Style-based (F)Traditional (B)\fFigure 10. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN BEDROOM dataset at 2562. FID computed for 50K images was 2.65.",
        "Figure 11. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN CAR dataset at 512 × 384. FID computed for 50K images was 3.27.",
        "These datasets were trained using the same setup as FFHQ for the duration of 70M images for BEDROOM and CATS, and 46M for CARS. We suspect that the results for BEDROOM are starting to approach the limits of the training data, as in many images the most objectionable issues are the severe compression artifacts that have been inherited from the low-quality training data. CARS has much higher quality training data that also allows higher spatial resolution (512×384 instead of 2562), and CATS continues to be a difﬁcult dataset due to the high intrinsic variation in poses, zoom levels, and backgrounds.",
        "[4] M. Ben-Yosef and D. Weinshall. Gaussian mixture generative adversarial networks for diverse datasets, and the unsupervised clustering of images. CoRR, abs/1808.10356, 2018. 3",
        "Figure 12. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN CAT dataset at 2562. FID computed for 50K images was 8.53.",
        "I. Higgins, D. Hassabis, and A. Lerchtesting sprites dataset.",
        "ner. https://github.com/deepmind/dsprites-dataset/, 2017. 6 [44] L. Mescheder, A. Geiger, and S. Nowozin. Which trainCoRR,",
        "[60] T. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. CoRR, abs/1711.11585, 2017. 3 [61] T. White. Sampling generative networks: Notes on a few effective techniques. CoRR, abs/1609.04468, 2016. 7 [62] F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015. 9"
    ]
}