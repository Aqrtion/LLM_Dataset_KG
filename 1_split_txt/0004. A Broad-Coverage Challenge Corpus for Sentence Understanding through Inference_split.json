{
    "title_author_abstract_introduction": "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference\nAdina Williams1 adinawilliams@nyu.edu\nNikita Nangia2 nikitanangia@nyu.edu\nSamuel R. Bowman1,2,3 bowman@nyu.edu\n1Department of Linguistics New York University\n2Center for Data Science New York University\n3Department of Computer Science New York University\nAbstract\nThis paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, thisresourceis oneofthelargestcorporaavailable for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difﬁculty. MultiNLI accomplishes this by offering data from ten distinct genres of written andspokenEnglish,makingit possibletoevaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difﬁcult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.\n1 Introduction\nMany of the most actively studied problems in NLP, including question answering, translation, and dialog, depend in large part on natural language understanding (NLU) for success. While there has been a great deal of work that uses representation learning techniques to pursue progress on these applied NLU problems directly, in order for a representation learning model to fully succeed at one of these problems, it must simultaneously succeed both at NLU, and at one or more additional hard machine learning problems like structured prediction or memory access. This makes it difﬁcult to accurately judge the degree to\nwhich current models extract reasonable representations of language meaning in these settings.\nThe task of natural language inference (NLI) is well positioned to serve as a benchmark task for research on NLU. In this task, also known as recognizing textual entailment (Fyodorov et al., 2000; Condoravdi et al., 2003; Bos and Markert, 2005; Dagan et al., 2006; MacCartney and Manning, 2009), a model is presented with a pair of sentences—like one of those in Figure 1— and asked to judge the relationship between their meanings by picking a label from a small set: typically ENTAILMENT, NEUTRAL, and CONTRADICTION. Succeeding at NLI does not require a system to solve any difﬁcult machine learning problems except, crucially, that of extracting an effective and thorough representations for the meanings of sentences (i.e., their lexical and compositional semantics). In particular, a model must handle phenomena like lexical entailment, quantiﬁcation, coreference, tense, belief, modality, and lexical and syntactic ambiguity.\nAs the only large human-annotated corpus for NLI currently available, the Stanford NLI Corpus (SNLI; Bowman et al., 2015) has enabled a good deal of progress on NLU, serving as a major benchmark for machine learning work on sentence understanding and spurring work on core representation learning techniques for NLU, such as attention (Wang and Jiang, 2016; Parikh et al., 2016), memory (Munkhdalai and Yu, 2017), and the use of parse structure (Mou et al., 2016b; Bowman et al., 2016; Chen et al., 2017). However, SNLI falls short of providing a sufﬁcient testing ground for machine learning models in two ways.\nMet my ﬁrst girlfriend that way.\n8 million in relief in the form of emergency housing.\nFACE-TO-FACE contradiction C C N C\nGOVERNMENT neutral N N N N\nNow, aschildren tend their gardens, they have a new appreciation of their relationship to the land, their cultural heritage, and their community.\nLETTERS neutral N N N N\nAt 8:34, the Boston Center controller received a third transmission from American 11\nI am a lacto-vegetarian.\n9/11 entailment E E E E\nSLATE neutral N N E N\nsomeone else noticed it and i saidwell i guess that’strue and it was somewhat melodious in other words it wasn’t just you know it was really funny\nTELEPHONE contradiction C C C C\nI didn’t meet my ﬁrst girlfriend until later.\nThe 8 million dollars for emergency housing was still not enough to solve the problem.\nAll of the children love working in their gardens.\nThe Boston Center controller got a third transmission from American 11.\nI enjoy eating cheese too much to abstain from dairy.\nNo one noticed and it wasn’t funny at all.\nTable 1: Randomly chosen examples from the development set of our new corpus, shown with their genre labels, their selected gold labels, and the validation labels (abbreviated E, N, C) assigned by individual annotators.\nFirst, the sentences in SNLI are derived from only a single text genre—image captions—and are thus limited to descriptions of concrete visual scenes, rendering the hypothesis sentences used to describe these scenes short and simple, and rendering many important phenomena—like temporal reasoning (e.g., yesterday), belief (e.g., know), and modality (e.g., should)—rare enough to be irrelevant to task performance. Second, because of these issues, SNLI is not sufﬁciently demanding to serve as an effective benchmark for NLU, with the best current model performance falling within a few percentage points of human accuracy and limited room left for ﬁne-grained comparisons between strong models.\nThis paper introduces a new challenge dataset, the Multi-Genre NLI Corpus (MultiNLI), whose chief purpose is to remedy these limitations by making it possible to run large-scale NLI evaluations that capture more of the complexity of modern English. While its size (433k pairs) and mode of collection are modeled closely on SNLI, unlike that corpus, MultiNLI represents both written and spoken speech in a wide range of styles, degrees of formality, and topics.\nOur chief motivation in creating this corpus is to provide a benchmark for ambitious machine learning research on the core problems of NLU, but we are additionally interested in constructing a corpus that facilitates work on domain adaptation and cross-domain transfer learning. In many application areas outside NLU, artiﬁcial neural network\ntechniques have made it possible to train generalpurpose feature extractors that, with no or minimal retraining, can extract useful features for a variety of styles of data (Krizhevsky et al., 2012; Zeiler and Fergus, 2014; Donahue et al., 2014). However, attempts to bring this kind of general purpose representation learning to NLU have seen only very limited success (see, for example, Mou et al., 2016a). Nearly all successful applications of representation learning to NLU have involved models that are trained on data that closely resembles the target evaluation data, both in task and style. This fact limits the usefulness of these tools for problems involving styles of language not represented in large annotated training sets.\nWith this in mind, we construct MultiNLI so as to make it possible to explicitly evaluate models both on the quality of their sentence representations within the training domain and on their ability to derive reasonable representations in unfamiliar domains. The corpus is derived from ten different genres of written and spoken English, which are collectively meant to approximate the full diversity of ways in which modern standard American English is used. All of the genres appear in the test and development sets, but only ﬁve are included in the training set. Models thus can be evaluated on both the matched test examples, which are derived from the same sources as those in the training set, and on the mismatched examples, which do not closely resemble any of those seen at training time.\nThis task will involve reading a line from a non-ﬁction article and writing three sentences that relate to it. The line will describe a situation or event. Using only this description and what you know about the world:\n• Write one sentence that is deﬁnitely correct about\nthe situation or event in the line.\n• Writeone sentence that might be correct about the\nsituation or event in the line.\n• Write one sentence that is deﬁnitely incorrect\nabout the situation or event in the line.\nFigure 1: The main text of a prompt (truncated) that was presented to our annotators. This version is used for the written non-ﬁction genres.",
    "data_related_paragraphs": [
        "2.1 Data Collection",
        "The data collection methodology for MultiNLI is similar to that of SNLI: We create each sentence pair by selecting a premise sentence from a preexisting text source and asking a human annotator to compose a novel sentence to pair with it as a hypothesis. This section discusses the sources of our premise sentences, our collection method for hypotheses, and our validation (relabeling) strategy.",
        "OANC data constitutes the following nine gentranscriptions from the Charlotte Narrative res: and Conversation Collection of two-sided, inperson conversations that took place in the early 2000s (FACE-TO-FACE); reports, speeches, letters, and press releases from public domain government websites (GOVERNMENT); letters from the Indiana Center forIntercultural Communication of Philanthropic Fundraising Discourse written in the late 1990s–early 2000s (LETTERS); the public re-",
        "port from the National Commission on Terrorist Attacks Upon the United States released on July 22, 20042 (9/11); ﬁve non-ﬁction works on the textile industry and child development published by the Oxford University Press (OUP); popular culture articles from the archives of Slate Magazine (SLATE) written between 1996–2000; transcriptions from University of Pennsylvania’s Linguistic Data Consortium Switchboard corpus of two-sided, telephone conversations that took place in 1990 or 1991 (TELEPHONE); travel guides published by Berlitz Publishing in the early 2000s (TRAVEL); and short posts about linguistics for non-specialists from the Verbatim archives written between 1990 and 1996 (VERBATIM).",
        "Hypothesis Collection To collect a sentence pair, we present a crowdworker with a sentence from a source text and ask them to compose three novel sentences (the hypotheses): one which is necessarily true or appropriate whenever the premise is true (paired with the premise and labeled ENTAILMENT), one which is necessarily false or inappropriate whenever the premise is true (CONTRADICTION), and one where neither condition applies (NEUTRAL). This method of data collection ensures that the three classes will be represented equally in the raw corpus.",
        "The prompts that surround each premise sentence during hypothesis collection are slightly tailored to ﬁt the genre of that premise sentence. We pilot these prompts prior to data collection to ensure that the instructions are clear and that they yield hypothesis sentences that ﬁt the intended meanings of the three classes. There are ﬁve unique prompts in total: one for written non-ﬁction genres (SLATE, OUP, GOVERNMENT, VERBATIM, TRAVEL; Figure 1), one for spoken genres (TELEPHONE, FACE-TO-FACE), one for each of the less formal written genres (FICTION, LETTERS), and a specialized one for 9/11, tailored to ﬁt its potentially emotional content. Each prompt is accompanied by example premises and hypothesis that are speciﬁc to each genre.",
        "sented with pairs of sentences and asked to supply a single label (ENTAILMENT, CONTRADICTION, NEUTRAL) for the pair. Each pair is relabeled by four workers, yielding a total of ﬁve labels per example. Validation instructions are tailored by genre, based on the main data collection prompt (Figure 1); a single FAQ, modeled after the validation FAQ from SNLI, is provided for reference. In order to encourage thoughtful labeling, we manually label one percent of the validation examples and offer a $1 bonus each time a worker selects a label that matches ours.",
        "Table 1 shows randomly chosen development set examples from the collected corpus. Hypotheses tend to be ﬂuent and correctly spelled, though not all are complete sentences. Punctuation is often omitted. Hypotheses can rely heavily on knowledge about the world, and often don’t correspond closely with their premises in syntactic structure. Unlabeled test data is available on Kaggle for both matched and mismatched sets as competitions that will be open indeﬁnitely; Evaluations on a subset of the test set have previously been conducted with different leaderboards through the RepEval 2017 Workshop (Nangia et al., 2017).",
        "the The corpus at freely nyu.edu/projects/bowman/multinli/ for typical machine learning uses, and may be modiﬁed and redistributed. The majority of the corpus is released under the OANC’s license, which allows all content to be freely used, modiﬁed, and shared under permissive terms. The data in the FICTION section falls under several permissive licenses; Seven Swords is available under a Creative Commons Share-Alike 3.0 Unported License, and with the explicit permission of the author, Living History and Password Incorrect are available under Creative Commons Attribution 3.0 Unported Licenses; the remaining works of ﬁction are in the public domain in the United States (but may be licensed differently elsewhere).",
        "4.1 Data Collection",
        "In data collection for NLI, different annotator decisions about the coreference between entities and events across the two sentences in a pair can lead to very different assignments of pairs to labels (de Marneffe et al., 2008; Marelli et al., 2014a; Bowman et al., 2015). Drawing an example from Bowman et al., the pair “a boat sank in the Paciﬁc Ocean” and “a boat sank in the Atlantic Ocean” can be labeled either CONTRADICTION or NEUTRAL depending on (among other things) whether the two mentions of boats are assumed to refer to the same entity in the world. This uncertainty can present a serious problem for inter-annotator agreement, since it is not clear that it is possible to deﬁne an explicit set of rules around coreference that would be easily intelligible to an untrained annotator (or any non-expert).",
        "As expected, both the increase in the diversity of linguistic phenomena in MultiNLI and its longer average sentence length conspire to make MultiNLI dramatically more difﬁcult than SNLI. Our three baseline models perform better on SNLI than MultiNLI by about 15% when trained on the respective datasets. All three models achieve accuracy above 80% on the SNLI test set when trained only on SNLI. However, when trained on MultiNLI, only ESIM surpasses 70% accuracy on MultiNLI’s test sets. When we train models on MultiNLI and downsampled SNLI, we see an expected signiﬁcant improvement on SNLI,",
        "Natural language inference makes it easy to judge the degree to which neural network models for sentence understanding capture the full meanings for natural Existing NLI datasets like SNLI have facilitated substantial advances in modeling, but have limited headroom and coverage of the full diversity of meanings expressed in English. This paper presents a new dataset that offers dramatically greater linguistic difﬁculty and diversity, and also serves as a benchmark for cross-genre domain adaptation.",
        "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. 2017. Supervised learning of universal sentence representations from arXiv preprint natural language inference data. arXiv:1705.02364."
    ]
}