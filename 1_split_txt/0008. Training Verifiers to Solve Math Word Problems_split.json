{
    "title_author_abstract_introduction": "Training Veriﬁers to Solve Math Word Problems\nKarl Cobbe∗\nVineet Kosaraju∗\nMohammad Bavarian\nMark Chen\nHeewoo Jun\n(cid:32)Lukasz Kaiser\nMatthias Plappert\nJerry Tworek\nJacob Hilton\nReiichiro Nakano\nChristopher Hesse\nJohn Schulman\nOpenAI\nAbstract\nState-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We ﬁnd that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training veriﬁers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the veriﬁer. We demonstrate that veriﬁcation signiﬁcantly improves performance on GSM8K, and we provide strong empirical evidence that veriﬁcation scales more eﬀectively with increased data than a ﬁnetuning baseline.\nIntroduction\nIn recent years, large language models have demonstrated impressive skills across many diverse tasks (Wang et al., 2019; Brown et al., 2020). Kaplan et al. (2020) describe the consistent beneﬁts of increasing model size, characterizing scaling trends that hold across many orders of magnitude. However, even the largest models falter when required to perform multi-step mathematical reasoning (Hendrycks et al., 2021). Model samples frequently contain catastrophic mistakes, even after the model has been appropriately ﬁnetuned. Mathematical reasoning thus reveals a critical weakness in modern language models.\nOne signiﬁcant challenge in mathematical reasoning is the high sensitivity to individual mistakes (Shen et al., 2021a). When generating a solution, autoregressive models have no mechanism to correct their own errors. Solutions that veer oﬀ-course quickly become unrecoverable. If we rely purely on generative methods and extrapolate from current trends, we will require an exorbitant\n∗Equal contribution. Correspondence to: Karl Cobbe <karl@openai.com>, Vineet\nKosaraju <vineet@openai.com>\nFigure 1: Three example problems from GSM8K. Calculation annotations are highlighted in red.\nparameter count to achieve even moderate performance on distributions as challenging as the MATH dataset (Hendrycks et al., 2021). This evidence strongly motivates the search for methods with more favorable scaling laws.\nWe propose training veriﬁers to evaluate the correctness of model generated solutions, similar to concurrent work by Shen et al. (2021a). At test time, we sample a ﬁxed number of candidate solutions and select the solution ranked highest by the veriﬁer. Veriﬁers beneﬁt both from their inherent optionality and from veriﬁcation being a simpler task than generation in general.\nTo facilitate research, we are releasing GSM8K, a dataset of 8.5K high quality problems at the grade school math level. We designed this dataset to have high linguistic diversity while relying on relatively simple grade school math concepts. State-of-the-art language models struggle to achieve high performance on this dataset, primarily due to the high diversity among problems. At the same time, GSM8K solutions depend only on elementary concepts, so achieving high test performance is a tractable goal.\nOur main contributions are as follows:\n1. We present a curated dataset of 8.5K grade school math questions and natural language solutions, useful for probing the informal reasoning ability of large language models.",
    "data_related_paragraphs": [
        "2. We show that, compared to a ﬁnetuning baseline, the use of veriﬁers results in approximately the same performance boost as a 30x model size increase, and that veriﬁers scale signiﬁcantly better with increased data.",
        "2 Dataset",
        "• High Diversity We strive for high diversity among problems. We actively avoid designing problems that are drawn from the same linguistic template or diﬀer only in superﬁcial details, an issue that is prevalent among many other datasets. By creating each individual problem to be relatively unique, held-out test performance becomes a far more relevant metric.",
        "• Moderate Diﬃculty We choose a problem distribution that is challenging for large state-of-the-art language models, without being completely intractable. GSM8K will help us better understand the data scaling trends of diﬀerent models and methods in this diﬃculty sweet spot. Problems require no concepts beyond the level of early Algebra, and the vast majority of problems can be solved without explicitly deﬁning a variable.",
        "• Natural Language Solutions We collect solutions in natural language rather than as pure math expressions. We believe this is the most generally useful data format, and we expect it to shed light on the properties of large language models’ internal monologues. We instructed problem writers to explain their work as much as possible, but we allowed them to write solutions in their own diverse linguistic styles.",
        "The full GSM8K dataset can be found at https://github.com/openai/gradeschool-math. Example problems are shown in Figure 1, and we discuss additional dataset details in Appendix A.",
        "3.1 Related Datasets",
        "Early math word problem datasets (Kushman et al., 2014; Roy and Roth, 2015) are relatively small and are not well suited for testing the limits of modern language models. Dolphin18K (Huang et al., 2016) is a larger dataset containing",
        "18K problems, but solutions are provided only in the form of equations or ﬁ- nal answers. AQuA-RAT (Ling et al., 2017) contains 100K problems, but this dataset unfortunately suﬀers from both a high degree of problem templatization and poor quality control of the natural language solutions. MathQA is a recently released subset of AQuA-RAT focused on correcting these mistakes (Amini et al., 2019), but even the corrected dataset has data quality issues, with around 30% of the data having inconsistencies (Miao et al., 2021). Ape210K (Zhao et al., 2020) is the largest publicly available dataset, consisting of 210K Chinese elementary school-level math problems. However, due to the language barrier and the lack of natural language solutions, we’re unable to evaluate our methods on this dataset.",
        "The recently developed ASDiv dataset (Miao et al., 2021), which contains 2.3K math word problems, addresses common ﬂaws in prior datasets by ensuring problems have both high diversity and high quality. We share those design principles in the creation of GSM8K. However, we note that GSM8K is larger, provides natural language solutions, and consists of problems that on average require more steps to solve. The MATH dataset (Hendrycks et al., 2021) is larger and signiﬁcantly more complex than GSM8K, but the high diﬃculty makes it challenging to accurately measure progress given the current capabilities of state-of-the-art language models.",
        "Other recent reasoning-related datasets have focused on mathematical reasoning on symbolic math (Lample and Charton, 2019), reading comprehension (LogiQA) (Liu et al., 2020), and commonsense question answering (CommonsenseQA) (Talmor et al., 2018). Similar to CommonsenseQA, GSM8K includes questions that require basic background knowledge, like the number of days in a week. Similar to LogiQA, which requires a mix of reading comprehension and logical reasoning, GSM8K’s main diﬃculty lies in both properly interpreting a question and reasoning through the steps to solve it.",
        "select among many model completions. Nichols et al. (2020) proposed a sampleand-rank approach to improve the collaborative storytelling ability of large language models, with the training signal coming from the preferences of human workers. In concurrent work closely related to our own, Shen et al. (2021a) applied a similar approach to solving math word problems, jointly training a model to both generate and rank solutions. Our work shares many fundamental similarities with their approach, though we diﬀer in several key respects. First, we focus attention on the space of natural language solutions, as this is a richer and more general solution format than pure mathematical expressions. Moreover, this choice enables our models to develop verbal analytical skills and to produce solutions that are more readily interpretable by humans. Second, we provide evidence that veriﬁers scale far more favorably with additional data than baseline methods. Finally, we use separate generator and veriﬁer networks, in order to prevent the generator from overﬁtting.",
        "We perform ﬁnetuning by updating model parameters to minimize the crossentropy loss over all training tokens. Figure 2 shows test performance after ﬁnetuning on training sets of varying sizes for 20 epochs. We visualize the same data both as a function of training set size and as a function of model size. Test performance is determined by a single low temperature (T = 0) sample for each test problem. Unsurprisingly, we see that the 175B model signiﬁcantly outperforms the smaller models. Assuming a log-linear trend, we can naively extrapolate these results to estimate that a model with 1016 parameters would be required to reach an 80% solve rate, when using the full GSM8K training set. It is even harder to extrapolate along the data dimension, since performance does not appear to follow a log-linear trend. Nevertheless, it appears likely that the 175B model would require at least two additional orders of magnitude of training data to reach an 80% solve rate.",
        "training epochs. We use test@N to denote the percentage of problems solved correctly at least once when allowing the model to make N separate guesses for each problem. We use a low temperature (T = 0) to generate test@1 samples and we use a higher temperature (T = 0.7) to generate test@100 samples. Both temperature values were chosen empirically to produce the best results. Test@1 performance improves approximately monotonically, even though we quickly begin overﬁtting on test loss. Unfortunately, test@100 performance degrades much more sharply than test@1 as we increase the number of epochs. This is to be expected: as the model repeatedly encounters the same data, it becomes increasingly uncalibrated and overconﬁdent in its predictions. At test time, this overconﬁdence leads to poor coverage of the solution space, an eﬀect which only becomes noticeable when we are considering multiple samples at test time.",
        "3. Train a veriﬁer for a single epoch on this dataset.",
        "At test time, we sample 100 completions to each test problem, rank them with the veriﬁer, and then return the one with the highest veriﬁer score. A comparison between veriﬁcation and ﬁnetuning is shown in Figure 5 for both the 6B and 175B model sizes. We ﬁnd that it is not beneﬁcial to use veriﬁcation at low dataset sizes. We believe this is due to the pressure to overﬁt to the correct answer: with small datasets, overﬁtting to the correct answer happens faster than learning more generalizable properties of correct reasoning. However, once we use a suﬃciently large dataset, we see a strong boost from veriﬁers.",
        "We have seen that veriﬁcation provides a signiﬁcant performance boost relative to a ﬁnetuning baseline. On the full dataset, 6B veriﬁcation slightly outperforms a ﬁnetuned 175B model, thereby oﬀering a boost approximately equivalent to a 30x model size increase. We have also seen that token-level veriﬁers are less prone to overﬁtting than solution-level veriﬁers, and that all methods beneﬁt from regularization with residual dropout. We expect veriﬁcation to scale well to problem distributions that require more complex mathematical reasoning, and we hope GSM8K supports the development of new methods that scale even better.",
        "We thank Dan Hendrycks, Leo Gao, Alec Radford, and Giambattista Parascandolo for their valuable feedback on this paper; Harri Edwards, Yura Burda, Michael Wu, and Nick Ryder for many insightful conversations; Michael Petrov, Alethea Power, and Jacob Jackson for their technical assistance; the OpenAI Supercomputing team for the infrastructure that made these experiments possible; and the team at Surge AI for performing the GSM8K data collection.",
        "D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.",
        "D. Huang, S. Shi, C.-Y. Lin, J. Yin, and W.-Y. Ma. How well do computers solve math word problems? large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 887–896, 2016.",
        "J. Liu, L. Cui, H. Liu, D. Huang, Y. Wang, and Y. Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. In IJCAI, 2020.",
        "scale and template-rich dataset of math word problems. arXiv:2009.11506, 2020.",
        "A Dataset Details",
        "We initially collected a starting set of a thousand problems and natural language solutions by hiring freelance contractors on Upwork (upwork.com). We then worked with Surge AI (surgehq.ai), an NLP data labeling platform, to scale up our data collection. After collecting the full dataset, we asked workers to re-solve all problems, with no workers re-solving problems they originally wrote. We checked whether their ﬁnal answers agreed with the original solutions, and any problems that produced disagreements were either repaired or discarded. We then performed another round of agreement checks on a smaller subset of problems, ﬁnding that 1.7% of problems still produce disagreements among contractors. We estimate this to be the fraction of problems that contain breaking errors or ambiguities. It is possible that a larger percentage of problems contain subtle errors.",
        "When training veriﬁers with the joint objective, we use an equal mix of language data and veriﬁer data. Because we sample 100 completions for each original training example to generate the veriﬁer data, using an equal mix means we eﬀectively upsample the original language data by a factor of 100. To form the joint objective, we simply add the veriﬁer loss and language modeling loss unweighted, and deﬁne an epoch of this joint objective as having seen each veriﬁer example once. With both objectives, we mask out tokens in the question and only train on tokens in the solutions, as visualized in Figure 12."
    ]
}