{
    "title_author_abstract_introduction": "Natural Questions: A Benchmark for Question Answering Research\nTom Kwiatkowski♣♦♠∗ Jennimaria Palomaki♠ Olivia Redfield♦♠ Michael Collins♣♦♠♥ Ankur Parikh♥ Chris Alberti♥ Danielle Epstein♤♦ Illia Polosukhin♤♦ Jacob Devlin♤ Kenton Lee♥ Kristina Toutanova♥ Llion Jones♤ Matthew Kelcey♤♦ Ming-Wei Chang♥\nAndrew M. Dai♣♦\nJakob Uszkoreit♣ Quoc Le♣♦ Slav Petrov♣\nGoogle Research natural-questions@google.com\nAbstract\nWe present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.\nIntroduction\nIn recent years there has been dramatic progress in machine learning approaches to problems such as machine translation, speech recognition, and image recognition. One major factor in these successes has been the development of neural methods that far exceed the performance of previous approaches. A second major factor has\nbeen the existence of large quantities of training data for these systems.\nOpen-domain question answering (QA) is a benchmarktaskinnaturallanguageunderstanding (NLU), which has significant utility to users, and in addition is potentially a challenge task that can drive the development of methods for NLU. Several pieces of recent work have introduced QA data sets (e.g., Rajpurkar et al., 2016; Reddy et al., 2018). However, in contrast to tasks where it is relatively easy to gather naturally occurring examples,1 the definition of a suitable QA task, and the development of a methodology for annotationandevaluation,ischallenging.Keyissues include the methods and sources used to obtain questions; the methods used to annotate and collect answers; the methods used to measure and ensureannotationquality;andthemetricsusedfor evaluation. For more discussion of the limitations of previous work with respect to these issues, see Section 2 of this paper.\nThis paper introduces Natural Questions2 (NQ), a new data set for QA research, along with methods for QA system evaluation. Our goals are three-fold: 1) To provide large-scale end-to-end training data for the QA problem. 2) To provide a data set that drives research in natural language understanding. 3) To study human performance in providing QA annotations for naturally occurring questions.\nInbrief,ourannotationprocessisasfollows.An annotator is presented with a (question, Wikipedia page) pair. The annotator returns a (long answer, short answer) pair. The long answer (l) can be an HTML bounding box on the Wikipedia\n∗♣Project\ninitiation; ♦Project design; ♠Data creation; ♥Model development; ♤Project support; ♥Also affiliated with Columbia University, work done at Google; ♦No longer at Google, work done at Google.\n1For example, for machine translation/speech recognition humans provide translations/transcriptions relatively easily. 2Availableat:https://ai.google.com/research/\nNaturalQuestions.\nTransactions of the Association for Computational Linguistics, vol. 7, pp. 453–466, 2019. https://doi.org/10.1162/tacl a 00276. Action Editor: Jing Jiang. Submission batch: 4/2018; Revision batch: 6/2018; Published 7/2019. c(cid:7) 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.\nTask definition The input to a model is a question together with an entire Wikipedia page. The target output from the model is: 1) a long-answer (e.g., a paragraph) from the page that answers the question, or alternatively an indication that there is no answer on the page; 2) a short answer where applicable. The task was designed to be close to an end-to-end question answering application.\nEnsuring high-quality annotations at scale Comprehensive guidelines were developed for the task. These are summarized in Section 3. Annotation quality was constantly monitored.\nEvaluation of quality Section 4 describes posthoc evaluation of annotation quality. Long/short answers have 90%/84% precision, respectively.\nStudy of variability One clear finding in NQ is thatfornaturallyoccurringquestionsthereisoften genuine ambiguity in whether or not an answer is acceptable. There are also often a number of acceptable answers. Section 4 examines this variability using 25-way annotations.\nRobust evaluation metrics Section 5 introduces methods of measuring answer quality that account for variability in acceptable answers. We demonstrate a high human upper bound on these measures for both long answers (90% precision, 85% recall), and short answers (79% precision, 72% recall).\nWepropose NQasanewbenchmarkforresearch in QA. In Section 6.4 we present baseline results fromrecentmodelsdevelopedoncomparabledata sets(ClarkandGardner,2018),aswellasasimple pipelined model designed for the NQ task. We demonstrate a large gap between the performance of these baselines and a human upper bound. We argue that closing this gap will require significant advances in NLU.",
    "data_related_paragraphs": [
        "The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017)",
        "Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Section 5.",
        "The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018) data sets contain dialogues between a questioner, who is trying to learn about a text, and an answerer. QuAC also prevents the questioner from seeing the evidence text. Conversational QA is an exciting new area, but it is significantly different from the single turn QA task in NQ. In both QuAC and CoQA, conversations tend to explore evidence texts incrementally, progressing from the start to the end of the text.",
        "Thiscontrastswith NQ,whereindividualquestions often require reasoning over large bodies of text. TheWikiQA(Yangetal.,2015)andMSMarco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages.Free-formtextanswersallowmoreflexibilityinprovidingabstractiveanswers,butleadto difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discuss issues of variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric.",
        "There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylovetal.,2018;Richardsonetal.,2013;Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU.",
        "3 Task Definition and Data Collection",
        "The goal of these heuristics is to discard a large proportion of queries that are non-questions, while retaining the majority of queries of 8 words or more in length that are questions. A manual inspectionshowedthatthemajorityofquestionsin the data, with the exclusion of question beginning with ‘‘how to’’, are accepted by the filters. We focus on longer queries as they are more complex, andarethusamorechallengingtestfordeepNLU.",
        "Figure 2: Annotation decision process with path proportions from NQ training data. Percentages are proportions of entire data set. A total of 49% of all examples have a long answer.",
        "We focus on Wikipedia as it is a very important source of factual information, and we believe that stylistically it is similar to other sources of factual information on the Web; however, like any data set there may be biases in this choice. Future datacollection efforts may introduce shorter queries, ‘‘how to’’ questions, or domains other than Wikipedia.",
        "3.3 Data Statistics",
        "This section describes evaluation of the quality of the human annotations in our data. We use a combination of two methods: 1) post hoc evaluation of correctness of non-null answers, under consensus judgments from four ‘‘experts’’;",
        "2) k-way annotations (with k = 25) on a subset of the data.",
        "Post hoc evaluation of non-null answers leads directlytoameasureofannotationprecision.Asis common in information-retrieval style problems such as long-answer identification, measuring recall is more challenging. However, we describe how25-wayannotateddataprovideusefulinsights into recall, particularly when combined with expert judgments.",
        "Each item in our data consists of a four-tuple (q,d,l,s) where q is a question, d is a document, l is a long answer, and s is a short answer. Thus we introduce random variables Q, D, L, and S corresponding to these items. Note that L, can be a span within the document, or NULL. Similarly, S can be one or more spans within L, a boolean, or NULL.",
        "Eachdataitem(q,d,l)isindependentandiden-",
        "quality. In that section, we also show that a model trained on (l,q,d) triples can outperform a single annotator on this metric by accounting for the uncertaintyofwhetherornotanansweris present. As well as disagreeing about whether (q,d) contains a valid answer, annotators can disagree about the location of the best answer. In many cases there are multiple valid long answers in multiple distinct locations on the page.8 The most extreme example of this that we see in our 25-way annotated data is for the question ‘‘name the substance used to make the filament of bulb’’ paired with the Wikipedia page about incandescent light bulbs. Annotators identify 7 passages that discuss tungsten wire filaments.",
        "Short answers can be arbitrarily delimited and this can lead to extreme variation. The most extreme example of this that we see in the 25-way annotated data is the 11 distinct, but correct, answers for the question ‘‘where is blood pumped after it leaves the right ventricle’’. Here, 14 annotators identify a substring of ‘‘to the lungs’’ as the best possible short answer. Of these, 6 label the entire string, 4 reduce it to ‘‘the lungs’’, and 4 reduce it to ‘‘lungs’’. A further 6 annotators do not consider this short answer to be sufficient and choose more precise phrases such as ‘‘through the semilunar pulmonary valve into the left and right main pulmonary arteries (one for each lung)’’. The remaining 5 annotators decide that there is no adequate short answer.",
        "NQ includes 5-way annotations on 7,830 items for development data, and we will sequester a further 7,842 items, 5-way annotated, for test data. This section describes evaluation metrics using this data, and gives justification for these metrics.",
        "Toplaceanupperboundonthemetricsintroduced abovewecreatea‘‘super-annotator’’fromthe25- way annotated data introduced in Section 4. From thisdata,wecreatefourtuples(q(i),d(i),a(i),b(i)). The first three terms in this tuple are the question, document, and vector of five reference annotations. b(i) is a vector of annotations b(i) for j j = 1...20 drawn from the same distribution as a(i). The super-annotator predicts NULL if g(b(i)) < α, and l∗ = argmaxl∈d 20 j=1[[l = bj]] otherwise.",
        "TheNQcorpusisdesignedtoprovideabenchmark with which we can evaluate the performance of QA systems. Every question in NQ is unique under exact string match, and we split questions randomly in NQ into separate train/development/test sets. To facilitate comparison, we introduce baselines that either make use of high-level data set regularities,oraretrainedonthe307kexamplesin the training set. Here, we present well-established baselines that were state of the art at the time of submission. We also refer readers to Alberti et al. (2019) for more recent advances in modeling. All of our baselines focus on the long and short answer extraction tasks. We leave boolean answers to future work.",
        "Table 3: Precision (P), recall (R), and the harmonic mean of these (F1) of all baselines, a single annotator, and the super-annotator upper bound. The human performances marked with † are evaluated on a sample of five annotations from the 25-way annotated data introduced in Section 5.",
        "Finally, we break the long answer identification results down according to long answer type. From Table3weknowthatDecAtt+DocReaderpredicts long answers with 54.8% F1. If we only measure performance on examples that should have a paragraph long answer, this increases to 65.1%. For tables and table rows it is 66.4%. And for lists and list items it is 32.0%. All other examples have a NULL label. Clearly, the model is struggling to learn some aspect of list-formatted data from the 6% of the non NULL examples that have this type.",
        "We argue that progress on QA has been hindered by a lack of appropriate training and test data. To address this, we present the Natural Questions corpus. This is the first large publicly available data set to pair real user queries with high-quality annotations of answers in documents. We also presentmetricstobeusedwith NQ,forthepurposes ofevaluatingtheperformanceofquestionanswering systems. We demonstrate a high upper bound onthesemetricsandshowthatexistingmethodsdo not approach this upper bound. We argue that for them to do so will require significant advances in NLU. Figure 5 shows example questions from the data set. Figure 6 shows example question/answer pairs from the data set, together with expert judgments and statistics from the 25-way annotations.",
        "Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yuan Liu, Yizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wang. 2018. Dureader: A Chinese machine reading comprehension dataset from real-world applications. In Proceedings of the Workshop on Machine Reading for Question Answering, pages 37–46, Melbourne.",
        "Mandar Joshi, Eunsol Choi, Daniel Weld, and LukeZettlemoyer.2017.Triviaqa:Alargescale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601–1611.",
        "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Largescale reading comprehension dataset from examinations. In Proceedings of the 2017 Conin Natuference on Empirical Methods ral Language Processing, pages 785–794. Copenhagen.",
        "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can A suit of armor conduct electricity? A new datasetforopenbook question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing,pages2381–2391,Brussels.",
        "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In Proceedings of the Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches.",
        "Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, and David McAllester. 2016. Who did what: A large-scale person-centered cloze dataset. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2230–2235. Austin, TX.",
        "Denis Paperno, Germ´an Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. 2016. The LAMBADAdataset:Wordpredictionrequiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1525–1534, Berlin.",
        "Matthew Richardson, Christopher J. C. Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of text. In Proceedings of the",
        "Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: A challenge dataset for opendomain question answering. In Proceedings of the 2015 Conference on Empirical Methods inNaturalLanguageProcessing,pages2013–2018, Lisbon.",
        "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,WilliamCohen,RuslanSalakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 ConferenceonEmpiricalMethodsinNaturalLanguage Processing, pages 2369–2380, Brussels."
    ]
}