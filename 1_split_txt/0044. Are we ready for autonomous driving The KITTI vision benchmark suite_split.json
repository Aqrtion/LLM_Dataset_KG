{
    "title_author_abstract_introduction": "Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite\nAndreas Geiger and Philip Lenz Karlsruhe Institute of Technology {geiger,lenz}@kit.edu\nRaquel Urtasun Toyota Technological Institute at Chicago rurtasun@ttic.edu\nAbstract\nToday, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical ﬂow, visualodometry/SLAMand3Dobjectdetection. Ourrecording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical ﬂow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difﬁculties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti\n1. Introduction\nDeveloping autonomous systems that are able to assist humans in everyday tasks is one of the grand challenges in modern computer science. One example are autonomous driving systems which can help decrease fatalities caused by trafﬁc accidents. While a variety of novel sensors have beenusedinthepastfewyearsfortaskssuchasrecognition, navigation and manipulation of objects, visual sensors are rarely exploited in robotics applications: Autonomous driving systems rely mostly on GPS, laser range ﬁnders, radar as well as very accurate maps of the environment.\nIn the past few years an increasing number of benchmarks have been developed to push forward the performance of visual recognitions systems, e.g., Caltech-101\nFigure 1. Recording platform with sensors (top-left), trajectory from our visual odometry benchmark (top-center), disparity and optical ﬂow map (top-right) and 3D object labels (bottom).\n[17], Middlebury for stereo [41] and optical ﬂow [2] evaluation. However, most of these datasets are simplistic, e.g., are taken in a controlled environment. A notable exception is the PASCAL VOC challenge [16] for detection and segmentation.\nIn this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for stereo, optical ﬂow, visual odometry / SLAM and 3D object detection. Ourbenchmarksarecapturedbydrivingarounda mid-size city, in rural areas and on highways. Our recording platform is equipped with two high resolution stereo camera systems (grayscale and color), a Velodyne HDL-64E laser scanner that produces more than one million 3D points per second and a state-of-the-art OXTS RT 3003 localization system which combines GPS, GLONASS, an IMU and RTK correction signals. The cameras, laser scanner and localization system are calibrated and synchronized, providing us with accurate ground truth. Table 1 summarizes our benchmarks and provides a comparison to existing datasets. Our stereo matching and optical ﬂow estimation benchmark comprises 194 training and 195 test image pairs at a resolution of 1240 × 376 pixels after rectiﬁcation with semi-dense (50%) ground truth. Compared to previous datasets [41, 2, 30, 29], this is the ﬁrst one with realistic non-synthetic imagery and accurate ground truth. Dif-\n978-1-4673-1228-8/12/$31.00 ©2012 IEEE\nAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 03:29:15 UTC from IEEE Xplore.  Restrictions apply.\nﬁculties include non-lambertian surfaces (e.g., reﬂectance, transparency) large displacements (e.g., high speed), a large variety of materials (e.g., matte vs. shiny), as well as different lighting conditions (e.g., sunny vs. cloudy).\nOur 3D visual odometry / SLAM dataset consists of 22 stereo sequences, with a total length of 39.2 km. To date, datasets falling into this category are either monocular and short [43] or consist of low quality imagery [42, 4, 35]. They typically do not provide an evaluation metric, and as a consequence there is no consensus on which benchmark should be used to evaluate visual odometry / SLAM approaches. Thus often only qualitative results are presented, with the notable exception of laser-based SLAM [28]. We believe a fair comparison is possible in our benchmark due to its large scale nature as well as the novel metrics we propose, which capture different sources of error by evaluating error statistics over all sub-sequences of a given trajectory length or driving speed.\nOur 3D object benchmark focuses on computer vision algorithms for object detection and 3D orientation estimation. While existing benchmarks for those tasks do not provide accurate 3D information [17, 39, 15, 16] or lack realism [33, 31, 34], our dataset provides accurate 3D bounding boxes for object classes such as cars, vans, trucks, pedestrians, cyclists and trams. We obtain this information by manually labeling objects in 3D point clouds produced by our Velodyne system, and projecting them back into the image. This results in tracklets with accurate 3D poses, which can be used to asses the performance of algorithms for 3D orientation estimation and 3D tracking.\nIn our experiments, we evaluate a representative set of state-of-the-art systems using our benchmarks and novel metrics. Perhaps not surprisingly, many algorithms that do well on established datasets such as Middlebury [41, 2] struggle on our benchmark. We conjecture that this might be due to their assumptions which are violated in our scenarios, as well as overﬁtting to a small set of training (test) images.\nIn addition to the benchmarks, we provide MATLAB/C++ development kits for easy access. We also maintain an up-to-date online evaluation server1. We hope that our efforts will help increase the impact that visual recognition systems have in robotics applications.",
    "data_related_paragraphs": [
        "Generating large-scale and realistic evaluation benchmarks for the aforementioned tasks poses a number of challenges, including the collection of large amounts of data in real time, the calibration of diverse sensors working at different rates, the generation of ground truth minimizing the amount of supervision required, the selection of the appro-",
        "1www.cvlibs.net/datasets/kitti",
        "2.1. Sensors and Data Acquisition",
        "We equipped a standard station wagon with two color and two grayscale PointGrey Flea2 video cameras (10 Hz, resolution: 1392×512 pixels, opening: 90◦ ×35◦), a Velodyne HDL-64E 3D laser scanner (10 Hz, 64 laser beams, range: 100 m), a GPS/IMU localization unit with RTK correction signals (open sky localization errors < 5 cm) and a powerful computer running a real-time database [22].",
        "Stereo Matching EISATS [30] Middlebury [41] Make3D Stereo [40] Ladicky [29] Proposed Dataset",
        "Optical Flow EISATS [30] Middlebury [2] Proposed Dataset",
        "Visual Odometry / SLAM TUM RGB-D [43] New College [42] Malaga 2009 [4] Ford Campus [35] Proposed Dataset",
        "Object Detection / 3D Estimation Caltech 101 [17] MIT StreetScenes [3] LabelMe [39] ETHZ Pedestrian [15] PASCAL 2011 [16] Daimler [8] Caltech Pedestrian [13] COIL-100 [33] EPFL Multi-View Car [34] Caltech 3D Objects [31] Proposed Dataset",
        "Table 1. Comparison of current State-of-the-Art Benchmarks and Datasets.",
        "Figure 2. Object Occurence and Object Geometry Statistics of our Dataset. This ﬁgure shows (from left to right and top to bottom): The different types of objects occuring in our sequences, the power-law shaped distribution of the number of instances within an image and the orientation histograms and object size distributions for the two most predominant categories ’cars’ and ’pedestrians’.",
        "We collected a total of ∼ 3 TB of data from which we select a representative subset to evaluate each task. In our experiments we currently concentrate on grayscale images, as they provide higher quality than their color counterparts. For our stereo and optical ﬂow benchmarks we select a subset of the sequences where the environment is static. To maximize diversity, we perform k-means (k = 400) clustering on the data using a novel representation, and chose the elements closest to the center of each cluster for the benchmark. We describe each image using a 144-dimensional image descriptor, obtained by subdividing the image into 12 × 4 rectangular blocks and computing the average disparity and optical ﬂow displacement for each block. After",
        "Our 3D object detection and orientation estimation benchmark is chosen according to the number of nonoccluded objects in the scene, as well as the entropy of the object orientation distribution. High entropy is desirable in order to ensure diversity. Towards this goal we utilize a greedy algorithm: We initialize our dataset X to the empty set ∅ and iteratively add images using the following rule",
        "where X is the current set, x is an image from our dataset, noc(x) stands for the number of non-occluded objects in image x and C denotes the number of object classes. Hc is the entropy of class c with respect to orientation (we use 8/16 orientation bins for pedestrians/cars). We further ensure that images from one sequence do not appear in both training and test set.",
        "For stereo matching, we run global [26, 37, 46], semiglobal [23], local [5, 20, 38] and seed-growing [27, 10, 9] methods. The parameter settings we have employed can be found on www.cvlibs.net/datasets/kitti. Missing disparities are ﬁlled-in for each algorithm using background interpolation [23] to produce dense disparity maps which can then be compared. As Table 2 shows, errors on our benchmark are higher than those reported on Middlebury [41], indicating",
        "theincreasedlevelofdifﬁcultyofourreal-worlddataset. Interestingly, methods ranking high on Middlebury, perform particularly bad on our dataset, e.g., guided cost-volume ﬁltering [38], pixel-wise graph cuts [26]. This is mainly due to the differences in the data sets: Since the Middlebury benchmark is largely well textured and provides a smaller label set, methods concentrating on accurate object boundary segmentation peform well. In contrast, our data requires more global reasoning about areas with little, ambiguous or no texture where segmentation performance is less critical. Purely local methods [5, 38] fail if fronto-parallel surfaces are assumed, as this assumption is often strongly violated in real-world scenes (e.g., road or buildings).",
        "world. Previously hampered by the lack of sufﬁcient training data, such approaches will become feasible in the near future with larger training sets as the one we provide.",
        "We evaluate ﬁve different approaches on our visual odometry / SLAM dataset: VISO2-S/M [21], a real-time stereo/monocular visual odometry library based on incremental motion estimates, the approach of [1] with and withoutLocalBundleAdjustment(LBA)[32]aswellastheﬂow separation approach of [25]. All algorithms are comparable as none of them uses loop-closure information. All approaches use stereo with the exception of VISO2-M [21] which employs only monocular images. Fig. 5 depicts the rotational and translational errors as a function of the trajectory length and driving speed.",
        "rate of 10 frames per second, the vehicle moved up to 2.8 meters per frame. Additionally, large motions mainly occur on highways which are less rich in terms of 3D structure. Large errors at lower speeds stem from the fact that incremental or sliding-window based methods slowly drift over time, with the strongest relative impact at slow speeds. This problem can be easily alleviated if larger timespans are optimized when the vehicle moves slowly or is standing still. In our experiments, no ground truth information has been used to train the model parameters. We expect detecting loop closures, utilizing more enhanced bundle adjustment techniques as well as utilizing the training data for parameter ﬁtting to further boost performance.",
        "We evaluate object detection as well as joint detection and orientation estimation using average precision and average orientation similarity as described in Sec. 2.5. Our benchmark extracted from the full dataset comprises 12,000 images with 40,000 objects. We ﬁrst subdivide the training set into 16 orientation classes and use 100 nonoccluded examples per class for training the part-based object detector of [18] using three different settings: We train the model in an unsupervised fashion (variable), by initializing the components to the 16 classes but letting the components vary during optimization (ﬁxed init) and by initializing the components and additionally ﬁxing the latent variables to the 16 classes (ﬁxed).",
        "Throwing new light on existing methods, we hope that the proposed benchmarks will complement others and help to reduce overﬁtting to datasets with little training or test examples and contribute to the development of algorithms that work well in practice. As our recorded data provides more information than compiled into the benchmarks so far, our intention is to gradually increase their difﬁculties. Furthermore, we also plan to include visual SLAM with loop-closure capabilities, object tracking, segmentation, structure-from-motion and 3D scene understanding into our evaluation framework.",
        "[2] S. Baker, D. Scharstein, J. Lewis, S. Roth, M. Black, and R. Szeliski. A database and evaluation methodology for optical ﬂow. IJCV, 92:1–31, 2011. 1, 2, 3, 4, 5",
        "[4] J.-L. Blanco, F.-A. Moreno, and J. Gonzalez. A collection of outdoor robotic datasets with centimeter-accuracy ground truth. Auton. Robots, 27:327–351, 2009. 2, 3",
        "vision and lidar data set. IJRR, 2011. 2, 3",
        "[39] B. Russell, A. Torralba, K. Murphy, and W. Freeman. Labelme: A database and web-based tool for image annotation. IJCV, 77:157–173, 2008. 2, 3",
        "[42] M. Smith, I. Baldwin, W. Churchill, R. Paul, and P. Newman. The new college vision and laser data set. IJRR, 28:595–599, 2009. 2, 3"
    ]
}