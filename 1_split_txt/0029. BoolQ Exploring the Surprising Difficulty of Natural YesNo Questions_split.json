{
    "title_author_abstract_introduction": "BoolQ: Exploring the Surprising Difﬁculty of Natural Yes/No Questions\nChristopher Clark∗1, Kenton Lee†, Ming-Wei Chang†, Tom Kwiatkowski†\nMichael Collins †2, Kristina Toutanova†\n∗Paul G. Allen School of CSE, University of Washington csquared@cs.uw.edu\n†Google AI Language {kentonl, mingweichang, tomkwiat, mjcollins, kristout}@google.com\nAbstract\nInthispaperwestudyyes/noquestionsthatare naturally occurring — meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information,andrequiredifﬁcultentailment-likeinference to solve. We also explore the effectiveness of a range of transfer learning baselines. We ﬁnd that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneﬁcial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a signiﬁcant gap for future work.\n1 Introduction\nUnderstanding whatfacts canbeinferred tobe true or false from text is an essential part of natural language understanding. In many cases, these inferences can go well beyond what is immediately stated in the text. For example, a simple sentence like “Hanna Huyskova won the gold medal for Belarus in freestyle skiing.” implies that (1) Belarus is a country, (2) Hanna Huyskova is an athlete, (3) Belarus won at least one Olympic event, (4) the USA did not win the freestyle skiing event, and so on.\nTo test a model’s ability to make these kinds of inferences, previous work in natural language in-\n1Work completed while interning at Google. 2Also afﬁliated with Columbia University, work done at\nGoogle.\nQ: Has the UK been hit by a hurricane? P:\nThe Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands ... Yes. [An example event is given.]\nQ: Does Francehave a PrimeMinister and a President? P: ... The extent to which those decisions lie with the Prime Minister or President depends upon ... Yes. [Both are mentioned, so it can be inferred both exist.]\nQ: Have the San Jose Sharks won a Stanley Cup? P:\n... The Sharks have advanced to the Stanley Cup ﬁ- nals once, losing to the Pittsburgh Penguins in 2016 ... No. [They were in the ﬁnals once, and lost.]\nFigure 1: Example yes/no questions from the BoolQ dataset. Each example consists of a question (Q), an excerpt from a passage (P), and an answer (A) with an explanation added for clarity.\nference (NLI) proposed the task of labeling candidate statements as being entailed or contradicted by a given passage. However, in practice, generating candidate statements that test for complex inferential abilities is challenging. For instance, evidence suggests (Gururangan et al., 2018; Jia and Liang, 2017; McCoy et al., 2019) that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning.\nIn this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no questions. That is, questions that were authored by people who were not prompted to write particular kinds of questions, including even being required to write yes/no questions, and who did not know the answer to the question they were asking. Figure 1 contains some examples from our dataset. We ﬁnd such questions often query for\nnon-factoid information, and that human annotators need to apply a wide range of inferential abilities when answering them. As a result, they can be used to construct highly inferential reading comprehension datasets that have the added beneﬁt of being directly related to the practical end-task of answering user yes/no questions.\nYes/No questions do appear as a subset of some existing datasets (Reddy et al., 2018; Choi et al., 2018; Yang et al., 2018). However, these datasets are primarily intended to test other aspects of question answering (QA), such as conversational QA or multi-step reasoning, and do not contain naturally occurring questions.\nWe follow the data collection method used by Natural Questions (NQ) (Kwiatkowski et al., 2019) to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions). Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer. The task is then to take a question and passage as input, and to return “yes” or “no” as output. Figure 1 contains some examples, and Appendix A.1 contains additional randomly selected examples.\nFollowing recent work (Wang et al., 2018), we focus on using transfer learning to establish baselines for our dataset. Yes/No QA is closely related to many other NLP tasks, including other forms of question answering, entailment, and paraphrasit is not clear what the best ing. Therefore, data sources to transfer from are, or if it will be sufﬁcient to just transfer from powerful pretrained language models such as BERT (Devlin et al., 2018) or ELMo (Peters et al., 2018). We experiment with state-of-the-art unsupervised approaches, using existing entailment datasets, three methods of leveraging extractive QA data, and using a few other supervised datasets.\nWe found that transferring from MultiNLI, and the unsupervised pre-training inBERT,gave us the best results. Notably, we found these approaches are surprisingly complementary and can be combined to achieve a large gain in performance. Overall, our best model reaches 80.43% accuracy, compared to 62.31% for the majority baseline and 90% human accuracy. In light of the fact BERT on its own has achieved human-like performance on several NLP tasks, this demonstrates the high degree of difﬁculty of our dataset. We present our data and code at https://goo.gl/boolq.",
    "data_related_paragraphs": [
        "Yes/No questions make up a subset of the reading comprehension datasets CoQA (Reddy et al., 2018), QuAC (Choi et al., 2018), and HotPotQA (Yang et al., 2018), and are present in the ShARC (Saeidi et al., 2018) dataset. These datasets were built to challenge models to understand conversational QA (for CoQA, ShARC and QuAC) or multi-step reasoning (for HotPotQA), which complicates our goal of using yes/no questions totest inferential abilities. Ofthe four, QuAC is the only one where the question authors were not allowed to view the text being used to answer their questions, making it the best candidate to contain naturally occurring questions. However, QuAC still heavily prompts users, including limiting their questions to be about pre-selected Wikipedia articles, and is highly class imbalanced with 80% “yes” answers.",
        "The MS Marco dataset (Nguyen et al., 2016), which contains questions with free-form text answers, also includes some yes/no questions. We experiment with heuristically identifying them in Section 4, but this process can be noisy and the quality of the resulting annotations is unknown. We also found the resulting dataset is class imbalanced, with 80% “yes” answers.",
        "Yes/No QA has been used in other contexts, such as the templated bAbI stories (Weston et al., 2015) or some Visual QA datasets (Antol et al., 2015; Wu et al., 2017). We focus on answering yes/no questions using natural language text.",
        "Question answering for reading comprehension ingeneral has seen a great deal ofrecent work (Rajpurkar et al., 2016; Joshi et al., 2017), and there have been many recent attempts to construct QA datasets that require advanced reasoning abilities (Yang et al., 2018; Welbl et al., 2018; Mihaylov et al., 2018; Zellers et al., 2018; Zhang et al., 2018). However, these attempts typically involve engineering data to be more difﬁcult by, for example, explicitly prompting users to write multi-step questions (Yang et al., 2018; Mihaylov etal.,2018), orﬁltering outeasy questions (Zellers et al., 2018). This risks resulting in models that do not have obvious end-use applications since they are optimized to perform in an artiﬁcial setting. In this paper, we show that yes/no questions have the beneﬁt of being very challenging even when they are gathered from natural sources.",
        "studied area of research, particularly on the MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015) datasets. Other sources of entailment data include the PASCAL RTE challenges (Bentivogli et al., 2009, 2011) or SciTail (Khot et al., 2018). We note that, although SciTail, RTE-6 and RTE-7 did not use crowd workers to generate candidate statements, they still use sources (multiple choices questions or document summaries) that were written by humans with knowledge of the premise text. Using naturally occurring yes/no questions ensures even greater independence between the questions and premise text, and ties our dataset to aclear end-task. BoolQ also requires detecting entailment in paragraphs instead of sentence pairs.",
        "Transfer learning for entailment has been studied in GLUE (Wang et al., 2018) and SentEval (Conneau and Kiela, 2018). Unsupervised pre-training in general has recently shown excellent results on many datasets, including entailment data (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018).",
        "This work builds upon the Natural Questions (NQ) (Kwiatkowski et al., 2019), which contains some natural yes/no questions. However, there are too few (about 1% of the corpus) to make yes/no QA a very important aspect of that task. In this paper, we gather a large number of additional yes/no questions in order to construct a dedicated yes/no QA dataset.",
        "3 The BoolQ Dataset",
        "An example in our dataset consists of a question, a paragraph from a Wikipedia article, the title of the article, and an answer, which is either “yes”",
        "3.1 Data Collection",
        "Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators ﬁnd a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable” if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question’s answer is “yes” or “no”. Annotating data in this manner is quite expensive since annotators need to search entire Wikipedia documents for relevant evidence and read the text carefully.",
        "Note that, unlike in NQ, we only use questions that were marked as having a yes/no answer, and pair each question with the selected passage instead of the entire document. This helps reduce ambiguity (ex., avoiding cases where the document supplies conﬂicting answers in different paragraphs), and keeps the input small enough so that existing entailment models can easily be applied to our dataset.",
        "In the following section we analyze our dataset to better understand the nature of the questions, the annotation quality, and the kinds of reasoning abilities required to answer them.",
        "First, in order to assess annotation quality, three of the authors labelled 110 randomly chosen examples. the auIf there was a disagreement, thors conferred and selected a single answer by mutual agreement. We call the resulting labels “gold-standard” labels. On the 110 selected examples, the answer annotations reached 90% accuracy compared to the gold-standard labels. Of the cases where the answer annotation differed from the gold-standard, six were ambiguous or debatable cases, and ﬁve were errors where the annotator misunderstood the passage. Since the agreement was sufﬁciently high, we elected to use singly-annotated examples in the training/dev/test sets in order to be able to gather a larger dataset.",
        "Part of the value of this dataset is that it contains questions that people genuinely want to answer. To explore this further, we manually deﬁne a set of topics that questions can be about. An author categorized 200 questions into these topics. The results can be found in the upper half of Table 1.",
        "Table 2: Kinds of reasoning needed in the BoolQ dataset.",
        "4Note the dataset has been updated since we carried out",
        "We also think it was important that annotators only had to answer questions, rather than generate them. For example, imagine trying to construct questions that fall into the categories of “Missing Mention” or “Implicit”. While possible, it would require a great deal of thought and creativity. On the other hand, detecting when a yes/no question can be answered using these strategies seems much easier and more intuitive. Thus, having annotators answer pre-existing questions opens the door to building datasets that contain more inference and have higher quality labels.",
        "Models on this dataset need to predict an output class given two pieces of input text, which is a well studied paradigm (Wang et al., 2018). We ﬁnd training models on our train set alone to be relatively ineffective. Our best model reaches 69.6% accuracy, only 8% better than the majority baseline. Therefore, we follow the recent trend in NLP of using transfer learning. In particular, we experiment with pre-training models on related tasks that have larger datasets, and then ﬁne-tuning them on our training data. We list the sources we consider for pre-training below.",
        "Entailment: We two entailment datasets, MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015). We choose these datasets since they are widely-used and large enough to use for pre-training. We also experiment with ablating classes from MultiNLI. During ﬁne-tuning we use the probability the model assigns to the “entailment” class as the probability of predicting a “yes” answer.",
        "Multiple-Choice QA: We use a multiple choice reading comprehension dataset, RACE (Lai et al., 2017), which contains stories or short essays paired with questions built to test the reader’s comprehension of the text. Following what was done in SciTail (Khot et al., 2018), we convert questions and answer-options to statements by either substituting the answer-option for the blanks in ﬁll-in-the-blank questions, or appending a separator token and the answer-option to the question. During training, we have models independently assign a score to each statement, and then apply the softmax operator between all statements per each question to get statement probabilities. We use the negative log probability of the correct statement as a loss function. To ﬁne-tune on BoolQ, we apply the sigmoid operator to the score of the question given its passage to get the probability of a“yes” answer.",
        "Extractive QA: We consider several methods of leveraging extractive QA datasets, where the model must answer questions by selecting text from a relevant passage. Preliminary experiments found that simply transferring the lower-level weights of extractive QA models was ineffective, so we instead consider three methods of con-",
        "structing entailment-like data from extractive QA data.",
        "First, we use the QNLI task from GLUE (Wang et al., 2018), where the model must determine if a sentence from SQuAD 1.1 (Rajpurkar et al., 2016) contains the answer to an input question or not. Following previous work (Hu et al., 2018), we also try building entailment-like training data from SQuAD 2.0 (Rajpurkar et al., 2018). We concatenate questions with either the correct answer, or with the incorrect “distractor” answer candidate provided by the dataset, and train the model to classify which is which given the question’s supporting text.",
        "Paraphrasing: We use the Quora Question Paraphrasing (QQP) dataset, which consists of pairs of questions labelled as being paraphrases or not.5 Paraphrasing is related to entailment since we expect, at least in some cases, passages will contain a paraphrase of the question.",
        "5data.quora.com/First-Quora-Dataset-Release-Question-",
        "The results of our transfer learning methods are shown in Table 3. All results are averaged over ﬁve runs. For models pre-trained on supervised datasets, both the pre-training and the ﬁne-tuning stages were repeated. For unsupervised pretraining, we use the pre-trained models provided by the authors, but continue to average over ﬁve runs of ﬁne-tuning.",
        "Transfer Data",
        "Entailment Results: The MultiNLI dataset out-performed all other supervised methods by a large margin. Remarkably, this approach is only a few points behind BERT despite using orders of magnitude less training data and a much more light-weight model, showing high-quality pre-training data can help compensate for these deﬁciencies.",
        "Our ablation results show that removing the neutral class from MultiNLI hurt transfer slightly, and removing either of the other classes was very harmful, suggesting the neutral examples had limited value. SNLI transferred better than other datasets, but worse than MultiNLI. We suspect this is due to limitations of the photo-caption domain it was constructed from.",
        "Marco. Although Y/N MS Marco is a yes/no QA dataset, its small size and class imbalance likely contributed to its limited effectiveness. The web snippets it uses as passages also present a large domain shift from the Wikipedia passages in BoolQ.",
        "Unsupervised Results: Following results on other datasets (Wang et al., 2018), we found BERTL to be the most effective unsupervised method, surpassing all other methods of pretraining.",
        "In Figure 2, we graph model accuracy as more of the training data is used for ﬁne-tuning, both with and without initially pre-training on MultiNLI. Pre-training on MultiNLI gives at least a 5-6 point gain, and nearly a 10 point gain for BERTL when only using 1000 examples. For small numbers of examples, the recurrent model with MultiNLI pretraining actually out-performs BERTL.",
        "A surprising result from our work is that the datasets that more closely resemble the format of BoolQ, meaning they contain questions and multisentence passages, such as SQuAD 2.0, RACE, or",
        "Y/N MS Marco, were not very useful for transfer. The entailment datasets were stronger despite consisting of sentence pairs. This suggests that adapting from sentence-pair input to question/passage input wasnot alarge obstacle to achieving transfer. Preliminary work found attempting to convert the yes/no questions in BoolQ into declarative statements did not improve transfer from MultiNLI, which supports this hypothesis.",
        "The success of MultiNLI might also be surprising given recent concerns about the generalization abilities of models trained on it (Glockner et al., 2018), particularly related to“annotation artifacts” caused by using crowd workers to write the hypothesis statements (Gururangan et al., 2018). We have shown that, despite these weaknesses, it can still be an important starting point for models being used on natural data.",
        "Note that it is possible to pre-train a model on several of the suggested datasets, either in succession or in a multi-task setup. We leave these experiments to future work. Our results also suggest pre-training on MultiNLIwould behelpful for other corpora that contain yes/no questions.",
        "We have introduced BoolQ, a new reading comprehension dataset of naturally occurring yes/no questions. We have shown these questions are challenging and require a wide range of inference abilities to solve. We have also studied how transfer learning performs on this task, and found crowd-sourced entailment datasets can be leveraged to boost performance even on top of language model pre-training. Future work could include building a document-level version of this task, which would increase its difﬁculty and its correspondence to an end-user application.",
        "Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming Question Answering Datasets Into Natural Language Inference Datasets. Computing Research Repository, arXiv:1809.02922. Version 2.",
        "Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R Bowman, and Noah A Smith. 2018. Annotation Artifacts in Natural Language Inference Data. In NAACL.",
        "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In ACL.",
        "TusharKhot,Ashish Sabharwal,and PeterClark. 2018. SciTail: A Textual Entailment Dataset from Science Question Answering. In AAAI.",
        "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-Scale Reading Comprehension Dataset from Examinations. In EMNLP.",
        "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In EMNLP.",
        "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A Human Generated Machine Reading Comprehension Dataset. Computing Research Repository, arXiv:1611.09268. Version 3.",
        "Jason Phang, Thibault F´evry, and Samuel R Bowman. 2018. Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks. ComputingResearchRepository,arXiv:1811.01088. Version 2.",
        "Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing Datasets for Multi-hop Reading Comprehension Across Documents. In ACL.",
        "Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. 2017. Visual Question Answering: A Survey of Methods In Computer Vision and Image Unand Datasets. derstanding. Elsevier.",
        "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and ChristopherD Manning.2018. Hotpotqa: A Dataset for Diverse, Explainable Multi-hop Question Answering. In EMNLP.",
        "Rowan Zellers, YonatanBisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference. In EMNLP."
    ]
}