{
    "title_author_abstract_introduction": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering visualreasoning.net\nDrew A. Hudson Stanford University 353 Serra Mall, Stanford, CA 94305 dorarad@cs.stanford.edu\nChristopher D. Manning Stanford University 353 Serra Mall, Stanford, CA 94305 manning@cs.stanford.edu\nAbstract\nWe introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages Visual Genome scene graph structures to create 22M diverse reasoning questions, which all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate question biases. Accompanying the dataset is a suite of new metricsthatevaluateessentialqualitiessuchasconsistency, grounding and plausibility. A careful analysis is performed for baselines as well as state-of-the-art models, providing ﬁne-grained results for different question types and topologies. Whereas a blind LSTM obtains a mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3%, offering ample opportunity for new research to explore. We hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding of vision and language.\n1. Introduction\nIt takes more than a smart guess to answer a good question. The ability to assimilate knowledge and use it to draw inferences is among the holy grails of artiﬁcial intelligence. A tangible form of this goal is embodied in the task of Visual Question Answering (VQA), where a system has to answer free-form questions by reasoning about presented images. Thetaskdemandsarichsetofabilitiesasvariedasobject recognition, commonsense understanding and relation extraction, spanning both the visual and linguistic domains. In recent years, it has sparked substantial interest throughout the research community, becoming extremely popular\nFigure 1: Examples from the new GQA dataset for visual reasoning and compositional question answering: Is the bowl to the right of the green apple? What type of fruit in the image is round? What color is the fruit on the right side, red or green? Is there any milk in the bowl to the left of the apple?\nacross the board, with a host of datasets being constructed [4, 11, 15, 41, 20] and numerous models being proposed [5, 38, 6, 10, 12].\nThe multi-modal nature of the task and the diversity of skills required to address different questions make VQA particularly challenging. Yet, designing a good test that will reﬂect its full qualities and complications may not be that trivial. Despite the great strides that the ﬁeld recently made, it has been established through a series of studies that existing benchmarks suffer from critical vulnerabilities that render them highly unreliable in measuring the actual degree of visual understanding capacities [39, 11, 2, 8, 3, 13, 18]. Most notable among the ﬂaws of current benchmarks is the strong and prevalent real-world priors displayed throughout the data [39, 11, 3] – most tomatoes are red and most tables are wooden. These in turn are exploited\nby VQA models, which become heavily reliant upon such statistical biases and tendencies within the answer distribution to largely circumvent the need for true visual scene understanding [2, 11, 15, 8]. This situation is exacerbated by the simplicity of many of the questions, from both linguistic and semantic perspectives, which in practice rarely require much beyond object recognition [33]. Consequently, early benchmarks led to an inﬂated sense of the state of scene understanding, severely diminishing their credibility [37]. Aside from that, the lack of annotations regarding question structure and content leaves it difﬁcult to understand the factors affecting models’ behavior and performance and to identify the root causes behind their mistakes.\nTo address these shortcomings, while retaining the visual and semantic richness of real-world images, we introduce GQA, a new dataset for visual reasoning and compositional question answering. We have developed and carefully reﬁned a robust question engine, leveraging content: information about objects, attributes and relations provided through Visual Genome Scene Graphs [20], along with structure: a newly-created extensive linguistic grammar which couples hundreds of structural patterns and detailed lexical semantic resources. Together, they are combined in our engine to generate over 22 million novel and diverse questions, which all come with structured representations in the form of functional programs that specify their contents and semantics, and are visually grounded in the image scene graphs.\nGQA questions involve varied reasoning skills, and multi-step inference in particular. We further use the associated semantic representations to greatly reduce biases within the dataset and control for its question type composition, downsampling it to create a 1.7M balanced dataset. ContrarytoVQA2.0, herewebalancenotonlybinaryquestions, but also open ones, by applying a tunable smoothing technique that makes the answer distribution for each question group more uniform. Just like a well-designed exam, our benchmark makes the educated guesses strategy far less rewarding, and demands instead more reﬁned comprehension of both the visual and linguistic contents.\nAlong with the dataset, we have designed a suite of new metrics, which include consistency, validity, plausibility, grounding and distribution scores, to complement the standard accuracy measure commonly used in assessing methods’ performance. Indeed, studies have shown that the accuracy metric alone does not account for a range of anomalous behaviors that models demonstrate, such as ignoring key question words or attending to irrelevant image regions [2, 8]. Other works have argued for the need to devise new evaluation measures and techniques to shed more light on systems’ inner workings [18, 34, 35, 17]. In fact, beyond providing new metrics, GQA can even directly support the development of more interpretable models, as it provides\na sentence-long explanation that corroborates each answer, and further associates each word from both the questions and the responses with a visual pointer to the relevant region in the image, similar in nature to datasets by Zhu et al. [41], Park et al. [29], and Li et al. [22]. These in turn can serve as a strong supervision signal to train models with enhanced transparency and accessibility.\nGQA combines the best of both worlds, having clearly deﬁned and crisp semantic representations on the one hand but enjoying the semantic and visual richness of real-world images on the other. Our three main contributions are (1) the GQA dataset as a resource for studying visual reasoning; (2) development of an effective method for generating a large number of semantically varied questions, which marries scene graph representations with computational linguistic methods; (3) new metrics for GQA, that allow for better assessment of system success and failure modes, as demonstrated through a comprehensive performance analysis of existing models on this task. We hope that the GQA dataset will provide fertile ground for the development of novel methods that push the boundaries of question answering and visual reasoning.",
    "data_related_paragraphs": [
        "Recent years have witnessed tremendous progress in visual understanding. Multiple attempts have been made to mitigate the systematic biases of VQA datasets as discussed in section 1 [11, 39, 3, 15], but they fall short in providing an adequate solution: Some approaches operate over constrained and synthetic images [39, 15], neglecting the realismanddiversitynaturalphotosprovide. Meanwhile, Goyal et al. [11] associate most of the questions in VQA1.0 with a pair of similar pictures that result in different answers. While offering partial relief, this technique fails to address open questions, leaving their answer distribution largely unbalanced. In fact, since the method does not cover 29% of the questions due to limitations of the annotation process, even within the binary ones biases still remain.1",
        "At the other extreme, Agrawal et al. [3] partition the questions into training and validation sets such that their respective answer distributions become intentionally dissimilar. While undoubtedly challenging, these adversarial settings penalize models, maybe unjustly, for learning salient properties of the training data. In the absence of other information, making an educated guess is a legitimate choice – a valid and beneﬁcial strategy pursued by machines and people alike [27, 7, 26]. What we essentially need is a balanced test that is more resilient to such gaming strategies, as we strive to achieve with GQA.",
        "In creating GQA, we drew inspiration from the CLEVR task [15], which consists of compositional questions over synthetic images. However, its artiﬁcial nature and low diversity, with only a handful of object classes and properties, makesitparticularlyvulnerabletomemorizationofallcombinations, thereby reducing its effective degree of compositionality. Conversely, GQA operates over real images and a large semantic space, making it much more challenging. Even though our questions are not natural as in other VQA datasets [11, 41], they display a rich vocabulary and diverse linguistic and grammatical structures. They may serve in fact as a cleaner benchmark to asses models in a more controlled and comprehensive fashion, as discussed below.",
        "The task of question generation has been explored in earlier work, mostly for the purpose of data augmentation. Contrary to GQA, those datasets are either small in scale [25] or use only a restricted set of objects and a handful of non-compositional templates [17, 24]. Neural alternatives to visual question generation have been recently proposed [28, 14, 40], but they aim at a quite different goal of creating engaging but potentially inaccurate questions about the wider context of the image such as subjective evoked feelings or speculative events that may lead to or result from the depicted scenes [28].",
        "3. The GQA Dataset",
        "The GQA dataset centers around real-world reasoning, sceneunderstandingandcompositionalquestionanswering. It consists of 113K images and 22M questions of assorted types and varying compositionality degrees, measuring performance on an array of reasoning skills such as object and attributerecognition, transitiverelationtracking, spatialrea-",
        "soning, logical inference and comparisons. Figure 2 provides a brief overview of the GQA components and generation process, and ﬁgure 3 presents multiple instances from the dataset. The dataset along with further information are available at visualreasoning.net.",
        "We proceed by describing the GQA question engine and the four-step dataset construction pipeline: First, we thoroughly clean, normalize, consolidate and augment the Visual Genome scene graphs [20] linked to each image. Then, we traverse the objects and relations within the graphs, and marry them with grammatical patterns gleaned from VQA 2.0 [11] and sundry probabilistic grammar rules to produce a semantically-rich and diverse set of questions. In the third stage, we use the underlying semantic forms to reduce bi-",
        "ases in the conditional answer distribution, resulting in a balanced dataset that is more robust against shortcuts and guesses. Finally, we discuss the question functional representation, and explain how we use it to compute entailment between questions, supporting new evaluation metrics.",
        "Our starting point in creating the GQA dataset is the Visual Genome Scene Graph annotations [20] that cover 113k images from COCO [23] and Flickr [36].2 The scene graph serves as a formalized representation of the image: each node denotes an object, a visual entity within the image, like a person, an apple, grass or clouds. It is linked to a boundingboxspecifyingitspositionandsize, andismarked up with about 1–3 attributes, properties of the object: e.g., its color, shape, material or activity. The objects are connected by relation edges, representing actions (verbs), spatial relations (prepositions), and comparatives.",
        "2We extend Visual Genome dataset with 5k hidden scene graphs col-",
        "Figure 3: Examples of questions from the GQA dataset.",
        "The semantically unambiguous representations offer multiple advantages over free-form unrestricted questions. For one thing, they enable comprehensive assessment of methods by dissecting their performance along different axes of question textual and semantic lengths, type and topology, thus facilitating the diagnosis of their success and failuremodes(section4.2). Second, theyaidusinbalancing the dataset distribution, mitigating its question-conditional priors and guarding against educated guesses (section 3.4). Finally, they allow us to identify entailment and equivalence relations between different questions: knowing the answer to the question What color is the apple? allows a coherent learner to infer the answer to the questions Is the apple red? Is it green? etc. The same goes especially for questions that involve logical inference like or and and operations or spatial reasoning, e.g. left and right.",
        "One of the main issues of existing VQA datasets is the prevalent question-conditional biases that allow learners to make educated guesses without truly understanding the presented images, as explained in section 1. However, precise representation of the question semantics can allow tighter control over these biases, having the potential to greatly alleviate the problem. We leverage this observation and use the functionalprograms attached to each question to smooth out the answer distribution.",
        "Finally, we downsample the questions based on their type to control the dataset type composition, and ﬁlter out redundant questions that are too semantically similar to existing ones. We split the dataset into 70% train, 10% validation, 10% test and 10% challenge, making sure that all the questions about a given image appear in the same split.",
        "In the following, we provide an analysis of the GQA dataset and evaluate the performance of baselines, state-ofthe-art models and human subjects, revealing a large gap from the latter. To establish the diversity and realism of GQA questions, we test transfer performance between the GQA and VQA datasets. We then introduce the new metrics that complement our dataset, present quantitative results",
        "anddiscusstheirimplicationsandmerits. Inthesupplementary, we perform a head-to-head comparison between GQA and the popularVQA 2.0 dataset [11], and proceed with further diagnosis of the current top-performing model, MAC [12], evaluating it along multiple axes such as training-set size, question length and compositionality degree.",
        "4.1. Dataset Analysis and Comparison",
        "The GQA dataset consists of 22,669,678 questions over 113,018 images, which cover wide range of reasoning skills and vary in length and number of required inference-steps (ﬁgure 6). The dataset has a vocabulary size of 3097 words and 1878 possible answers. While smaller than natural language datasets, further investigation reveals that it covers 88.8% and 70.6% of VQA questions and answers respectively, corroborating its wide diversity. A wide selection of dataset visualizations is provided in the supplementary.",
        "We tested the transfer performance between the GQA and VQA datasets, training on one and testing on the other: A MAC model trained on GQA achieves 52.1% on VQA before ﬁne-tuning and 60.5% afterwards. Compare these with 51.6% for LSTM+CNN and 68.3% for MAC, when both are trained and tested on VQA. These quite good results demonstrate the realism and diversity of GQA questions, showing that the dataset can serve as a good proxy for human-like questions. In contrast, MAC trained on VQA gets 39.8% on GQA before ﬁne-tuning and 46.5% afterwards, illustrating the further challenge GQA poses.",
        "Apart from the standard accuracy metric and the more detailed type-based diagnosis our dataset supports, we introduce ﬁve new metrics to get further insight into visual reasoning methods and point to missing capabilities we believe coherent reasoning models should possess.",
        "Figure 6: Top: Dataset statistics, partitioned into structural types, semantic types, and the number of reasoning steps. Bottom: VQA datasets question length distribution.",
        "Validity and Plausibility. The validity metric checks whether a given answer is in the question scope, e.g. responding some color to a color question. The plausibility score goes a step further, measuring whether the answer is reasonable, or makes sense, given the question (e.g. elephant usually do not eat, say, pizza). Speciﬁcally, we check whether the answer occurs at least once in relation with the question’s subject, across the whole dataset. Thus, we consider e.g., red and green as plausible apple colors and, conversely, purple as implausible.6 The experiments show that models fail to respond with plausible or even valid answers at least 5–15% of the times, indicating limited com-",
        "6While the plausibility metric may not be fully precise especially for infrequent objects due to potential data scarcity issues, it may provide a goodsenseofthegenerallevelofworldknowledgethemodelhasacquired.",
        "Table 1: Results for baselines and state-of-the-art models on the GQA dataset. All results refer to the test set. Models are evaluated for overall accuracy as well as accuracy per type. In addition, they are evaluated by validity, plausibility, distribution, consistency, and when possible, grounding metrics. Please refer to the text for further detail.",
        "prehension of some questions. Given that these properties are noticeable statistics of the dataset’s conditional answer distribution, not even depending on the speciﬁc images, we would expect a sound method to achieve higher scores.",
        "Distribution. To get further insight into the extent to which methods manage to model the conditional answer distribution, we deﬁne the distribution metric, which measures the overall match between the true answer distribution and the model predicted distribution, using Chi-Square statistic [21]. It allows us to see if the model predicts not only the most common answers but also the less frequent ones. Indeed, the experiments demonstrate that the leading SOTA models score lower than the baselines (for this metric, lower is better), indicating increased capacity in ﬁtting more subtle trends of the dataset’s distribution.",
        "Grounding. For attention-based models, the grounding score checks whether the model attends to regions within the image that are relevant to the question. For each dataset instance, we deﬁne a pointer r to the visual region which the question or answer refer to, and measure the model’s visual attention (probability) over that region. This metric allows us to evaluate the degree to which the model grounds its reasoning in the image, rather than just making educated guesses based on question priors or world tendencies.",
        "In this paper, we introduced the GQA dataset for realworld visual reasoning and compositional question answering. We described the dataset generation process, provided baseline experiments and deﬁned new measures to get more insight into models’ behavior and performance. We believe this benchmark can help drive VQA research in the right directions of deeper semantic understanding, sound reasoning, enhanced robustness and improved consistency. A potential avenue towards such goals may involve more intimate integration between visual knowledge extraction and question answering, two ﬂourishing ﬁelds that oftentimes have been pursued independently. We strongly hope that GQA will motivate and support the development of more compositional, interpretable and cogent reasoning models, to advance research in scene understanding and visual question answering.",
        "[8] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra. Human attention in visual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding, 163:90–100, 2017. 1, 2 [9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 248–255. Ieee, 2009. 14 [10] A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach. Multimodal compact bilinear pooling for visual question answering and visual grounding. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016. 1",
        "[15] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1988–1997, 2017. 1, 2, 3, 7",
        "[18] K. Kaﬂe and C. Kanan. Visual question answering: Datasets, algorithms, and future challenges. Computer Vision and Image Understanding, 163:3–20, 2017. 1, 2, 16",
        "[36] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li. Yfcc100m: The new data in multimedia research. arXiv preprint arXiv:1503.01817, 2015. 4, 17",
        "[37] A. Torralba and A. A. Efros. Unbiased look at dataset bias. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1521–1528. IEEE, 2011. 2",
        "7. Dataset Visualizations",
        "34%11%6%6%6%5%5%4%4%3%2%2%GQA\tTYPE\tCOMPOSITION\t(DETAILED)queryRelqueryAttrexistRelchooseAttrverifyAttrlogicOrverifyRelqueryObjectchooseRelexistlogicAndverifyAttrsqueryStatechooseObjecttwoSametwoDiffverifyGlobalchooseGlobalcommonchooseObjRelcompareallDiffallSame\fFigure 8: Top left: Question length distribution for VQA datasets: we can see that GQA has a diverse range of lengths compared to all other datasets except synthetic CLEVR. Left: GQA Question structural and semantic type distributions. Right: The object class hierarchy we have created as part of the dataset construction process.",
        "8. Dataset Balancing",
        "Figure 10: Impact of the dataset balancing on the conditional answer distribution: The left side shows the distribution before any balancing. We show the top 10 answers for a selection of question groups, where the column height corresponds to the relative frequency of each answer. The top row shows global question groups such as color questions, questions about animals, etc. while the bottom row shows local ones e.g. apple-color, table-material etc (section 3.3, main paper). Indeed, we can see that the distributions are heavily biased. The right side shows the distributions after balancing, more uniform and with heavier tails, while intentionally retaining the original real-world tendencies up to a tunable degree.",
        "As discussed in section 3.4 (main paper), given the original 22M auto-generated questions, we have performed answerdistributionbalancing, similaritiesreductionandtype-based sampling, reducingitssizetoa1.7Mbalanaceddataset. The balancing is performed in an iterative manner: as explained in section 3.3, for each question group (e.g. color questions), weiterateovertheanswerdistribution, fromthemost to least frequent answers: (ai,ci) when ai is the answer and ci is its count. In each iteration i, we downsample the head distribution (aj,j ≤ i) such that the ratio between the head (cid:80) will be bounded by and its complementary tail 1−(cid:80) b. While doing so, we also make sure to set minimum and maximum bounds on the frequency ratio ci+1 of each pair ci of consequent answers ai,ai+1. The results of this process is shown in ﬁgure 10. Indeed we can see how the distribu-",
        "tion is “pushed” away from the head and spreads over the tail, while intentionally maintaining the original real-world tendencies presented in the data, to retain its authenticity.",
        "We have used a sigmoid-based classiﬁer and trained all models using Adam [19] for 15 epochs, each takes about an hour to complete. For MAC [12], we use the ofﬁcial authored code available online, with 4 cells. For BottomUp [5], since the ofﬁcial implementation is unfortunately not publicly available, we re-implemented the model, carefully following details presented in [5, 34]. To ensure the correctnessofourimplementation, wehavetestedthemodel onthe standard VQA dataset, achieving 67%, which matches the original scores reported by Anderson et al. [5].",
        "Following section 4.2 (main paper), and in order to get more insight into models’ behaviors and tendencies, we perform further analysis of the top-scoring model for the GQA dataset, MAC [12]. The MAC network is a recurrent attention network that reasons in multiple concurrent steps over both the question and the image, and is thus geared towards compositional reasoning as well as rich scenes with several regions of relevance.",
        "0.30.40.50.638131823Accuracy\tNumber\tof\tQuestion\tWordsAccuracy\t/\t#\twords0.40.50.60.70.812345Accuracy\tNumber\tof\tSemantic\tOperationsAccuracy\t/\t#\toperations0.40.450.50.5510100100010000Accuracy\tTraining\tSet\tSize\t(logarithmic\tscale)Accuracy\t/\t\ttraining\tset\tsize0.50.510.520.530.540.5512345Accuracy\tNumber\tof\tCellsAccuracy\t/\t\t#\tcells0.20.30.40.50.60.70.80.91spatial\tfeatures(CNN)object\tfeatures(bottom-up)scene\tscene\tgraph(perfect\tsight)functional\tprogramsINPUT\tREPRESENTATION16%242%344%4+8%GQA\tSEMANTIC\tSTEPS\fFigure 14: Entailment relations between different question types. In section 3.3 (main paper) we discuss the entailment and equivalences between questions. Since every question in the dataset has a matching logical representation of the sequence of reasoning steps, we can formally compute all the entailment and equivalence relations between different questions. Indeed, a cogent and reasonable learner should be consistent between its own answers, e.g. should not answer “red” to a question about the color of an object it has just identiﬁed as blue. Some more subtle relations also occur, such as those involving relations, e.g. if X is above Y, than Y is below X, and X is not below Y, etc. ﬁgure 14 shows all the logical relations between the various question types. Refer to table 2 for a complete catalog of the different types. Experiments show that while people excel at consistency, achieving the impressive 98.4%, deep learning models perform much worse in this task, with 69% - 82%. These results cast a doubt about the reliability of existing models and their true visual understanding skills. We therefore believe that improving their skills towards enhanced consistency and cogency is an important direction, which we hope our dataset will encourage.",
        "We can further see that longer MAC networks with more cells are more competent in performing the GQA task, substantiating its increased compositionality. Other experiments show that increasing the training set size has signiﬁ- cant impact on the model’s performance, as found out also by Kaﬂe et al. [18]. Apparently, the training set size has not reached saturation yet and so models may beneﬁt from even larger datasets. Finally, we have measured the impact of different input representations on the performance. We encode the visual scene with three different methods, ranging from standard pretrained CNN-based spatial features, to object-informed features obtained through faster R-CNNs detectors [31], up to even a “perfect sight” model that has access to the precise semantic scene graph through direct node and edge embeddings. Asﬁgure11shows, themorehigh-levelandsemantic the representation is, the better are the results. On the question side, we explore both training on the standard textual questions as well as the semantic functional programs. MAC achieves 53.8% accuracy and 81.59% consistency on the textual questions and 59.7% and 85.85%",
        "We proceed by performing a comparison with the VQA 2.0 dataset [11], the ﬁndings of which are summarized in table 3. Apart from the higher average question length, we can see that GQA consequently contains more verbs and prepositions than VQA (as well as more nouns and adjectives)",
        "Some VQA question types are not covered by GQA, such as intention (why) questions or ones involving OCR or external knowledge. The GQA dataset focuses on factual questions and multi-hop reasoning in particular, rather than covering all types. Comparing to VQA, GQA questions are objective, unambiguous, more compositional and can be answered from the images only, potentially making this benchmark more controlled and convenient for making research progress on.",
        "Our starting point in creating the GQA dataset is the Visual Genome Scene Graph annotations [20] that cover 113k images from COCO [23] and Flickr [36].7 The scene graph serves as a formalized representation of the image: each node denotes an object, a visual entity within the image, like a person, an apple, grass or clouds. It is linked to a boundingboxspecifyingitspositionandsize, andismarked up with about 1-3 attributes, properties of the object: e.g., its color, shape, material or activity. The objects are connected by relation edges, representing actions (verbs), spatial relations (prepositions), and comparatives.",
        "7We extend the original Visual Genome dataset with 5k new hidden",
        "Figure 15: Question length distribution for Visual Question Answering datasets: we can see that GQA questions have a wide range of lengths and are longer on average than all other datasets except the synthetic CLEVR. Note that the long CLEVR questions tend to sound unnatural at times.",
        "At the next step, we prune graph edges that sound unnatural or are otherwise inadequate to be incoporated within the questions to be generated, such as (woman, in, shirt), (tail, attached to, giraffe), or (hand, hugging, bear). We ﬁlter these triplets using a combination of category-based rules, n-gram frquencies [1], dataset co-occurrence statistics, and manual curation."
    ]
}