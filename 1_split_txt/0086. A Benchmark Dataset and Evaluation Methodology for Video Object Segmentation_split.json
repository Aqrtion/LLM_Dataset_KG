{
    "title_author_abstract_introduction": "A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation\nF. Perazzi1,2\nJ. Pont-Tuset1 B. McWilliams2 1ETH Zurich\nL. Van Gool1 M. Gross1,2 A. Sorkine-Hornung2 2Disney Research\nAbstract\nOver the years, datasets and benchmarks have proven their fundamental importance in computer vision research, enabling targeted progress and objective comparisons in many ﬁelds. At the same time, legacy datasets may impend the evolution of a ﬁeld due to saturated algorithm performance and the lack of contemporary, high quality data. In this work we present a new benchmark dataset and evaluation methodology for the area of video object segmentation. The dataset, named DAVIS (Densely Annotated VIdeo Segmentation), consists of ﬁfty high quality, Full HD video sequences, spanning multiple occurrences of common video object segmentation challenges such as occlusions, motionblur and appearance changes. Each video is accompanied by densely annotated, pixel-accurate and per-frame ground truth segmentation. In addition, we provide a comprehensive analysis of several state-of-the-art segmentation approaches using three complementary metrics that measure the spatial extent of the segmentation, the accuracy of the silhouette contours and the temporal coherence. The results uncover strengths and weaknesses of current approaches, opening up promising directions for future works.\n1. Introduction\nVideo object segmentation is a binary labeling problem aiming to separate foreground object(s) from the background region of a video. A pixel-accurate, spatio-temporal bipartition of the video is instrumental to several applications including, among others, action recognition, object tracking, video summarization, and rotoscoping for video editing. Despite remarkable progress in recent years, video objectsegmentationstill remainsachallenging problem and most existing approaches still exhibit too severe limitations in terms of quality and efﬁciency to be applicable in practical applications, e.g. for processing large datasets, or video post-production and editing in the visual effects industry.\nWhat is most striking is the performance gap among state-of-the-art video object segmentation algorithms and closely related methods focusing on image segmentation\nFigure 1: Sample sequences from our dataset, with ground truthsegmentationmasksoverlayed. Pleaserefertothesupplemental material for the complete dataset.\nand object recognition, which have experienced remarkable progress in the recent years. A key factor bootstrapping this progress has been the availability of large scale datasets and benchmarks [12, 26, 29, 42]. This is in stark contrast to video object segmentation. While several datasets exists for various different video segmentation tasks [1,4,5,15,20,21,25,38,41,44,46,47], none of them targets the speciﬁc task of video object segmentation.\nTo date, the most widely adopted dataset is that of [47], which, however, was originally proposed for joint segmentation and tracking and only contains six low-resolution video sequences, which are not representative anymore for the image quality and resolution encountered in today’s video processing applications. As a consequence, evaluations performed on such datasets are likely to be overﬁtted, without reliable indicators regarding the differences between individual video segmentation approaches, and the real performance on unseen, more contemporary data becomes difﬁcult to determine [6]. Despite the effort of some authors toaugment their evaluation with additional datasets, a standardized and widely adopted evaluation methodology for video object segmentation does not yet exists.\nTo this end, we introduce a new dataset speciﬁcally designed for the task of video object segmentation. The\ndataset, which will be made publicly available, contains ﬁfty densely and professionally annotated high-resolution Full HD video sequences, with pixel-accurate ground-truth data provided for every video frame. The sequences have been carefully captured to cover multiple instances of major challenges typically faced in video object segmentation. The dataset is accompanied with a comprehensive evaluation of several state-of-the-art approaches [5,7,13,14,18, 21,24,33,35,40,43,45]. To evaluate the performance we employ three complementary metrics measuring the spatial accuracy of the segmentation, the quality of the silhouette and its temporal coherence. Furthermore, we annotated each video with speciﬁc attributes such as occlusions, fast-motion, non-linear deformation and motion-blur. Correlated with the performance of the tested approaches, these attributes enable a deeper understanding of the results and point towards promising avenues for future research. The components described above represent a complete benchmark suite, providing researchers with the necessary tools to facilitate the evaluation of their methods and advance the ﬁeld of video object segmentation.",
    "data_related_paragraphs": [
        "In this section we provide an overview of datasets designed for different video segmentation tasks, followed by a survey of techniques targeting video object segmentation.",
        "2.1. Datasets",
        "There exist several datasets for video segmentation, but none of them has been speciﬁcally designed for video object segmentation, the task of pixel-accurate separation of foreground object(s) from the background regions.",
        "The Freiburg-Berkeley Motion Segmentation dataset [5] MoSeg is a popular dataset for motion segmentation, i.e. clustering regions with similar motion. Despite being recently adopted by works focusing on video object segmentation [35,45], the dataset does not fulﬁll several important requirements. Most of the videos have low spatial resolution, segmentation is only provided on a sparse subset of the frames, and the content is not sufﬁciently diverse to provide a balanced distribution of challenging situations such as fast motion and occlusions.",
        "The Berkeley Video Segmentation Dataset (BVSD) [44] comprises a total 100, higher resolution sequences. It was originally meant to evaluate occlusions boundary detection and later extended to over- and motion-segmentation tasks (VSB100 [19]). However, several sequences do not contain a clear object. Furthermore, theground-truth, availableonly for a subset of the frames, is fragmented, with most of the objects being covered by multiple manually annotated, disjoint segments, and therefore this dataset is not well suited for evaluating video object segmentation.",
        "SegTrack [47] is a small dataset composed of 6 densely annotated videos of humans and animals. It is designed to bechallengingwithrespecttobackground-foregroundcolor similarity, fast motion and complex shape deformation. Although it has been extensively used by several approaches, its content does not sufﬁciently span the variety of challenges encountered in realistic video object segmentation applications. Furthermore, the image quality is not anymore representative of modern consumer devices, and due to the limited number of available video sequences, progress on In [25] this dataset was extended this dataset plateaued. with 8 additional sequences. While this is certainly an improvement over the predecessor, it still suffers of the same limitations. We refer the reader to the supplemental material for a comprehensive summary of the properties of the aforementioned datasets, including ours.",
        "Other datasets exist, but they are mostly provided to support speciﬁc ﬁndings and thus are either limited in terms of totalnumberofframes,[8,21,25,47], ordonotexhibitasufﬁcient variety in terms of content [1,4,5,15,17,20,41,46]. Others cover a broader range of content but do not provide enough ground-truth data for an accurate evaluation of the segmentation [21, 38]. Video datasets designed to benchmark tracking algorithms typically focus on surveillance scenarios with static cameras [9,16,32], and usually contain multiple instances of similar objects [50] (e.g. a crowd of people), and annotation is typically provided only in the form of axis-aligned bounding boxes, instead of pixelaccurate segmentation masks necessary to accurately evaluate video object segmentation. Importantly, none of the aforementioned methods includes contemporary high resolution videos, which is an absolute necessity to realistically evaluate the actual practical utility of such algorithms.",
        "We categorize the body of literature related to video objectsegmentationbasedonthelevelofsupervisionrequired. Unsupervised approaches have historically targeted over-segmentation [21, 51] or motion segmentation [5, 18] and only recently automatic methods for foregroundbackground separation have been proposed [13,25,33,43, 45,52]. These methods extend the concept of salient object detection [34] to videos. They do not require any manual annotation and do not assume any prior information on the object to be segmented. Typically they are based on the assumption that object motion is dissimilar from the surroundings. Some of these methods generate several ranked segmentation hypotheses [24]. While they are well suited for parsing large scale databases, they are bound to their underlying assumption and fail in cases it does not hold.",
        "Table 1: List of video attributes and corresponding description. We extend the annotations of [50] (top) with a complementary set of attributes relevant to video object segmentation (bottom). We refer the reader to the supplementary material for the list of attributes for each in video in the dataset, and corresponding visual examples.",
        "We evaluate a large set of the state-of-the-art approaches on our proposed dataset, providing new insights and several pointers to areas for future research.",
        "3. Dataset Description",
        "In this section we describe our new dataset DAVIS (Densely Annotated VIdeo Segmentation) speciﬁcally designed for the task of video object segmentation. Exam-",
        "ple frames of some of the sequences are shown in Figure 1. Based on experiences with existing datasets we ﬁrst identify fourkeyaspectsweadhereto, inordercreateabalancedand comprehensive dataset.",
        "Data Amount and Quality. A sufﬁciently large amount of data is necessary to ensure content diversity and to provide a uniformly distributed set of challenges. Furthermore, having enough data is crucial to avoid over-ﬁtting and to delay performance saturation, hence guaranteeing a longer lifespan of the dataset [6]. The quality of the data also plays a crucial role, as it should be representative of the current state of technology. To this end, DAVIS comprises a total of 50 sequences, 3455 annotated frames, all captured at 24fps and Full HD 1080p spatial resolution. Due to the computational complexity being a major bottleneck in video processing, the sequences have a short temporal extent (about 2-4 seconds), but include all major challenges typically found in longer video sequences, see Table 1.",
        "Experimental Validation. For each video frame, we provide pixel-accurate, manually created segmentation in the form of a binary mask. While we subdivide DAVIS into training- and a test-set to provide guidelines for future works, in our evaluation, we do not make use of the partition, and instead consider the dataset as a whole, since most oftheevaluatedapproachesarenottrainedandagrid-search estimation of the optimal parameters would be infeasible due to the involved computational complexity.",
        "Object Presence. Intuitively each sequence should contain at least one target foreground-object to be separated from the background regions. The clips in DAVIS contain either one single object or two spatially connected objects. We choose not to have multiple distinct objects with significant motion in order to be able to fairly compare segmentation approaches operating on individual objects against those that jointly segment multiple objects. Moreover, having a single objectper sequence disambiguates the detection performed by methods which are fully automatic. A similar design choice made in [27] has been successfully steering research in salient object detection from its beginnings to the current state-of-the-art. To ensure sufﬁcient content diversity, which is necessary to comprehensively assess the performance of different algorithms, the dataset spans four evenly distributed classes (humans, animals, vehicles, objects) and several actions.",
        "O V Figure 2: Left: Attributes distribution over the dataset. Each bin indicates the number of occurrences. Right: Mutual dependencies among attributes. The presence of a link indicates high probability of an attribute to appear in a sequence, if the one on the other end is also present.",
        "sive, therefore a sequence can be annotated with multiple attributes. Their distribution over the dataset, i.e. number of occurrences, and their pairwise dependencies are shown in Figure 2. The annotations enable us to decouple the analysis of the performance into different groups with dominant characteristics (e.g. occlusion), yielding a better understanding of each methods’ strengths and weaknesses.",
        "In order to judge the quality of a segmentation, the choice of a suitable metric is largely dependent on the end goal of the ﬁnal application [10]. Intuitively, when video segmentation is used primarily a classiﬁer within a larger processing pipeline, e.g. for parsing large scale datasets, it makes sense to seek the lowest amount of mislabeled pixels. On the other hand, in video editing applications the accuracy of the contours and their temporal stability is of highest importance, as these properties usually require the most painstaking and time-consuming manual input. In order to exhaustively cover the aforementioned aspects we evaluate the video segmentation results using three complementary error metrics. We describe the metrics in Section 4.1 and we empirically validate their complementary properties on the proposed dataset in Section 4.2.",
        "To verify that the use of these measures produces meaningful results on our dataset, we compute the pairwise correlation between the region similarity J and the contour",
        "For each of the methods we kept the default parameters ﬁxed throughout the entire dataset. Despite a considerable effort to speed-up the computation (parallelizing preprocessing steps such as motion estimation or extraction",
        "The evaluation scripts, the input data, and the output re-",
        "For a given error measure C we consider three different statistics. Let R = {Si} be the dataset of video sequences Si and let ¯C(Si) be the error measure average on Si. The mean is the average dataset error deﬁned as ¯C(Si). The decay quantiﬁes the perMC(R) = 1 formance loss (or gain) over time. Let Qi = {Q1 i,..,Q4 i} be a partition of Si in quartiles, we deﬁne the decay as DC(R) = 1 i). The object recall measures the fraction of sequences scoring higher than a ✶ ¯C(Si)>τ, with threshold, deﬁned as OC(R) = 1 τ = 0.5 in our experiments.",
        "BVS [31] scored MJ = 0.665, therefore being the best performer in terms of region similarity, with the advantage of having the parameters tuned on this speciﬁc dataset.",
        "V ∈ {1,...,16} and (unknown) edge set E. The absence of an edge between two attributes denotes that they are independent conditioned on the remaining attributes. Given a collection of n = 50 binary vectors denoting the presence of attributes in each video sequence, we estimate E via ℓ1 penalized logistic regression. To ensure robustness in the estimated graph we employ stability selection [30]. Brieﬂy, this amounts to performing the above procedure on n/2-sized subsamples of the data multiple times and computing the proportion of times each edge is selected. Setting an appropriate threshold on this selection probability allows us to control the number of wrongly estimated edges according to Theorem 1 in [30]. For example, for a threshold value of 0.6 and choosing a value of λ which on average selects neighbourhoods of size 4, the number of wrongly selected edges is at most 4 (out of 162 = 256 possible edges). The estimated dependencies are visualized in Figure 2 (right). As expected there is a mutual dependency between attributes such as fast-mostion (FM) and motionblur (MB), or interacting-object (IO) and shape-complexity (SC). We refer the reader to the supplementary material for further details.",
        "Results. In Table 3 we report the performance on subsets ofthedatasetscharacterizedbyaparticularattribute. Dueto space limitations we reduce the analysis in the paper to the most informative and recurrent attributes. Furthers details can be found in the supplementary material.",
        "To the best of our knowledge, this work represents the currently largest scale performance evaluation of video object segmentation algorithms. One of course has to consider that the evaluated approaches have been developed using different amounts and types of input data and ground-truth, or were partially even designed for different problems and only later adapted to the task of video object segmentation. However, the primary aim of our evaluation is not to determine a winner, but to provide researchers with high-quality, contemporary data, a solid standardized evaluation procedure, and valuable comparisons with the current state-ofthe-art. We hope that the public availability of this dataset and the identiﬁed areas for potential future works will motivate even more interest in such an active and fundamentally important ﬁeld for video processing.",
        "As any dataset, also DAVIS will have a limited life-span. Therefore we welcome external contributions to extend it,",
        "Currently, running time efﬁciency and memory requirements are a major bottleneck for the usability of several video segmentation algorithms. In our experiments we observed that a substantial amount of time is spent preprocessing images to extract boundary preserving regions, object proposals and motion estimates. We encourage future research to carefully select those components bearing in mind they could compromise the practical utility of their work. Efﬁcient algorithms will be able to take advantage of the Full HD videos and accurate segmentation masks made available with this dataset. Leveraging high resolution might not produce better results in terms of region-similarity, but it is essential to improve the segmentation of complex object contours and tiny object region.",
        "[4] G. J. Brostow, J. Fauqueur, and R. Cipolla. Semantic object classes in video: A high-deﬁnition ground truth database. Pattern Recognition Letters, 30(2), 2009. 1, 2",
        "[14] Q. Fan, F. Zhong, D. Lischinski, D. Cohen-Or, and B. Chen. Jumpcut: Non-successive mask transfer and interpolation for video cutout. ACM Trans. Graph., 34(6), 2015. 2, 3, 5, 6 [15] A. Fathi, X. Ren, and J. M. Rehg. Learning to recognize objects in egocentric activities. In CVPR, 2011. 1, 2 [16] R. B. Fisher. The pets04 surveillance ground-truth data sets.",
        "[29] D. R. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001. 1",
        "[32] S. Oh, A. Hoogs, A. G. A. Perera, and M. Desai. A largescale benchmark dataset for event recognition in surveillance video. In CVPR, 2011. 2"
    ]
}