{
    "title_author_abstract_introduction": "Building  a Large Annotated  Corpus  of English:  The  Penn Treebank\nMitchell P. Marcus* University of Pennsylvania\nBeatrice Santorini t Northwestern University\nMary Ann Marcinkiewicz~ University of Pennsylvania\n1. Introduction\nThere is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.\nIn this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992),  this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.\nThe paper is organized as follows. Section 2 discusses the POS tagging task. After outlining  the  considerations  that  informed the  design  of  our  POS  tagset  and  presenting  the  tagset  itself,  we  describe  our  two-stage  tagging  process,  in  which  text is  first  assigned  POS  tags  automatically and  then  corrected by  human  annotators. Section 3 briefly presents the results  of a  comparison between entirely manual  and semi-automated tagging, with the latter being shown to be superior on three counts: speed, consistency, and accuracy. In Section 4, we turn to the bracketing task. Just as with the tagging task, we have partially automated the bracketing task: the output of\n•  Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA\nf Department of Linguistics, Northwestern University, Evanston, IL 60208. :~ Department of Computer and Information Sciences, University of Pennsylvania, Philadelphia, PA\n1 A distinction is sometimes made between a corpus as a carefully structured set of materials gathered\ntogether to jointly meet some design principles, and a collection, which may be much more opportunistic in construction. We acknowledge that from this point of view, the raw materials of the Penn Treebank form a collection.\n(~) 1993 Association for Computational Linguistics\nComputational Linguistics\nVolume 19, Number 2\nthe POS tagging phase is automatically parsed and  simplified to yield a skeletal syntactic representation,  which is then corrected by human  annotators.  After presenting the set of syntactic tags that we use, we illustrate and discuss the bracketing process. In particular,  we will outline various factors that affect the speed with which annotators are able to correct bracketed structures, a task that--not  surprisingly--is  considerably more  difficult than  correcting  POS-tagged  text.  Finally,  Section 5 describes the  composition and size of the current Treebank corpus, briefly reviews some of the research projects that  have relied on it to date,  and  indicates  the  directions  that  the project is likely to take in the future.",
    "data_related_paragraphs": [
        "2.1.1 Recoverability.  Like the tagsets just mentioned, the Penn Treebank tagset is based on that  of the  Brown Corpus.  However, the  stochastic  orientation  of the  Penn  Treebank and  the resulting  concern with sparse data led us to modify the Brown Corpus tagset by paring  it down considerably.  A  key strategy  in reducing  the  tagset was  to eliminate  redundancy  by taking  into  account both lexical and  syntactic information. Thus, whereas many POS tags in the Brown Corpus tagset are unique  to a particular lexical item, the Penn Treebank tagset strives to eliminate such instances of lexical redundancy. For instance, the Brown Corpus distinguishes  five different forms for main verbs:  the  base  form  is  tagged  VB, and  forms  with  overt  endings  are  indicated  by appending  D  for  past  tense,  G  for present  participle/gerund,  N  for  past  participle, and  Z for third person singular  present.  Exactly the same paradigm  is recognized for have, but have  (regardless  of whether  it is used as an auxiliary  or a main verb) is assigned  its own base tag HV. The  Brown Corpus  further  distinguishes  three  forms  of do--the base form (DO), the past tense (DOD), and  the third  person singular  present (DOZ), 4 and  eight forms of be--the five forms distinguished  for regular verbs as well as  the  irregular  forms  am  (BEM),  are  (BER),  and  was  (BEDZ).  By contrast,  since  the distinctions between the forms of VB on the one hand  and  the forms of BE, DO, and HV on the  other are lexically recoverable, they are eliminated  in the Penn Treebank, as shown in Table 1. 5",
        "5.1 Composition and Size of Corpus Table 4 shows the output of the Penn Treebank project at the end of its first phase. All the materials listed in Table 4 are available on CD-ROM to members of the Linguistic Data Consortium. 12 About 3 million words of POS-tagged material and a small sampiing of skeletally parsed  text are available as part  of the first Association for Computational  Linguistics/Data Collection Initiative CD-ROM,  and  a  somewhat  larger subset  of materials  is  available  on  cartridge  tape  directly from  the  Penn  Treebank Project. For information, contact the first author of this paper  or send e-mail to treebank@unagi.cis.upenn.edu.",
        "11 This use of pseudo-attachment is identical to its original use in Church's parser (Church 1980). 12 Contact the Linguistic Data Consortium, 441 Williams Hall, University of Pennsylvania, Philadelphia",
        "•  All of the skeletally parsed Dow Jones Newswire materials are also available as digitally recorded read speech as part of the DARPA WSJ-CSR1 corpus, available through the Linguistic Data Consortium.",
        "5.2  Future  Directions A  large number  of research efforts, both at the University of Pennsylvania  and  elsewhere, have relied on the output of the Penn Treebank Project to date. A few examples already in print: a number of projects investigating stochastic parsing have used either the POS-tagged materials  (Magerman and Marcus  1990; Brill et al. 1990; Brill 1991) or the  skeletally parsed  corpus  (Weischedel et al.  1991;  Pereira  and  Schabes  1992).  The POS-tagged corpus has also been used to train a number  of different POS taggers including Meteer, Schwartz, and Weischedel (1991), and the skeletally parsed corpus has been used in connection with the development of new methods to exploit intonational cues in disambiguating the parsing of spoken sentences (Veilleux and Ostendorf 1992). The Penn Treebank has been used to bootstrap the development of lexicons for particular applications (Robert Ingria, personal communication) and is being used as a source of examples for linguistic theory and  psychological modelling  (e.g. Niv  1991). To aid in  the  search  for  specific examples  of grammatical  phenomena  using  the  Treebank, Richard  Pito has  developed tgrep,  a  tool for very fast context-free pattern  matching against  the  skeletally parsed  corpus,  which  is available  through  the  Linguistic  Data Consortium.",
        "Acknowledgments The work reported here was partially supported by DARPA grant No. N0014-85-K0018, by DARPA and AFOSR jointly under grant No. AFOSR-90-0066 and by ARO grant No. DAAL 03-89-C0031 PRI. Seed money was provided by the General Electric Corporation under grant No. J01746000. We gratefully acknowledge this support. We would also like to acknowledge the contribution of the annotators who have worked on the Penn Treebank Project: Florence Dong, Leslie Dossey, Mark Ferguson, Lisa Frank, Elizabeth Hamilton, Alissa Hinckley, Chris Hudson, Karen Katz, Grace Kim, Robert MacIntyre, Mark Parisi, Britta Schasberger,  Victoria Tredinnick and Matt Waters; in addition, Rob Foye, David Magerman, Richard Pito and Steven Shapiro deserve our special thanks for their administrative and programming support. We are grateful to AT&T Bell Labs for permission to use Kenneth Church's PARTS part-of-speech labeler and Donald Hindle's Fidditch parser.  Finally, we would like to thank Sue Marcus for sharing with us her statistical expertise and providing the analysis of the time data of the experiment reported in Section 3. The design of that experiment is due to the first two authors; they alone are responsible for its shortcomings."
    ]
}