{
    "title_author_abstract_introduction": "The Kinetics Human Action Video Dataset\nWill Kay wkay@google.com\nJo˜ao Carreira joaoluis@google.com\nKaren Simonyan simonyan@google.com\nBrian Zhang brianzhang@google.com\nChloe Hillier chillier@google.com\nSudheendra Vijayanarasimhan svnaras@google.com\nFabio Viola fviola@google.com\nTim Green tfgg@google.com\nTrevor Back back@google.com\nPaul Natsev natsev@google.com\nMustafa Suleyman mustafasul@google.com\nAndrew Zisserman zisserman@google.com\nAbstract\nWe describe the DeepMind Kinetics human action video dataset. The dataset contains 400 human action classes, with at least 400 video clips for each action. Each clip lasts around 10s and is taken from a different YouTube video. The actions are human focussed and cover a broad range of classes including human-object interactions such as playing instruments, as well as human-human interactions such as shaking hands. We describe the statistics of the dataset, how it was collected, and give some baseline performance ﬁgures for neural network architectures trained and tested for human action classiﬁcation on this dataset. We also carry out a preliminary analysis of whether imbalance in the dataset leads to bias in the classiﬁers.\n1. Introduction\nIn this paper we introduce a new, large, video dataset for human action classiﬁcation. We developed this dataset principally because there is a lack of such datasets for human action classiﬁcation, and we believe that having one will facilitate research in this area – both because the dataset is large enough to train deep networks from scratch, and also because the dataset is challenging enough to act as a performance benchmark where the advantages of different architectures can be teased apart.\nOur aim is to provide a large scale high quality dataset, covering a diverse range of human actions, that can be used for human action classiﬁcation, rather than temporal localization. Since the use case is classiﬁcation, only short clips of around 10s containing the action are included, and there are no untrimmed videos. However, the clips also contain sound so the dataset can potentially be used for many\npurposes, including multi-modal analysis. Our inspiration in providing a dataset for classiﬁcation is ImageNet [18], where the signiﬁcant beneﬁts of ﬁrst training deep networks on this dataset for classiﬁcation, and then using the trained network for other purposes (detection, image segmentation, non-visual modalities (e.g. sound, depth), etc) are well known.\nThe Kinetics dataset can be seen as the successor to the two human action video datasets that have emerged as the standard benchmarks for this area: HMDB-51 [15] and UCF-101 [20]. These datasets have served the community very well, but their usefulness is now expiring. This is because they are simply not large enough or have sufﬁcient variation to train and test the current generation of human action classiﬁcation models based on deep learning. Coincidentally, one of the motivations for introducing the HMDB dataset was that the then current generation of action datasets was too small. The increase then was from 10 to 51 classes, and we in turn increase this to 400 classes.\nTable 1 compares the size of Kinetics to a number of recent human action datasets. In terms of variation, although the UCF-101 dataset contains 101 actions with 100+ clips for each action, all the clips are taken from only 2.5k distinctvideos. Forexamplethereare7clipsfromonevideoof the same person brushing their hair. This means that there is far less variation than if the action in each clip was performed by a different person (and different viewpoint, lighting, etc). This problem is avoided in Kinetics as each clip is taken from a different video.\nThe clips are sourced from YouTube videos. Consequently, for the most part, they are not professionally videoed and edited material (as in TV and ﬁlm videos). There can be considerable camera motion/shake, illumination variations, shadows, background clutter, etc. More im-\nDataset HMDB-51 [15] UCF-101 [20] ActivityNet-200 [3] Kinetics\nYear Actions 2011 2012 2015 2017\nClips 51 min 102 101 min 101 200 avg 141 400 min 400\nTotal 6,766 13,320 28,108 306,245\nVideos 3,312 2,500 19,994 306,245\nTable 1: Statistics for recent human action recognition datasets. ‘Actions’, speciﬁes the number of action classes; ‘Clips’, the number of clips per class; ‘Total’, is the total number of clips; and ‘Videos’, the total number of videos from which these clips are extracted.\nportantly, there are a great variety of performers (since each clip is from a different video) with differences in how the action is performed (e.g. its speed), clothing, body pose and shape, age, and camera framing and viewpoint.\nOur hope is that the dataset will enable a new generation of neural network architectures to be developed for video. For example, architectures including multiple streams of information (RGB/appearance, optical ﬂow, human pose, object category recognition), architectures using attention, etc. That will enable the virtues (or otherwise) of the new architectures to be demonstrated. Issues such as the tension betweenstaticandmotionprediction, andtheopenquestionof the best method of temporal aggregation in video (recurrent vs convolutional) may ﬁnally be resolved.\nThe rest of the paper is organized as: Section 2 gives an overview of the new dataset; Section 3 describes how it was collected and discusses possible imbalances in the data and their consequences for classiﬁer bias. Section 4 gives the performance of a number of ConvNet architectures that are trained and tested on the dataset. Our companion paper [5] explores the beneﬁt of pre-training an action classiﬁcation network on Kinetics, and then using the features from the network for action classiﬁcation on other (smaller) datasets. The URLs of the YouTube videos and temporal intervals of the dataset can be obtained from http://deepmind. com/kinetics.",
    "data_related_paragraphs": [
        "2. An Overview of the Kinetics Dataset",
        "Content: The dataset is focused on human actions (rather than activities or events). The list of action classes covers: Person Actions (singular), e.g. drawing, drinking, laughing, pumping ﬁst; Person-Person Actions, e.g. hugging, kissing, shaking hands; and, Person-Object Actions, e.g. opening present, mowing lawn, washing dishes. Some actions are ﬁne grained and require temporal reasoning to distinguish, for example different types of swimming. Other actions require more emphasis on the object to distinguish, for example playing different types of wind instruments.",
        "Statistics: Thedataset has400 humanaction classes, with 400–1150 clips for each action, each from a unique video. Each clip lasts around 10s. The current version has 306,245 videos, and is divided into three splits, one for training having 250–1000 videos per class, one for validation with 50 videos per class and one for testing with 100 videos per class. The statistics are given in table 2. The clips are from YouTube videos and have a variable resolution and frame rate.",
        "Table 2: Kinetics Dataset Statistics. The number of clips for each class in the train/val/test partitions.",
        "Non-exhaustive annotation. Each class contains clips illustrating that action. However, a particular clip can contain several actions. Interesting examples in the dataset include: “texting” while “driving a car”; “Hula hooping” while “playing ukulele”; “brushing teeth” while “dancing” (of some type). In each case both of the actions are Kinetics classes, and the clip will probably only appear under only one of these classes not both, i.e. clips do not have complete (exhaustive) annotation. For this reason when evaluating classiﬁcation performance, a top-5 measure is more suitable than top-1. This is similar to the situation in ImageNet [18], where one of the reasons for using a top-5 measure is that images are only labelled for a single class, although it may contain multiple classes.",
        "3. How the Dataset was Built",
        "Figure 1: Example classes from the Kinetics dataset. Best seen in colour and with zoom. Note that in some cases a single image is not enough for recognizing the action (e.g. “headbanging”) or distinguishing classes (“dribbling basketball” vs “dunking basketball”). The dataset contains: Singular Person Actions (e.g. “robot dancing”, ”stretching leg”); Person-Person Actions (e.g. “shaking hands”, ”tickling”); Person-Object Actions (e.g. “riding a bike”); same verb different objects (e.g. “playing violin”, “playing trumpet”); and same object different verbs (e.g. “dribbling basketball”, “dunking basketball”). These are realistic (amateur) videos – there is often signiﬁcant camera shake, for instance.",
        "and clean up the dataset. We then discuss possible biases in the dataset due to the collection process.",
        "Overview: clips for each class were obtained by ﬁrst searching on YouTube for candidates, and then using Amazon Mechanical Turkers (AMT) to decide if the clip contains the action or not. Three or more conﬁrmations (out of ﬁve) were required before a clip was accepted. The dataset was de-duped, by checking that only one clip is taken from each video, and that clips do not contain common video material. Finally, classes were checked for overlap and denoised.",
        "Curating a large list of human actions is challenging, as there is no single listing available at this scale with suitable visual action classes. Consequently, we had to combine numerous sources together with our own observations of actions that surround us. These sources in(i) Action datasets – existing datasets like Acclude: tivityNet [3], HMDB [15], UCF101 [20], MPII Human Pose[2], ACT[25]haveusefulclassesandasuitablesub set of these were used; (ii) Motion capture – there are a numberofmotioncapturedatasetswhichwelookedthroughand extracted ﬁle titles. These titles described the motion within the ﬁle and were often quite creative; and, (iii) Crowdsourced – we asked Mechanical Turk workers to come up with a more appropriate action if the label we had presented to them for a clip was incorrect.",
        "Following annotating, the video ids, clip times and labels were exported from the database and handed on to be used for model training.",
        "In order for a clip to be added to the dataset, it needed to receive at least 3 positive responses from workers. We allowed each clip to be annotated 5 times except if it had been annotated by more than 2 of a speciﬁc response. For example, if 3 out of 3 workers had said it did not contain an exampleoftheactionwewouldimmediatelyremoveitfrom the pool and not continue until 5 workers had annotated it. Due to the large scale of the task it was necessary to quickly remove classes that were made up of low quality or completely irrelevant candidates. Failing to do this would have meant that we spent a lot of money paying workers to mark videos as negative or bad. Accuracies for each class were calculated after 20 clips from that class had been annotated. We adjusted the accuracy threshold between runs but would typically start at a high accuracy of 50% (1 in 2 videos were expected to contain the action).",
        "The amount of worker trafﬁc that the task generated meant that we could not rely on direct fetching and writes to the database even with appropriate indexes and optimised queries. We therefore created many caches which were made up of groups of clips for each worker. When a worker started a new task, the interface would fetch a set of clips for that speciﬁc worker. The cache was replenished often by background processes as clips received a sufﬁcient number of annotations. This also negated labelling collisions where previously > 1 worker might pick up the same video to annotate and we would quickly exceed 5 responses for any 1 clip.",
        "One of the dataset design goals was having a single clip from each given video sequence, different from existing datasets which slice videos containing repetitive actions into many (correlated) training examples. We also employed mechanisms for identifying structural problems as we grew the dataset, such as repeated classes due to synonymyordifferentwordorder(e.g. ridingmotorbike, riding motorcycle), classes that are too general and co-occur with many others (e.g. talking) and which are problematic for typical 1-of-K classiﬁcation learning approaches (instead of multi-label classiﬁcation). We will now describe these procedures.",
        "Detecting noisy classes. Classes can be ‘noisy’ in that they may overlap with other classes or they may contain several quite distinct (in terms of the action) groupings due to an ambiguity in the class name. For example, ‘skipping’ can be ‘skipping with a rope’ and also ‘skipping stones across water’. We trained two-stream action classiﬁers [19] repeatedly throughout the dataset development to identify these noise classes. This allowed us to ﬁnd the top confusions for each class, which sometimes were clear even by just verifying the class names (but went unnoticed due",
        "to the scale of the dataset), and other times required eyeballing the data to understand if the confusions were alright and the classes were just difﬁcult to distinguish because of shortcomings of the model. We merged, split or outright removed classes based on these detected confusions.",
        "Final ﬁltering. After all the data was collected, deduplicatedandtheclasseswereselected, weranaﬁnalmanual clip ﬁltering stage. Here the class scores from the twostream model were again useful as they allowed sorting the examples from most conﬁdent to least conﬁdent – a measure of how prototypical they were. We found that noisy examples were often among the lowest ranked examples and focused on those. The ranking also made adjacent any remaining duplicate videos, which made it easier to ﬁlter out those too.",
        "3.5. Discussion: dataset bias I",
        "We are familiar with the notion of dataset bias leading to lack of generalization: where a classiﬁer trained on one dataset, e.g. Caltech 256 [10], does not perform well when tested on another, e.g. PASCAL VOC [8]. Indeed it is even possible to train a classiﬁer to identify which dataset an image belongs to [22].",
        "There is another sense of bias which could arise from unbalanced categories within a dataset. For example, gender imbalance in a training set could lead to a corresponding performance bias for classiﬁers trained on this set. There are precedents for this, e.g. in publicly available face detectors not being race agnostic1, and more recently in learning a semantic bias in written texts [4]. It is thus an important question as to whether Kinetics leads to such bias.",
        "To this end we carried out a preliminary study on (i) whether the data for each action class of Kinetics is gender balanced, and (ii) if, there is an imbalance, whether it leads to a biased performance of the action classiﬁes.",
        "The outcome of (i) is that in 340 action classes out of the 400, the data is either not dominated by a single gender, or it is mostly not possible to determine the gender – the latter arises in classes where, for example, only hands appear, or the ‘actors’ are too small or heavily clothed. The classes that do show gender imbalance include ‘shaving beard’ and ‘dunking basketball’, that are mostly male, and ’ﬁlling eyebrows’ and ‘cheerleading’, that are mostly female.",
        "Imbalance can also be examined on other ‘axes’, for example age and race. Again, in a preliminary investigation wefoundverylittleclearbias. Thereisoneexceptionwhere there is clear bias to babies – in ‘crying’, where many of the videos of non-babies crying are misclassiﬁed; another example is ‘wrestling’, where the opposite happens: adults wrestling in a ring seem to be better classiﬁed than children wrestling in their homes, but it is hard to tell whether the deciding factor is age or the scenes where the actions happen. Nevertheless, these issues of dataset imbalance and any resulting classiﬁer bias warrant a more thorough investigation, and we return to this in section 5.",
        "3.6. Discussion: dataset bias II",
        "Another type of bias could arise because classiﬁers are involved in the dataset collection pipeline: it could be that these classiﬁers lead to a reduction in the visual variety of the clips obtained, which in turn leads to a bias in the action classiﬁer trained on these clips. In more detail, although the videos are selected based on their title (which is provided by the person uploading the video to YouTube), the position of the candidate clip within the video is provided by an image (RGB) classiﬁer, as described above. In practice, using a classiﬁer at this point does not seem to constrain the variety of the clips – since the video is about the action, the particular frame chosen as part of the clip may not be crucial; and, in any case, the clip contains hundreds of more frames where the appearance (RGB) and motion can vary considerably. For these reasons we are not so concerned about the intermediate use of image classiﬁers.",
        "In this section we ﬁrst brieﬂy describe three standard ConvNet architectures for human action recognition in video. We then use these architectures as baselines and compare their performance by training and testing on the Kinetics dataset. We also include their performance on UCF-101 and HMDB-51.",
        "We consider three typical approaches for video classiﬁ- cation: ConvNets with an LSTM on top [7, 26]; two-stream networks [9, 19]; and a 3D ConvNet [13, 21, 23]. There have been many improvements over these basic architectures, e.g. [9], but our intention here is not to perform a thorough study on what is the very best architecture on Kinetics, but instead to provide an indication of the level of difﬁculty of the dataset. A rough graphical overview of the threetypesofarchitectureswecompareisshowninﬁgure3, and the speciﬁcation of their temporal interfaces is given in table 3.",
        "For the experiments on the Kinetics dataset all three architectures are trained from scratch using Kinetics. How-",
        "ever, for the experiments on UCF-101 and HMDB-51 the architectures (apart from the 3D ConvNet) are pre-trained on ImageNet (since these datasets are too small to train the architectures from scratch).",
        "3D ConvNets [13, 21, 23] seem like a natural approach to video modeling. They are just like standard 2D convolutional networks, but with spatio-temporal ﬁlters, and have a very interesting characteristic: they directly create hierarchical representations of spatio-temporal data. One issue with these models is that they have many more parameters",
        "than 2D ConvNets because of the additional kernel dimension, and this makes them harder to train. Also, they seem to preclude the beneﬁts of ImageNet pre-training and previous work has deﬁned relatively shallow custom architectures and trained them from scratch [13, 14, 21, 23]. Results on benchmarks have shown promise but have not yet matched the state-of-the-art, possibly because they require more training data than their 2D counterparts. Thus 3D ConvNets are a good candidate for evaluation on our larger dataset.",
        "Data augmentation is known to be of crucial importance for the performance of deep architectures. We used random cropping both spatially – randomly cropping a 299 × 299",
        "Table 4: Baseline comparisons across datasets: (left) training and testing on split 1 of UCF-101; (middle) training and testing on split 1 of HMDB-51; (right) training and testing on Kinetics (showing top-1/top-5 performance). ConvNet+LSTM and Two-Stream use ResNet-50 ConvNet modules, pretrained on ImageNet for UCF-101 and HMDB-51 examples but not for the Kinetics experiments. Note that the Two-Stream architecture numbers on individual RGB and Flow streams can be interpreted as a simple baseline which applies a ConvNet independently on 25 uniformly sampled frames then averages the predictions.",
        "In this section we compare the performance of the three baseline architectures whilst varying the dataset used for training and testing.",
        "Thereareseveralnoteworthyobservations. First, theperformanceisfarloweronKineticsthanonUCF-101, anindication of the different levels of difﬁculty of the two datasets. On the other hand, the performance on HMDB-51 is worse than on Kinetics – it seems to have a truly difﬁcult test set, and it was designed to be difﬁcult to appearance-centered methods, while having little training data. The parameterrich 3D-ConvNet model is not pre-trained on ImageNet,",
        "unlike the other baselines. This translates into poor performance on all datasets but especially on UCF-101 and HMDB-51 – on Kinetics it is much closer to the performance of the other models, thanks to the much larger training set of Kinetics.",
        "We have described the Kinetics Human Action Video dataset, which has an order of magnitude more videos than previous datasets of its type. We have also discussed the procedures we employed collecting the data and for ensuring its quality. We have shown that the performance of standard existing models on this dataset is much lower than on UCF-101 and on par with HMDB-51, whilst allowing large models such as 3D ConvNets to be trained from scratch, unlike the existing human action datasets.",
        "Wehavealsocarriedoutapreliminaryanalysisofdataset imbalance and whether this leads to bias in the classiﬁers trained on the dataset. We found little evidence that the resulting classiﬁers demonstrate bias along sensitive axes, such as across gender. This is however a complex area that deserves further attention. We leave a thorough analysis for future work, in collaboration with specialists from complementary areas, namely social scientists and critical humanists.",
        "The collection of this dataset was funded by DeepMind. We are very grateful for help from Andreas Kirsch, JohnPaul Holt, Danielle Breen, Jonathan Fildes, James Besley and Brian Carver. We are grateful for advice and comments from Tom Duerig, Juan Carlos Niebles, Simon Osindero, Chuck Rosenberg and Sean Legassick; we would also like to thank Sandra and Aditya for data clean up.",
        "[5] J. Carreira and A. Zisserman. Quo vadis, action recognition? new models and the kinetics dataset. In IEEE International Conference on Computer Vision and Pattern Recognition CVPR, 2017.",
        "egory dataset. 2007.",
        "[15] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. HMDB: a large video database for human motion recogIn Proceedings of the International Conference on nition. Computer Vision (ICCV), 2011.",
        "[20] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.",
        "ference on computer vision, pages 140–153. Springer, 2010. [22] A. Torralba and A. A. Efros. Unbiased look at dataset bias. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1521–1528. IEEE, 2011. [23] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri. Learning spatiotemporal features with 3d convolutional networks. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 4489–4497. IEEE, 2015.",
        "This is the list of classes included in the human action video dataset. The number of clips for each action class is given by the number in brackets following each class name."
    ]
}