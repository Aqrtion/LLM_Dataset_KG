{
    "title_author_abstract_introduction": "Common Voice: A Massively-Multilingual Speech Corpus\nRosana Ardila,† Megan Branson,† Kelly Davis,† Michael Henretty, Michael Kohler, Josh Meyer,◦ Reuben Morais,† Lindsay Saunders,† Francis M. Tyers,‡ Gregor Weber† † Mozilla ‡ Indiana University ◦ Artie, Inc. Various Cities Bloomington, IN, USA Los Angeles, CA, USA {rosana, mbranson, kdavis, reuben, lsaunders, gweber}@mozilla.com, ftyers@iu.edu, michael.henretty@gmail.com, me@michaelkohler.info, josh.meyer@artie.com\nAbstract The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identiﬁcation). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla’s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we ﬁnd an average Character Error Rate improvement of 5.99 ± 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the ﬁrst ever published results on end-to-end Automatic Speech Recognition. Keywords:spoken corpus, Automatic Speech Recognition, low-resource languages\nIntroduction\nThe Common Voice project1 is a response to the current state of affairs in speech technology, in which training data is either prohibitively expensive or unavailable for most languages (Roter, 2019). We believe that speech technology (like all technology) should be open and decentralized, and the Common Voice project achieves this goal via a mix of community building, open source tooling, and a permissive licensing scheme. The corpus is designed to organically scale to new languages as community members use the provided tools to translate the interface, submit text sentences, and ﬁnally record and validate voices in their new language 2. The project was started with an initial focus on English in July 2017 and then in June 2018 was made available for any language. The remainder of the paper is organized as follows: In Section (2) we motivate Common Voice and review previous multilingual corpora. Next, in Section (3) we describe the recording and validation process used to create the corpus. Next, in Section (4) we describe the current contents of Common Voice, and lastly in Section (5) we show multilingual Automatic Speech Recognition experiments using the corpus.",
    "data_related_paragraphs": [
        "2. Prior work Some notable multilingual speech corpora include VoxForge (VoxForge, 2019), Babel (Gales et al., 2014), and MAILABS (M-AILABS, 2019). Even though the Babel corpus contains high-quality data from 22 minority languages, it is not released under an open license. VoxForge is most similar to Common Voice in that it is community-driven,",
        "multilingual (17 languages), and released under an open license (GNU General Public License). However, the VoxForge does not have a sustainable data collection pipeline compared to Common Voice, and there is no data validation step in place. M-AILABS data contains 9 language varieties with a modiﬁed BSD 3-Clause License, however there is no community-driven aspect. Common Voice is a sustainable, open alternative to these projects which allows for collection of minority and majority languages alike.",
        "3. Corpus Creation The data presented in this paper was collected and validated via Mozilla’s Common Voice initiative. Using either the Common Voice website or iPhone app, contributors record their voice by reading sentences displayed on the screen (see Figure (1)). The recordings are later veriﬁed by other contributors using a simple voting system. Shown in Figure (2), this validation interface has contributors mark <audio,transcript> pairs as being either correct (up-vote) or incorrect (down-vote). A maximum of three contributors will listen to any audio clip.3 If an <audio,transcript> pair ﬁrst receives two upvotes, then the clip is marked as valid. If instead the clip ﬁrst receives two down-votes, then it is marked as invalid. A contributor may switch between recording and validation as they wish. Only clips marked as valid are included in the ofﬁcial training, development, and testing sets for each language. Clips which did not receive enough votes to be validated or invalidated by the time of release are released as “other”. The train, test, and development sets are bucketed such that",
        "3In the early days of Common Voice, this voting mechanism contained bugs, and some clips in the ofﬁcial release received over three votes. In these cases we use a simple majority rule. The total number of up and down votes is released with the dataset.",
        "any given speaker may appear in only one. This ensures that contributors seen at train time are not seen at test time, which would skew results. Additionally, repetitions of text sentencesareremovedfromthetrain, test, anddevelopment sets of the corpus.4 The number of clips is divided among the three datasets according to statistical power analyses. Given the total number of validated clips in a language, the number of clips in the test set is equal to the number needed to achieve a conﬁdence level of 99% with a margin of error of 1% relative to the number of clips in the training set. The same is true of the development set.5 The audio clips are released as mono-channel, 16bit",
        "Figure 3: Interface for reporting problematic data. It is possible to report any text or audio as problematic during either recording or validation.",
        "4.1. Released Languages The data presented in Table (1) shows the currently available data. Each of the released languages is available for individual download as a compressed directory from the Mozilla Common Voice website.6 The directory contains six ﬁles with Tab-Separated Values TSV ﬁles), and a single clips sub-directory (i.e. which contains all of the audio data. Each of the six TSV ﬁles represents a different segment of the voice data, with all six having the following column head[client id, path, sentence, up votes, ers: down votes, age, gender, accent]. The ﬁrst three columns refer to an anonymized ID for the speaker, the location of the audio ﬁle, and the text that was read. The next two columns contain information on how listeners judged the <audio,transcript> pair. The last three columns represent demographic data which was optionally self-reported by the speaker of the audio.",
        "mozilla.org/en/datasets",
        "Table 1: Current data statistics for Common Voice. Data in italics is as of yet unreleased. Other numbers refer to the data published in the June 12, 2019 release.",
        "with more than 500,000 Wikipedia articles, text sentences are extracted from Wikipedia using community provided rule-sets per language8. These sentences make up the initial text prompts for the languages in question. Any language community can gather additional sentences through the Sentence Collector9 taking advantage of automatic validation mechanisms such as checks for sentence length, foreign alphabets, and numbers. Every sentence submitted through the Sentence Collector needs to be approved by two out of three reviewers, leading to a weekly export of new sentences into the Common Voice database. Once the website is translated and at least 5,000 sentences have been added, the language is enabled for voice recordings.",
        "The following experiments demonstrate the potential to use theCommonVoicecorpusformultilingualspeechresearch. These results represent work on an internal version of Common Voice from February 2019. The current corpus contains more languages and more data per language. These experiments use an End-to-End Transfer Learning approach which bypasses the need for linguistic resources or domain expertise (Meyer, 2019). Certain layers are copied from a pre-trained English source model, new layers are initialized for a target language, the old and new layers are stitched together, and all layers are ﬁne-tuned via gradient descent.",
        "5.1. Data We made dataset splits (c.f. Table (2)) such that one speaker’s recordings are only present in one data split. This allows us to make a fair evaluation of speaker generalization, but as a result some training sets have very few speakers, making this an even more challenging scenario. The splits per language were made as close as possible to 80% train, 10% development, and 10% test.10 Results from this dataset are interesting because the text and audio are challenging, the range of languages is wider than any openly available speech corpus, and the amount of data per language ranges from very small (less than 1,000 clips for Slovenian) to relatively large (over 65,000 clips for German).",
        "Dataset Size",
        "Table 2: Data used in the experiments, from an earlier multilingual versionof CommonVoice. Number of audioclips andunique speakers.",
        "Table 3: Fine-Tuned Transfer Learning Character Error Rate for each language, in addition to a baseline trained from scratch on the target language data. Bolded values display best model per language. Shading indicates relative performance per language, with darker indicating better models.",
        "7. Concluding remarks We have presented Common Voice: a crowd-sourced, multilingual speech corpus which can scale to any language via community effort. All of the speech data is released under a Creative Commons CC0 license, making Common",
        "Graves, A., Fern´andez, S., Gomez, F., and Schmidhuber, J. (2006). Connectionist temporal classiﬁcation: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, pages 369–376. ACM.",
        "The m-ailabs speech dataset.",
        "https://www.caito.de/2019/01/ the-m-ailabs-speech-dataset/. 11/25/2019.",
        "Roter, G. mozilla lic https://blog.mozilla.org/blog/2019/02/28/sharingour-common-voices-mozilla-releases-the-largest-todate-public-domain-transcribed-voice-dataset/.",
        "to-date dataset,",
        "Voice the largest public domain corpus designed for Automatic Speech Recognition. In Section (3) we described the recording and validation process used to create the corpus. In Section (4) we presented the current contents of Common Voice, and lastly in Section (5) we show multilingual Automatic Speech Recognition experiments using the corpus. There are currently 38 language communities collecting data via Common Voice, and we welcome more languages and more volunteers."
    ]
}