{
    "title_author_abstract_introduction": "Deeper, Broader and Artier Domain Generalization\nDa Li Yongxin Yang Yi-Zhe Song\nTimothy M. Hospedales\nQueen Mary University of London University of Edinburgh {da.li, yongxin.yang, yizhe.song}@qmul.ac.uk, t.hospedales@ed.ac.uk\nAbstract\nThe problem of domain generalization is to learn from multiple training domains, and extract a domain-agnostic model that can then be applied to an unseen domain. Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training. For example recognitioninsketchimages, whicharedistinctlymoreabstractand rarer than photos. Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on alleviating the dataset bias where both problems of domain distinctiveness and data sparsity can be minimal. We argue that these benchmarks are overly straightforward, and show that simple deep learning baselines perform surprisingly well on them.\nIn this paper, we make two main contributions: Firstly, we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning. Secondly, we develop a DG benchmark dataset covering photo, sketch, cartoon and painting domains. This is both more practically relevant, and harder (bigger domain shift) than existing benchmarks. The results show that our method outperforms existing DG alternatives, and our dataset provides a more signiﬁcant DG challenge to drive future research.\n1. Introduction\nLearning models that can bridge train-test domain-shift is a topical issue in computer vision and beyond. In vision this has been motivated recently by the observation of signiﬁcant bias across popular datasets [27], and the poor performance of state-of-the-art models when applied across datasets. Existing approaches can broadly be categorized into domain adaptation (DA) methods, that use (un)labeled target data to adapt source model(s) to a speciﬁc target domain [23]; and domain generalization (DG) approaches, that learn a domain agnostic model from multiple sources that can be applied to any target domain [12, 10]. While DA has been more commonly studied, DG is the more valuable\nFigure 1: Contrast between prior Caltech Ofﬁce and VLCS datasets versus our new PACS dataset. The domain generalization task is to recognize categories in an unseen testing domain. PACS provides more diverse domains with bigger more challenging domain-shifts between them.\nyet challenging setting, as it does not require acquisition of a large target domain set for off-line analysis to drive adaptation. Such data may not even exist if the target domain is sparse. Instead it aims to produce a more human-like model, where there is a deeper semantic sharing across different domains – a dog is a dog no matter if it is depicted in the form of a photo, cartoon, painting, or indeed, a sketch. The most popular existing DA/DG benchmarks deﬁne domains as photos of objects spanning different camera types [23], or datasets collected with different composition biases [27]. While these benchmarks provide a good start, we argue that they are neither well motivated nor hard enough to drive the ﬁeld. Motivation: The constituent domains/datasets in existing benchmarks are based upon conventional photos, albeit with different camera types or composition bias. However there exist enough photos, that\nPhotoArt paintingSketchTrainTestVLCSCaltech-Office TrainTestCartoonDomain shift\none could in principle collect enough target domain-speciﬁc data to train a good model, or enough diverse data to cover all domains and minimize bias (thus negating the need for DA). A more compelling motivation is domains where the total available images is fundamentally constrained, such as for particular styles of art [5, 29], and sketches [33, 6, 34, 26]. Compared to photos, there may simply not be enough examples of a given art style to train a good model, even if we are willing to spend the effort. Difﬁculty: The camera type and bias differences between domains in existing benchmarks are already partially bridged by contemporary Deep features [4, 32], thus questioning the need for DA or DG methods. In this paper, we show that multidomain deep learning provides a very simple but highly effective approach to DG that outperforms existing purposedesigned methods.\nTo address these limitations, we provide a harder and better motivated benchmark dataset PACS, consisting of images from photo (P), art painting (A), cartoon (C), and sketch (S) domains. This benchmark carries two important advancements over prior examples: (i) it extends the previously photo-only setting in DA/DG research, and uniquely includes domains that are maximally distinct from each other, spanning a wide spectrum of visual abstraction, from photos that are the least abstract to human sketches which are the most abstract; (ii) it is more reﬂective of a real-world task where a target domain (such as sketch) is intrinsically sparse, and so DG from a more abundant domain (such as photos) is really necessary. As illustrated qualitatively in Fig. 1, the benchmark is harder, as the domains are visually more distinct than in prior datasets. We explore these differences quantitatively in Sec. 4.2.\nThere have been a variety of prior approaches to DG based on SVM [12, 30], subspace learning [19], metric learning [7], and autoencoders [10]. Despite their differences, most of these have looked at ﬁxed shallow features. In this paper, we address the question of how end-to-end learning of deep features impacts the DG setting. Our deep learning approach trains on multiple source domains, and extracts both domain agnostic features (e.g., convolutional kernels), and classiﬁer (e.g., ﬁnal FC layer) for transfer to a new target domain. This approach can be seen as a deep multi-class generalization of the shallow binary Undo Bias method [12], which takes the form of a dynamically parameterized deep neural network [25]. However, the resulting number of parameters grows linearly with the number of source domains (of which ultimately, we expect many for DG), increasing overﬁtting risk. To address this we develop a low-rank parameterized neural network which reduces the number of parameters. Furthermore the low-rank approach provides an additional route to knowledge sharing besides through explicit parameterization. In particular it has the further beneﬁt of automatically modeling how related the\ndifferent domains are (e.g., perhaps sketch is similar to cartoon; and cartoon is similar to painting), and also how the degree of sharing should vary at each layer of the CNN.\nTo summarize our contributions: Firstly, we highlight the weaknesses of existing methods (they lose to a simple deep learning baseline) and datasets (their domain shift is small). Second, we introduce a new, better motivated, and more challenging DG benchmark. Finally, we develop a novel DG method based on low-rank parameterized CNNs that shows favorable performance compared to prior work.",
    "data_related_paragraphs": [
        "Domain Generalization Despite different methodological tools (SVM, subspace learning, autoencoders, etc), existing methods approach DG based on a few different intuitions. One is to project the data to a new domain invariant representation where the differences between training domains is minimized [19, 10], with the intuition that such a space will also be good for an unseen testing domain. Another intuition is to predict which known domain a testing sample seems most relevant to, and use that classiﬁer [30]. Finally, there is the idea of generating a domain agnostic classiﬁer, for example by asserting that each training domain’s classiﬁer is the sum of a domain-speciﬁc and domain-agnostic weight vector [12]. The resulting domainagnostic weight vector can then be extracted and applied to held out domains. Our approach lies in this latter category. However, prior work in this area has dealt with shallow, linear models only. We show how to extend this intuition to end-to-end learning in CNNs, while limiting the resulting parameter growth, and making the sharing structure richer than an unweighted sum.",
        "Neural Network Methods Our DG method is related to parameterized neural networks [1, 25], in that the parameters are set based on external metadata. In our case, based on a description of the current domain, rather than an instance[1], oradditionalsensor[25]. Itisalsorelatedtolowrank neural network models, typically used to compress [13] and speed up [16] CNNs, and have very recently been explored for cross-category CNN knowledge transfer [31]. In our case we exploit this idea both for compression – but across rather than within domains [13], as well as for crossdomain (rather than cross-category [31]) knowledge sharing. Different domains can share parameters via common latent factors. [2] also addresses the DG setting, but learns shared parameters based on image reconstruction, whereas ours is learned via paramaterizing each domain’s CNN. As a parameterized neural network, our approach also differs from all those other low-rank methods [13, 16, 31], which have a ﬁxed parameterization.",
        "2.1. Benchmarks and Datasets",
        "DG Benchmarks The most popular DG benchmarks are: ‘Ofﬁce’ [23] (containing Amazon/Webcam/DSLR images), later extended to include a fourth Caltech 101 domain [11] (OfﬁceCaltech) and Pascal 2007, LabelMe, Caltech, SUN09 (VLCS) [27, 12]. The domains within Ofﬁce relate to different camera types, and the others are created by the biases of different data collection procedures [27]. Despite the famous analysis of dataset bias [27] that motivated the creationoftheVLCSbenchmark, it waslatershownthat the domain shift is much smaller with recent deep features [4]. Thus recent DG studies have used deep features [10], to obtain better results. Nevertheless, we show that a very simple baseline of ﬁne-tuning deep features on multiple source domains performs comparably or better than prior DG methods. ThismotivatesourdesignofaCNN-basedDGmethod, as well as our new dataset (Fig 1) which has greater domain shift than the prior benchmarks. Our dataset draws on nonphotorealistic and abstract visual domains which provide a better motivated example of the sort of relatively sparse data domain where DG would be of practical value. Non-photorealistic Image Analysis Non-photorealistic image analysis is a growing subﬁeld of computer vision that extends the conventional photo-only setting of vision research to include other visual depictions (often more abstract) such as paintings and sketches. Typical tasks include instance-level matching between sketch-photo [33, 24], and art-photo domains [3], and transferring of object recognizers trained on photos to detect objects in art [5, 29]. Most prior work focuses on two domains (such as photo and painting [5, 29], or photo and sketch [33, 24]). Studies have",
        "Assume we observe S domains, and the ith domain contains Ni labeledinstances {(x(i) isthe input data (e.g., an image) for which we assume they are of the same size among all domains (e.g., all images are cropped into the same size), and y(i) j ∈ {1...C} is the class label. We assume the label space is consistent across domains. The objective of DG is to learn a domain agnostic model which can be applied to unseen domains in the future. In contrast to domain adaptation, we can not access the labeled or unlabeled examples from those domains to which the model is eventually applied. So the model is supposed to extract the domain agnostic knowledge within the observed domains. In the training stage, we will minimize the empirical error for all observed domains,",
        "4.1. New Domain Generalization Dataset: PACS",
        "Our PACS DG dataset is created by intersecting the classes found in Caltech256 (Photo), Sketchy (Photo, Sketch) [24], TU-Berlin (Sketch) [6] and Google Images (Art painting, Cartoon, Photo). Our dataset and code, together with latest results using alternative state-of-the-art base networks, can be found at: http://sketchx. eecs.qmul.ac.uk/. PACS: Our new benchmark includes 4 domains (Photo, Sketch, Cartoon, Painting), and 7 common categories ‘dog’, ‘guitar’, ‘horse’, ‘house’, ‘person’. ‘elephant’, ‘giraffe’. The total number of images is 9991.",
        "We ﬁrst perform a preliminary analysis to contrast the domain shift within our PACS dataset to that of prior popular datasets such as VLCS. We make this contrast from both a feature space and a classiﬁer performance perspective. Feature Space Analysis Given the DG setting of training on source domains and applying to held out test domain(s), we measure the shift between source and target domains based on the Kullback-Leibler divergence as: m (cid:80) Dshift(Ds,Dt) = 1 j and m are the number of source and target domains, and λi weights the i th source domain, to account for data imbalance. To encode each domain as a probability, we calculate the mean DECAF7 representation over instances and then apply softmax normalization.",
        "Classiﬁer Performance Analysis We also compare the datasets by the margin between multiclass classiﬁcation accuracy of within-domain learning, and a simple crossdomain baseline of training a CNN on all the source domains before testing on the held out target domain (as we shall see later, this baseline is very competitive). Assuming within-domain learning performance is an upper bound, then this difference indicates the space which a DG method has to make a contribution, and hence roughly reﬂects size of the domain-shift/difﬁculty of the DG task.",
        "Results Fig. 2(a) shows the average domain-shift in terms of KLD across all choices of held out domain in our new PACS benchmark, compared with the VLCS benchmark [27]. Clearly the domain shift is signiﬁcantly higher in our new benchmark, as is visually intuitive from the illustrative examples in Fig. 1. To provide a qualitative summarization, we also show the distribution of features in our PACS compared to VLCS in Fig. 2(b,c) as visualized by a 2 dimensional t-SNE [18] plot, where the features are categorized and colored by their associated domain. From this result, we can see that the VLCS data are generally hard to sepa-",
        "rate by domain, while our PACS data are much more separated by domain. This illustrates the greater degree of shift between the domains in PACS over VLCS.",
        "4.3.1 Datasets and Settings",
        "We evaluate our proposed method on two datasets: VLCS, and our proposed PACS dataset. VLCS [27] aggregates photos from Caltech, LabelMe, Pascal VOC 2007 and SUN09. It provides a 5-way multiclass benchmark on the ﬁve common classes: ’bird’,’car’,’chair’,’dog’ and ’person’. Our PACS (described in Sec. 4.1) with 7 classes from Photo, Sketch, Cartoon, Painting domains. All results are evaluated by multi-class accuracy, following [10]. We explore features including Classic SIFT features (for direct comparison with earlier work), DECAF pre-extracted deep features following [10], and E2E end-to-end CNN learning.",
        "Baselines: We evaluate our contributions by comparison with number of alternatives including variants designed to reveal insights, and state of the art competitors: Ours-MLP: Our DG method applied to a 1 hidden layer multi-layer perception. For use with pre-extracted features. Ours-Full: Our full low-rank parameterized CNN trained end-to-end on images. SVM: Linear SVM, applied on the aggregation of data from all source domains. Deep-All: Pretrained Alexnet CNN [14], ﬁne-tuned on the aggregation of all source domains. Undo-Bias: Modiﬁes traditional SVM to include a domain-speciﬁc and global weight vector which can be extracted for DG [12]. The original",
        "servations: (i) Given the ﬁxed DECAF6 feature, most prior DG methods improve on vanilla SVM, and D-MTAE [10] is the best of these. (ii) Ours-MLP outperforms 1HNN, which uses the same type of architecture and the same feature. This margin is due to our low-rank domain-generalization approach. (iii) The very simple baseline of ﬁne-tuning a deep model on the aggregation of source domains (DeepAll) performs surprisingly well and actually outperforms all thepriorDGmethods. (iii)Ours-FulloutperformsDeep-All slightly. This small margin is understandable. Our model does have more parameters to learn than Deep-All, despite the low rank; and the cost of doing this is not justiﬁed by the relatively small domain gap between the VLCS datasets.",
        "Figure 4: Visualization of the preferred images of output neurons ‘horse’, ‘giraffe’ and ‘house’ in the domains of the PACS dataset. Left: real images. Middle: synthesized images for PACS domains. Right: synthesized images for agnostic domain.",
        "We presented a new dataset and deep learning-based method for domain generalization. Our PACS (Photo-ArtCartoon-Sketch) dataset is aligned with a practical application of domain generalization, and we showed it has more challenging domain shift than prior datasets, making it suitable to drive the ﬁeld in future. Our new domain generalization method integrates the idea of learning a domainagnostic classiﬁer with a robust deep learning approach for end-to-end learning of domain generalization. The result performs comparably or better than prior approaches.",
        "[7] C. Fang, Y. Xu, and D. N. Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In ICCV, 2013.",
        "[11] B. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic ﬂow kernel for unsupervised domain adaptation. In CVPR, 2012. [12] A. Khosla, T. Zhou, T. Malisiewicz, A. Efros, and A. Torralba. Undoing the damage of dataset bias. In ECCV, 2012. [13] Y.-D. Kim, E. Park, S. Yoo, T. Choi, L. Yang, and D. Shin. Compression of deep convolutional neural networks for fast and low power mobile applications. In ICLR, 2016.",
        "[24] P. Sangkloy, N. Burnell, C. Ham, and J. Hays. The sketchy database: learning to retrieve badly drawn bunnies. TOG, 2016.",
        "[27] A. Torralba and A. A. Efros. Unbiased look at dataset bias.",
        "[18] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne."
    ]
}