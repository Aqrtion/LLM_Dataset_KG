{
    "title_author_abstract_introduction": "ShapeNet: An Information-Rich 3D Model Repository http://www.shapenet.org\nAngel X. Chang1, Thomas Funkhouser2, Leonidas Guibas1, Pat Hanrahan1, Qixing Huang3, Zimo Li3, Silvio Savarese1, Manolis Savva∗1, Shuran Song2, Hao Su∗1, Jianxiong Xiao2, Li Yi1, and Fisher Yu2\n1Stanford University — 2Princeton University — 3Toyota Technological Institute at Chicago Authors listed alphabetically\nAbstract\nWe present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection of datasets providing many semantic annotations for each 3D model such as consistent rigid alignments, parts and bilateral symmetry planes, physical sizes, keywords, as well as other planned annotations. Annotations are made available through a public web-based interface to enable data visualization of object attributes, promote data-driven geometric analysis, and provide a large-scale quantitative benchmark for research in computer graphics and vision. At the time of this technical report, ShapeNet has indexedmore than 3,000,000 models, 220,000 models out of which are classiﬁed into 3,135 categories (WordNet synsets). In this report we describe the ShapeNet effort as a whole, provide details for all currently available datasets, and summarize future plans.\n1. Introduction\nRecent technological developments have led to an explosion in the amount of 3D data that we can generate and store. Repositories of 3D CAD models are expanding continuously, predominantlythroughaggregationof3Dcontent on the web. RGB-D sensors and other technology for scanning and reconstruction are providing increasingly higher ﬁdelity geometric representations of objects and real environments that can eventually become CAD-quality models. At the same time, there are many open research problems due to fundamental challenges in using 3D content. Computing segmentations of 3D shapes, and establishing correspondences between them are two basic problems in geometric shape analysis. Recognition of shapes from par-\ntial scans is a research goal shared by computer graphics and vision. Scene understanding from 2D images is a grand challenge in vision that has recently beneﬁted tremendously from 3D CAD models [28, 34]. Navigation of autonomous robots and planning of grasping manipulations are two large areas in robotics that beneﬁt from an understanding of 3D shapes. At the root of all these research problems lies the need for attaching semantics to representations of 3D shapes, and doing so at large scale.\nRecently, data-driven methods from the machine learning community have been exploited by researchers in vision and NLP (natural language processing). “Big data” in the visual and textual domains has led to tremendous progress towards associating semantics with content in both ﬁelds. Mirroring this pattern, recent work in computer graphics has also applied similar approaches to speciﬁc problems in the synthesis of new shape variations [10] and new arrangements of shapes [6]. However, a critical bottleneck facing the adoption of data-driven methods for 3D content is the lack of large-scale, curated datasets of 3D models that are available to the community.\nMotivated by the far-reaching impact of dataset efforts such as the Penn Treebank [20], WordNet [21] and ImageNet [4], which collectively have tens of thousands of citations, we propose establishing ShapeNet: a large-scale 3D model dataset. Making a comprehensive, semantically enriched shape dataset available to the community can have immense impact, enabling many avenues of future research. In constructing ShapeNet we aim to fulﬁll several goals: • Collect and centralize 3D model datasets, helping to\norganize effort in the research community.\n• Support data-driven methods requiring 3D model data.\n• Enable evaluation and comparison of algorithms for fundamental tasks involving geometry (e.g., segmentation, alignment, correspondence).\n• Serve as a knowledge base for representing real-world\n∗Contact authors: {msavva,haosu}@cs.stanford.edu\nobjects and their semantics.\nThese goals imply several desiderata for ShapeNet: • Broad and deep coverage of objects observed in the real world, with thousands of object categories and millions of total instances.\n• Categorization scheme connected to other modalities\nof knowledge such as 2D images and language.\n• Annotation of salient physical attributes on models, such as canonical orientations, planes of symmetry, and part decompositions.\n• Web-based interfaces for searching, viewing and retrieving models in the dataset through several modalities: textual keywords, taxonomy traversal, image and shape similarity search.\nAchievingthesegoalsandprovidingtheresultingdataset to the community will enable many advances and applications in computer graphics and vision.\nIn this report, we ﬁrst situate ShapeNet, explaining the overall goals of the effort and the types of data it is intended to contain, as well as motivating the long-term vision and infrastructural design decisions (Section 3). We then describe the acquisition and validation of annotations collected so far (Section 4), summarize the current state of all available ShapeNet datasets, and provide basic statistics on the collected annotations (Section 5). We end with a discussion of ShapeNet’s future trajectory and connect it with several research directions (Section 7).",
    "data_related_paragraphs": [
        "There have been previous efforts to build organized collections of 3D models (e.g., [5, 7]). However, they have provided quite small datasets, covered only a small number of semantic categories, and included few structural and semantic annotations. Most of these previous collections have been developed for evaluating shape retrieval and classiﬁcation algorithms. For example, datasets are created annually for the Shape Retrieval Contest (SHREC) that commonly contains sets of models organized in object categories. However, those datasets are very small — the most recent SHREC iteration in 2014 [17] contains a “large” datasetwitharound9,000modelsconsistingofmodelsfrom a variety of sources organized into 171 categories (Table 1).",
        "The Princeton Shape Benchmark is probably the most well-known and frequently used 3D shape collection to date (with over 1000 citations) [27]. It contains around 1,800 3D models grouped into 90 categories, but has no annotations beyond category labels. Other commonly-used datasets contain segmentations [2], correspondences [13, 12], hierarchies [19], symmetries [11], salient features [3], semantic segmentations and labels [36], alignments of 3D models with images [35], semantic ontologies [5], and other functional annotations — but again only for small size datasets. For example, the Benchmark for 3D Mesh Segmentation contains just 380 models in 19 object classes [2].",
        "In contrast, there has been a ﬂurry of activity on collecting, organizing, and labeling large datasets in computer vision and related ﬁelds. For example, ImageNet [4] provides a set of 14M images organized into 20K categories associated with “synsets” of WordNet [21]. LabelMe provides segmentations and label annotations of hundreds of thousands of objects in tens of thousands of images [24]. The SUN dataset provides 3M annotations of objects in 4K categories appearing in 131K images of 900 types of scenes. Recent work demonstrated the beneﬁt of a large dataset of 120K 3D CAD models in training a convolutional neural network for object recognition and next-best view prediction in RGB-D data [34]. Large datasets such as this and others (e.g., [14, 18]) have revitalized data-driven algorithms for recognition, detection, and editing of images, which have revolutionized computer vision.",
        "Similarly, large collections of annotated 3D data have had great inﬂuence on progress in other disciplines. For example, the Protein Data Bank [1] provides a database with 100K protein 3D structures, each labeled with its source and links to structural and functional annotations [15]. This databaseisacommonrepositoryofall3Dproteinstructures solved to date and provides a shared infrastructure for the collectionandtransferofknowledgeabouteachentry. Ithas accelerated the development of data-driven algorithms, facilitated the creation of benchmarks, and linked researchers and industry from around the world. We aim to provide a similar resource for 3D models of everyday objects.",
        "ShapeNet is a large, information-rich repository of 3D models. It contains models spanning a multitude of semantic categories. Unlike previous 3D model repositories, it provides extensive sets of annotations for every model and links between models in the repository and other multimedia data outside the repository.",
        "Like ImageNet, ShapeNet provides a view of the contained data in a hierarchical categorization according to WordNet synsets (Figure 1). Unlike other model repositories, ShapeNet also provides a rich set of annotations for",
        "Table 1. Source datasets from SHREC 2014: Princeton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark (SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Benchmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape Benchmark (ESB) [9].",
        "We have currently collected approximately 3 million shapes from online 3D model repositories, and categorized 300 thousand of them against the WordNet taxonomy. We have also annotated a subset of these models with shape properties such as upright and front orientations, symmetries, and hierarchical part decompositions. We are continuing the process of expanding the annotated set of models and also collecting new models from new data sources.",
        "In the following sections, we discuss how 3D models are collected for ShapeNet, what annotations will be added, how those annotations will be generated, how annotations will be updated as the dataset evolves over time, and what tools will be provided for the community to search, browse, and utilize existing data, as well as contribute new data.",
        "3.1. Data Collection",
        "The raw 3D model data for ShapeNet comes from public online repositories or existing research datasets. ShapeNet isintendedtobeanevolvingrepositorywithregularupdates as more and more 3D models become available, as more people contribute annotations, and as the data captured with new 3D sensors become prevalent.",
        "We envision ShapeNet as far more than a collection of 3D models. ShapeNet will include a rich set of annotations that provide semantic information about those models, establish links between them, and links to other modalities of data (e.g., images). These annotations are exactly what make ShapeNet uniquely valuable. Figure 2 illustrates the value of this dense network of interlinked attributes on shapes, which we describe below.",
        "Language-related Annotations: Naming objects by their basic category is useful for indexing, grouping, and linking to related sources of data. As described in the previous section, we organize ShapeNet based on the WordNet [21] taxonomy. Synsets are interlinked with various relations, such as hyper and hyponym, and part-whole relations. Due to the popularity of WordNet, we can leverage other resources linked to WordNet such as ImageNet, ConceptNet, Freebase, and Wikipedia. In particular, linking to ImageNet [4] will help transport information between images and shapes. We assign each 3D model in ShapeNet to one or more synsets in the WordNet taxonomy (i.e., we populate each synset with a collection of shapes). Please refer to Section 4.1 for details on the acquisition and validation of basic category annotations. Future planned annotations include natural language descriptions of objects and object part-part relation descriptions.",
        "Geometric Annotations: A critical property that distinguishes ShapeNet from image and video datasets is the ﬁ- delity with which 3D geometry represents real-world structures. We combine algorithmic predictions and manual annotations to organize shapes by category-level geometric properties and further derive rich geometric annotations from the raw 3D model geometry.",
        "• Rigid Alignments: Establishing a consistent canonical orientation (e.g., upright and front) for every model is important for various tasks such as visualizing shapes [13], shape classiﬁcation [8] and shape recognition [34]. Fortunately, most raw 3D model data is by default placed in an upright orientation, and the front orientations are typically aligned with an axis. This allows us to use a hierarchical clustering and alignment approach to ensure consistent rigid alignments within each category (see Section 4.2).",
        "Though at ﬁrst glance it might seem reasonable to collect the annotations we describe purely through manual human effort, we will in general take a hybrid approach. For annotationtypeswhereitispossible, wewillﬁrstalgorithmically predict the annotation for each model instance (e.g., global symmetryplanes, consistentrigidalignments). Wewillthen verify these predictions through crowd-sourcing pipelines and inspection by human experts. This hybrid strategy is sensible in the context of 3D shape data as there are already various algorithms we can leverage, and collecting corresponding annotations entirely through manual effort can be extremely labor intensive. In particular, since objects in a 3D representation are both more pure and more complete than objects in images, we can expect better and easier to establish correspondences between 3D shapes, enabling algorithmic transport of semantic annotations. In many cases, the design of the human annotation interfaces themselves is anopenquestion—whichstandsincontrasttolargelymanual image labeling efforts such as ImageNet. As a concrete example, shape part annotation can be presented and performed in various ways with different trade-offs in the type of obtained part annotation, the accuracy and the efﬁciency of the annotation process.",
        "To provide convenient access to all of the model and annotation data contained within ShapeNet, we construct an index over all the 3D models and their associated annotations using the Apache Solr framework.3 Each stored annotation for a given 3D model is contained within the index as a separate attribute that can be easily queried and ﬁltered through a simple web-based UI. In addition, to make the dataset conveniently accessible to researchers, we provide a batched download capability.",
        "Clean-up Inorderforthedatasettobeeasilyusablebyresearchers it should contain clean and high quality 3D models. Through inspection, we identify and group 3D models into the following categories: single 3D models, 3D scenes, billboards, and big ground plane.",
        "Category Distribution Figure 4 shows the distributions of the number of shapes per synset at various taxonomy levels for the current ShapeNetCore corpus. To the best of our knowledge, ShapeNet is the largest clean shape dataset available in terms of total number of shapes, average num-",
        "We observe that ShapeNet as a whole is strongly biased towards categories of rigid man-made artifacts, due to the bias of the source 3D model repositories. This is in contrast to common image database statistics that contain more natural objects such as plants and animals [30]. This distribution bias is probably due to a combination of factors: 1) meshes of natural objects are more difﬁcult to design using common CAD software; 2) 3D model consumers are typically more interested in artiﬁcial objects such as those observed in modern urban lifestyles. The former factor can be mitigated in the near future by using the rapidly improving depth sensing and 3D scanning technology.",
        "ShapeNetCore is a subset of the full ShapeNet dataset with single clean 3D models and manually veriﬁed category and alignment annotations. It covers 55 common object categories with about 51,300 unique 3D models. The 12 object categories of PASCAL 3D+[35], a popular computer vision 3D benchmark dataset, are all covered by ShapeNetCore. The category distribution of ShapeNetCore is shown in Table 2.",
        "RGB-D data The rapid proliferation of commodity RGB-D sensors is already making the process of capturing real-world environments better and more efﬁcient. Expanding ShapeNet to include shapes reconstructed from scanned RGB-D data is a critical goal. We foresee that over time, the amount of available reconstructed shape data will overshadow the existing designed 3D model data and as such this is a natural growth direction for ShapeNet. A related effort that we are currently undertaking is to align 3D models to objects observed in RGB-D frames. This will establish a powerful connection between real world observations and 3D models.",
        "Data-driven research By establishing ShapeNet as the ﬁrst large-scale 3D shape dataset of its kind we can help to move computer graphics research toward a data-driven direction following recent developments in vision and NLP. Additionally, we can help to enable larger-scale quantitative analysis of proposed systems that can clarify the beneﬁts of particular methodologies against a broader and more representative variety of 3D model data.",
        "Training resource By providing a large-scale, richly annotated dataset we can also promote a broad class of recentlyresurgentmachinelearningandneuralnetworkmethods for applications dealing with geometric data. Much like research in computer vision and natural language understanding, computational geometry and graphics stand to beneﬁt immensely from these data-driven learning approaches.",
        "Benchmark dataset We hope that ShapeNet will grow to become a canonical benchmark dataset for several evaluation tasks and challenges. In this way, we would like to engage the broader research community in helping us deﬁne and grow ShapeNet to be a pivotal dataset with long-lasting impact.",
        "[1] Helen M Berman, John Westbrook, Zukang Feng, Gary Gilliland, TN Bhat, Helge Weissig, Ilya N Shindyalov, and Philip E Bourne. The protein data bank. Nucleic Acids Res, 28:235–242, 2000. 2",
        "[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. 1, 2, 4",
        "[15] Roman A Laskowski, E Gail Hutchinson, Alex D Michie, Andrew C Wallace, Martin L Jones, and Janet M Thornton. PDBsum: A web-based database of summaries and analyses of all PDB structures. Trends Biochem. Sci., 22:488–490, 1997. 2",
        "[21] George A. Miller. WordNet: a lexical database for English.",
        "[24] Bryan C Russell and Antonio Torralba. Building a database of 3D scenes from user annotations. In CVPR, 2009. 2",
        "[33] Raoul Wessel, Ina Bl¨umel, and Reinhard Klein. A 3D shape benchmark for retrieval and automatic classiﬁcation of architectural data. In Eurographics 2009 Workshop on 3D Object Retrieval, pages 53–56. The Eurographics Association, 2009. 3",
        "[36] Jianxiong Xiao, Andrew Owens, and Antonio Torralba. SUN3D: A database of big spaces reconstructed using SfM and object labels. In ICCV, pages 1625–1632, 2013. 2"
    ]
}