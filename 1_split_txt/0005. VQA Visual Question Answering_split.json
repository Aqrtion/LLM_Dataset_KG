{
    "title_author_abstract_introduction": "VQA: Visual Question Answering www.visualqa.org\nAishwarya Agrawal∗, Jiasen Lu∗, Stanislaw Antol∗, Margaret Mitchell, C. Lawrence Zitnick, Dhruv Batra, Devi Parikh\nAbstract—We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines and methods for VQA are provided and compared with human performance. Our VQA demo is available on CloudCV (http://cloudcv.org/vqa).\n(cid:70)\n1 INTRODUCTION\nWe are witnessing a renewed excitement in multi-discipline Artiﬁcial Intelligence (AI) research problems. In particular, research in image and video captioning that combines Computer Vision (CV), Natural Language Processing (NLP), and Knowledge Representation & Reasoning (KR) has dramatically increased in the past year [16], [9], [12], [38], [26], [24], [53]. Part of this excitement stems from a belief that multi-discipline tasks like image captioning are a step towards solving AI. However, the current state of the art demonstrates that a coarse scene-level understanding of an image paired with word n-gram statistics sufﬁces to generate reasonable image captions, which suggests image captioning may not be as “AI-complete” as desired. What makes for a compelling “AI-complete” task? We believe that in order to spawn the next generation of AI algorithms, an ideal task should (i) require multi-modal knowledge beyond a single sub-domain (such as CV) and (ii) have a well-deﬁned quantitative evaluation metric to track progress. For some tasks, such as image captioning, automatic evaluation is still a difﬁcult and open research problem [51], [13], [22]. In this paper, we introduce the task of free-form and openended Visual Question Answering (VQA). A VQA system takes as input an image and a free-form, open-ended, naturallanguage question about the image and produces a naturallanguage answer as the output. This goal-driven task is applicable to scenarios encountered when visually-impaired users [3] or intelligence analysts actively elicit visual information. Example questions are shown in Fig. 1. Open-ended questions require a potentially vast set of AI capabilities to answer – ﬁne-grained recognition (e.g., “What kind of cheese is on the pizza?”), object detection (e.g., “How\n• ∗The ﬁrst three authors contributed equally. • A. Agrawal, J. Lu and S. Antol are with Virginia Tech. • M. Mitchell is with Microsoft Research, Redmond. • C. L. Zitnick is with Facebook AI Research. • D. Batra and D. Parikh are with Georgia Institute of Technology.\nFig. 1: Examples of free-form, open-ended questions collected for images via Amazon Mechanical Turk. Note that commonsense knowledge is needed along with a visual understanding of the scene to answer many questions.\nmany bikes are there?”), activity recognition (e.g., “Is this man crying?”), knowledge base reasoning (e.g., “Is this a vegetarian pizza?”), and commonsense reasoning (e.g., “Does this person have 20/20 vision?”, “Is this person expecting company?”). VQA [19], [36], [50], [3] is also amenable to automatic quantitative evaluation, making it possible to effectively track progress on this task. While the answer to many questions is simply “yes” or “no”, the process for determining a correct answer is typically far from trivial (e.g. in Fig. 1, “Does this person have 20/20 vision?”). Moreover, since questions about images often tend to seek speciﬁc information, simple oneto-three word answers are sufﬁcient for many questions. In such scenarios, we can easily evaluate a proposed algorithm by the number of questions it answers correctly. In this paper, we present both an open-ended answering task and a multiplechoice task [45], [33]. Unlike the open-ended task that requires a free-form response, the multiple-choice task only requires an\nDoes it appear to be rainy? Does this person have 20/20 vision? Is this person expecting company? What is just under the tree? How many slices of pizza are there? Is this a vegetarian pizza? What color are her eyes? What is the mustache made of?\nalgorithm to pick from a predeﬁned list of possible answers. We present a large dataset that contains 204,721 images from the MS COCO dataset [32] and a newly created abstract scene dataset [57], [2] that contains 50,000 scenes. The MS COCO dataset has images depicting diverse and complex scenes that are effective at eliciting compelling and diverse questions. We collected a new dataset of “realistic” abstract scenes to enable research focused only on the high-level reasoning required for VQA by removing the need to parse real images. Three questions were collected for each image or scene. Each question was answered by ten subjects along with their conﬁdence. The dataset contains over 760K questions with around 10M answers. While the use of open-ended questions offers many beneﬁts, it is still useful to understand the types of questions that are being asked and which types various algorithms may be good at answering. To this end, we analyze the types of questions asked and the types of answers provided. Through several visualizations, we demonstrate the astonishing diversity of the questions asked. We also explore how the information content of questions and their answers differs from image captions. For baselines, we offer several approaches that use a combination of both text and state-of-the-art visual features [29]. As part of the VQA initiative, we will organize an annual challenge and associated workshop to discuss state-of-the-art methods and best practices. VQA poses a rich set of challenges, many of which have been viewed as the holy grail of automatic image understanding and AI in general. However, it includes as building blocks several components that the CV, NLP, and KR [5], [8], [31], [35], [4] communities have made signiﬁcant progress on during the past few decades. VQA provides an attractive balance between pushing the state of the art, while being accessible enough for the communities to start making progress on the task.",
    "data_related_paragraphs": [
        "VQA Efforts. Several recent papers have begun to study visual question answering [19], [36], [50], [3]. However, unlike our work, these are fairly restricted (sometimes synthetic) settings with small datasets. For instance, [36] only considers questions whose answers come from a predeﬁned closed world of 16 basic colors or 894 object categories. [19] also considers questions generated from templates from a ﬁxed vocabulary of objects, attributes, relationships between objects, etc. In contrast, our proposed task involves open-ended, free-form questions and answers provided by humans. Our goal is to increase the diversity of knowledge and kinds of reasoning needed to provide correct answers. Critical to achieving success on this more difﬁcult and unconstrained task, our VQA dataset is two orders of magnitude larger than [19], [36] (>250,000 vs. 2,591 and 1,449 images respectively). The proposed VQA task has connections to other related work: [50] has studied joint parsing of videos and corresponding text to answer queries on two datasets containing 15 video clips each. [3] uses crowdsourced workers to answer questions about visual content asked by visually-impaired users. In concurrent work, [37] proposed combining an LSTM for the",
        "question with a CNN for the image to generate an answer. In their model, the LSTM question representation is conditioned on the CNN image features at each time step, and the ﬁnal LSTM hidden state is used to sequentially decode the answer phrase. In contrast, the model developed in this paper explores “late fusion” – i.e., the LSTM question representation and the CNN image features are computed independently, fused via an element-wise multiplication, and then passed through fullyconnected layers to generate a softmax distribution over output answer classes. [34] generates abstract scenes to capture visual common sense relevant to answering (purely textual) ﬁll-inthe-blank and visual paraphrasing questions. [47] and [52] use visual information to assess the plausibility of common sense assertions. [55] introduced a dataset of 10k images and prompted captions that describe speciﬁc aspects of a scene (e.g., individual objects, what will happen next). Concurrent with our work, [18] collected questions & answers in Chinese (later translated to English by humans) for COCO images. [44] automatically generated four types of questions (object, count, color, location) using COCO captions. Text-based Q&A is a well studied problem in the NLP and text processing communities (recent examples being [15], [14], [54], [45]). Other related textual tasks include sentence completion (e.g., [45] with multiple-choice answers). These approaches provide inspiration for VQA techniques. One key concern in text is the grounding of questions. For instance, [54] synthesized textual descriptions and QA-pairs grounded in a simulation of actors and objects in a ﬁxed set of locations. VQA is naturally grounded in images – requiring the understanding of both text (questions) and vision (images). Our questions are generated by humans, making the need for commonsense knowledge and complex reasoning more essential. Describing Visual Content. Related to VQA are the tasks of image tagging [11], [29], image captioning [30], [17], [40], [9], [16], [53], [12], [24], [38], [26] and video captioning [46], [21], where words or sentences are generated to describe visual content. While these tasks require both visual and semantic knowledge, captions can often be non-speciﬁc (e.g., observed by [53]). The questions in VQA require detailed speciﬁc information about the image for which generic image captions are of little use [3]. Other Vision+Language Tasks. Several recent papers have explored tasks at the intersection of vision and language that are easier to evaluate than image captioning, such as coreference resolution [28], [43] or generating referring expressions [25], [42] for a particular object in an image that would allow a human to identify which object is being referred to (e.g., “the one in a red shirt”, “the dog on the left”). While task-driven and concrete, a limited set of visual concepts (e.g., color, location) tend to be captured by referring expressions. As we demonstrate, a richer variety of visual concepts emerge from visual questions and their answers.",
        "3 VQA DATASET COLLECTION",
        "We now describe the Visual Question Answering (VQA) dataset. We begin by describing the real images and abstract",
        "Fig. 2: Examples of questions (black), (a subset of the) answers given when looking at the image (green), and answers given when not looking at the image (blue) for numerous representative examples of the dataset. See the appendix for more examples.",
        "scenes used to collect the questions. Next, we describe our process of collecting questions and their corresponding answers. Analysis of the questions and answers gathered as well as baselines’ & methods’ results are provided in following sections. Real Images. We use the 123,287 training and validation images and 81,434 test images from the newly-released Microsoft Common Objects in Context (MS COCO) [32] dataset. The MS COCO dataset was gathered to ﬁnd images containing multiple objects and rich contextual information. Given the visual complexity of these images, they are well-suited for our VQA task. The more diverse our collection of images, the more diverse, comprehensive, and interesting the resultant set of questions and their answers. Abstract Scenes. The VQA task with real images requires the use of complex and often noisy visual recognizers. To attract researchers interested in exploring the high-level reasoning required for VQA, but not the low-level vision tasks, we create a new abstract scenes dataset [2], [57], [58], [59] containing 50K scenes. The dataset contains 20 “paperdoll” human models [2] spanning genders, races, and ages with 8 different expressions. The limbs are adjustable to allow for continuous pose variations. The clipart may be used to depict both indoor and outdoor scenes. The set contains over 100 objects and 31 animals in various poses. The use of this clipart enables the creation of more realistic scenes (see bottom row of Fig. 2) that more closely mirror real images than previous papers [57], [58], [59]. See the appendix for the user interface,",
        "additional details, and examples. Splits. For real images, we follow the same train/val/test split strategy as the MC COCO dataset [32] (including testdev, test-standard, test-challenge, test-reserve). For the VQA challenge (see section 6), test-dev is used for debugging and validation experiments and allows for unlimited submission to the evaluation server. Test-standard is the ‘default’ test data for the VQA competition. When comparing to the state of the art (e.g., in papers), results should be reported on test-standard. Test-standard is also used to maintain a public leaderboard that is updated upon submission. Test-reserve is used to protect against possible overﬁtting. If there are substantial differences between a method’s scores on test-standard and test-reserve, this raises a red-ﬂag and prompts further investigation. Results on test-reserve are not publicly revealed. Finally, test-challenge is used to determine the winners of the challenge. For abstract scenes, we created splits for standardization, separating the scenes into 20K/10K/20K for train/val/test splits, respectively. There are no subsplits (test-dev, test-standard, test-challenge, test-reserve) for abstract scenes. Captions. The MS COCO dataset [32], [7] already contains ﬁve single-sentence captions for all images. We also collected ﬁve single-captions for all abstract scenes using the same user interface1 for collection. Questions. Collecting interesting, diverse, and well-posed questions is a signiﬁcant challenge. Many simple questions",
        "To bias against generic image-independent questions, subjects were instructed to ask questions that require the image to answer. The same user interface was used for both the real images and abstract scenes. In total, three questions from unique workers were gathered for each image/scene. When writing a question, the subjects were shown the previous questions already asked for that image to increase the question diversity. In total, the dataset contains over ∼0.76M questions. Answers. Open-ended questions result in a diverse set of possible answers. For many questions, a simple “yes” or “no” response is sufﬁcient. However, other questions may require a short phrase. Multiple different answers may also be correct. For instance, the answers “white”, “tan”, or “off-white” may all be correct answers to the same question. Human subjects may also disagree on the “correct” answer, e.g., some saying “yes” while others say “no”. To handle these discrepancies, we gather 10 answers for each question from unique workers, while also ensuring that the worker answering a question did not ask it. We ask the subjects to provide answers that are “a brief phrase and not a complete sentence. Respond matter-offactly and avoid using conversational language or inserting your opinion.” In addition to answering the questions, the subjects were asked “Do you think you were able to answer the question correctly?” and given the choices of “no”, “maybe”, and “yes”. See the appendix for more details about the user interface to collect answers. See Section 4 for an analysis of the answers provided. For testing, we offer two modalities for answering the ques-",
        "i.e., an answer is deemed 100% accurate if at least 3 workers provided that exact answer.2 Before comparison, all responses are made lowercase, numbers converted to digits, and punctuation & articles removed. We avoid using soft metrics such as Word2Vec [39], since they often group together words that we wish to distinguish, such as “left” and “right”. We also avoid using evaluation metrics from machine translation such as BLEU and ROUGE because such metrics are typically applicable and reliable for sentences containing multiple words. In VQA, most answers (89.32%) are single word; thus there no high-order n-gram matches between predicted answers and ground-truth answers, and low-order n-gram matches degenerate to exact-string matching. Moreover, these automatic metrics such as BLEU and ROUGE have been found to poorly correlate with human judgement for tasks such as image caption evaluation [6]. For multiple-choice task, 18 candidate answers are created for each question. As with the open-ended task, the accuracy of a chosen option is computed based on the number of human subjects who provided that answer (divided by 3 and clipped at 1). We generate a candidate set of correct and incorrect answers from four sets of answers: Correct: The most common (out of ten) correct answer. Plausible: To generate incorrect, but still plausible answers we ask three subjects to answer the questions without seeing the image. See the appendix for more details about the user interface to collect these answers. If three unique answers are not found, we gather additional answers from nearest neighbor questions using a bag-of-words model. The use of these answers helps ensure the image, and not just commonsense knowledge, is necessary to answer the question. Popular: These are the 10 most popular answers. For instance, these are “yes”, “no”, “2”, “1”, “white”, “3”, “red”, “blue”, “4”, “green” for real images. The inclusion of the most popular answers makes it more difﬁcult for algorithms to infer the type of question from the set of answers provided, i.e., learning that it is a “yes or no” question just because “yes” and “no” are present in the answers. Random: Correct answers from random questions in the dataset. To generate a total of 18 candidate answers, we ﬁrst ﬁnd the union of the correct, plausible, and popular answers. We include random answers until 18 unique answers are found. The order of the answers is randomized. Example multiple choice questions are in the appendix. Note that all 18 candidate answers are unique. But since 10 different subjects answered every question, it is possible that more than one of those 10 answers be present in the 18 choices. In such cases, according to the accuracy metric, multiple options could have a non-zero accuracy.",
        "4 VQA DATASET ANALYSIS",
        "In this section, we provide an analysis of the questions and answers in the VQA train dataset. To gain an understanding of the types of questions asked and answers provided, we visualize the distribution of question types and answers. We also explore how often the questions may be answered without the image using just commonsense information. Finally, we analyze whether the information contained in an image caption is sufﬁcient to answer the questions. The dataset includes 614,163 questions and 7,984,119 answers (including answers provided by workers with and without looking at the image) for 204,721 images from the MS COCO dataset [32] and 150,000 questions with 1,950,000 answers for 50,000 abstract scenes.",
        "with image captions that generically describe the entire image and hence tend to be longer. The brevity of our answers makes automatic evaluation feasible. While it may be tempting to believe the brevity of the answers makes the problem easier, recall that they are human-provided open-ended answers to open-ended questions. The questions typically require complex reasoning to arrive at these deceptively simple answers (see Fig. 2). There are currently 23,234 unique one-word answers in our dataset for real images and 3,770 for abstract scenes. ‘Yes/No’ and ‘Number’ Answers. Many questions are answered using either “yes” or “no” (or sometimes “maybe”) – 38.37% and 40.66% of the questions on real images and abstract scenes respectively. Among these ‘yes/no’ questions, there is a bias towards “yes” – 58.83% and 55.86% of ‘yes/no’ answers are “yes” for real images and abstract scenes. Question types such as “How many...” are answered using",
        "Dataset",
        "In this section, we explore the difﬁculty of the VQA dataset for the MS COCO images using several baselines and novel methods. We train on VQA train+val. Unless stated otherwise, all human accuracies are on test-standard, machine accuracies are on test-dev, and results involving human captions (in gray font) are trained on train and tested on val (because captions are not available for test).",
        "1K answers of the VQA train/val dataset.",
        "2) LSTM Q: An LSTM with one hidden layer is used to obtain 1024-dim embedding for the question. The embedding obtained from the LSTM is a concatenation of last cell state and last hidden state representations (each being 512-dim) from the hidden layer of the LSTM. Each question word is encoded with 300-dim embedding by a fully-connected layer + tanh non-linearity which is then fed to the LSTM. The input vocabulary to the embedding layer consists of all the question words seen in the training dataset.",
        "the age and commonsense perceived by MTurk workers that would be required to answer the question. See the appendix for details. We further analyzed the performance of the model for different age groups on the validation questions for which we have age annotations. In Fig. 11, we computed the average accuracy of the predictions made by the model for questions belonging to different age groups. Perhaps as expected, the accuracy of the model decreases as the age of the question increases (from 61.07% at 3 − 4 age group to 47.83% at 18+ age group). In Fig. 12, we show the distribution of age of questions for different levels of accuracies achieved by our system on the validation questions for which we have age annotations. It is interesting to see that the relative proportions of different age groups is consistent across all accuracy bins with questions belonging to the age group 5-8 comprising the majority of the predictions which is expected because 5-8 is the most common age group in the dataset (see Fig. 7). Table 4 shows the accuracy of different ablated versions of our best model (deeper LSTM Q + norm I) for both the openended and multiple-choice tasks on the VQA test-dev for real images. The different ablated versions are as follows –",
        "the input vocabulary to the embedding layer (which encodes the question words) consists of only those question words which occur atleast 5 times in the training dataset, thus reducing the vocabulary size from 14770 (when all question words are used) to 5134 (65.24% reduction). Remaining question words are replaced with UNK (unknown) tokens. Comparing the accuracies in Table 4 and Table 2, we can see that truncating the question vocabulary @ 5 performs better than using all questions words by 0.24% for openended task and by 0.17% for multiple-choice task.",
        "6) Truncated Q Vocab @ 11: In this model, the input vocabulary to the embedding layer (which encodes the question words) consists of only those question words which occur atleast 11 times in the training dataset, thus reducing the vocabulary size from 14770 (when all question words are used) to 3561 (75.89% reduction). Remaining question words are replaced with UNK (unknown) tokens. Comparing the accuracies in Table 4 and Table 2, we can see that truncating the question vocabulary @ 11 performs better than using all questions words by 0.06% for open-ended task and by 0.02% for multiple-choice task.",
        "7) Filtered Dataset: We created a ﬁltered version of the VQA train + val dataset in which we only keep the answers with subject conﬁdence “yes”. Also, we keep only those questions for which at least 50% (5 out of 10) answers are annotated with subject conﬁdence “yes”. The resulting ﬁltered dataset consists of 344600 questions,",
        "57.59 80.41 Without I Norm 56.80 78.49 Concatenation 56.93 80.61 K = 500 58.15 80.56 K = 2000 Truncated Q Vocab @ 5 57.99 80.67 Truncated Q Vocab @ 11 57.81 80.42 56.62 80.19 Filtered Dataset",
        "compared to 369861 questions in the original dataset, thus leading to only 6.83% reduction in the size of the dataset. The ﬁltered dataset has 8.77 answers per question on average. We did not ﬁlter the test set so that accuracies of the model trained on the ﬁltered dataset can be compared with that of the model trained on the original dataset. The row “Filtered Dataset” in Table 4 shows the performance of the deeper LSTM Q + norm I model when trained on the ﬁltered dataset. Comparing these accuracies with the corresponding accuracies in Table 2, we can see that the model trained on ﬁltered version performs worse by 1.13% for open-ended task and by 1.88% for multiplechoice task.",
        "We have set up an evaluation server3 where results may be uploaded for the test set and it returns an accuracy breakdown. We are organizing an annual challenge and workshop to facilitate systematic progress in this area; the ﬁrst instance of the workshop will be held at CVPR 20164. We suggest that papers reporting results on the VQA dataset –",
        "language question about the image, the task is to provide an accurate natural language answer. We provide a dataset containing over 250K images, 760K questions, and around 10M answers. We demonstrate the wide variety of questions and answers in our dataset, as well as the diverse set of AI capabilities in computer vision, natural language processing, and commonsense reasoning required to answer these questions accurately. The questions we solicited from our human subjects were open-ended and not task-speciﬁc. For some application domains, it would be useful to collect task-speciﬁc questions. For instance, questions may be gathered from subjects who are visually impaired [3], or the questions could focused on one speciﬁc domain (say sports). Bigham et al. [3] created an application that allows the visually impaired to capture images and ask open-ended questions that are answered by human subjects. Interestingly, these questions can rarely be answered using generic captions. Training on task-speciﬁc datasets may help enable practical VQA applications. We believe VQA has the distinctive advantage of pushing the frontiers on “AI-complete” problems, while being amenable to automatic evaluation. Given the recent progress in the community, we believe the time is ripe to take on such an endeavor. Acknowledgements. We would like to acknowledge the countless hours of effort provided by the workers on Amazon Mechanical Turk. This work was supported in part by the The Paul G. Allen Family Foundation via an award to D.P., ICTAS at Virginia Tech via awards to D.B. and D.P., Google Faculty the National Science Research Awards to D.P. and D.B., Foundation CAREER award to D.B., the Army Research Ofﬁce YIP Award to D.B., and a Ofﬁce of Naval Research grant to D.B.",
        "I - Additional analysis comparing captions and Q&A data II - Qualitative visualizations for “What is” questions III - Human accuracy on multiple-choice questions IV - Details on VQA baselines V - “Age” and “Commonsense” of our model VI - Details on the abstract scene dataset VII - User interfaces used to collect the dataset VIII - List of the top answers in the dataset",
        "IX - Additional examples from the VQA dataset",
        "Do questions and answers provide further information about the visual world beyond that captured by captions? One method for determining whether the information captured by questions & answers is different from the information captured by captions is to measure some of the differences in the word distributions from the two datasets. We cast this comparison in terms of nouns, verbs, and adjectives by extracting all words from the caption data (MS COCO captions for real images and captions collected by us for abstract scenes) using the Stanford part-of-speech (POS)6 tagger [49]. We normalize the word frequencies from captions, questions, and answers per image, and compare captions vs. questions and answers combined. Using a Kolmogorov-Smirnov test to determine whether the underlying distributions of the two datasets differ, we ﬁnd a signiﬁcant difference for all three parts of speech (p < .001) for both real images and abstract scenes. This helps motivate the VQA task as a way to learn information about visual scenes; although both captions and questions & answers provide information about the visual world, they do it from different perspectives, with different underlying biases [20], and can function as complementary to one another.",
        "We illustrate the similarities and differences between the word distributions in captions vs. questions & answers as Venn-style word clouds [10] with size indicating the normalized count – Fig. 15 (nouns), Fig. 16 (verbs), and Fig. 17 (adjectives) for real images and Fig. 18 (nouns), Fig. 19 (verbs), and Fig. 20 (adjectives) for abstract scenes.7 The left side shows the top words in questions & answers, the right the top words in captions, and the center the words common to both, with size indicating the harmonic mean of the counts. We see that adjectives in captions capture some clearly visual properties discussed in previous work on vision to language [41], such as material and pattern, while the questions & answers have more adjectives that capture what is usual (e.g., “dominant”, “approximate”, “higher”) and other kinds of commonsense properties (e.g., “edible”, “possible”, “unsafe”, “acceptable”). Interestingly, we see that question & answer nouns capture information about “ethnicity” and “hairstyle”, while caption nouns capture information about pluralized visible objects (e.g., “cellphones”, “daughters”) and groups (e.g., “trio”, “some”), among other differences. “Man” and “people” are common in both captions and questions & answers. One key piece to understanding the visual world is understanding spatial relationships, and so we additionally extract spatial prepositions and plot their proportions in the captions vs. the questions & answers data in Fig. 14 (left) for real images and Fig. 14 (right) for abstract scenes. We see that questions &",
        "Dataset Accuracy Metric",
        "TABLE 6: For each of the two datasets, real and abstract, ﬁrst two rows are the human accuracies for multiple-choice questions when subjects were shown both the image and the question. Majority vote means we consider the answer picked by majority of the three subjects to be the predicted answer by humans and compute accuracy of that answer for each question. Average means we compute the accuracy of each of the answers picked by the subjects and record their average for each question. The last row is the inter-human agreement for open-ended answers task when subjects were shown both the image and the question. All accuracies are evaluated on a random subset of 3000 questions.",
        "“per Q-type prior” baseline. We decide on different question types based on ﬁrst few words of questions in the real images training set and ensure that each question type has at least 30 questions in the training dataset. The most popular answer for each question type is also computed on real images training set. “nearest neighbor” baseline. For every question in the VQA test-standard set, we ﬁnd its k nearest neighbor questions",
        "APPENDIX VI: ABSTRACT SCENES DATASET",
        "In Fig. 23 (left), we show a subset of the objects that are present in the abstract scenes dataset. For more examples of the scenes generated, please see Fig. 28. The user interface used to create the scenes is shown in Fig. 23 (right). Subjects used a drag-and-drop interface to create the scenes. Each object could be ﬂipped horizontally and scaled. The scale of the object determined the rendering order of the objects. Many objects have different attributes corresponding to different poses or types. Most animals have ﬁve different discrete poses. Humans have eight discrete expressions and their poses may be continuously adjusted using a “paperdoll” model [2].",
        "In Fig. 24, we show the AMT interface that we used to collect questions for images. Note that we tell the workers that the robot already knows the answer to the previously asked question(s), inspiring them to ask different kinds of questions, thereby increasing the diversity of our dataset. Fig. 25 shows the AMT interface used for collecting answers to the previously collected questions when subjects were shown the corresponding images. Fig. 26 shows the interface that was used to collect answers to questions when subjects were not shown the corresponding image (i.e., to help in gathering incorrect, but plausible, answers for the multiplechoice task and to assess how accurately the questions can be answered using common sense knowledge alone).",
        "Fig. 23: Left: A small subset of the objects present in the abstract scene dataset. Right: The AMT interface for collecting abstract scenes. The light green circles indicate where users can select to manipulate a person’s pose. Different objects may be added to the scene using the folders to the right.",
        "The top 250 answers in our real images dataset along with their counts and percentage counts are given below. The answers have been presented in different colors to show the different Part-of-Speech (POS) tagging of the answers with the following color code: yes/no, noun, verb, adjective, adverb, and numeral. “yes” (566613, 22.82%), “no” (381307, 15.35%), “2” (80031, 3.22%), “1” (46537, 1.87%), “white” (41753, 1.68%), “3” (41334, 1.66%), “red” (33834, 1.36%), “blue” (28881, 1.16%), “4” (27174, 1.09%), “green” (22453, 0.9%), “black” (21852, 0.88%), “yellow” (17312, 0.7%), “brown” (14488, 0.58%), “5” (14373, 0.58%), “tennis” (10941, 0.44%),“baseball” (10299, 0.41%), “6” (10103, 0.41%), “orange” (9136, 0.37%), “0” (8812, 0.35%), “bathroom” (8473, 0.34%), “wood” (8219, 0.33%), “right” (8209, 0.33%), “left” (8058, 0.32%), “frisbee” (7671, 0.31%), “pink” (7519, 0.3%), “gray” (7385, 0.3%), “pizza” (6892, 0.28%), “7” (6005, 0.24%), “kitchen” (5926, 0.24%), “8” (5592, 0.23%), “cat” (5514, 0.22%), “skiing” (5189, 0.21%), “skateboarding” (5122, 0.21%), “dog” (5092, 0.21%), “snow” (4867, 0.2%), “black and white” (4852, 0.2%), “skateboard” (4697, 0.19%), “surﬁng” (4544, 0.18%), “water” (4513, 0.18%), “giraffe” (4027, 0.16%), “grass” (3979, 0.16%), “surfboard” (3934, 0.16%), “wii” (3898, 0.16%), “kite” (3852, 0.16%), “10” (3756, 0.15%), “purple” (3722, 0.15%), “elephant” (3646, 0.15%), “broccoli” (3604, 0.15%), “man” (3590, 0.14%), “winter” (3490, 0.14%), “stop” (3413, 0.14%), “train” (3226, 0.13%), “9” (3217, 0.13%), “apple” (3189, 0.13%), “silver” (3186, 0.13%), “horse” (3159, 0.13%), “banana” (3151, 0.13%), “umbrella” (3139, 0.13%), “eating” (3117, 0.13%), “sheep” (2927, 0.12%), “bear” (2803, 0.11%), “phone” (2772, 0.11%), “12” (2633, 0.11%), “motorcycle” (2608, 0.11%), “cake” (2602, 0.1%), “wine” (2574, 0.1%), “beach” (2536, 0.1%), “soccer” (2504, 0.1%), “sunny” (2475, 0.1%), “zebra” (2403, 0.1%), “tan” (2402, 0.1%), “brick” (2395, 0.1%), “female” (2372, 0.1%), “bananas” (2350, 0.09%), “table” (2331, 0.09%), “laptop” (2316, 0.09%), “hat” (2277, 0.09%), “bench” (2259, 0.09%), “ﬂowers” (2219, 0.09%), “woman” (2197, 0.09%), “male” (2170, 0.09%), “cow” (2084, 0.08%), “food” (2083, 0.08%), “living room” (2022, 0.08%), “bus” (2011, 0.08%), “snowboarding” (1990, 0.08%), “kites” (1979, 0.08%), “cell phone” (1943, 0.08%), “helmet” (1885, 0.08%), “maybe” (1853, 0.07%), “outside” (1846, 0.07%), “hot dog” (1809, 0.07%), “night” (1805, 0.07%), “trees” (1785, 0.07%), “11” (1753, 0.07%), “bird” (1739, 0.07%), “down” (1732, 0.07%), “bed” (1587, 0.06%), “camera” (1560, 0.06%), “tree” (1547, 0.06%), “christmas” (1544, 0.06%), “fence” (1543, 0.06%), “nothing” (1538, 0.06%), “unknown” (1532, 0.06%), “tennis racket” (1525, 0.06%), “red and white” (1518, 0.06%), “bedroom” (1500, 0.06%), “bat” (1494, 0.06%), “glasses” (1491, 0.06%), “tile” (1487, 0.06%), “metal” (1470, 0.06%), “blue and white” (1440, 0.06%), “fork” (1439, 0.06%), “plane” (1439, 0.06%), “airport” (1422, 0.06%), “cloudy” (1413, 0.06%), “15” (1407, 0.06%), “up” (1399, 0.06%), “blonde” (1398, 0.06%), “day” (1396, 0.06%), “teddy bear” (1386, 0.06%), “glass” (1379, 0.06%), “20” (1365, 0.05%), “beer” (1345, 0.05%), “car” (1331, 0.05%), “sitting” (1328, 0.05%), “boat” (1326, 0.05%),",
        "To provide insight into the dataset, we provide additional examples. In Fig. 27, Fig. 28, and Fig. 29, we show a random selection of the VQA dataset for the MS COCO [32] images, abstract scenes, and multiple-choice questions, respectively.",
        "Fig. 27: Random examples of questions (black), (a subset of the) answers given when looking at the image (green), and answers given when not looking at the image (blue) for numerous representative examples of the real image dataset.",
        "Fig. 28: Random examples of questions (black), (a subset of the) answers given when looking at the image (green), and answers given when not looking at the image (blue) for numerous representative examples of the abstract scene dataset.",
        "Fig. 29: Random examples of multiple-choice questions for numerous representative examples of the real and abstract scene dataset.",
        "[4] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge. In International Conference on Management of Data, 2008. 2",
        "[6] X. Chen, H. Fang, T. Lin, R. Vedantam, S. Gupta, P. Doll´ar, and C. L. Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR, abs/1504.00325, 2015. 4",
        "[7] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Doll´ar, and C. L. Zitnick. Microsoft COCO Captions: Data Collection and Evaluation Server. arXiv preprint arXiv:1504.00325, 2015. 3",
        "Knowledge from Web Data. In ICCV, 2013. 2",
        "exploratory data analysis. Learning and Visualization, 2014. 13, 15, 16",
        "[15] A. Fader, L. Zettlemoyer, and O. Etzioni. Open Question Answering over Curated and Extracted Knowledge Bases. In International Conference on Knowledge Discovery and Data Mining, 2014. 2",
        "[18] H. Gao, J. Mao, J. Zhou, Z. Huang, and A. Yuille. Are you talking to a machine? dataset and methods for multilingual image question answering. In NIPS, 2015. 2",
        "[22] M. Hodosh, P. Young, and J. Hockenmaier. Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. JAIR, 2013. 1",
        "[43] V. Ramanathan, A. Joulin, P. Liang, and L. Fei-Fei. Linking People with “Their” Names using Coreference Resolution. In ECCV, 2014. 2 [44] M. Ren, R. Kiros, and R. Zemel. Exploring models and data for image",
        "[45] M. Richardson, C. J. Burges, and E. Renshaw. MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text. In EMNLP, 2013. 1, 2"
    ]
}