{
    "title_author_abstract_introduction": "A Database of Human Segmented Natural Images and its Application to Evaluating Segmentation Algorithms and Measuring Ecological Statistics\nJitendra Malik David Martin  Charless Fowlkes  Doron Tal Department of Electrical Engineering and Computer Sciences University of California, Berkeley Berkeley, CA 94720 { dmartin,fowlkes,doron,malik} @eecs.berkeley.edu\nAbstract\nThis  paper  presents  a  database  containing  ‘ground truth ’ segmentations prodiiced by humans for images of a wide variety of natural scenes.  We dejine an error nieasiire which qiiantiBes the consistency between segmentations of differing granularities and find  that different hiinian segmentations of the same image are highly consistent, Use of this dataset is demonstrated in two applications:  ( I )  evaluating the perjorniunce of  segmentation algorithms and (2) measuring probability distributions associated with Gestalt grouping factors as well as statistics of image region properties.\n1. Introduction\nTwo central problems in vision are image segmentation and recognition’.  Both problems are hard, and we do not yet have any general purpose solution approaching human level competence for either one.\nWhile it is unreasonable to expect quick solutions to either problem, there is one dimension on which research in recognition is on much more solid grounds-it  is considerably easier to quantify the performance of computer vision algorithms at recognition  than at segmentation.  Recognition is classification, and one can empirically estimate the probability of misclassification by simply counting classification errors on a test set. The ready availability of test sets - two of most significant ones are the MNIST handwritten digit dataset and the FERET face data set-has  meant that different algorithms can be compared directly using the same quantitative error measures.  It is well accepted that one cannot evaluate a recognition algorithm by showing a few images of correct classification. In contrast, image seg-\nI It could be argued that they are aspects of the same problem.  We do\nnot necessarily disagree!\nFigure  1 :  Sample of IO images from the segmentation database.  Each image has been segmented by 3 different  people. A total of  10 people are represented  in this data.\n0-7695-1143-0/01 $10.00 0 2001 IEEE\nAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:40:18 UTC from IEEE Xplore.  Restrictions apply.\nand 3 different segmentations for each image.  The images are of complex, natural  scenes.  In such images, multiple cues are available for segmentation by a human or a computer program-low  level cues such as coherence of brightness,  texture or continuity of contour, intermediate level cues such as symmetry and convexity, as well as high level cues based on recognition of familiar objects. The instructions to the human,observet-s made no attempt to restrict or encourage the use of any particular  type of cues.  For instance, it is perfectly reasonable for observers to use their familiarity with faces to guide their segmentation of the image in the second row of Figure 1.  We realize that this implies that a computational approach based purely  on, say, low-level coherence of color and texture, would find it difficult to attain perfect performance. In our view, this is perfectly  fine.  We wish  to define a  ‘gold standard’ for segmentation results without any prior biases on what cues and algorithms are to be exploited to obtain those results.  We expect that as segmentation and perceptual organization algorithms evolve to make richer use of multiple cues, their performance could continue to be evaluated  on  the  same dataset.\nNote that the segmentations produced by different humans for a given image in Figure 1 are not identical.  But, are they consistent?  One can think of a human’s perceptual organization as imposing a hierarchical  tree structure on the image. Even if two observers have exactly the same perceptual organization of  an  image, they may  choose to segment at varying levels of granularity.  See e.g. Figure 3. This implies that we need  to define segmentation consistency measures that do not penalize such differences.  We demonstrate empirically that human segmentations for the wide variety of images in the database are quite consistent according to these  criteria, suggesting that  we have  a reliable standard with  which  to evaluate different computer algorithms for image segmentation. We exploit this fact to develop a quantitative performance measure for image segmentation algorithms.\nThere has been a limited amount of previous work evaluating segmentation performance using datasets with human observers providing the ground truth.  Heath et al. [8] evaluated the output of different edge detectors on a subjective quantitative scale using the criterion  of ease of recognizability of objects (for human observers) in the edge images. Closer to our work is the Sowerby image dataset that has been used by Huang [9] and Konishi et al. [ 121. This dataset is small, not publicly available, and contains only one segmentation for each image. In spite of these limitations, the dataset has proved quite useful for work such as that of Konishi et al.  who used it to evaluate the effectiveness of different edge filters as indicators of boundaries.  We expect that our dataset would find far wider use, by virtue of being considerably more varied and extensive, and the fact that\n- Figure 2:  Using the segmentation tool. See s2.1 for details.\nFigure 3:  Motivation for making segmentation error measures tolerant to refinement.  (a) shows the original image.  (b)-(d) show three segmentations in our database by different subjects.  (b) and (d) are both simple refinements of (c), while (b) and (d) illustrate murual rejinement.\nmentation performance evaluation remains subjective. Typically, researchers will show their results on a few images and point out why the results ‘look good’. We never know from such studies whether the results are best examples or typical examples, whether the technique will work only on images that have no texture, and so on.\nThe major challenge is that the question “What is a correct segmentation” is a subtler question than “Is this digit a 5”.  This has led researchers e.g.  Borra and Sarkar[3] to argue that segmentation or grouping performance can be evaluated only in the context of a task such as object recognition. We don’t wish to deny the importance of evaluating segmentations in the context of a task.  However, the thesis of this paper is that segmentations can also be evaluated purely as segmentations by comparing them to those produced by multiple human observers and that there is considerable consistency among different human segmentations of the same image so as to make such a comparison reliable.\nFigure 1 shows some example images from the database\nAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 06:40:18 UTC from IEEE Xplore.  Restrictions apply.\nIdeal Measure\nFigure 4:  Distributions of the GCE (left) and LCE (right) measures over the segmentation database. The error measures are applied to all pairs of segmentations. The upper graphs show the error for segmentations of the same image.  The lower graphs show the error for segmentations of different images.  The spike at zero in the different-image graphs is due to degenerate segmentations of one particular image, of which everything else is a refinement. Clockwise from the top-left, the means are 0.1 1,0.07, 0.39, 0.30.\nwe provide a mechanism for computing the consistency of different segmentations.\nThe database that  we  have  collected is a  valuable resource for studying statistics of natural images. Most such studies in the past have concentrated on first and second order statistics such as the power spectrum or covariances, either on pixel brightnesses directly or on wavelet coefficients [lo, 15, 16, 11, 12, 5, 13, 181.  We can go much further given the additional information provided by the segmentations.  For instance, we can evaluate prior distributions corresponding to the various Gestalt factors such as similarity, proximity, convexity etc. and thus provide objective justifications for the use of these cues in grouping.  While this way of thinking about the Gestalt factors was suggested nearly 50 years ago by Brunswik [4], so far empirical measurements of probability distributions have been limited to the factor of good continuation, e.g.  [2]. Another application of the database is in studying the empirical distribution of sizes of regions in an image.  This turns out to follow a power law, consistent with the work of Alvarez, Gousseau and Morel [I] with a rather different definition of sizes.\nThis paper is organized as follows. In 5 2, we describe in detail the construction of the database of image segmentations. In  3 we define measures for evaluating consistency of different segmentations of an image.  4 puts the database to use by evaluating the performance of the Normalized cut algorithm on the different images. Performance is evaluated by  computing the consistency of the computer segmentations with those made by human observers and comparing that to consistency among human observers. In 3 5, we find another use for the database, namely in evaluating the ecological statistics of various Gestalt grouping factors.  We conclude in 5 6.\nFigure 5:  Error matrix for all image pairs, for GCE (left) and LCE (middle).  Mij corresponds to the error between segmentations i and j, where black signifies zero error.  Segmentations are sorted by image, so segmentations of the same image are adjacent.  The spurious horizontal and vertical bands confirm that the spike in the different-image graphs of Figure 4 are caused by degenerate segmentations of one image. The rightmost matrix shows the block-diagonal structure of the ideal error measure applied to a flawless dataset.\nMeasure Correlation (all segmentations)  Measure Correlation (same images)\nFigure 6:  LCE vs.  GCE for segmentations of different images (left) and the same image (right). The dashed line x = y shows that GCE is a stricter measure than LCE.",
    "data_related_paragraphs": [
        "2. Image Segmentation Database",
        "The first task in constructing the segmentation database was to select a set of images.  We chose 1000 representative 48 1 x32 1 RGB images from the Core1 image database. This database of 40,000 images is widely used in computer vision (e.g.  [6, 71).  The criterion for selecting images was simple:  We chose images of natural scenes that contain at least one discernible object. This criterion culls images that are inappropriate for the task of recognition, such as photographs of reflections of neon signs on wet concrete sidewalks, or photographs of marble textures.",
        "In  order to  easily  collect  segmentations from a wide range of people, we have developed a Java application that one can use to divide an image into segments, where a segment is simply a set of pixels.  This approach has several advantages. First, anyone with Internet access can segment images. Second, the process produces an explicit partition of the pixels into groups (segments). Third, a server process can dynamically assign images to users, which gives precise control over the database content as it evolves.",
        "2.3. Database Status and Plans",
        "The results  in this paper were generated using our first version of the dataset that contains 150 grayscale segmentations by 10 people of 50 images, with 30 images with 3 or more segmentations. The data collection is ongoing, and at this time, we have 3000 segmentations by 25 people of 800 images.  We  aim to ultimately  collect at least 4 grayscale and 4 color segmentations of 1000 images.",
        "There are two reasons  to develop a measure that  provides an empirical comparison between two segmentations of an image.  First,  we can use it to validate the segmentation database by showing that segmentations of the same image by different people are consistent.  Second, we can",
        "Figure 9:  The GCE for human vs. human (gray) and NCuts vs. human (white) for each image for which we have 2 3 human segmentations. The LCE data is similar.",
        "A potential  problem  for a measure of consistency between  segmentations is that there is no unique segmentation of an image. For example, two people may segment an image differently because either (1) they perceive the scene differently, or (2) they segment at different granularities.  If two different segmentations arise from different perceptual organizations of the scene, then it is fair to declare the segmentations inconsistent.  If, however, one segmentation is simply a refinement of the other, then the error should be small, or even zero. Figure 3 shows examples of both simple and mutual refinement from our database. We do not penalize simple refinement in our measures, since it does not preclude identical perceptual organizations of the scene.",
        "In  addition  to  being  tolerant  to  refinement,  any  error measure should also be (1) independent of the coarseness of pixelation, (2) robust to noise along region boundaries, and (3) tolerant of different segment counts between the two segmentations. The third point is due to the complexity of the images: We need to be able to compare two segmentations when they have different numbers of segments. In the remainder of this section, we present two error measures that meet all of the aforementioned criteria. We then apply the measures to the database of human segmentations.",
        "We apply the GCE and LCE measures to all pairs of segmentations in our dataset with two goals. First, we hope to show that given the arguably ambiguous task of segmenting an image into an unspecified number of segments, different people produce consistent results on each image.  Second, we hope to validate the measures by showing that the error",
        "We characterize the separation  of the two distributions by  noting that for LCE, 5.9% of  segmentation pairs  lie above 0.12 for the same image or below 0.12 for different images. For GCE, 5.9% of pairs lie above 0.16 for the same image or below 0.16 for different images.  Note the good behavior of both measures despite the fact that the number of segments in each segmentation of a particular image can vary by a factor of 10. Figure 5 shows the raw data used to the compute the histograms.",
        "In this section, we use the segmentation database and error measures to evaluate the Normalized Cuts (NCuts) image segmentation algorithm.",
        "In collecting our dataset, we permitted  a great deal  of flexibility in how many segments each subject created for an image. This is desirable from the point of view of creating an information-rich dataset.  However, when comparing a human segmentation to a computer segmentation, our measures are most meaningful when the number of segments is approximately equal.  For example, an algorithm could thwart the benchmark by  producing one segment for the whole image, or one segment for each pixel.  Due to the tolerance of GCE and LCE to refinement, both of these degenerate segmentations have zero error.",
        "Figure 9 shows both the human error (blue) and NCuts error (red) for each image separately.  In most cases, the human segmentations form a tight distribution near zero. In virtually all cases, NCuts performs worse than humans, but it fares better on some images than others. This data can be used to find the type of images for which an algorithm has the most difficulty.",
        "belong  to  the  same region,  it  is justified  to  group them. In computer vision,  we would similarly like grouping algorithms  to be based  on these  ecological  statistics.  The Bayesian  framework provides a rigorous  approach to exploiting this knowledge in the form of prior probability distributions. Our database enables the empirical measurement of these distributions.",
        "One commonly posited  mid-level  grouping cue is  the convexity  of  foreground  object  boundaries.  We  capture the notion of convexity for discrete, pixel-based regions by measuring the ratio of a region’s area to the area of its convex hull. This yields a number between zero and one where one indicates a perfectly convex region.  Since the regions in our dataset have no’labels that designate them as foreground or background we are forced to look at the distribution of the convexity of all image regions. This on its own is arguably instructive and we imagine that since there can be many foreground groups and only a few background groups in a given image, the distribution  for only foreground regions would look very similar. Figure 12 shows our results. As expected, grouped pixels commonly form a convex region.",
        "$5.2 suggest that intensity bilevel sets are only a rough approximation to perceptual segments in the image. Figure I3 shows the distribution of region areas in our data set. We get an excellent fit from a power law curve of the form y = -$ yielding an CY = 1.008.",
        "In this paper, we presented a database of natural images segmented by human subjects along with two applications of the dataset.  First, we developed an image segmentation benchmark by which one can objectively evaluate segmentation algorithms.  Second, we measured ecological statistics related to Gestalt grouping factors. In time, we expect the database to grow to cover  1000 images, with 4 human segmentations of each image in both grayscale and color. This data is to be made available to the community in the hope that we can place the problem of image segmentation on firm, quantitative ground.",
        "We  would  like  to  thank  Dave  Patterson  for his  valuable input, particularly  in  the data collection and benchmark portions of this paper.  We also graciously thank the Fall 2000 students of UCB CS294, who provided our image segmentations. This work was supported in part by the UC Berkeley MICRO Fellowship (to CF), the NIH Training Grant in Vision Science T32EY 07043-22 (to DT), ARO contract DAAH04-96- 1-034 1, the Digital Library grant IRI941 1334, Defense Advanced Research Projects Agency of the Department of Defense contract DABT63-96-C-0056, the National Science Foundation infrastructure grant EIA9802069, and by a grant from Intel Corporation. The infor-"
    ]
}