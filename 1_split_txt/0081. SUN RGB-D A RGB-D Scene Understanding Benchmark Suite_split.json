{
    "title_author_abstract_introduction": "SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite\nShuran Song\nSamuel P. Lichtenberg Princeton University http://rgbd.cs.princeton.edu\nJianxiong Xiao\nAbstract\nAlthough RGB-D sensors have enabled major breakthroughs for several vision tasks, such as 3D reconstruction, we have not attained the same level of success in highlevel scene understanding. Perhaps one of the main reasons is the lack of a large-scale benchmark with 3D annotations and 3D evaluation metrics. In this paper, we introduce an RGB-D benchmark suite for the goal of advancing the state-of-the-arts in all major scene understanding tasks. Our dataset is captured by four different sensors and contains 10,335 RGB-D images, at a similar scale as PASCAL VOC. The whole dataset is densely annotated and includes 146,617 2D polygons and 64,595 3D bounding boxes with accurate object orientations, as well as a 3D room layout and scene category for each image. This dataset enables us to train data-hungry algorithms for scene-understanding tasks, evaluate them using meaningful 3D metrics, avoid overﬁtting to a small testing set, and study cross-sensor bias.\n1. Introduction\nScene understanding is one of the most fundamental problems in computer vision. Although remarkable progress has been achieved in the past decades, generalpurpose scene understanding is still considered to be very challenging. Meanwhile, the recent arrival of affordable depth sensors in consumer markets enables us to acquire reliable depth maps at a very low cost, stimulating breakthroughs in several vision tasks, such as body pose recognition [56, 58], intrinsic image estimation [4], 3D modeling [27] and SfM reconstruction [72].\nRGB-D sensors have also enabled rapid progress for scene understanding (e.g. [20, 19, 53, 38, 30, 17, 32, 49]). However, while we can crawl color images from the Internet easily, it is not possible to obtain large-scale RGB-D data online. Consequently, the existing RGB-D recognition benchmarks, such as NYU Depth v2 [49], are an orderof-magnitude smaller than modern recognition datasets for color images (e.g. PASCAL VOC [9]). Although these\nFigure 1. Comparison of RGB-D recognition benchmarks. Apart from 2D annotation, our benchmark provided high quality 3D annotation for both objects and room layout.\nsmall datasets successfully bootstrapped initial progress in RGB-D scene understanding in the past few years, the size limit is now becoming the critical common bottleneck in advancing research to the next level. Besides causing overﬁtting of the algorithm during evaluation, they cannot support training data-hungry algorithms that are currently the state-of-the-arts in color-based recognition (e.g. [15, 36]). If a large-scale RGB-D dataset were available, we could borrow the same success to the RGB-D domain as well. (Table 1 shows the performance improvement for a RGBD deep learning algorithm [20] when a bigger training set is used.) Furthermore, although the RGB-D images in these datasets contain depth maps, the annotation and evaluation metrics are mostly in 2D image domain, but not directly in 3D (Figure 1). Scene understanding is much more useful in the real 3D space for most applications. We desire to reason about scenes and evaluate algorithms in 3D.\nTo this end, we introduce SUN RGB-D, a dataset containing 10,335 RGB-D images with dense annotations in both 2D and 3D, for both objects and rooms. Based on this dataset, we focus on six important recognition tasks\n(a) NYU Depth v2(b) UW Object Dataset(c) SUN3D(d) Ours\fTraining Set\nTesting Set\nNYU (795 images) SUN RGB-D (5,285 images)\nIntel Realsense\nAsus Xtion\nKinect v1\nKinect v2\nNYU SUN RGB-D\n32.50 15.78 Table 1. Performance improves as the size of training data increases. We trained the Depth-RCNN [20] for 2D object detection using RGB-D images, and evaluated the mean average precision. Bigger training set produces better result. Especially for the ﬁrst row using NYU as the testing set, the performance is still better using the bigger SUN RGB-D that is a superset of NYU, despite the domain gap due to dataset bias.\nRealSense 0.077 0.25 × 2.5W USB 468 628 1080 1920\nXtion weight (pound) 0.5 size (inch) 1.4 × power 2.5W USB depth resolution 480 640 424 color resolution 640 480 1080 Table 2. Speciﬁcation of sensors. RealSense is very light, while Kinect v2 is heavier and has much higher power consumption.\nKinect v1 4 2.3 × × 12.96W 480 640 480 640\nKinect v2 4.5 2.7 × × 115W\nh t p e d d e n fi e r\ns t n o p d e n fi e r\ntowards total scene understanding, which recognizes objects, room layouts and scene categories. For each task, we propose metrics in 3D and evaluate baseline algorithms derived from the state-of-the-arts. Since there are several popular RGB-D sensors available, each with different size andpowerconsumption, weconstructourdatasetusingfour different kinds of sensors to study how well the algorithms generalizeacrosssensors. ByconstructingaPASCAL-scale dataset and deﬁning a benchmark with 3D evaluation metrics, we hope to lay the foundation for advancing RGB-D scene understanding in the coming years.\n1.1. Related work\nThere are many interesting works on RGB-D scene understanding, including semantic segmentation [53, 49, 19] object classiﬁcation [69], object detection [59, 20, 62], context reasoning [38], mid-level recognition [32, 31], and surface orientation and room layout estimation [13, 14, 74]. Having a solid benchmark suite to evaluate these tasks will be very helpful in further advancing the ﬁeld.\nThere are many existing RGB-D datasets available [54, 47, 1, 25, 44, 60, 49, 45, 29, 66, 57, 52, 46, 16, 21, 73, 35, 67, 3, 41, 10, 63, 42, 64, 65, 48, 12, 33, 8, 50, 26, 6]. Figure 1 shows some of them. Here we will brieﬂy describe several most relevant ones1. There are datasets [61, 37] that capture objects on a turntable instead of real-world scenes. For natural indoor scene datasets, NYU Depth v2 [49] is probably the most popular one. They labeled 1,449 selected frames from short RGB-D videos using 2D semantic segmentation on the image domain. [18] annotates each object by aligning a CAD model with the 3D point cloud. However, the 3D annotation is quite noisy, and in our benchmark we reuse the 2D segmentation but recreate the 3D an-\n1 A full list with brief descriptions is available at http://www0.cs.\nucl.ac.uk/staff/M.Firman/RGBDdatasets/.\nFigure 2. Comparison of the four RGB-D sensors. The raw depth map from Intel RealSense is noisier and has more missing values. Asus Xtion and Kinect v1’s depth map have observable quantization effect. Kinect v2 is more accurate to measure the details in depth, but it is more sensitive to reﬂection and dark color. Across different sensors our depth improvement algorithm manages to robustly improve the depth map quality.\nnotation by ourselves. Although this dataset is very good, the size is still small compared to other modern recognition datasets, suchasPASCALVOC[9]orImageNet[7]. B3DO [28] is another dataset with 2D bounding box annotations on the RGB-D images. But its size is smaller than NYU and it has many images with an unrealistic scene layouts (e.g. snapshot of a computer mouse on the ﬂoor). The Cornell RGBD dataset [2, 34] contains 52 indoors scenes with per-point annotations on the stitched point clouds. SUN3D [72] contains 415 RGB-D video sequence with 2D polygon annotation on some key frames. Although they stitched the point cloud in 3D, the annotation is still purely in the 2D image domain, and there are only 8 annotated sequences.",
    "data_related_paragraphs": [
        "2. Dataset construction",
        "The goal of our dataset construction is to obtain an image dataset captured by various RGB-D sensors at a similar scale as the PASCAL VOC object detection benchmark. To improve the depth map quality, we take short videos and use multiple frames to obtain a reﬁned depth map. For each image, we annotate the objects with both 2D polygons and 3D bounding boxes and the room layout with 3D polygons.",
        "Since there are several popular sensors available, with different size and power consumption, we construct our dataset using four kinds – Intel RealSense 3D Camera for",
        "Figure 3. Example images with annotation from our dataset.",
        "The depth maps from these cameras are not perfect, due to measurement noise, view angle to the regularly reﬂective surface, and occlusion boundary. Because all the RGBD sensors operate as a video camera, we can use nearby frames to improve the depth map, providing redundant data to denoise and ﬁll in missing depth.",
        "Figure 4. Statistics of semantic annotation in our dataset.",
        "Figure 5. Data Capturing Process. (a) RealSense attached to laptop, (b) Kinect v2 with battery, (c) Capturing setup for Kinect v2.",
        "2.4. Data acquisition",
        "To construct a dataset at the PASCAL VOC scale, we capture a signiﬁcant amount of new data by ourselves and combine some existing RGB-D datasets. We capture 3,784 images using Kinect v2 and 1,159 images using Intel RealSense. We included the 1,449 images from the NYU Depth V2 [49], and also manually selected 554 realistic scene images from the Berkeley B3DO Dataset [28], both captured by Kinect v1. We manually selected 3,389 distinguished frames without signiﬁcant motion blur from the SUN3D videos [72] captured by Asus Xtion. In total, we obtain 10,335 RGB-D images.",
        "As shown in Figure 5, we attach an Intel RealSense to a laptop and carry it around to capture data. For Kinect v2 we use a mobile laptop harness and camera stabilizer. Because Kinect v2 consumes a signiﬁcant amount of power, we use a 12V car battery and a 5V smartphone battery to power the sensor and the adaptor circuit. The RGB-D sensors only work well for indoors. And we focus on universities, houses, and furniture stores in North America and Asia. Some example images are shown in Figure 3.",
        "For each RGB-D image, we obtain LabelMe-style 2D polygon annotations, 3D bounding box annotations for objects, and 3D polygon annotations for room layouts. To ensure annotation quality and consistency, we obtain our own ground truth labels for images from other datasets; the only exception is NYU, whose 2D segmentation we use.",
        "report the result on Table 3. Since our dataset is quite large, we expect non-parametric label transfer to work well. We ﬁrst use Places-CNN features [75] to ﬁnd the nearest neighbor and directly copy its segmentation as the result. We surprisingly found that this simple method performs quite well, especially for big objects (e.g. ﬂoor, bed). We then adapt the SIFT-ﬂow algorithm [40, 39], on both color and depth to estimation ﬂow. But it only slightly improves performance.",
        "For evaluation, we carefully split the data into training and testing set, ensuring each sensor has around half for training and half for testing, Since some images are captured from the same building or house with similar furniture styles, to ensure fairness, we carefully split the training and testing sets by making sure that those images from the same building either all go into the training set or the testing set and do not spread across both sets. For data from NYU Depth v2 [49], we use the original split.",
        "Scene Categorization For this task, we use the 19 scene categories with more than 80 images. We choose GIST [51] with a RBF kernel one-vs-all SVM as the baseline. We also choose the state-of-the-art Places-CNN [75] scene feature, which achieves the best performance in color-based scene classiﬁcation on the SUN database [70]. This feature is learned using a Deep Convolutional Neural Net (AlexNet [36]) with 2.5 million scene images [75]. We use both linear SVM and RBF kernel SVM with this CNN feature. Also, empirical experiments [20] suggest that both traditional image features and deep learning features for color image can be used to extract powerful features for depth maps as well. Therefore, we also compute the GIST and Places-CNN on the depth images. We also evaluate the concatenation of depth and color features. The depth image is encoded as HHA image as in [20] before extract the feature. Figure 6 reports the accuracy for these experiments. We can see that the deep learning features indeed perform much better, and the combination of color and depth features also helps.",
        "Cross sensor Because real data likely come from different sensors, it is important that an algorithm can generalize across them. Similar to dataset bias [68], we study sensor bias for different RGB-D sensors. We conduct an experiment to train a DPM object detector using data captured by one sensor and test on data captured by another to evaluate the cross-sensor generality. To separate out the dataset biases, we do this experiment on a subset of our data, where a Xtion and a Kinect v2 are mounted on a rig with large overlapping views of the same places. From the result in Table 7, we can see that sensor bias does exist. Both color and depth based algorithms exhibit some performance drop. We hope this benchmark can stimulate the development of RGB-D algorithms with better sensor generalization ability.",
        "Acknowledgement. This work is supported by gift funds from Intel Corporation. We thank Thomas Funkhouser, Jitendra Malik, Alexi A. Efros and Szymon Rusinkiewicz for valuable discussion. We also thank Linguang Zhang, Fisher Yu, Yinda Zhang, Luna Song, Zhirong Wu, Pingmei Xu, Guoxuan Zhang and others for data capturing and labeling.",
        "[7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 2",
        "[13] D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3d primitives for single image understanding. In ICCV, 2013. 2 [14] D. F. Fouhey, A. Gupta, and M. Hebert. Unfolding an indoor",
        "[26] C. Ionescu, D. Papava, V. Olaru, and C. Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. PAMI, 2014. 2",
        "[28] A. Janoch, S. Karayev, Y. Jia, J. T. Barron, M. Fritz, K. Saenko, and T. Darrell. A category-level 3-d object dataset: Putting the kinect to work. In ICCV Workshop on Consumer Depth Cameras for Computer Vision, 2011. 2, 4 [29] A. Janoch, S. Karayev, Y. Jia, J. T. Barron, M. Fritz, K.Saenko, andT.Darrell. Acategory-level3dobjectdataset: Putting the kinect to work. 2013. 2",
        "multi-view rgb-d object dataset. In ICRA, 2011. 2",
        "[61] A. Singh, J. Sha, K. S. Narayan, T. Achim, and P. Abbeel. Bigbird: A large-scale 3d database of object instances. In ICRA, 2014. 2",
        "[63] L. Spinello and K. O. Arras. People detection in rgb-d data.",
        "[68] A. Torralba and A. A. Efros. Unbiased look at dataset bias.",
        "[70] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010. 5, 7",
        "[72] J. Xiao, A. Owens, and A. Torralba. SUN3D: A database of big spaces reconstructed using SfM and object labels. In ICCV, 2013. 1, 2, 4",
        "[75] B. Zhou, J. Xiao, A. Lapedriza, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In NIPS, 2014. 5, 7",
        "from rgb-d video data. In IJCAI, 2013. 2",
        "[42] M. Luber, L. Spinello, and K. O. Arras. People tracking in rgb-d data with on-line boosted target models. In IROS, 2011. 2",
        "database for face recognition. 2",
        "[50] B.Ni, G.Wang, andP.Moulin. Rgbd-hudaact: Acolor-depth video database for human daily activity recognition. 2011. 2 [51] A. Oliva and A. Torralba. Modeling the shape of the scene: A holistic representation of the spatial envelope. IJCV, 2001. 5, 7",
        "[55] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman. Labelme: a database and web-based tool for image annotation. IJCV, 2008. 4"
    ]
}