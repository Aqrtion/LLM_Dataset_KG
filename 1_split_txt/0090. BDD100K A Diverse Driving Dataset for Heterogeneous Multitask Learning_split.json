{
    "title_author_abstract_introduction": "BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning\nFisher Yu1 Haofeng Chen1 Xin Wang1 Wenqi Xian2∗ Yingying Chen1 Fangchen Liu3∗ Vashisht Madhavan4∗ Trevor Darrell1\n1UC Berkeley 2Cornell University\n3UC San Diego\n4Element, Inc.\nAbstract\nDatasets drive vision progress, yet existing driving datasets are impoverished in terms of visual content and supported tasks to study multitask learning for autonomous driving. Researchers are usually constrained to study a small set of problems on one dataset, while real-world computer vision applications require performing tasks of various complexities. We construct BDD100K 1, the largest driving video dataset with 100K videos and 10 tasks to evaluate the exciting progress of image recognition algorithms on autonomous driving. The dataset possesses geographic, environmental, and weather diversity, which is useful for training models that are less likely to be surprised by new conditions. Based on this diverse dataset, we build a benchmark for heterogeneous multitask learning and study how to solve the tasks together. Our experiments show that special training strategies are needed for existing models to perform such heterogeneous tasks. BDD100K opens the door for future studies in this important venue.\n1. Introduction\nDiverse, large-scale annotated visual datasets, such as ImageNet [8] and COCO [19], have been the driving force behind recent advances in supervised learning tasks in computer vision. Typical deep learning models can require millions of training examples to achieve state-of-the-art performance for a task [17, 28, 16].\nFor autonomous driving applications, however, leveraging the power of deep learning is not as simple due to the lack of comprehensive datasets. Existing datasets for autonomous driving [15, 7, 24] are limited in one or more signiﬁcant aspects, including the scene variation, the richness of annotations, and the geographic distribution. Additionally, models trained on existing datasets tend to overﬁt speciﬁc domain characteristics [26].\nReal-world applications require performing a combina-\n∗Work done at UC Berkeley. 1The data is available at https://bdd-data.berkeley.edu\ntion of perception tasks with different complexities, instead of only homogeneous multiple tasks with the same prediction structure [27, 38, 1, 21]. Although it may be feasible to label a large number of images with simple annotations such as drivable areas and object bounding boxes [11, 19], it remains challenging to obtain more complicated annotations such as instance segmentation [3], not to mention multi-object detection and segmentation tracking [31, 22]. As a result, even though a considerable amount of effort has been put into constructing large-scale visual datasets, research on those complicated tasks is still limited to small datasets [7, 15]. In production environments, it is also unclear how to allocate resources for various annotations to support the applications requiring heterogeneous tasks with various output structures.\nWe aim to facilitate algorithmic study on large-scale diverse visual data and multiple tasks. We build BDD100K, a new, diverse, and large-scale dataset of visual driving scenes, together with various tasks, to overcome the limitations. We have been able to collect and annotate the largest available dataset of annotated driving scenes, consisting of over 100K diverse video clips. BDD100K covers more realistic driving scenarios and captures more of the “long-tail” ofappearancevariationandposeconﬁgurationofcategories of interest in diverse environmental domains. Our benchmarks are comprised of ten tasks: image tagging, lane detection, drivable area segmentation, road object detection, semantic segmentation, instance segmentation, multi-object detection tracking, multi-object segmentation tracking, domain adaptation, and imitation learning, as shown in Figure 1. These diverse tasks make the study of heterogeneous multitask learning possible. In our benchmarks, the models can perform a series of tasks with increasing complexities. We conduct extensive evaluations of existing algorithms on our new benchmarks. Special attention is paid to multitask learning in homogeneous, cascaded, and heterogeneous settings. Our experiments present many new ﬁndings, made possible by the diverse set of tasks on a single dataset. Our benchmark models on heterogeneous multitask learning shed light on the challenges of designing one single model to support multiple tasks.\nFigure 1: Overview of our dataset. Our dataset includes a diverse set of driving videos under various weather conditions, time, and scene types. The dataset also comes with a rich set of annotations: scene tagging, object bounding box, lane marking, drivable area, full-frame semantic and instance segmentation, multiple object tracking, and multiple object tracking with segmentation.\nThe major contributions of our paper are: 1) a comprehensive diverse 100K driving video dataset supporting tasks of multiple complexities, which can serve as an evaluation benchmark for computer vision research for autonomous driving; 2) a benchmark for heterogeneous multitask learning and baseline studies to facilitate future study.",
    "data_related_paragraphs": [
        "Visual datasets are necessary for numerous recognition tasks in computer vision. Especially with the advent of deep learning methods, large scale visual datasets, such as [8, 36, 40, 24], are essential for learning high-level image representations. They are general-purpose and include millions of images with image-level categorical labels. These large datasets are useful in learning representations for image recognition, but most of the complex visual understanding tasks in the real world require more ﬁne-grained recognition such as object localization and segmentation [11]. Our proposed dataset provides these multi-granularity annotations for more in-depth visual reasoning. In addition, weprovidetheseannotationsinthecontextofvideos, which provides an additional dimension of visual information. Although large video datasets exist [5, 2, 29], they usually are restricted to image-level labels. Driving datasets have received increasing attention in the recent years, due to the popularity of autonomous vehicle technology. The goal is to understand the challenge of computer vision systems in the context of self-driving. Some of the datasets focus on particular objects such as pedestrians [9, 39]. Cityscapes [7] provides instance-level semantic segmentation on sampled frames of videos collected by their own vehicle. RobotCar [20] and KITTI [15] also",
        "provide data of multiple sources such as LiDAR scanned points. Because it is very difﬁcult to collect data that covers a broad range of time and location, the data diversity of these datasets is limited. For a vehicle perception system to be robust, it needs to learn from a variety of road conditions in numerous cities. Our data was collected from the same original source as the videos in [33]. However, the primary contribution of our paper is the video annotations with benchmarks on heterogeneous tasks. Mapillary Vistas [24] provides ﬁne-grained annotations for user uploaded data, which is much more diverse with respect to location. However, these images are one-off frames that are not placed in the context of videos with temporal structure. Like Vistas, our data is crowdsourced, however, our dataset is collected solely from drivers, with each annotated image corresponding to a video sequence, which enables interesting applications for modeling temporal dynamics.",
        "Figure 2: Geographical distribution of our data sources. Each dot represents the starting location of every video clip. Our videos are from many cities and regions in the populous areas in the US.",
        "We aim to provide a large-scale diverse driving video dataset with comprehensive annotations that can expose the challenges of street-scene understanding. To achieve good diversity, we obtain our videos in a crowd-sourcing manner uploaded by tens of thousands of drivers, supported by Nexar 2. The dataset contains not only images with high resolution (720p) and high frame rate (30fps), but also GPS/IMU recordings to preserve the driving trajectories. In total, we have 100K driving videos (40 seconds each) collected from more than 50K rides, covering New York, San Francisco Bay Area, and other regions as shown in Figure 2. The dataset contains diverse scene types such as city streets, residential areas, and highways. Furthermore, the videos were recorded in diverse weather conditions at different times of the day. The videos are split into training (70K), validation (10K) and testing (20K) sets. The frame at the 10th second in each video is annotated for image tasks and the entire sequences are used for tracking tasks.",
        "for each image. The videos contain large portions of extreme weather conditions, such as snow and rain. They also include a diverse number of different scenes across the world. Notably, our dataset contains approximately an equal number of day-time and night-time videos. Such diversity allows us to study domain transfer and generalize our object detection model well on new test sets. Detailed distributions of images with weather, scene, and day hours tags are shown in the Appendix. We provide image tagging classiﬁcation results using DLA-34 [37] in Figure 4. The average classiﬁcation accuracy across different weather and scenes are around 50 to 60%.",
        "The lane marking detection is critical for vision-based vehicle localization and trajectory planning. However, available datasets are often limited in scale and diversity. For example, the Caltech Lanes Dataset [4] only contains 1,224 images, and the Road Marking Dataset [32] has 1,443 images labeled in 11 classes of lane markings. The most recent work, VPGNet [18], consists of about 20,000 images taken during three weeks of driving in Seoul.",
        "Datasets",
        "Caltech Lanes Dataset [4] Road Marking Dataset [32] KITTI-ROAD [14] VPGNet [18]",
        "We provide ﬁne-grained, pixel-level annotations for images from each of the 10,000 video clips randomly sampled from the whole dataset. Each pixel is given a label and a corresponding identiﬁer denoting the instance number of that object label in the image. Since many classes (e.g., sky) are not amenable to being split into instances, only a small subset of class labels are assigned instance identiﬁers. The entire label set consists of 40 object classes, which are chosen to capture the diversity of objects in road scenes as well as maximizing the number of labeled pixels in each image. Besides a large number of labels, our dataset exceeds previous efforts in terms of scene diversity and complexity. The",
        "Figure 7: Cumulative distributions of the box size (left), the ratio between the max and min box size for each track (middle) and track length (right). Our dataset is more diverse in object scale.",
        "Figure 8: Number of occlusions by track (left) and number of occluded frames for each occlusion (right). Our dataset covers complicated occlusion and reappearing patterns.",
        "whole set is split into 3 parts: 7K images for training, 1K images for validation, and 2K images for testing. The distribution of classes in the semantic instance segmentation dataset is shown in the Appendix.",
        "To understand the temporal association of objects within the videos, we provide a multiple object tracking (MOT) dataset including 2,000 videos with about 400K frames. Each video is approximately 40 seconds and annotated at 5 fps, resulting in approximately 200 frames per video. We observe a total number of 130.6K track identities and 3.3M bounding boxes in the training and validation set. The dataset splits are 1400 videos for training, 200 videos for validation and 400 videos for testing. Table 2 shows the comparison of BDD100K with previous MOT datasets. Our tracking benchmark provides one orderof-magnitude bigger than the previously popular tracking dataset, MOT17 [22]. A recent dataset released by Waymo [30] has fewer tracking sequences (1150 vs 2000) and fewer frames (230K vs 398K) in total, compared to ours. But Waymo data has more 2D boxes (9.9M vs 4.2M), while ours has better diversity including different weather conditions and more locations. Distributions of tracks and bounding boxes by category are shown in the Appendix.",
        "Datasets",
        "Table 2: MOT datasets statistics of training and validation sets. Our dataset has more sequences, frames, identities as well as more box annotations.",
        "length of each track. The distributions show that the MOT dataset is not only diverse in visual scale among and within tracks, but also in the temporal range of each track.",
        "Objects in our tracking data also present complicated occlusion and reappearing patterns are shown in Figure 8. An object may be fully occluded or move out of the frame, and then reappear later. We observe 49,418 occurrences of occlusion in the dataset, or one occurrence of occlusion every 3.51 tracks. Our dataset shows the real challenges of object re-identiﬁcation for tracking in autonomous driving.",
        "We further provide a multiple object tracking and segmentation (MOTS) dataset with 90 videos. We split the dataset into 60 training videos, 10 validation videos, and 20 testing videos.",
        "Datasets",
        "Table 3: Comparisons with other MOTS and VOS datasets.",
        "Table 3 shows the details of the BDD MOTS dataset and the comparison with existing multiple object tracking and segmentation (MOTS) and video object segmentation (VOS) datasets. MOTS aims to perform segmentation and tracking of multiple objects in crowded scenes. Therefore, MOTS datasets like KITTI MOTS and MOTS Challenge [31] require denser annotations per frame and therefore are smaller in size than VOS datasets. BDD100K MOTS provides a MOTS dataset that is larger than the KITTI and MOTS Challenge datasets, with the number of annotations comparable with the large-scale YouTube VOS [34] dataset. Detailed distributions of the MOTS dataset by category are shown in the Appendix.",
        "Figure 9: Visual comparisons of the same model (DRN [35]) trained on different datasets. We ﬁnd that there is a dramatic domain shift between Cityscapes and our new dataset. For example, due to infrastructure difference, the model trained on Cityscapes is confused by some simple categories such as sky and trafﬁc signs.",
        "GPS/IMU recordings in our dataset show the human driver action given the visual input and the driving trajectories. We can use those recordings as a demonstration supervision for the imitation learning algorithms and use perplexity to measure the similarity of driving behaviors on the validation and testing set. We refer to Xu et al. [33] for details on the evaluation protocols. Visualizations of the driving trajectories are shown in the Appendix.",
        "One distinct featureof our data is diversity, besides video and scale. We can study new challenges that the diversity bringstoexistingalgorithmsandhowourdatacomplements existing datasets. We conduct two sets of experiments on object detection and semantic segmentation. In object detection experiments, we study the different domains within our dataset. While in semantic segmentation, we investigate the domains between our data and Cityscapes [7].",
        "Our dataset has an advantage in diversity, compared to other popular driving datasets. We investigate the inﬂuence of domain differences on object detection. The whole dataset is partitioned into several domains based on time of day and scene types. City street and daytime are chosen as validation domains. The training sets have the same",
        "We also compare the models trained on Cityscapes and ours, to understand the difference between our new datasets and existing driving datasets. Cityscapes data is collected in German cities, while our data is mainly from the US. We observe that there is a dramatic domain shift between the two datasets for semantic segmentation models. The models perform much worse when tested on a different dataset. This suggests that even for the domain of other datasets, our new dataset is complementary, which augments existing datasets. Figure 9 shows the discrepancy visually. We can observe that the model trained on Cityscape can not recognize the trafﬁc sign in the US.",
        "Weﬁrstinvestigatetheeffectsofjointlyperformingtasks with similar output structures. The BDD100K lane marking and drivable area datasets share the same set of 70K training images. Drivable area annotations consist of 2 foreground classes and lane marking annotations have 3 attributes (direction, continuity, and category). We formulate the detection of drivable area as segmentation and lane marking as contour detection. We evaluate drivable area segmentation by mean IoU, and lane marking by the Optimal Dataset Scale F-measure (ODS-F) for each category of the three attributes using the Structured Edge Detection Toolbox [10] with tolerance τ = 1, 2, and 10 pixels. We employ morphological thinning for each score threshold during evaluation. We employ DLA-34 [37] as the base model for the segmentation tasks. We implement the segmentation head with four 3 × 3 convolution blocks followed by an 1 × 1 convolution to produce segmentation maps in a 4x down-sampled scale, and use bilinear interpolation to upsample the output to the original scale. For lane marking, we use three segmentation heads for the three attributes. We employ the weighted cross-entropy loss with foreground weight 10 for the lane marking heads, and the gradient-based nonmaximum suppression for post-processing. We construct three train sets with 10K, 20K and the full 70K images and report the evaluation results of models trained on individual tasks and both tasks in Table 5. Full evaluation results for lane marking are shown in the Appendix.",
        "Object detection and instance segmentation. The BDD instance segmentation dataset contains 7K images, whereas the detection dataset has 70K images. We ﬁrst study whether adding more object detection annotations can help instance segmentation. We use Mask R-CNN [16] with ResNet-50 [17] as the backbone, and train detection and instance segmentation in a batch-level round-robin manner. As shown in Table 6, AP increases from 21.8 to 24.5 with joint training. The instance segmentation model is able to learn better object appearance features and localization from the detection set with a much richer diversity of images and object examples. Zhou et al. [41] explore the shape priors in the detection supervision and improve the semi-supervised instance segmentation results further.",
        "MOT and object detection. BDD100K MOT has 278K training frames from 1,400 videos, whereas the detection set contains 70K images sampled from 70K videos. For the detection and MOT models, we use a modiﬁed version of Faster R-CNN [28] with a shared DLA-34 [37] backbone. The implementation details of the tracking model are shown in the Appendix. Table 7 shows that joint training of detection and multiple object tracking improves the singletask MOT model with detection AP increasing from 28.1 to 30.7 and MOTA from 55.0 to 56.7, with a slight increase in identity switch. Semantic segmentation with other tasks. Following a similar manner, we ﬁne-tune a base semantic segmentation model byjointly trainingsemantic segmentationwith detectionandlanemarking/drivableareaasshowninTable8. We observe that training with the additional 70K object detection dataset improves the overall mIoU from 56.9 to 58.3, with the improvement mostly attributed to the object classes that are present in the object detection dataset. When jointly training with the lane marking and drivable area sets, the IOU of the stuff classes (e.g., road and sidewalk) improves though the overall IOU across all classes decreases.",
        "The ultimate goal of our benchmark is to study how to perform all the heterogeneous tasks together for autonomous driving. To understand the potential and difﬁ- culty, we study joint training for multiple object tracking and segmentation, a downstream task to object detection, instance segmentation, and multiple object tracking. Since the MOTS dataset requires time-consuming instance segmentationannotationsateachframe, thedatasetisrelatively limited in video diversity, with 12K frames from 60 videos in the training set. We aim to improve the performance on the task of MOTS by leveraging the diversity from the detection set with 70K images from 70K videos, the MOT set with 278K frames from 1,400 videos, and the instance segmentation set with 7K images from 7K videos.",
        "tracking and segmentation accuracy (MOTSA), precision (MOTSP), and other metrics used by [31] in Table 9. We ﬁrst ﬁne-tune the MOTS model from pre-trained models of upstream tasks. Compared with training MOTS from scratch, ﬁne-tuning from the pre-trained instance segmentation model improves segmentation AP and MOTSP. Finetuning from the pre-trained MOT model, on the other hand, reduces identity switch (IDSW). The extra training examples from the instance segmentation and MOT datasets improve the segmentation and box propagation respectively, thus improving the overall MOTSA results by a large margin. We ﬁnally ﬁne-tune the jointly trained detection and tracking model mentioned in Table 7 by jointly training the four tasks together. We achieve an overall segmentation AP of 23.3 and MOTSA of 41.4.",
        "In this work, we presented BDD100K, a large-scale driving video dataset with extensive annotations for heterogeneous tasks. We built a benchmark for heterogeneous multitask learning where the tasks have various prediction structures and serve different aspects of a complete driving system. Our experiments provided extensive analysis to different multitask learning scenarios: homogeneous multitask learning and cascaded multitask learning. The results presented interesting ﬁndings about allocating the annotation budgets in multitask learning. We hope our work can foster future studies on heterogeneous multitask learning and shed light on this important direction.",
        "[3] D. Acuna, H. Ling, A. Kar, and S. Fidler. Efﬁcient interactive annotation of segmentation datasets with polygonrnn++. 2018. 1",
        "[7] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3213–3223, 2016. 1, 2, 6",
        "[8] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009. 1, 2",
        "[12] T. Evgeniou and M. Pontil. Regularized multi–task learning. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 109–117. ACM, 2004. 2",
        "[15] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231–1237, 2013. 1, 2, 5, 10",
        "1000 km: The oxford robotcar dataset. 36(1):3–15, 2017. 2",
        "[24] G. Neuhold, T. Ollmann, S. R. Bul`o, and P. Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In International Conference on Computer Vision (ICCV), 2017. 1, 2",
        "[29] K. Soomro, A. R. Zamir, and M. Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 2",
        "[30] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. arXiv, pages arXiv–1912, 2019. 5",
        "[33] H. Xu, Y. Gao, F. Yu, and T. Darrell. End-to-end learning of driving models from large-scale video datasets. arXiv preprint, 2017. 2, 6",
        "[36] F. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and J. Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 2",
        "[39] S. Zhang, R. Benenson, and B. Schiele. Citypersons: A diverse dataset for pedestrian detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2, 10",
        "[40] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In Advances in neural information processing systems, pages 487–495, 2014. 2",
        "A. Dataset Details",
        "We present more details about our dataset and annota-",
        "Table 10: Comparisons on number of pedestrians with other datasets. The statistics are based on the training set in each dataset.",
        "Table 10 compares the number of pedestrians of the detection dataset with other datasets. Our dataset has more examples of pedestrians, but because our dataset contains non-city scenes such as highways, the number of person per image is lower than Cityscapes.",
        "Our choice of annotated lane attributes is based on their inﬂuence on driving decisions. The continuity of a lane marking is essential for making a “driving-across” decision, so we labeled it independently as an important attribute. Similarly, the direction of a lane marking is also signiﬁcant for autonomous driving. For example, if a lane marking is parallel to the passing car, it may serve to guide cars and separate lanes; if it is perpendicular, it can be treated as a sign of deceleration or stop. The distribution of the number of annotations in varied driving scenes are shown in Figure 12a, Figure 12b, and Figure 12c. The detailed evaluation results for the lane marking benchmark are in Table 14. Drivable area detection is a new task, so we show results of a baseline method on the task here. First, the drivable area detection is converted to 3-way segmentation task (background, directly, and alternatively drivable) by ignoring the region ID. Then, we train DRN-D-22 model [35] on the 70,000 training images. We ﬁnd that after learning from the large-scale image dataset, the model learns to split the road according to the lanes and extrapolate the drivable area to unmarked space. The mIoU for directly and alternatively drivable areas is 79.4% and 63.3%. However, the same model can achieve much higher accuracy on road segmentation, which indicates that techniques beyond segmentation may be required to solve the drivable area problem.",
        "Table 11: Annotations of the BDD100K MOT dataset by category.",
        "Figure 13 shows GPS trajectories of example sequences. Our data presents diverse driving behaviors, like starting, stopping, turning and passing. The data is suitable to train and test imitation learning algorithms on real driving data.",
        "The model is also confused by the US highway trafﬁc sign. However, the same model trained on our dataset does not suffer these problems. Also, the model of Cityscapes may over-ﬁt the hood of the data collecting vehicle and produces erroneous segmentation for the lower part of the images.",
        "Figure 14 shows the distribution of number of instances observed in the segmentation annotations. BDD100K has a good coverage on rare categories (e.g. trailer, train) and large number of instances of common trafﬁc objects such as persons and cars. We also observe long-tail effects on our dataset. There are almost 60 thousand car instances, a few hundred rider and motorcycle instances, and mere dozens of trailer and train instances.",
        "RoadCurbDoubleWhiteDoubleYellowDoubleOtherSingleWhiteSingleYellowSingleOtherCross-walk100103106Instances1574018222534263735328828993394154108ParallelVertical100103106Instances562157193712FullDashed100103106Instances75586991626DrivableAlternative100103106Instances9162688392\fFigure 14: Distribution of classes in semantic instance segmentation. It presents a long-tail effect with more than 10 cars and poles per image, but only tens of trains in the whole dataset."
    ]
}