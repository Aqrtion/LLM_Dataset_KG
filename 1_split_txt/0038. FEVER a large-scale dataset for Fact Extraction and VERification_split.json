{
    "title_author_abstract_introduction": "FEVER: a large-scale dataset for Fact Extraction and VERiﬁcation\nJames Thorne1, Andreas Vlachos1, Christos Christodoulopoulos2, and Arpit Mittal2\n1Department of Computer Science, University of Shefﬁeld 2Amazon Research Cambridge {j.thorne, a.vlachos}@sheffield.ac.uk {chrchrs, mitarpit}@amazon.co.uk\nAbstract\nIn this paper we introduce a new publicly available dataset for veriﬁcation against textual sources, FEVER: Fact Extraction and VERiﬁcation. It consists of 185,445 claims generated by altering sentences extracted from Wikipedia and subsequently veriﬁed without knowledge of the sentence they were derived from. The claims are classiﬁed as SUPPORTED, REFUTED or NOTENOUGHINFO by annotators achieving 0.6841 in Fleiss κ. For the ﬁrst two classes, the annotators also recorded the sentence(s) forming the necessary evidence for their judgment. To characterize the challenge of the dataset presented, we develop a pipeline approach and compare it to suitably designed oracles. The best accuracy we achieve on labeling a claim accompanied by the correct evidence is 31.87%, while if we ignore the evidence we achieve 50.91%. Thus we believe that FEVER is a challenging testbed that will help stimulate progress on claim veriﬁcation against textual sources.\nIntroduction\nThe ever-increasing amounts of textual information available combined with the ease in sharing it through the web has increased the demand for veriﬁcation, also referred to as fact checking. While it has received a lot of attention in the context of journalism, veriﬁcation is important for other domains, e.g. information in scientiﬁc publications, product reviews, etc.\nIn this paper we focus on veriﬁcation of textual claims against textual sources. When compared to textual entailment (TE)/natural language inference (Dagan et al., 2009; Bowman et al., 2015),\nthe key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in veriﬁcation systems it is retrieved from a large set of documents in order to form the evidence. Another related task is question answering (QA), for which approaches have recently been extended to handle large-scale resources such as Wikipedia (Chen et al., 2017). However, questions typically providetheinformationneededtoidentifytheanswer, while information missing from a claim can often be crucial in retrieving refuting evidence. For example, a claim stating “Fiji’s largest island is Kauai.” can be refuted by retrieving “Kauai is the oldest Hawaiian Island.” as evidence.\nProgress on the aforementioned tasks has beneﬁted from the availability of large-scale datasets (Bowman et al., 2015; Rajpurkar et al., 2016). However, despite the rising interest in veriﬁcation and fact checking among researchers, the datasets currently used for this task are limited to a few hundred claims. Indicatively, the recently conductedFakeNewsChallenge (Pomerleau and Rao, 2017) with 50 participating teams used a dataset consisting of 300 claims veriﬁed against 2,595 associated news articles which is orders of magnitude smaller than those used for TE and QA.\nIn this paper we present a new dataset for claim veriﬁcation, FEVER: Fact Extraction and VERIt consists of 185,445 claims manuiﬁcation. ally veriﬁed against the introductory sections of Wikipedia pages and classiﬁed as SUPPORTED, REFUTED or NOTENOUGHINFO. For the ﬁrst two classes, systems and annotators need to also return the combination of sentences forming the necessary evidence supporting or refuting the claim (see Figure 1). The claims were generated by human annotators extracting claims from Wikipedia and mutating them in a variety of ways, some of which were meaning-altering. The veriﬁcation of each\nclaim was conducted in a separate annotation process by annotators who were aware of the page but not the sentence from which original claim was extracted and thus in 31.75% of the claims more than one sentence was considered appropriate evidence. Claims require composition of evidence from multiple sentences in 16.82% of cases. Furthermore, in 12.15% of the claims, this evidence was taken from multiple pages.\nTo ensure annotation consistency, we developed suitable guidelines and user interfaces, resulting in inter-annotator agreement of 0.6841 in Fleiss κ (Fleiss, 1971) in claim veriﬁcation classiﬁcation, and 95.42% precision and 72.36% recall in evidence retrieval.\nTo characterize the challenges posedby FEVER we develop a pipeline approach which, given a claim, ﬁrst identiﬁes relevant documents, then selects sentences forming the evidence from the documents and ﬁnally classiﬁes the claim w.r.t. evidence. The best performing version achieves 31.87% accuracy in veriﬁcation when requiring correct evidence to be retrieved for claims SUPPORTED or REFUTED, and 50.91% if the correctness of the evidence is ignored, both indicating the difﬁculty but also the feasibility of the task. We also conducted oracle experiments in which components of the pipeline were replaced by the gold standard annotations, and observed that the most challenging part of the task is selecting the sentences containing the evidence. In addition to publishing the data via our website1, we also publish theannotationinterfaces2 andthebaselinesystem3 to stimulate further research on veriﬁcation.",
    "data_related_paragraphs": [
        "Vlachos and Riedel (2014) constructed a dataset for claim veriﬁcation consisting of 106 claims, selecting data from fact-checking websites such as PolitiFact, taking advantage of the labelled claims available there. However, in order to develop claim veriﬁcation components we typically require the justiﬁcation for each verdict, including the sources used. While this information is usually available in justiﬁcations provided by the they are not in a machine-readable journalists, form. Thus, also considering the small number of claims, the task deﬁned by the dataset proposed",
        "remains too challenging for the ML/NLP methods currentlyavailable. Wang (2017)extended thisapproach by including all 12.8K claims available by Politifact via its API, however the justiﬁcation and the evidence contained in it was ignored in the experiments as it was not machine-readable. Instead, theclaimswereclassiﬁedconsideringonlythetext and the metadata related to the person making the claim. While this rendered the task amenable to current NLP/ML methods, it does not allow for veriﬁcation against any sources and no evidence needs to be returned to justify the verdicts.",
        "The Fake News challenge (Pomerleau and Rao, 2017) modelled veriﬁcation as stance classiﬁcation: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, combining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by journalists in the context of the Emergent Project (Silverman, 2015), and the dataset was ﬁrst proposed by Ferreira and Vlachos (2016), who only classiﬁed the claim w.r.t. the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009), the systems were provided with the sources to verify against, instead of having to retrieve them.",
        "A differently motivated but closely related dataset is the one developed by Angeli and Manning (2014) to evaluate natural logic inference for common sense reasoning, as it evaluated sim-",
        "Claim veriﬁcation is also related to the multilingual Answer Validation Exercise (Rodrigo et al., 2009) conducted in the context of the TREC shared tasks. Apart from the difference in dataset size (1,000 instances per language), the key difference is that the claims being validated were answers returned to questions by QA systems. The questions and the QA systems themselves provide additional context to the claim, while in our task deﬁnition the claims are outside any particular context. In the same vein, Kobayashi et al. (2017) collected a dataset of 412 statements in context from high-school student exams that were validated against Wikipedia and history textbooks.",
        "3 Fact extraction and veriﬁcation dataset",
        "The dataset was constructed in two stages:",
        "3.4 Data Validation",
        "Given the complexity of the second task (claim labeling), we conducted three forms of data validation: 5-way inter-annotator agreement, agreement against super-annotators (deﬁned in Section 3.4.2), and manual validation by the authors. The validation for claim generation was done im-",
        "3.4.2 Agreement against Super-Annotators We randomly selected 1% of the data to be annotated by super-annotators: expert annotators with no suggested time restrictions. The purpose of this exercise was to provide as much coverage of evidence as possible. We instructed the superannotators to search over the whole Wikipedia for every possible sentence that could be used as evidence. We compared the regular annotations against this set of evidence and the precision/recall was 95.42% and 72.36% respectively.",
        "A related issue is entity resolution. For a claim like “David Beckham was with United.”, it might be trivial for an annotator to accept “David Beckham made his European League debut playing for Manchester United.” as supporting evidence. This implicitly assumes that “United” refers to “Manchester United”, however there are many Uniteds in Wikipedia and not just football clubs, e.g. United Airlines. The annotators knew the page of the main entity and thus it was relatively easy to resolve ambiguous entities. While we provide this information as part of the dataset, we argue that it should only be used for system training/development.",
        "5.1 Dataset Statistics",
        "Table 1: Dataset split sizes for SUPPORTED, REFUTED and NOTENOUGHINFO (NEI) classes",
        "Predicting whether a claim is SUPPORTED, REFUTED or NOTENOUGHINFO is a 3-way classiﬁcation task that we evaluate using accuracy. In the case of the ﬁrst two classes, appropriate evidence must be provided, at a sentence-level, to justify the classiﬁcation. We consider an answer returned correct for the ﬁrst two classes only if correct evidence is returned. Given that the development and test datasets have balanced class distributions, a random baseline will have ∼ 33% accuracy if one ignores the requirement for evidence for SUPPORTED and REFUTED.",
        "The RTE component is trained on labeled claims paired with sentence-level. Where multiple sentences are required as evidence, the strings are concatenated. As discussed in Section 4, such data is not annotated for claims labeled NOTENOUGHINFO, thus we compare random samplingbased and similarity-based strategies for generating it. We evaluate classiﬁcation accuracy on the development set in an oracle evaluation, assuming correct evidence sentences are selected (Table 3). Additionally, for the DA model, we predict entailment given evidence, using the AllenNLP (Gardner et al., 2017) pre-trained Stanford Natural Language Inference (SNLI) model for comparison.",
        "5.9 Ablation of Training Data",
        "To evaluate whether the size of the dataset is suitable for training the RTE component of the pipeline, we plot the learning curves for the DA and MLP models (Fig. 3). For each model, we trained 5 models with different random initializations using the NEARESTP method (see Section 5.5). We selected the highest performing",
        "model when evaluated on development set and report the oracle RTE accuracy on the test set. We observe that with fewer than 6000 training instances, the accuracy of DA is unstable. However, with more data, its accuracy increases with respect to the log of the number of training instances and exceeds that of MLP. While both learning curves exhibit the typical diminishing return trends, they indicate that the dataset is large enough to demonstrate the differences of models with different learning capabilities.",
        "The pipeline presented and evaluated in the previous section is one possible approach to the task proposed in our dataset, but we envisage different ones to be equally valid and possibly better performing. For instance, it would be interesting to test how approaches similar to natural logic inference (Angeli and Manning, 2014) can be applied, where a knowledge base/graph is constructed by reading the textual sources and then a reasoning process over the claim is applied, possibly using recent advances in neural theorem proving (Rockt¨aschel and Riedel, 2017). A different approach could be to consider a combination of question generation (Heilman and Smith, 2010) followed by a question answering model such as BiDAF(Seoetal.,2016), possiblyrequiringmodiﬁcation as they are designed to select a single span of text from a document rather than return one or more sentences as per our scoring criteria. The sentence-level evidence annotation in our dataset will help develop models selecting and attending to the relevant information from multiple documents and non-contiguous passages. Not only will",
        "Another use case for the FEVER dataset is claim extraction: generating short concise textual facts from longer encyclopedic texts. For sources like Wikipedia or news articles, the sentences can contain multiple individual claims, making them not only difﬁcult to parse, but also hard to evaluate against evidence. During the construction on the FEVER dataset, we allowed for an extension of the task where simple claims can be extracted from multiple complex sentences.",
        "Finally, we would like to note that while we chose Wikipedia as our textual source, we do not consider it to be the only source of information worth considering in veriﬁcation, hence not using TRUE or FALSE in our classiﬁcation scheme. We expect systems developed on the dataset presented to be portable to different textual sources.",
        "In this paper we have introduced FEVER, a publicly available dataset for fact extraction and veriﬁcation against textual sources. We discussed the data collection and annotation methods and shared some of the insights obtained during the annotation process that we hope will be useful to other large-scale annotation efforts.",
        "In order to evaluate the challenge this dataset presents, we developed a pipeline approach that comprises information retrieval and textual entailment components. We showed that the task is challenging yet feasible, with the best performing system achieving an accuracy of 31.87%.",
        "We also discussed other uses for the FEVER dataset and presented some further extensions that we would like to work on in the future. We believe that FEVER will provide a stimulating challenge for claim extraction and veriﬁcation systems.",
        "The work reported was partly conducted while James Thorne was at Amazon Research Cambridge. Andreas Vlachos is supported by the EU H2020 SUMMA project (grant agreement number 688139). The authors would like to thank the following people for their advice and suggestions: David Hardcastle, Marie Hanabusa, Timothy Howd, Neil Lawrence, Benjamin Riedel, Craig Saunders and Iris Spik. The authors also wish to thank the team of annotators involved in preparing this dataset.",
        "William Ferreira and Andreas Vlachos. 2016. Emergent: a novel data-set for stance classiﬁcation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. San Diego, California, pages 1163–1168.",
        "Niket Tandon, Gerard de Melo, and Gerhard Weikum. 2011. Deriving a Web-scale common sense fact database. In Proceedings of the 25th AAAI Conference on Artiﬁcial Intelligence (AAAI 2011). AAAI Press, Palo Alto, CA, USA, pages 152–157.",
        "Andreas Vlachos and Sebastian Riedel. 2014. Fact checking: Task deﬁnition and dataset construction. In Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science. ACL. http://www.aclweb.org/anthology/W14- 2508.",
        "William Yang Wang. 2017. “Liar, Liar Pants on Fire”: A new benchmark dataset for fake news detection. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). http://aclweb.org/anthology/P17- 2067.",
        "Claim A claim is a single sentence expressing information (true or mutated) about a single aspect of one target entity. Using only the source sentence to generate claims will result in simple claims that are not challenging. But, allowing world knowledge to be incorporated is too unconstrained and will result in claims that cannot be evidenced by this dataset. We address this gap by introducing a dictionary that provides additional knowledge that can be used to increase the complexity of claims in a controlled manner.",
        "See Tables 7 and 8 for examples from the real data.",
        "The manual error analysis was conducted with the decision process described in Figure 8. For the cases of ﬁnding new evidence, recommended actions have been listed. While these were not performed for this version of the dataset, this may form a future update following a pool-based evaluation in the FEVER Shared Task."
    ]
}