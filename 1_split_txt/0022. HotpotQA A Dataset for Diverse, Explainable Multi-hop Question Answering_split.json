{
    "title_author_abstract_introduction": "HOTPOTQA: A Dataset for Diverse, Explainable Multi-hop Question Answering\nZhilin Yang*♠ Peng Qi*♥\nSaizheng Zhang*♣\nYoshua Bengio♣♦ William W. Cohen† Ruslan Salakhutdinov♠ Christopher D. Manning♥ ♠ Carnegie Mellon University ♥ Stanford University ♣ Mila, Universit´e de Montr´eal ♦ CIFAR Senior Fellow † Google AI {zhiliny, rsalakhu}@cs.cmu.edu, {pengqi, manning}@cs.stanford.edu saizheng.zhang@umontreal.ca, yoshua.bengio@gmail.com, wcohen@google.com\nAbstract\nExisting question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HOTPOTQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require ﬁnding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HOTPOTQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.\nIntroduction\nThe ability to perform reasoning and inference over natural language is an important aspect of intelligence. The task of question answering (QA) provides a quantiﬁable and objective way to test the reasoning ability of intelligent systems. To this end, a few large-scale QA datasets have been proposed, which sparked signiﬁcant progress in this direction. However, existing datasets have limitations that hinder further advancements of machine reasoning over natural language, especially in testing QA systems’ ability to perform multi-hop reasoning, where the system has to reason with information taken from more than one document to arrive at the answer.\n∗These authors contributed equally. The order of author-\nship is decided through dice rolling.\n†Work done when WWC was at CMU.\nParagraph A, Return to Olympus: [1] Return to Olympus is the only album by the alternative rock band Malfunkshun. [2] It was released after the band had broken up and after lead singer Andrew Wood (later of Mother Love Bone) had died of a drug overdose in 1990. [3] Stone Gossard, of Pearl Jam, had compiled the songs and released the album on his label, Loosegroove Records. Paragraph B, Mother Love Bone: [4] Mother Love Bone was an American rock band that formed in Seattle, Washington in 1987. [5] The band [6] Frontman Andrew was active from 1987 to 1990. Wood’s personality and compositions helped to catapult the group to the top of the burgeoning late 1980s/early 1990s Seattle music scene. [7] Wood died only days before the scheduled release of the band’s debut album, “Apple”, thus ending the group’s hopes of success. [8] The album was ﬁnally released a few months later. Q: What was the former band of the member of Mother Love Bone who died just before the release of “Apple”? A: Malfunkshun Supporting facts: 1, 2, 4, 6, 7\nFigure 1: An example of the multi-hop questions in HOTPOTQA. We also highlight the supporting facts in blue italics, which are also part of the dataset.\nFirst, some datasets mainly focus on testing the ability of reasoning within a single paragraph or document, or single-hop reasoning. For example, in SQuAD (Rajpurkar et al., 2016) questions are designed to be answered given a single paragraph as the context, and most of the questions can in fact be answered by matching the question with a single sentence in that paragraph. As a result, it hasfallenshortattestingsystems’abilitytoreason over a larger context. TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) create a more challenging setting by using information retrieval to collect multiple documents to form the context given existing question-answer pairs. Nevertheless, most of the questions can be answered by matching the question with a few nearby sentences in one single paragraph, which is limited as it does not require more complex reasoning (e.g.,\nover multiple paragraphs).",
    "data_related_paragraphs": [
        "2 Data Collection",
        "Second, existing datasets that target multi-hop reasoning, such as QAngaroo (Welbl et al., 2018) and COMPLEXWEBQUESTIONS (Talmor and Berant, 2018), are constructed using existing knowledge bases (KBs). As a result, these datasets are constrained by the schema of the KBs they use, and therefore the diversity of questions and answers is inherently limited.",
        "Third, all of the above datasets only provide distant supervision; i.e., the systems only know what the answer is, but do not know what supporting facts lead to it. This makes it difﬁcult for models to learn about the underlying reasoning process, as well as to make explainable predictions.",
        "To address the above challenges, we aim at creating a QA dataset that requires reasoning over multiple documents, and does so in natural language, without constraining itself to an existing knowledge base or knowledge schema. We also want it to provide the system with strong supervision about what text the answer is actually derived from, to help guide systems to perform meaningful and explainable reasoning.",
        "We present HOTPOTQA1, a large-scale dataset that satisﬁes these desiderata. HOTPOTQA is collected by crowdsourcing based on Wikipedia articles, where crowd workers are shown multiple supporting context documents and asked explicitly to come up with questions requiring reasoning about all of the documents. This ensures it covers multi-hop questions that are more natural, and are not designed with any pre-existing knowledge base schema in mind. Moreover, we also ask the crowd workers to provide the supporting facts they use to answer the question, which we also provide as part of the dataset (see Figure 1 for an example). We have carefully designed a data collection pipeline for HOTPOTQA, since the collection of high-quality multi-hop questions is nontrivial. We hope that this pipeline also sheds light on future work in this direction. Finally, we also collected a novel type of questions—comparison questions—as part of HOTPOTQA, in which we require systems to compare two entities on some shared properties to test their understanding of both language and common concepts such as numerical magnitude. We make HOTPOTQA publicly available at https://HotpotQA.github.io.",
        "The main goal of our work is to collect a diverse and explainable question answering dataset that requires multi-hop reasoning. One way to do so is to deﬁne reasoning chains based on a knowledge base (Welbl et al., 2018; Talmor and Berant, 2018). However, the resulting datasets are limited by the incompleteness of entity relations and the lack of diversity in the question types. Instead, in this work, we focus on text-based question answering in order to diversify the questions and answers. The overall setting is that given some context paragraphs (e.g., a few paragraphs, or the entire Web) and a question, a QA system answers the question by extracting a span of text from the context, similar to Rajpurkar et al. (2016). We additionally ensure that it is necessary to perform multi-hop reasoning to correctly answer the question.",
        "Algorithm 1 Overall data collection procedure",
        "To the best of our knowledge, text-based comparisonquestionsareanoveltypeofquestionsthat have not been considered by previous datasets. More importantly, answering these questions usually requires arithmetic comparison, such as comparing ages given birth dates, which presents a new challenge for future model development.",
        "The overall procedure of data collection is illus-",
        "We collected 112,779 valid examples in total on Amazon Mechanical Turk4 using the ParlAI interface (Miller et al., 2017) (see Appendix A).To isolate potential single-hop questions from the desired multi-hop ones, we ﬁrst split out a subset of data called train-easy. Speciﬁcally, we randomly sampled questions (∼3–10 per Turker) from top-contributing turkers, and categorized all",
        "Table 1: Data split. The splits train-easy, trainmedium, and train-hard are combined for training. The distractor and full wiki settings use different test sets so that the gold paragraphs in the full wiki test set remain unknown to any models.",
        "their questions into the train-easy set if an overwhelming percentage in the sample only required reasoning over one of the paragraphs. We sampled these turkers because they contributed more than 70% of our data. This train-easy set contains 18,089 mostly single-hop examples.",
        "After splitting out train-easy and train-medium, we are left with hard examples. As our ultimate goal is to solve multi-hop question answering, we focus on questions that the latest modeling techniques are not able to answer. Thus we constrain our dev and test sets to be hard examples. Specifically, we randomly divide the hard examples into four subsets, train-hard, dev, test-distractor, and test-fullwiki. Statistics about the data split can be found in Table 1. In Section 5, we will show that combining train-easy, train-medium, and trainhard to train models yields the best performance, so we use the combined set as our default training set. The two test sets test-distractor and testfullwiki are used in two different benchmark settings, which we introduce next.",
        "We also try to understand the model’s good performance on the train-medium split. Manual analysis shows that the ratio of multi-hop questions in train-medium is similar to that of the hard examples (93.3% in train-medium vs. 92.0% in dev), but one of the question types appears more frequently in train-medium compared to the hard splits (Type II: 32.0% in train-medium vs. 15.0% in dev, see Section 4 for the deﬁnition of Type II questions). These observations demonstrate that given enough training data, existing neural architectures can be trained to answer certain types and certain subsets of the multi-hop questions. However, train-medium remains challenging when not just the gold paragraphs are present—we show in Appendix C that the retrieval problem on these examplesareasdifﬁcultasthatontheirhardcousins.",
        "4 Dataset Analysis",
        "In this section, we analyze the types of questions, typesofanswers, andtypesofmulti-hopreasoning covered in the dataset.",
        "We visualize the distribution of question types in Figure 2, and label the ones shared among more than 250 questions. As is shown, our dataset covers a diverse variety of questions centered around entities, locations, events, dates, and numbers, as wellasyes/noquestionsdirectedatcomparingtwo entities (“Are both A and B ...?”), to name a few.",
        "Answer Types. We further sample 100 examples from the dataset, and present the types of answers in Table 2. As can be seen, HOTPOTQA covers a broad range of answer types, which matches our initial analysis of question types. We ﬁnd that a majority of the questions are about entities in the articles (68%), and a non-negligible amountofquestionsalsoaskaboutvariousproperties like date (9%) and other descriptive properties such as numbers (8%) and adjectives (4%).",
        "To test the performance of leading QA systems on our data, we reimplemented the architecture described in Clark and Gardner (2017) as our baseline model. We note that our implementation without weight averaging achieves performance very close to what the authors reported on SQuAD (about 1 point worse in F1). Our implemented model subsumes the latest techni-",
        "Table 7: Ablation study of question answering performance on the dev set in the distractor setting. “– sup fact” means removing strong supervision over supporting facts from our model. “– train-easy” and “– trainmedium” means discarding the according data splits from training. “gold only” and “sup fact only” refer to using the gold paragraphs or the supporting facts as the only context input to the model.",
        "model, whichachievesa10+F1 improvementover not using the supporting facts. Compared with the gain of strong supervision in our model (∼2 points in F1), our proposed method of incorporating supporting facts supervision is most likely suboptimal, and we leave the challenge of better modeling to future work. At last, we show that combining all data splits (train-easy, train-medium, and train-hard) yields the best performance, which is adopted as the default setting.",
        "To establish human performance on our dataset, we randomly sampled 1,000 examples from the dev and test sets, and had at least three additional Turkers provide answers and supporting facts for these examples. As a baseline, we treat the original Turker during data collection as the prediction, and the newly collected answers and supporting facts as references, to evaluate human performance. For each example, we choose the answer and supporting fact reference that maximize the F1 score to report the ﬁnal metrics to reduce the effect of ambiguity (Rajpurkar et al., 2016).",
        "KB-based multi-hop datasets. Recent datasets like QAngaroo (Welbl et al., 2018) and COMPLEXWEBQUESTIONS (TalmorandBerant,2018) explore different approaches of using pre-existing knowledgebases(KB)withpre-deﬁnedlogicrules to generate valid QA pairs, to test QA models’ capability of performing multi-hop reasoning. The diversity of questions and answers is largely limited by the ﬁxed KB schemas or logical forms. Furthermore, some of the questions might be answerable by one text sentence due to the incompleteness of KBs.",
        "Free-form answer-generation datasets. MS MARCO (Nguyen et al., 2016) contains 100k user queries from Bing Search with human generated answers. Systems generate free-form answers and are evaluated by automatic metrics such as ROUGE-L and BLEU-1. However, the reliability of these metrics is questionable because they have been shown to correlate poorly with human judgement (Novikova et al., 2017).",
        "We present HOTPOTQA, a large-scale question answering dataset aimed at facilitating the development of QA systems capable of performing explainable, multi-hop reasoning over diverse natural language. We also offer a new type of factoid comparison questions to test systems’ ability to extract and compare various entity properties in text.",
        "Various recently-proposed large-scale QA datasets can be categorized in four categories.",
        "Single-document datasets. SQuAD (Rajpurkar et al., 2016, 2018) questions that are relatively simple because they usually require no more than one sentence in the paragraph to answer.",
        "Multi-document datasets. TriviaQA (Joshi et al., 2017) and SearchQA (Dunn et al., 2017) contain question answer pairs that are accompanied with more than one document as the context. This further challenges QA systems’ ability to accommodate longer contexts. However, since the",
        "Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association of Computational Linguistics.",
        "Matthew Dunn, Levent Sagun, Mike Higgins, Ugur Guney, Volkan Cirik, and Kyunghyun Cho. 2017. SearchQA: A new Q&A dataset augmented with arXiv preprint context from a search engine. arXiv:1704.05179.",
        "Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comIn Proceedings of the 55th Annual prehension. Meeting of the Association for Computational Linguistics.",
        "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine In Proceedings of reading comprehension dataset. the 30th Annual Conference on Neural Information Processing Systems (NIPS).",
        "A Data Collection Details",
        "A.1 Data Preprocessing",
        "A.2 Further Data Collection Details",
        "Bonus Structures. To incentivize crowd workers to produce higher-quality data more efﬁciently, we follow Yang et al. (2018), and employ bonus structures. We mix two settings in our data collection process. In the ﬁrst setting, we reward the top (in terms of numbers of examples) workers every 200 examples. In the second setting, the workers get bonuses based on their productivity (measured as the number of examples per hour).",
        "Our crowd worker interface is based on ParlAI (Miller et al., 2017), an open-source project that facilitates the development of dialog systems and data collection with a dialog interface. We adapt ParlAI for collecting question answer pairs by converting the collection workﬂow into a systemoriented dialog. This allows us to have more control over the turkers input, as well as provide turkers with in-the-loop feedbacks or helpful hints to help Turkers ﬁnish the task, and therefore speed up the collection process.",
        "interface during data collection.",
        "B Further Data Analysis",
        "To further look into the diversity of the data in HOTPOTQA, we further visualized the distribution of question lengths in the dataset in Figure 5. Besides being diverse in terms of types as is show in the main text, questions also vary greatly in length, indicating different levels of complexity and details covered."
    ]
}