{
    "title_author_abstract_introduction": "2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops\nNTIRE 2017 Challenge on Single Image Super-Resolution: Dataset and Study\nEirikur Agustsson CVL, ETH Zurich, Switzerland aeirikur@vision.ee.ethz.ch\nRadu Timofte CVL, ETH Zurich & Merantix GmbH radu.timofte@vision.ee.ethz.ch\nAbstract\nThis paper introduces a novel large dataset for examplebased single image super-resolution and studies the stateof-the-art as emerged from the NTIRE 2017 challenge. The challenge is the ﬁrst challenge of its kind, with 6 competitions, hundreds of participants and tens of proposed solutions. Our newly collected DIVerse 2K resolution image dataset (DIV2K) was employed by the challenge. In our study we compare the solutions from the challenge to a set of representative methods from the literature and evaluate them using diverse measures on our proposed DIV2K dataset. Moreover, we conduct a number of experiments and draw conclusions on several topics of interest. We conclude that the NTIRE 2017 challenge pushes the state-ofthe-art in single-image super-resolution, reaching the best results to date on the popular Set5, Set14, B100, Urban100 datasets and on our newly proposed DIV2K.\n1. Introduction\nExample-based single image super-resolution (SR) aims at full restoration of rich details (high frequencies) in images based on prior examples under the form of low resolution (LR) and corresponding high resolution (HR) images. The loss of details / contents can be due to various degrading factors such as blur, decimation, noise or hardware limitations (e.g. camera sensors). SR is an ill-posed problem because for each LR image patch the number of corresponding HR image patches can be very large.\nSingle image super-resolution as well as image restoration researchliteraturespansoverdecades[36,20,4, 13,16, 3, 15, 14, 6, 32, 54, 30, 17, 23, 12, 47, 48, 10, 21]. Nonetheless, the recent years showed tremendous progress as shown in Fig. 1. The performance of the top methods have continuously improved [54, 48, 21, 26] as the ﬁeld has reached maturity.\nThere is a continuous need for standardized SR benchmarks to allow for comparison of different proposed methods under the same conditions. Most of the recent SR works adopted a couple of datasets like the 91 train images\nUrban100\nSNU CVPRW17 [46] HelloSR CVPRW17 [46] Lab402 CVPRW17 [46] SRResNet CVPR17 [26] DRRN CVPR17 [44] DRCN CVPR16 [22] VDSR CVPR16 [21] FSRCNN ECCV16 [11] ESPCN CVPR16 [42] CSCN-MV ICCV16 [51] SRCNN PAMI16 [10] WSDSR arxiv17 [7] IA CVPR16 [49] PSyCo CVPR16 [33] ARFL+ CVPR15 [39] SelfEx CVPR15 [19] A+ ACCV14 [48] bicubic PSNR (dB) :\nvery deep\nshallow\nFigure 1. Representative methods from the recent years and their average PSNR performance on ﬁve datasets for scale ×4.\nproposed by Yang et al. [54] and the validation datasets Set5 [5], Set14 [56], B100 [31, 48] brought together by Timofte et al. [47, 48], or the more recent Urban100 [19]. The bicubic downscaling (imresize from Matlab) is the most used degradation operator to simulate the HR to LR transformation.\nIn this work, we propose a novel DIV2K dataset with DIVerse 2K resolution high quality images collected from Internet. It has 1000 images with considerable higher resolution than the popular datasets mentioned before. Moreover, we organized the ﬁrst example-based single image superresolution online challenge which used the DIV2K dataset. The NTIRE 2017 SR challenge 1 [46] employs two types of degradations: the standard bicubic and the unknown downscaling operators aka downscaling operators known only through train data of LR and corresponding HR images.\nAnother contributionof thispaper isa study of our newly proposed DIV2K in relation with the achieved performance by the winners of the NTIRE 2017 SR Challenge and representative methods from recent years. We report results using a selection of image quality measures and investigate correlations and limits in SR benchmarking.\nThe remainder of the paper is structured as follows. Section 2 introduces the DIV2K dataset. Section 3 reviews the NTIRE 2017 SR Challenge and its settings. Section 4 intro-\n1http://www.vision.ee.ethz.ch/ntire17/",
    "data_related_paragraphs": [
        "duces the image quality assessment (IQA) measures, Section 5 - the datasets, and Section 6 - the methods from our study. Section 7 discusses the experiments and interprets the achieved results, while Section 8 concludes the paper.",
        "2. Proposed DIV2K dataset",
        "We propose the DIV2K dataset 2, a novel DIVerse 2K resolution image dataset for benchmarking example-based single image super-resolution (see Fig. 2. DIV2K is intended to complement the existing SR datasets (see Fig. 5) and to further increase the (content) diversity. Source: We manually crawled 1000 color RGB images from Internet paying special attention to the image quality, to the diversity of sources (sites and cameras), to the image contentsandtothecopyrights. DIV2Kismeantforresearch purposes. Resolution and quality: All the 1000 images are 2K resolution, that is they have 2K pixels on at least one of the axes (vertical or horizontal). All the images were processed using the same tools. For simplicity, since the most common magniﬁcation factors in the recent SR literature are of ×2, ×3 and ×4 we cropped the images to multiple of 12 pixels on both axes. Most of the crawled images were originally above 20M pixels. The images are of high quality both aesthetically and in the terms of small amounts of noise and other corruptions (like blur and color shifts). Diversity: We collected our images from dozens of sites. A preference was made for sites with freely shared high quality photography (such as https://www.pexels. com/ ). Note that we did not use images from Flickr, Instagram, or other legally binding or copyright restricted images. We only seldom used keywords to assure the diversity for our dataset. DIV2K covers a large diversity of contents, ranging from people, handmade objects and environments",
        "2https://data.vision.ee.ethz.ch/cvl/DIV2K/",
        "(cities, villages), to ﬂora and fauna, and natural sceneries including underwater and dim light conditions. Partitions: After collecting the DIV2K 1000 images we computed image entropy, bit per pixel (bpp) PNG compressionratesand CORNIAscores(seeSection7.6)and applied bicubic downscaling ×3 and then upscaling ×3 with bicubic interpolation (imresize Matlab function), ANR [47] and A+ [48] methods and default settings. We randomly generated partitions of 800 train, 100 validation and 100 test images until we achieved a good balance ﬁrstly in visual contents and then on the average entropy, average bpp, average number of pixels per image (ppi), average CORNIA quality scores and also in the relative differences between the average PSNR scores of bicubic, ANR and A+ methods. Table 1 summarizes the main characteristics of DIV2K validation and test partitions in comparison with the most popular SR datasets. Fig. 2 visualizes the 100 images for validation and the 100 images for testing of the DIV2K dataset.",
        "The NTIRE 2017 challenge on example-based single image super-resolution [46] was the ﬁrst of its kind and had as objectives: to gauge the state-of-the-art in SR, to facilitate comparison of different solutions on a novel large dataset - DIV2K, and to propose more challenging SR settings. Tracks and competitions The challenge had two tracks: Track 1 for bicubic downscaling (‘classic’) and Track 2 for Unknown downscaling. For Track 1 the degradation is the popular bicubic downscaling (Matlab imresize function) and facilitates easy deployment of the recent solutions that assumed this degradation. Track 2 is more challenging as it uses a combination of blur and decimation ‘unknown’ under explicit form to the challenge participants, but known through exemplars of LR and corresponding HR images. Each track corresponds to 3 competitions for the",
        "usual 3 downscaling factors (×2,×3,×4). A visualization of the tracks and competitions is shown in Fig. 3. The hosting platform for the competitions is CodaLab 3. For each competition the LR and HR train images (from the DIV2K train set) were provided for learning models during the development (training) phase. The following validation phase gave the opportunity to the participants to test their solutions on the LR validation images (from DIV2K validation set) and compare their scores through an online validation server and associated leaderboard. The ﬁnal evaluation (test) phase provided the LR test images (from DIV2K testing set) and invited the submission of the HR results before the challenge deadline. PSNR and SSIM (see Section 4) are the challenge main quantitative quality assessment measures for the image restoration results. A 6 + scalefactor pixels image boundary is ignored in the evaluation. Challenge results Each competition had on average 100 registered participants and 20 teams submitted results, code/executables and factsheets for the ﬁnal test phase. All these competing solutions and the achieved results on the DIV2K test data are described in the NTIRE 2017 SR challenge report [46]. All the proposed challenge solutions, except WSDSR [7], employ end-to-end deep learning of convolutional neural networks (CNN) [25] and use GPU for both training and testing. They propose a diversity of ideas and design details and generally build upon and go beyond theveryrecentproposedSRworks[10,49,21,26]. InFig.4 we plot the average PSNR vs. runtime results of the challenge solutions in comparison with several other representative methods and in Table 2 we show results for a selection of them. The top challenge solutions are consistent across all 6 competitions, showing that the solutions proposed for Track 1 with bicubic downscaling generalize well to Track 2 unknown downscaling if sufﬁcient training examples are provided. The PSNR and the SSIM scores correlate well. The scores on Track 2 are generally worse than on Track 1 for the same methods/solutions and reﬂects the increased difﬁculty of the unknown downscaling setup.",
        "All the above selected measures were not intended for measuring the quality of a super-resolved image. However, they tend to generalize well for different kinds of image distortions. In particular, IFC was shown to have a strong correlation with the human assessed perceptual quality for image super-resolution results [53]. While working without a reference image, CORNIA proved superior to many full reference measures in assessing perceptual quality. Very recent CORNIA was shown to achieve high correlation to the human perception also for the image super-resolution task onalargedatasetwithandwithoutretrainingitsmodel[29]. WeusetheCORNIAwiththeoriginalmodelandthedefault settings.",
        "Figure 4. Runtime vs. PSNR results on NTIRE 2017 Challenge tracks, DIV2K test data.",
        "Dataset Set5 Set14 B100 Urban100 DIV2K validation DIV2K test DIV2K test ⇓ 2 bic. DIV2K test ⇓ 4 bic. DIV2K test ⇓ 8 bic. DIV2K test ⇓ 16 bic. DIV2K test ⇓ 2 crop DIV2K test ⇓ 4 crop DIV2K test ⇓ 8 crop DIV2K test ⇓ 16 crop",
        "Table 1. Main characteristics of the SR datasets. We report average and standard deviation.",
        "5. Datasets",
        "In this study we use the most common datasets from SR literature (shown in Fig. 5). We mention also LIVE1 [41, 53], L20 [49, 27], ImageNet [38], Kodak 7 or SuperTex136 [8] that are less popular for single-image SR. Train91 was proposed by Yang et al. [54] for training. It has 91 RGB images with mainly small sized ﬂower images. Set5 was used in [5] and adopted under the name ‘Set5’ in [47]. It contains ﬁve popular images: one medium size image (‘baby’, 512 × 512) and four smaller ones (‘bird’, ‘butterﬂy’,‘head’, ‘women’). Set14 was proposed by Zeyde et al. [56]. It contains 14 commonly used images in the image processing literature. The images in Set14 are larger and more diverse than those in Set5. B100 represents the set of 100 testing images from the Berkeley Segmentation Dataset [31] as adopted in [48]. It covers a large variety of real-life scenes. Urban100 was introduced by Huang et al. [19]. It consists from 100 clean from urban environments with repetitive patterns and high self-similarity. DIV2K is our proposed dataset as introduced in Section 2 and is used for the NTIRE 2017 SR Challenge.",
        "In Table 1 we summarize main characteristics of the SR datasets. According to the perceptual image quality assessed by CORNIA all the datasets have good image quality, Urban100 and DIV2K being at the top. The average image size (pixels per image or ppi) varies from 113491",
        "Set14 Figure 5. Visualization of standard popular SR datasets: Set5, Set14, B100, and Urban100.",
        "pixels for Set5 to 2.8million pixels for DIV2K. DIV2K images are about 4 times larger than those from Urban100. In terms of entropy computed over the grayscale images the datasets are comparable, but Set14 has the lowest entropy. Also, the datasets are comparable in terms of bits per pixel (bpp) required by PNG for lossless compressing the images. Both bpp and image entropy are indicators of the amount of information present in the image per image pixel. Since CORNIA score, bpp, and entropy are comparable for all the datasets, the differences are made by the number and the resolution of the images. Thus, as intended, DIV2K has the best diversity in semantic contents (similar with B100) and the highest resolution (∼ 4× more than Urban100).",
        "SNU CVLab1 of Lim et al. [28] is the winner of the NTIRE 2017 challenge. It builds onto SRResNet architecture [26]. The building block removes the batch normalization layers from the residual block (ResBlock) in [18] and a residual scaling layer (constant multiplication with 0.1) is added after the 2nd convolutional layer of the block. The method has 36 ResBlocks end-to-end trained on the DIV2K train data, and for the Track 2 of the challenge used also crawled Flickr images to generate additional train data besides the DIV2K train data. SNU CVLab2 isacompactlydesignedapproachmeantfor HR estimation at multiple scales simultaneously. Most of the implementation and architecture design is shared with the SNU CVLab1 single-scale solution. SNU CVLab2 trades the brute performance of SNU CVLab1 for increased efﬁciency at train and runtime. HelloSR is a winner of NTIRE 2017 challenge based on a stacked residual-reﬁned network design. Lab402 proposed by Bae et al. [2] is the third winner of NTIRE 2017 challenge. Lab402 solution consists from a 41 layers Haar wavelet residual network.",
        "We use a selection of methods from [48, 49, 33], the seminal CNN methods [9, 21, 26], and additionally a selfsimilarity based method [19]. Bicubic interpolation is probably the most employed technique for interpolation of images in practice and often a basic component in more involved SR methods (such as A+ [48] and VDSR [21]). Each pixel in the upscaled image is a bicubic interpolation over a support LR patch of pixels. Yang of Yang et al. [54] employs sparse coding and sparse dictionaries for compact modeling of the LR-HR train examples and sharp HR reconstruction. Zeyde method of Zeyde et al. [56] builds upon Yang and efﬁciently learns sparse dictionaries using K-SVD [1] and Orthogonal Matching Pursuit for sparse solutions. ANR (Anchored Neighborhood Regression) of Timofte et al. [47] relaxes the sparse decomposition from Yang and Zeyde to a ridge regression solved ofﬂine and stored per each dictionary anchor for large speed beneﬁts. A+ or Adjusted ANR of Timofte et al. [48] improves over ANR by learning regressors from all the training patches in the local neighborhood of the anchor. IA is the Improved A+ method [49] which uses a couple of proposed techniques such as: data augmentation, enhanced prediction, cascading, hierarchical search with larger dictionaries, and context reasoning. SRCNN of Dong et al. [9, 10] directly learns to map patches from LR to HR images with a deep CNN model [25]. VDSR is a VGG16 architecture [43] based CNN model proposed by Kim et al. [21]. In comparison with SRCNN it goes ‘very deep’ with the convolutional layers and signiﬁ- cantly boosts the achieved performance. PSyCo proposedbyPerezetal.[33]buildsuponA+framework and efﬁciently reduces the manifold span to better use the train data and improve the model capacity.",
        "MostrecentSRworksvalidatetheirmethodsfor 3downscaling factors. The Pearson correlation is above 0.97 (ρ > 0.97) for the PSNR scores on the DIV2K test data of any two competitions (scaling factors) of Track 1 bicubic of the NTIRE 2017 SR Challenge [46]. The SSIM scores reported for ×2 and ×3 have ρ = 0.99, while between ×2 and ×4 ρ = 0.88. It is clear that validating on all these scaling factors for the same degradation operator (bicubic downscaling) is redundant and perhaps the efforts should be placed elsewhere, on better IQA measures and more challenging SR setups. The PSNR differences between the results of different challenge methods are larger for the smallest scale factor ×2, while for SSIM the differences get larger for higher scales. The higher scales are the more challenging ones and the perceptual differences are also potentially larger between different methods. If we analyze the reported results on the Set5, Set14, B100, Urban100 we come to the same conclusion that for low scaling factors (×2) both PSNR and SSIM scores of the recent methods are rather large and difﬁcult to assess by the human perception. Therefore we recommend to work with ×4 and to push further to ×8 and above for extreme SR validation and benchmarking. Already a couple of SR works [53, 26] report on ×8 settings.",
        "The top ranked entries SNU CVLab1, SNU CVLab2, HelloSR, UIUC-IFP and ‘I hate mosaic’ in the NTIRE 2017 SR Challenge use ensembles of HR predictions. They are ﬂipping and/or rotating by 90◦ the LR input image then process them to achieve HR corresponding results and align these results back for the ﬁnal HR average result. This enhanced prediction is one of the seven ways to improve SR described in [49]. In our experiments the top methods without the enhanced prediction achieve 0.1 to 0.25dB lower PSNR results on DIV2K test data. Since the unknown downscaling depends on the image orientation, SNU CVLab1, SNU CVLab2, and HelloSR propose",
        "to train different models eventually with different losses and to average their predictions. In this case the improvements are only marginal on the DIV2K dataset.",
        "NTIRE 2017 SR Challenge works on RGB images and uses the RGB color space for evaluation. We computed also the performance results on the luma component Y from the YCbCr color space after converting the RGB images. Some results are in Table 2. There is a very strong correlation (Pearson correlation ρ > 0.96) between the PSNR results computedonYlumaandthoseonRGBfortheNTIRE2017 SR Challenge methods on each DIV2K validation and testing datasets. However, the correlation is weaker for SSIM which is closer correlated to the human perception. We conclude that if for reconstruction ﬁdelity (measured by MSE or PSNR) reporting on Y is comparable with reporting on RGB,whenevertheperceptualqualityistargeted(measured by full-reference SSIM, IFC or no-reference CORNIA) for color images the RGB space could be better suited.",
        "WSDSR [7] is the slowest and the best performing among the non-CNN methods on bicubic downscaling settings. The best performing methods are also among the slowest ones. Fig. 1 reports a timeline performance (×4, bicubic downscaling, PSNR on Y channel) on ﬁve datasets for representative top methods. NTIRE 2017 challenge methods largely improve over prior art.",
        "How critical for a SR evaluation dataset is the image resolution in terms of ppi? To answer this we conduct a couple of experiments by reducing the HR images of the DIV2K test and then evaluating the best NTIRE 2017 challenge methods. In Fig. 7 we depict the two ways we use to reduce the ppi of the images: (i) by bicubic downscaling with a factor and thus preserving mainly much of the low frequencies and (ii) by centered cropping of a subimage corresponding to a downscaling factor. In Table 1 we show the main characteristics of the DIV2K test datasets derived for the 2, 4, 8, and 16 downscaling factors. In Fig. 8 we report the IQA scores of the selected methods for ×4 magniﬁcation versus the factors used to reduce the images of the DIV2K test dataset. Surprisingly, there is little to no effect of the test image resolution (ppi) on the relative performance achieved by the methods, the ranking is preserved and this regardless the IQA measures used. We note also the clear drop in performance when using bicubic downscaling and the relatively smaller drop when using the centered cropping strategy (or uniform sampling). As our experiments shows, given sufﬁciently large number of diverse images in the dataset, the image resolution is, in comparison, less important for benchmarking SR methods. We could easily use DIV2K ⇓ 8 and still achieve meaningful benchmarking results.",
        "In this paper we introduced DIV2K, a new dataset for super-resolution benchmarking, studied the winning solutions from the NTIRE 2017 SR challenge in comparison with representative methods from the recent literature, and investigated topics of interest such as predictors for superresolution performance, image resolution and quality, image quality assessment measures, magniﬁcation factors for benchmarking and challenges.",
        "[31] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological",
        "[34] N. Ponomarenko, L. Jin, O. Ieremeiev, V. Lukin, K. Egiazarian, J. Astola, B. Vozel, K. Chehdi, M. Carli, F. Battisti, and C.-C. J. Kuo. Image database tid2013: Peculiarities, results andperspectives. SignalProcessing: ImageCommunication, 30:57 – 77, 2015."
    ]
}