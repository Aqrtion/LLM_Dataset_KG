{
    "title_author_abstract_introduction": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension\nMandar Joshi†\nEunsol Choi†\nDaniel S. Weld†\nLuke Zettlemoyer†‡\n† Paul G. Allen School of Computer Science & Engineering, Univ. of Washington, Seattle, WA {mandar90, eunsol, weld, lsz}@cs.washington.edu\n‡ Allen Institute for Artiﬁcial Intelligence, Seattle, WA lukez@allenai.org\nAbstract\nWe present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K questionanswer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to ﬁnd answers. We also present two baseline algorithms: a featurebased classiﬁer and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23% and 40% vs. 80%), suggesting that TriviaQA is a challenging testbed that is worth signiﬁcant future study.1\nIntroduction\nReading comprehension (RC) systems aim to answer any question that could be posed against the facts in some reference text. This goal is challenging for a number of reasons: (1) the questions can be complex (e.g. have highly compositional semantics), (2) ﬁnding the correct answer can require complex reasoning (e.g. combining facts from multiple sentences or background knowledge) and (3) individual facts can be difﬁcult to\nand code washington.edu/triviaqa/\navailable\nat http://nlp.cs.\nQuestion: The Dodecanese Campaign of WWII that was an attempt by the Allied forces to capture islands in the Aegean Sea was the inspiration for which acclaimed 1961 commando ﬁlm? Answer: The Guns of Navarone Excerpt: The Dodecanese Campaign of World War II was an attempt by Allied forces to capture the Italianheld Dodecanese islands in the Aegean Sea following the surrender of Italy in September 1943, and use them as bases against the German-controlled Balkans. The failed campaign, and in particular the Battle of Leros, inspired the 1957 novel The Guns of Navarone and the successful 1961 movie of the same name.\nQuestion: American Callan Pinckney’s eponymously named system became a best-selling (1980s-2000s) book/video franchise in what genre? Answer: Fitness Excerpt: Callan Pinckney was an American ﬁtness professional. She achieved unprecedented success with her Callanetics exercises. Her 9 books all became international best-sellers and the video series that followed went on to sell over 6 million copies. Pinckney’s ﬁrst video release ”Callanetics: 10 Years Younger In 10 Hours” outsold every other ﬁtness video in the US.\nFigure 1: Question-answer pairs with sample excerpts from evidence documents from TriviaQA exhibiting lexical and syntactic variability, and requiring reasoning from multiple sentences.\nrecover from text (e.g. due to lexical and syntactic variation). Figure 1 shows examples of all these phenomena. This paper presents TriviaQA, a new reading comprehension dataset designed to simultaneously test all of these challenges.\nRecently, signiﬁcant progress has been made by introducing large new reading comprehension datasets that primarily focus on one of the challenges listed above, for example by crowdsourcing the gathering of question answer pairs (Rajpurkar et al., 2016) or using cloze-style sentences instead of questions (Hermann et al., 2015; Onishi et al., 2016) (see Table 1 for more examples). In general, system performance has improved rapidly as each resource is released. The best models of-\nDataset\nTriviaQA\nSQuAD (Rajpurkar et al., 2016) MS Marco (Nguyen et al., 2016) NewsQA(Trischler et al., 2016) WikiQA (Yang et al., 2016) TREC (Voorhees and Tice, 2000)\nLarge scale\n(cid:51)\n(cid:51) (cid:51) (cid:51) (cid:55) (cid:55)\nFreeform Answer (cid:51)\n(cid:51) (cid:51) (cid:51) (cid:55) (cid:51)\nWell formed\n(cid:51)\n(cid:51) (cid:55) (cid:51) (cid:55) (cid:51)\nIndependent of Evidence (cid:51)\nVaried Evidence (cid:51)\n(cid:55) (cid:51) (cid:55)* (cid:51) (cid:51)\n(cid:55) (cid:51) (cid:55) (cid:55) (cid:51)\nTable 1: Comparison of TriviaQA with existing QA datasets. Our dataset is unique in that it is naturally occurring, well-formed questions collected independent of the evidences. *NewsQA uses evidence articles indirectly by using only article summaries.\nten achieve near-human performance levels within months or a year, fueling a continual need to build ever more difﬁcult datasets. We argue that TriviaQA is such a dataset, by demonstrating that a high percentage of its questions require solving these challenges and showing that there is a large gap between state-of-the-art methods and human performance levels.\nTriviaQA contains over 650K question-answerevidence triples, that are derived by combining 95K Trivia enthusiast authored question-answer pairs with on average six supporting evidence documents per question. To our knowledge, TriviaQA is the ﬁrst dataset where full-sentence questions are authored organically (i.e. independently of an NLP task) and evidence documents are collected retrospectively from Wikipedia and the Web. This decoupling of question generation from evidence collection allows us to control for potential bias in question style or content, while offering organically generated questions from various topics. Designed to engage humans, TriviaQA presents a new challenge for RC models. They should be able to deal with large amount of text from various sources such as news articles, encyclopedic entries and blog articles, and should handle inference over multiple sentences. For example, our dataset contains three times as many questions that require inference over multiple sentences than the recently released SQuAD (Rajpurkar et al., 2016) dataset. Section 4 present a more detailed discussion of these challenges.\nFinally, we present baseline experiments on the TriviaQA dataset, including a linear classiﬁer inspired by work on CNN Dailymail and MCTest (Chen et al., 2016; Richardson et al., 2013) and a state-of-the-art neural network baseline (Seo et al., 2017). The neural model performs best, but only achieves 40% for TriviaQA in comparison to 68%\non SQuAD, perhaps due to the challenges listed above. The baseline results also fall far short of human performance levels, 79.7%, suggesting signiﬁcant room for the future work. In summary, we make the following contributions.\n• We collect over 650K question-answerevidence triples, with questions originating from trivia enthusiasts independent of the evidence documents. A high percentage of the questions are challenging, with substantial syntactic and lexical variability and often requiring multi-sentence reasoning. The dataset and code are available at http://nlp.cs.washington. edu/triviaqa/, offering resources for training new reading-comprehension models.\n• We present a manual analysis quantifying the quality of the dataset and the challenges involved in solving the task.\n• We present experiments with two baseline methods, demonstrating that the TriviaQA tasks are not easily solved and are worthy of future study.\n• In addition to the automatically gathered large-scale (but noisy) dataset, we present a clean, human-annotated subset of 1975 question-document-answer triples whose documents are certiﬁed to contain all facts required to answer the questions.",
    "data_related_paragraphs": [
        "Problem Formulation We frame reading comprehension as the problem of answering a question q given the textual evidence provided by document set D. We assume access to a dataset of tuples {(qi,ai,Di)|i = 1...n} where ai is a text string that deﬁnes the correct answer",
        "Data and Distant Supervision Our evidence documents are automatically gathered from either Wikipedia or more general Web search results (details in Section 3). Because we gather evidence using an automated process, the documents are not guaranteed to contain all facts needed to answer the question. Therefore, they are best seen as a source of distant supervision, based on the assumption that the presence of the answer string in an evidence document implies that the document does answer the question.3 Section 4 shows that this assumption is valid over 75% of the time, making evidence documents a strong source of distant supervision for training machine reading systems.",
        "In particular, we consider two types of distant supervision, depending on the source of our documents. For web search results, we expect the documents that contain the correct answer a to be highly redundant, and therefore let each questionanswer-document tuple be an independent data point. (|Di| = 1 for all i and qi = qj for many i,j pairs). However, in Wikipedia we generally expect most facts to be stated only once, so we instead pool all of the evidence documents and never repeat the same question in the dataset (|Di| = 1.8 on average and qi (cid:54)= qj for all i,j). In other words, each question (paired with the union of all of its evidence documents) is a single data point.",
        "These are far from the only assumptions that could be made in this distant supervision setup. For example, our data would also support multiinstance learning, which makes the at least once assumption, from relation extraction (Riedel et al., 2010; Hoffmann et al., 2011) or many other possibilities. However, the experiments in Section 6 show that these assumptions do present a strong",
        "2The data we will present in Section 3 would further support a task formulation where some documents D do not have the correct answer and the model must learn when to abstain. We leave this to future work.",
        "Table 2: TriviaQA: Dataset statistics.",
        "signal for learning; we believe the data will fuel signiﬁcant future study.",
        "3 Dataset Collection",
        "We collected a large dataset to support the reading comprehension task described above. First we gathered question-answer pairs from 14 trivia and quiz-league websites. We removed questions with less than four tokens, since these were generally either too simple or too vague.",
        "ing a single (combined) evidence document, and (2) 78K examples for the Wikipedia reading comprehension domain, containing on average 1.8 evidence documents per example. Table 2 contains the dataset statistics. While not the focus of this paper, we have also released the full unﬁltered dataset which contains 110,495 QA pairs and 740K evidence documents to support research in allied problems such as open domain and IRstyle question answering.",
        "4 Dataset Analysis",
        "Challenging problem A comparison of evidence with respect to the questions shows a high proportion of questions require reasoning over multiple sentences. To compare our datasetagainstpreviousdatasets, weclassiﬁed100 question-evidence pairs each from Wikipedia and the Web according to the form of reasoning required to answer them. We focus the analysis on Wikipedia since the analysis on Web documents are similar. Categories are not mutually exclusive: single example can fall into multiple categories. A summary of the analysis is presented in Table 5.",
        "To quantify the difﬁculty level of the dataset for current methods, we present results on neural and other models. We used a random entity baseline and a simple classiﬁer inspired from previous work (Wang et al., 2015; Chen et al., 2016), and compare these to BiDAF (Seo et al., 2017), one of the best performing models for the SQuAD dataset.",
        "Authored independently from the evidence document, TriviaQA does not contain the exact spans of the answers. We approximate the answer span by ﬁnding the ﬁrst match of answer string in the evidence document. Developed for a dataset wheretheevidencedocumentisasingleparagraph (average 122 words), the BiDAF model does not scale to long documents. To overcome this, we truncate the evidence document to the ﬁrst 800 words.8",
        "When the data contains more than one evidence document, as in our Wikipedia domain, we predict for each document separately and aggregate the predictions by taking a sum of conﬁdence scores. More speciﬁcally, when the model outputs a candidate answer Ai from n documents Di,1,...Di,n with conﬁdences ci,1,...ci,n, the score of Ai is given by",
        "An evaluation of our baselines shows that both of our tasks are challenging, and that the TriviaQA dataset supports signiﬁcant future work.",
        "Table 6: Data statistics for each task setup. The Wikipedia domain is evaluated over questions while the web domain is evaluated over documents.",
        "We randomly partition QA pairs in the dataset into train (80%), development (10%), and test set (10%). In addition to distant supervision evaluation, we also evaluate baselines on veriﬁed subsets (see section 4) of the dev and test partitions. Table 6 contains the number of questions and documents for each task. We trained the entity classiﬁer on a random sample of 50,000 questions from the training set. For training BiDAF on the web domain, we ﬁrst randomly sampled 80,000 documents. For both domains, we used only those (training) documents where the answer appears in the ﬁrst 400 tokens to keep training time manageable. Designing scalable techniques that can use the entirety of the data is an interesting direction for future work.",
        "Recent interest in question answering has resulted in the creation of several datasets. However, they are either limited in scale or suffer from biases stemming from their construction process. We group existing datasets according to their associated tasks, and compare them against TriviaQA. The analysis is summarized in Table 1.",
        "Reading comprehension tasks aims to test the ability of a system to understand a document using questions based upon its contents. Researchers have constructed cloze-style datasets (Hill et al., 2015; Hermann et al., 2015; Paperno et al., 2016; Onishi et al., 2016), where the task is to predict missing words, often entities, in a document. Cloze-style datasets, while easier to construct large-scale automatically, do not contain natural language questions.",
        "Datasets with natural language questions include MCTest (Richardson et al., 2013), SQuAD (Rajpurkar et al., 2016), and NewsQA (Trischler et al., 2016). MCTest is limited in scale with only2640multiplechoicequestions. SQuADcontains 100K crowdsourced questions and answers paired with short Wikipedia passages. NewsQA uses crowdsourcing to create questions solely from news article summaries in order to control potential bias. The crucial difference between SQuAD/NewsQA and TriviaQA is that TriviaQA questions have not been crowdsourced from preselected passages. Additionally, our evidence set consists of web documents, while SQuAD and NewsQA are limited to Wikipedia and news articles respectively. Other recently released datasets include (Lai et al., 2017).",
        "The recently released MS Marco dataset (Nguyen et al., 2016) also contains independently authored questions and documents drawn from the search results. However, the questions in the dataset are derived from search logs and the answers are crowdsourced. On the other hand, trivia enthusiasts provided both questions and answers for our dataset.",
        "Knowledge base question answering involves converting natural language questions to logical forms that can be executed over a KB. Proposed datasets (Cai and Yates, 2013; Berant et al., 2013; Bordes et al., 2015) are either limited in scale or in the complexity of questions, and can only retrieve facts covered by the KB.",
        "A standard task for open domain IR-style QA is the annual TREC competitions (Voorhees and Tice, 2000), which contains questions from various domains but is limited in size. Many advances from the TREC competitions were used in the IBM Watson system for Jeopardy! (Ferrucci et al., 2010). Other datasets includes SearchQA",
        "(Dunn et al., 2017) where Jeopardy! questions are paired with search engine snippets, the WikiQA dataset (Yang et al., 2015) for answer sentence selection, and the Chinese language WebQA (Li et al., 2016) dataset, which focuses on the task of answer phrase extraction. TriviaQA contains examples that could be used for both stages of the pipeline, although our focus on this paper is instead on using the data for reading comprehension where the answer is always present.",
        "We present TriviaQA, a new dataset of 650K To our question-document-evidence triples. knowledge, TriviaQA is the ﬁrst dataset where questions are authored by trivia enthusiasts, independently of the evidence documents. The evidence documents come from two domains – Web search results and Wikipedia pages – with highly differing levels of information redundancy. Results from current state-of-the-art baselines indi-",
        "While not the focus of this paper, TriviaQA also provides a provides a benchmark for a variety of other tasks such as IR-style question answering, QA over structured KBs and joint modeling of KBs and text, with much more data than previously available.",
        "Matthew Dunn, Levent Sagun, Mike Higgins, Ugur and Kyunghyun Cho. Guney, Volkan Cirik, 2017. Searchqa: A new q&a dataset augmented with context from a search engine. CoRR https://arxiv.org/abs/1704.05179.",
        "Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and In Proceedings of extracted knowledge bases. the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, New York, NY, USA, KDD ’14, pages 1156–1165. https://doi.org/10.1145/2623330.2623677.",
        "Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. CoRR https://arxiv.org/abs/1704.04683.",
        "Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, and Wei Xu. 2016. Dataset and neural recurrent sequence labeling model for open-domain factoid question answering. CoRR https://arxiv.org/abs/1607.06275.",
        "Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In Workshop in Advances in Neural Information Processing Systems. https://arxiv.org/pdf/1611.09268.pdf.",
        "Takeshi Onishi, Hai Wang, Mohit Bansal, Kevin Gimpel, and David McAllester. 2016. Who did what: A large-scale person-centered cloze dataset. In Proceedingsofthe2016ConferenceonEmpiricalMethods in Natural Language Processing. Association for Computational Linguistics, Austin, Texas, pages 2230–2235. https://aclweb.org/anthology/D16- 1241.",
        "Denis Paperno, Germ´an Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. 2016. The lambada dataset: Word prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Berlin, Germany, pages 1525– 1534. http://www.aclweb.org/anthology/P16-1144.",
        "Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. Wikiqa: A challenge dataset for open-domain quesIn Proceedings of the 2015 Contion answering. ference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Lisbon, Portugal, pages 2013–2018. http://aclweb.org/anthology/D15-1237.",
        "Matthew Richardson, Christopher J.C. Burges, and Erin Renshaw. 2013. MCTest: A challenge dataset for the open-domain machine comprehension of the 2013 Conference text. on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Seattle, Washington, USA, pages 193–203. http://www.aclweb.org/anthology/D13-1020.",
        "Sebastian Riedel, Limin Yao, and Andrew McModeling relations and their Callum. 2010. In Proceedings labeled text. mentions without of the 2010 European Conference on Machine Learning and Knowledge Discovery in Databases: III. Springer-Verlag, Berlin, Heidelberg, ECML PKDD’10, pages 148–163. http://dl.acm.org/citation.cfm?id=1889788.1889799.",
        "comprehension dataset."
    ]
}