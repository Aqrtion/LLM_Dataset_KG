{
    "title_author_abstract_introduction": "CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning\nJustin Johnson1,2∗ Li Fei-Fei1\nBharath Hariharan2 C. Lawrence Zitnick2\nLaurens van der Maaten2 Ross Girshick2\n1Stanford University\n2Facebook AI Research\nAbstract\nWhen building artiﬁcial intelligence systems that can reason and answer questions about visual data, we need diagnostic tests to analyze our progress and discover shortcomings. Existing benchmarks for visual question answering can help, but have strong biases that models can exploit to correctly answer questions without reasoning. They also conﬂatemultiplesourcesoferror, makingit hardto pinpoint model weaknesses. We present a diagnostic dataset that tests a range of visual reasoning abilities. It contains minimal biases and has detailed annotations describing the kind of reasoning each question requires. We use this dataset to analyze a variety of modern visual reasoning systems, providing novel insights into their abilities and limitations.\n1. Introduction\nA long-standing goal of artiﬁcial intelligence research is to develop systems that can reason and answer questions about visual information. Recently, several datasets have been introduced to study this problem [4, 10, 21, 26, 32, 46, 49]. Each of these Visual Question Answering (VQA)datasetscontainschallengingnaturallanguagequestions about images. Correctly answering these questions requires perceptual abilities such as recognizing objects, attributes, and spatial relationships as well as higher-level skills such as counting, performing logical inference, making comparisons, or leveraging commonsense world knowledge [31]. Numerous methods have attacked these problems [2, 3, 9, 24, 44], but many show only marginal improvementsoverstrongbaselines[4,16,48]. Unfortunately, our ability to understand the limitations of these methods is impeded by the inherent complexity of the VQA task. Are methods hampered by failures in recognition, poor reasoning, lack of commonsense knowledge, or something else?\nThe difﬁculty of understanding a system’s competences\n∗Work done during an internship at FAIR.\nQ: Are there an equal number of large things and metal spheres? Q: What size is the cylinder that is left of the brown metal thing that is left of the big sphere? Q: There is a sphere with the same size as the metal cube; is it made of the same material as the small red sphere? Q: How many objects are either small cylinders or metal things?\nFigure 1. A sample image and questions from CLEVR. Questions test aspects of visual reasoning such as attribute identiﬁcation, counting, comparison, multiple attention, and logical operations.\nis exempliﬁed by Clever Hans, a 1900s era horse who appeared to be able to answer arithmetic questions. Careful observation revealed that Hans was correctly “answering” questions by reacting to cues read off his human observers [30]. Statistical learning systems, like those used for VQA, may develop similar “cheating” approaches to superﬁcially “solve” tasks without learning the underlying reasoning processes [35, 36]. For instance, a statistical learner may correctly answer the question “What covers the ground?” not because it understands the scene but because biased datasets often ask questions about the ground when it is snow-covered [1, 47]. How can we determine whether a system is capable of sophisticated reasoning and not just exploiting biases of the world, similar to Clever Hans?\nIn this paper we propose a diagnostic dataset for studying the ability of VQA systems to perform visual reasoning. We refer to this dataset as the Compositional Language and Elementary Visual Reasoning diagnostics dataset (CLEVR; pronounced as clever in homage to Hans). CLEVR contains 100k rendered images and about one million automaticallygenerated questions, of which 853k are unique. It has chal-\nlenging images and questions that test visual reasoning abilities such as counting, comparing, logical reasoning, and storing information in memory, as illustrated in Figure 1.\nWe designed CLEVR with the explicit goal of enabling detailed analysis of visual reasoning. Our images depict simple 3D shapes; this simpliﬁes recognition and allows us to focus on reasoning skills. We ensure that the information in each image is complete and exclusive so that external information sources, such as commonsense knowledge, cannot increase the chance of correctly answering questions. We minimize question-conditional bias via rejection sampling within families of related questions, and avoid degenerate questions that are seemingly complex but contain simple shortcuts to the correct answer. Finally, we use structured ground-truth representations for both images and questions: images are annotated with ground-truth object positions and attributes, and questions are represented as functional programs that can be executed to answer the question (see Section 3). These representations facilitate indepth analyses not possible with traditional VQA datasets. These design choices also mean that while images in CLEVR may be visually simple, its questions are complex and require a range of reasoning skills. For instance, factorizedrepresentationsmayberequiredtogeneralizetounseen combinations of objects and attributes. Tasks such as counting or comparing may require short-term memory [15] or attending to speciﬁc objects [24, 44]. Questions that combine multiple subtasks in diverse ways may require compositional systems [2, 3] to answer.\nWe use CLEVR to analyze a suite of VQA models and discover weaknesses that are not widely known. For example, we ﬁnd that current state-of-the-art VQA models struggleontasksrequiringshorttermmemory, suchascomparing the attributes of objects, or compositional reasoning, suchasrecognizingnovelattributecombinations. Theseobservations point to novel avenues for further research.\nFinally, we stress that accuracy on CLEVR is not an end goal in itself: a hand-crafted system with explicit knowledge of the CLEVR universe might work well, but will not generalize to real-world settings. Therefore CLEVR should be used in conjunction with other VQA datasets in order to study the reasoning abilities of general VQA systems.\nThe CLEVR dataset, as well as code for generating new\nimages and questions, will be made publicly available.",
    "data_related_paragraphs": [
        "In recent years, a range of benchmarks for visual understanding have been proposed, including datasets for image captioning [7, 8, 23, 45], referring to objects [19], relationalgraphprediction[21], andvisualTuringtests[12,27]. CLEVR, our diagnostic dataset, is most closely related to benchmarks for visual question answering [4, 10, 21, 26, 32, 37, 46, 49], as it involves answering natural-language",
        "questions about images. The two main differences between CLEVR and other VQA datasets are that: (1) CLEVR minimizes biases of prior VQA datasets that can be used by learning systems to answer questions correctly without visual reasoning and (2) CLEVR’s synthetic nature and detailed annotations facilitate in-depth analyses of reasoning abilities that are impossible with existing datasets.",
        "Prior work has attempted to mitigate biases in VQA datasets in simple cases such as yes/no questions [12, 47], but it is difﬁcult to apply such bias-reduction approaches to more complex questions without a high-quality semantic representation of both questions and answers. In CLEVR, this semantic representation is provided by the functional program underlying each image-question pair, and biases are largely eliminated via sampling. Winograd schemas [22] are another approach for controlling bias in question answering: these questions are carefully designed to be ambiguous based on syntax alone and require commonsense knowledge. Unfortunately this approach does not scale gracefully: the ﬁrst phase of the 2016 Winograd Schema Challenge consists of just 60 hand-designed questions. CLEVR is also related to the bAbI question answering tasks [38] in that it aims to diagnose a set of clearly deﬁned competences of a system, but CLEVR focuses on visual reasoning whereas bAbI is purely textual.",
        "We are also not the ﬁrst to consider synthetic data for studying (visual) reasoning. SHRDLU performed simple, interactive visual reasoning with the goal of moving speciﬁc objects in the visual scene [40]; this study was one of the ﬁrst to demonstrate the brittleness of manually programmed semantic understanding. The pioneering DAQUAR dataset [28] contains both synthetic and human-written questions, but they only generate 420 synthetic questions using eight text templates. VQA [4] contains 150,000 natural-language questions about abstract scenes [50], but these questions do not control for questionconditional bias and are not equipped with functional program representations. CLEVR is similar in spirit to the SHAPES dataset [3], but it is more complex and varied both intermsofvisualcontentandquestionvarietyandcomplexity: SHAPES contains 15,616 total questions with just 244 unique questions while CLEVR contains nearly a million questions of which 853,554 are unique.",
        "3. The CLEVR Diagnostic Dataset",
        "CLEVR provides a dataset that requires complex reasoning to solve and that can be used to conduct rich diagnostics to better understand the visual reasoning capabilities of VQA systems. This requires tight control over the dataset, which we achieve by using synthetic images and automatically generated questions. The images have associated ground-truth object locations and attributes, and the questions have an associated machine-readable form. These",
        "Question families. We must overcome several key challenges to generate a VQA dataset using functional programs. Functional building blocks can be used to construct an inﬁnite number of possible functional programs, and we must decide which program structures to consider. We also need a method for converting functional programs to natural language in a way that minimizes question-conditional bias. We solve these problems using question families.",
        "Figure 3. Top: Statistics for CLEVR; the majority of questions are unique and few questions from the val and test sets appear in the training set. Bottom left: Comparison of question lengths for different VQA datasets; CLEVR questions are generally much longer. Bottom right: Distribution of question types in CLEVR.",
        "\u0000Ϭ\u0000ϭ\u0000Ϭ\u0000Ϯ\u0000Ϭ\u0000ϯ\u0000Ϭ\u0000ϰ\u0000Ϭ\u0000t\u0000Ž\u0000ƌ\u0000Ě\u0000Ɛ\u0000\u0003\u0000Ɖ\u0000Ğ\u0000ƌ\u0000\u0003\u0000Ƌ\u0000Ƶ\u0000Ğ\u0000Ɛ\u0000ƚ\u0000ŝ\u0000Ž\u0000Ŷ\u0000Ϭ\u0000й\u0000ϭ\u0000Ϭ\u0000й\u0000Ϯ\u0000Ϭ\u0000й\u0000ϯ\u0000Ϭ\u0000й\u0000Y\u0000Ƶ\u0000Ğ\u0000Ɛ\u0000ƚ\u0000ŝ\u0000Ž\u0000Ŷ\u0000\u0003\u0000Ĩ\u0000ƌ\u0000Ă\u0000Đ\u0000ƚ\u0000ŝ\u0000Ž\u0000Ŷ\u0000>\u0000Ğ\u0000Ŷ\u0000Ő\u0000ƚ\u0000Ś\u0000\u0003\u0000Ě\u0000ŝ\u0000Ɛ\u0000ƚ\u0000ƌ\u0000ŝ\u0000ď\u0000Ƶ\u0000ƚ\u0000ŝ\u0000Ž\u0000Ŷ\u0000\u0018\u0000\u0004\u0000Y\u0000h\u0000\u0004\u0000Z\u0000s\u0000Y\u0000\u0004\u0000s\u0000ϳ\u0000t\u0000\u0012\u0000>\u0000\u001c\u0000s\u0000Z\u0000\u001c\u0000ǆ\u0000ŝ\u0000Ɛ\u0000ƚ\u0000ϭ\u0000ϯ\u0000й\u0000\u0012\u0000Ž\u0000Ƶ\u0000Ŷ\u0000ƚ\u0000Ϯ\u0000ϰ\u0000й\u0000\u001c\u0000Ƌ\u0000Ƶ\u0000Ă\u0000ů\u0000Ϯ\u0000й\u0000>\u0000Ğ\u0000Ɛ\u0000Ɛ\u0000ϯ\u0000й\u0000'\u0000ƌ\u0000Ğ\u0000Ă\u0000ƚ\u0000Ğ\u0000ƌ\u0000ϯ\u0000й\u0000^\u0000ŝ\u0000ǌ\u0000Ğ\u0000ϵ\u0000й\u0000\u0012\u0000Ž\u0000ů\u0000Ž\u0000ƌ\u0000ϵ\u0000й\u0000D\u0000Ă\u0000ƚ\u0000͘\u0000ϵ\u0000й\u0000^\u0000Ś\u0000Ă\u0000Ɖ\u0000Ğ\u0000ϵ\u0000й\u0000^\u0000ŝ\u0000ǌ\u0000Ğ\u0000ϰ\u0000й\u0000\u0012\u0000Ž\u0000ů\u0000Ž\u0000ƌ\u0000ϰ\u0000й\u0000D\u0000Ă\u0000ƚ\u0000͘\u0000ϰ\u0000й\u0000^\u0000Ś\u0000Ă\u0000Ɖ\u0000Ğ\u0000ϰ\u0000й\u0000\u0012\u0000Ž\u0000ŵ\u0000Ɖ\u0000Ă\u0000ƌ\u0000Ğ\u0000\u0003\u0000/\u0000Ŷ\u0000ƚ\u0000Ğ\u0000Ő\u0000Ğ\u0000ƌ\u0000Y\u0000Ƶ\u0000Ğ\u0000ƌ\u0000Ǉ\u0000\u0012\u0000Ž\u0000ŵ\u0000Ɖ\u0000Ă\u0000ƌ\u0000Ğ\fFigure 4. Accuracy per question type of the six VQA methods on the CLEVR dataset (higher is better). Figure best viewed in color.",
        "Querying attributes: Query questions ask about an attribute of a particular object (e.g. “What color is the thing right of the red sphere?”). The CLEVR world has two sizes, eight colors, two materials, and three shapes. On questions asking about these different attributes, Q-type mode and LSTM obtain accuracies close to 50%, 12.5%, 50%, and 33.3% respectively, showing that the dataset has minimal question-conditional bias for these questions. CNN+LSTM+SA substantially outperforms all other models on these questions; its attention mechanism may help it focus on the target object and identify its attributes.",
        "Comparing attributes: Attribute comparison questions ask whether two objects have the same value for some attribute(e.g.“Isthecubethesamesizeasthesphere?”). The only valid answers are “yes” and “no”. Q-Type mode and LSTM achieve accuracies close to 50%, conﬁrming there is no dataset bias for these questions. Unlike attribute-query",
        "This paper has introduced CLEVR, a dataset designed to aid in diagnostic evaluation of visual question answering (VQA) systems by minimizing dataset bias and providing rich ground-truth representations for both images and questions. Our experiments demonstrate that CLEVR facilitates in-depth analysis not possible with other VQA datasets: our question representations allow us to slice the dataset along different axes (question type, relationship type, question topology, etc.), and comparing performance along these different axes allows us to better understand the reasoning capabilities of VQA systems. Our analysis has revealed several key shortcomings of current VQA systems:",
        "• Disentangled Representations: By training and testing models on different data distributions (Section 4.7) we argue that models do not learn representations that properly disentangle object attributes; they seem to learn strong biases from the training data and cannot overcome these biases when conditions change.",
        "These observations present clear avenues for future work on VQA. We plan to use CLEVR to study models with explicit short-term memory, facilitating comparisons between values [13, 18, 39, 42]; explore approaches that encourage learning disentangled representations [5]; and investigate methods that compile custom network architectures for different patterns of reasoning [2, 3]. We hope that diagnostic datasets like CLEVR will help guide future research in VQA and enable rapid progress on this important task.",
        "Data Types. Our basic functional building blocks operate on values of the following types:",
        "Module networks [2, 3] are a novel approach to visual question answering where a set of differentiable modules are used to assemble a custom network architecture to answer each question. Each module is responsible for performing a speciﬁc function such as ﬁnding a particular type of object, describing the current object of attention, or performing a logical and operation to merge attention masks. This approach seems like a natural ﬁt for the rich, compositional questions in CLEVR; unfortunately we found that parsing heuristics tuned for the VQA dataset did not generalize to the longer, more complex questions in CLEVR.",
        "For some questions, the heuristics are unable to produce any layout fragments; in this case, the system uses a simple default network architecture as a fallback for answering that question. On a random sample of 10,000 questions from the VQA dataset [4], we found that dynamic module networks resorted to default architecture for 7.8% of questions; on a random sample of 10,000 questions from CLEVR, the default network architecture was used for 28.9% of questions. This suggests that the same parsing heuristics used for VQA do not apply to the questions in CLEVR; therefore the method of [2] did not work out-of-the box on CLEVR.",
        "[10] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu. Are you talking to a machine? Dataset and methods for mulIn NIPS, 2015. 1, 2, tilingual image question answering. 4",
        "[32] M. Ren, R. Kiros, and R. Zemel. Exploring models and data for image question answering. In NIPS, 2015. 1, 2, 4 [33] K. Shih, S. Singh, and D. Hoiem. Where to look: Focus regions for visual question answering. In CVPR, 2016. 4 [34] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting. JMLR, 15(1):1929–1958, 2014. 5 [35] B. Sturm. A simple method to determine if a music inforIEEE Transactions on"
    ]
}