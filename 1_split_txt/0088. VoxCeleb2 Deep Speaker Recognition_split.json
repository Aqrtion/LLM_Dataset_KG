{
    "title_author_abstract_introduction": "VoxCeleb2: Deep Speaker Recognition\nJoon Son Chung†, Arsha Nagrani†, Andrew Zisserman\nVisual Geometry Group, Department of Engineering Science, University of Oxford, UK {joon,arsha,az}@robots.ox.ac.uk\nAbstract\nThe objective of this paper is speaker recognition under\nnoisy and unconstrained conditions.\nWe make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset.\nSecond, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a signiﬁcant margin.\nIndex Terms: large-scale, dataset, convolutional neural network\nspeaker identiﬁcation, speaker veriﬁcation,\n1. Introduction\nDespite recent advances in the ﬁeld of speaker recognition, producing single, compact representations for speaker segments that can be used efﬁciently under noisy and unconstrained conditions is still a signiﬁcant challenge. In this paper, we present a deep CNN based neural speaker embedding system, named VGGVox, trained to map voice spectrograms to a compact Euclidean space where distances directly correspond to a measure of speaker similarity. Once such a space has been produced, other tasks such as speaker veriﬁcation, clustering and diarisation can be straightforwardly implemented using standard techniques, with our embeddings as features.\nSuch a mapping has been learnt effectively for face images, through the use of deep CNN architectures [1, 2, 3] trained on large-scale face datasets [4, 5, 6]. Unfortunately, speaker recognition still faces a dearth of large-scale freely available datasets in the wild. VoxCeleb1 [7] and SITW [8] are valuable contributions, however they are still an order of magnitude smaller thanpopularfacedatasets, whichcontainmillionsofimages. To address this issue we curate VoxCeleb2, a large scale speaker recognition dataset obtained automatically from open-source media. VoxCeleb2 consists of over a million utterances from over 6k speakers. Since the dataset is collected ‘in the wild’, the speech segments are corrupted with real world noise including laughter, cross-talk, channel effects, music and other sounds. The dataset is also multilingual, with speech from speakers of 145 different nationalities, covering a wide range of accents, ages, ethnicities and languages. The dataset is audio-visual, so is also useful for a number of other applications, for example – visual speech synthesis [9, 10], speech separation [11, 12],\n†These authors contributed equally to this work.\ncross-modal transfer from face to voice or vice versa [13, 14] and training face recognition from video to complement existing face recognition datasets [4, 5, 6]. Both audio and video for the dataset will be released.\nWe train VGGVox on this dataset in order to learn speaker discriminative embeddings. Our system consists of three main variable parts: an underlying deep CNN trunk architecture, which is used to extract the features, a pooling method which is used to aggregate features to provide a single embedding for a given utterance, and a pairwise loss trained on the features to directly optimise the mapping itself. We experiment with both VGG-M [15] and ResNet [16] based trunk CNN architectures. Wemakethefollowingfourcontributions: (i)wecurateand release a large-scale dataset which is signiﬁcantly larger than any other speaker veriﬁcation dataset. It also addresses a lack of ethnic diversity in the VoxCeleb1 dataset (section 3); (ii) we propose deep ResNet-based architectures for speaker embedding suitable for spectrogram inputs (section 4); (iii) we beat the current state of the art for speaker veriﬁcation on the VoxCeleb1 test set using our embeddings (section 5); and (iv) we propose and evaluate on a new veriﬁcation benchmark test set which involves the entire VoxCeleb1 dataset.\nThe VoxCeleb2 dataset can be downloaded from http: //www.robots.ox.ac.uk/˜vgg/data/voxceleb2.",
    "data_related_paragraphs": [
        "Traditional methods. Traditionally, the ﬁeld of speaker recognition has been dominated by i-vectors [17], classiﬁed using techniques such as heavy-tailed PLDA [18] and GaussPLDA [19]. While deﬁning the state-of-the-art for a long time, such methods are disadvantaged by their reliance on handcrafted feature engineering. An in-depth review of these traditional methods is given in [20]. Deep learning methods. The success of deep learning in computer vision and speech recognition has motivated the use of deep neural networks (DNN) as feature extractors combined with classiﬁers, though not trained end-to-end [21, 22, 23, 24, 25]. While such fusion methods are highly effective, they still require hand-crafted engineering. In contrast, CNN architectures can be applied directly to raw spectrograms and trained in an end-to-end manner. For example, [26] uses a Siamese feedforward DNN to discriminatively compare two voices, however this relies on pre-computed MFCC features, whilst [27] also learns the features instead of using MFCCs. The most relevant to our work is [28], who train a neural embedding system using the triplet loss. However, they use private internal datasets for both training and evaluation, and hence a direct comparison with their work is not possible. Datasets. Existing speaker recognition datasets usually suffer from one or more of the following limitations: (i) they are ei-",
        "Dataset # of POIs # of male POIs # of videos # of hours # of utterances Avg # of videos per POI Avg # of utterances per POI Avg length of utterances (s)",
        "Table 1: Dataset statistics for both VoxCeleb1 and VoxCeleb2. Note VoxCeleb2 is more than 5 times larger than VoxCeleb1. POI: Person of Interest.",
        "Dataset # of POIs # of videos # of utterances",
        "ther obtained under controlled conditions (e.g., from telephone calls [29, 30] or acoustic laboratories [31, 32, 33]), (ii) they are manually annotated and hence limited in size [8], or (iii) not freely available to the speaker community [34, 33] (see [7] for a full review of existing datasets). In contrast, the VoxCeleb2 dataset does not suffer from any of these limitations.",
        "3. The VoxCeleb2 Dataset",
        "3.1. Description VoxCeleb2 contains over 1 million utterances for over 6,000 celebrities, extracted from videos uploaded to YouTube. The dataset is fairly gender balanced, with 61% of the speakers male. The speakers span a wide range of different ethnicities, accents, professions and ages. Videos included in the dataset are shot in a large number of challenging visual and auditory environments. These include interviews from red carpets, outdoor stadiums and quiet indoor studios, speeches given to large audiences, excerpts from professionally shot multimedia, and even crude videos shot on hand-held devices. Audio segments present in the dataset are degraded with background chatter, laughter, overlapping speech and varying room acoustics. We also provide face detections and face-tracks for the speakers in the dataset, and the face images are similarly ‘in the wild’, with variations in pose (including proﬁles), lighting, image quality and motion blur. Table 1 gives the general statistics, and Figure 1 shows examples of cropped faces as well as utterance length, gender and nationality distributions.",
        "The dataset contains both development (train/val) and test sets. However, since we use the VoxCeleb1 dataset for testing, onlythedevelopmentset willbeusedforthespeakerrecognition task (Sections 4 and 5). The VoxCeleb2 test set should prove useful for other applications of audio-visual learning for which the dataset might be used. The split is given in Table 2. The development set of VoxCeleb2 has no overlap with the identities in the VoxCeleb1 or SITW datasets.",
        "cate removal. This pipeline allows us to obtain a dataset that is ﬁve times the size of [7]. We also note that the list of celebrity namesspansawiderrangeofnationalities, andhenceunlike[7], the dataset obtained is multi-lingual. For the sake of clarity, the key stages are discussed in the following paragraphs: Stage 1. Candidate list of Persons of Interest (POIs). The ﬁrststageistoobtainalistofPOIs. Westartfromthelistofpeople that appear in the VGGFace2 dataset [4], which has considerable ethnic diversity and diversity in profession. This list containsover9,000identities, rangingfromactorsandsportspeople to politicians. Identities that overlap with those of VoxCeleb1 and SITW are removed from the development set. Stage 2. Downloading videos. The top 100 videos for each of the POIs are automatically downloaded using YouTube search. The word ‘interview’ is appended to the name of the POI in search queries to increase the likelihood that the videos contain an instance of the POI speaking, as opposed to sports or music videos. Stage 3. Face tracking. The CNN face detector based on the Single Shot MultiBox Detector (SSD) [35] is used to detect face appearances on every frame of the video. This detector is a distinct improvement from that used in [7], allowing the detection of faces in proﬁle and extreme poses. We used the same tracker as [7] based on ROI overlap. Stage 4. Face veriﬁcation. A face recognition CNN is used to classify the face tracks into whether they are of the POI or not. The classiﬁcation network used here is based on the ResNet50 [16] trained on the VGGFace2 dataset. Veriﬁcation is done by directly using this classiﬁcation score. Stage 5. Active speaker veriﬁcation. The goal of this stage is to determine if the visible face is the speaker. This is done by using a multi-view adaptation [36] of ‘SyncNet’ [37, 38], a twostream CNN which determines the active speaker by estimating the correlation between the audio track and the mouth motion of the video. The method can reject clips that contain dubbing or voice-over. Stage 6. Duplicate removal. A caveat of using YouTube as a source for videos is that often the same video (or a section of a video) can be uploaded twice, albeit with different URLs. Duplicates are identiﬁed and removed as follows: each speech segment is represented by a 1024D vector using the model in [7] as a feature extractor. The Euclidean distance is computed between all pairs of features from the same speaker. If any two speech segments have a distance smaller than a very conservative threshold (of 0.1), then the the speech segments are deemed to be identical, and one is removed. This method will certainly identify all exact duplicates, and in practice we ﬁnd that it also succeeds in identifying near-duplicates, e.g. speech segments of the same source that are differently trimmed. Stage 7. Obtaining nationality labels. Nationality labels are crawled from Wikipedia for all the celebrities in the dataset. We crawl for country of citizenship, and not ethnicity, as this is often more indicative of accent. In total, nationality labels are obtained for all but 428 speakers, who were labelled as unknown. Speakers in the dataset were found to hail from 145 nationalities (compared to 36 for VoxCeleb1), yielding a far more ethnically diverse dataset (See Figure 1 (bottom, right) for the distribution of nationalities). Note also the percentage of U.S. speakers is smaller in VoxCeleb2 (29%) compared to VoxCeleb1 (64%) where it dominates. Discussion. In order to ensure that our system is extremely conﬁdent that a person has been correctly identiﬁed (Stage 4),",
        "Figure 1: Top row: Examples from the VoxCeleb2 dataset. We show cropped faces of some of the speakers in the dataset. Both audio and face detections are provided. Bottom row: (left) distribution of utterance lengths in the dataset – lengths shorter than 20s are binned in 1s intervals and all utterances of 20s+ are binned together; (middle) gender distribution and (right) nationality distribution of speakers. For readability, the percentage frequencies of only the top-5 nationalities are shown. Best viewed zoomed in and in colour.",
        "and that are speaking (Stage 5) without any manual interference, we set conservative thresholds in order to minimise the number of false positives. Since VoxCeleb2 is designed primarily as a training-only dataset, the thresholds are less strict compared to those used to compile VoxCeleb1, so that fewer videos are discarded. Despite this, we have only found very few label errors after manual inspection of a signiﬁcant subset of the dataset.",
        "4.1. Evaluation The model is trained on the VoxCeleb2 dataset. At train time, pairs are sampled on-line using the method described in Section 4.3. The testing is done on the VoxCeleb1 dataset, with the test pairs provided in that dataset.",
        "4.2. Trunk architectures VGG-M: The baseline trunk architecture is the CNN introduced in [7]. This architecture is a modiﬁcation of the VGGM [15] CNN, known for high efﬁciency and good classiﬁcation performance on image data. In particular, the fully connected fc6 layer from the original VGG-M is replaced by two layers – a fully connected layer of 9 × 1 (support in the frequency domain), and an average pool layer with support 1 × n, where n",
        "A key challenge associated with learning embeddings via the contrastive loss is that as the dataset gets larger, the number of possible pairs grows quadratically. In such a scenario, the network rapidly learns to correctly map the easy examples, and hard negative mining is often required to improve performance to provide the network with a more useful learning signal. We use an ofﬂine hard negative mining strategy, which allows us to select harder negatives (e.g. top 1-percent of randomly generated pairs) than is possible with online (in-batch) hard negative mining methods [42, 41, 43] limited by the batch size. We do notminehardpositives, sincefalsepositivepairsaremuchmore likely to occur than false negative pairs in a random sample (due to possible label noise on the face veriﬁcation), and these label errors will lead to poor learning dynamics.",
        "As might be expected, performance improves with greater network depth, and also with more training data (VoxCeleb2 vs VoxCeleb1). This also demonstrates that VoxCeleb2 provides a suitable training regime for use on other datasets.",
        "New VoxCeleb1-E test set – using the entire dataset. Popular speaker veriﬁcation test sets in the wild [7, 8] are limited in the number of speakers. This yields the possible danger of optimising performance to overﬁt the small number of speakers in the test set, and results are not always indicative of good generalised performance. Hence we propose a new evaluation protocol consisting of 581,480 random pairs sampled from the entire VoxCeleb1 dataset, covering 1,251 speakers, and set benchmark performance for this test set. The result is given in Table 5.",
        "New VoxCeleb1-H test set – within the same nationality and gender. By using the whole of VoxCeleb1 as a test set, we are able to test only on the pairs with same nationality and gender. We propose a new evaluation list consisting of 552,536 pairs sampled from the VoxCeleb1 dataset, all of which are from the same nationality and gender. 18 nationality-gender combinations each with at least 5 individuals are used to generate this list, of which ‘USA-Male’ is the most common. The result is given in Table 5.",
        "In this paper, we have introduced new architectures and training strategies for the task of speaker veriﬁcation, and demonstrated state-of-the-art performance on the VoxCeleb1 dataset. Our learnt identity embeddings are compact (512D) and hence easy to store and useful for other tasks such as diarisation and retrieval. We have also introduced the VoxCeleb2 dataset, which is several times larger than any speaker recognition dataset, and have re-purposed the VoxCeleb1 dataset, so that the entire dataset of 1,251 speakers can be used as a test set for speaker veriﬁcation. Choosing pairs from all speakers allows a better assessment of performance than from the 40 speakers of the original test set. We hope that this new test set will be adopted, alongside SITW, as a standard for the speech community to evaluate on.",
        "[4] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman, “VGGFace2: a dataset for recognising faces across pose and age,” arXiv preprint arXiv:1710.08092, 2017.",
        "[6] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, “MS-Celeb-1M: A dataset and benchmark for large-scale face recognition,” in European Conference on Computer Vision. Springer, 2016, pp. 87–102.",
        "[7] A. Nagrani, J. S. Chung, and A. Zisserman, “VoxCeleb: a largescale speaker identiﬁcation dataset,” in INTERSPEECH, 2017.",
        "[8] M. McLaren, L. Ferrer, D. Castan, and A. Lawson, “The speakers in the wild (SITW) speaker recognition database,” in INTERSPEECH, 2016.",
        "[29] D. van der Vloed, J. Bouten, and D. A. van Leeuwen, “NFIFRITS: a forensic speaker recognition database and some ﬁrst experiments,” in The Speaker and Language Recognition Workshop, 2014.",
        "[30] J. Hennebert, H. Melin, D. Petrovska, and D. Genoud, “POLYCOST: a telephone-speech database for speaker recognition,” Speech communication, vol. 31, no. 2, pp. 265–270, 2000.",
        "[31] J. B. Millar, J. P. Vonwiller, J. M. Harrington, and P. J. Dermody, “The Australian national database of spoken language,” in Proc. ICASSP, vol. 1.",
        "[33] W. M. Fisher, G. R. Doddington, and K. M. Goudie-Marshall, “The DARPA speech recognition research database: speciﬁcations and status,” in Proc. DARPA Workshop on speech recognition, 1986, pp. 93–99."
    ]
}