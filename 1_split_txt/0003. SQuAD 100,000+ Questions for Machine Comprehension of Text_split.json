{
    "title_author_abstract_introduction": "SQuAD: 100,000+ Questions for Machine Comprehension of Text\nPranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang {pranavsr,zjian,klopyrev,pliang}@cs.stanford.edu Computer Science Department Stanford University\nAbstract\nWe present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a signiﬁcant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.\nIn meteorology, precipitation is any product of the condensation of atmospheric water vapor that falls under gravity. The main forms of precipitation include drizzle, rain, sleet, snow, graupel and hail... Precipitation forms as smaller droplets coalesce via collision with other rain drops or ice crystals within a cloud. Short, intense periods of rain in scattered locations are called “showers”.\nWhat causes precipitation to fall? gravity\nWhat is another main form of precipitation besides drizzle, rain, snow, sleet and hail? graupel\nWhere do water droplets collide with ice crystals to form precipitation? within a cloud\nFigure 1: Question-answer pairs for a sample passage in the\nSQuAD dataset. Each of the answers is a segment of text from\nIntroduction\nthe passage.\nReading Comprehension (RC), or the ability to read text and then answer questions about it, is a challenging task for machines, requiring both understanding of natural language and knowledge about the world. Consider the question “what causes precipitation to fall?” posed on the passage in Figure 1. In order to answer the question, one might ﬁrst locate the relevant part of the passage “precipitation ... falls under gravity”, then reason that “under” refers to a cause (not location), and thus determine the correct answer: “gravity”.\nHow can we get a machine to make progress on the challenging task of reading comprehension? large, realistic datasets have played Historically,\na critical role for driving ﬁelds forward—famous examples include ImageNet for object recognition (Deng et al., 2009) and the Penn Treebank for syntactic parsing (Marcus et al., 1993). Existing datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions.\nTo address the need for a large and high-quality reading comprehension dataset, we present the Stan-\nford Question Answering Dataset v1.0 (SQuAD), freely available at https://stanford-qa.com, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. SQuAD contains 107,785 question-answer pairs on 536 articles, and is almost two orders of magnitude larger than previous manually labeled RC datasets such as MCTest (Richardson et al., 2013).\nIn contrast to prior datasets, SQuAD does not provide a list of answer choices for each question. Rather, systems must select the answer from all possible spans in the passage, thus needing to cope with a fairly large number of candidates. While questions with span-based answers are more constrained than the more interpretative questions found in more advanced standardized tests, we still ﬁnd a rich diversity of questions and answer types in SQuAD. Wedevelopautomatictechniquesbasedondistances in dependency trees to quantify this diversity and stratify the questions by difﬁculty. The span constraint also comes with the important beneﬁt that span-based answers are easier to evaluate than freeform answers.\nTo assess the difﬁculty of SQuAD, we implemented a logistic regression model with a range of features. We ﬁnd that lexicalized and dependency tree path features are important to the performance of the model. We also ﬁnd that the model performance worsens with increasing complexity of (i) answer types and (ii) syntactic divergence between the question and the sentence containing the answer; interestingly, there is no such degradation for humans. Our best model achieves an F1 score of 51.0%,1 which is much better than the sliding window baseline (20%). Over the last four months (since June 2016), we have witnessed signiﬁcant improvements from more sophisticated neural network-based models. For example, Wang and Jiang (2016) obtained 70.3% F1 on SQuAD v1.1 (results on v1.0 are similar). These results are still well behind human performance, which is 86.8% F1 based on interannotator agreement. This suggests that there is plenty of room for advancement in modeling and learning on the SQuAD dataset.\n1All experimental results in this paper are on SQuAD v1.0.\nDataset\nQuestion source\nFormulation\ncrowdsourced RC,\nMCTest (Richardson et al., 2013) Algebra (Kushman et al., 2014) Science (Clark and Etzioni, 2016)\ncrowdsourced\nstandardized tests standardized tests\nin passage\nRC, multiple choice computation\nreasoning, multiple choice\nWikiQA (Yang et al., 2015) TREC-QA (Voorhees and Tice, 2000)\nquery logs\nquery logs + human editor\nIR, sentence selection IR, free form 1479\nCNN/Daily Mail (Hermann et al., 2015) CBT (Hill et al., 2015)\nsummary cloze cloze\nRC, ﬁll in single entity RC, ﬁll in single word\nTable 1: A survey of several reading comprehension and ques-\ntion answeringdatasets. SQuAD is much larger than all datasets\nexcept the semi-synthetic cloze-style datasets, and it is similar\nto TREC-QA in the open-endedness of the answers.",
    "data_related_paragraphs": [
        "2 Existing Datasets",
        "We begin with a survey of existing reading comprehension and question answering (QA) datasets, highlighting a variety of task formulation and creation strategies (see Table 1 for an overview).",
        "Reading comprehension. Adata-driven approach to reading comprehension goes back to Hirschman et al. (1999), who curated a dataset of 600 real 3rd– 6th grade reading comprehension questions. Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and difﬁcult, are too small to support very expressive statistical models.",
        "Some datasets focus on deeper reasoning abilities. Algebra word problems require understanding a story well enough to turn it into a system of equa-",
        "tions, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratiﬁed by different types of reasoning requiredtosolveeachtask. ClarkandEtzioni(2016) describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge.",
        "Open-domain question answering. The goal of open-domain QA is to answer a question from a large collection of documents. The annual evaluations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recently, Yang et al. (2015) created the WikiQA dataset, which, like SQuAD, use Wikipedia passages as a source of answers, but their task is sentence selection, while ours requires selecting a speciﬁc span in the sentence.",
        "Cloze datasets. Recently, researchers have constructed cloze datasets, in which the goal is to predict the missing word (often a named entity) in a passage. Since these datasets can be automatically generated from naturally occurring data, they can be extremely large. The Children’s Book Test (CBT) (Hill et al., 2015), for example, involves predicting a blanked-out word of a sentence given the 20 previous sentences. Hermann et al. (2015) constructed a corpus of cloze style questions by blanking out entities in abstractive summaries of CNN / Daily News articles; the goal is to ﬁll in the entity based on the original article. While the size of this dataset is impressive, Chen et al. (2016) showed that the dataset requires less reasoning than previously thought, and",
        "dataset encourages crowdworkers to use their own words while",
        "3 Dataset Collection",
        "We collect our dataset in three stages: curating passages, crowdsourcing question-answers on those passages, and obtaining additional answers.",
        "lowing categories. Our dataset consists of large number of an-",
        "4 Dataset Analysis",
        "Diversity in answers. We automatically categorize the answers as follows: We ﬁrst separate the numerical and non-numerical answers. The non-numerical answers are categorized using constituency parses and POS tags generated by Stanford CoreNLP. The proper noun phrases are further split into person, location and other entities using NER tags. In Table 2, we can see dates and other numbers make up 19.8% of the data; 32.6% of the answers are proper nouns of three different types; 31.8% are common noun phrases answers; and the remaining 15.8% are made up of adjective phrases, verb phrases, clauses and other types.",
        "Stratiﬁcation by syntactic divergence. We also develop an automatic method to quantify the syntactic divergence between a question and the sentence containing the answer. This provides another way to measure the difﬁculty of a question and to stratify the dataset, which we return to in Section 6.3.",
        "the anchor “ﬁrst” in the question to the wh-word “what”, and the other from the anchor in the answer sentence and to the answer span “Bainbridge’s”, are then extracted from the dependency parse trees. We measure the edit distance between these two paths, which we deﬁne as the minimum number of deletions or insertions to transform one path into the other. The syntactic divergence is then deﬁned as the minimum edit distance over all possible anchors. The histogram in Figure 4a shows that there is a wide range of syntactic divergence in our dataset. We also show a concrete example where the edit distance is 0 and another where it is 6. Note that our syntactic divergence ignores lexical variation. Also, small divergence does not mean that a question is easy since there could be other candidates with similarly small divergence.",
        "ing data.",
        "Feature ablations. In order to understand the features that are responsible for the performance of the logistic regression model, we perform a feature ablation where we remove one group of features from our model at a time. The results, shown in Table 6, indicate that lexicalized and dependency tree path features are most important. Comparing our analysis to the one in Chen et al. (2016), we note that the dependency tree path features play a much bigger role in our dataset. Additionally, we note that with lexicalized features, the model signiﬁcantly overﬁts the training set; however, we found that increasing L2 regularization hurts performance on the development set.",
        "the answer types explored in Table 2. The results (shown in Table 7) show that the model performs best on dates and other numbers, categories for which there are usually only a few plausible candidates, and most answers are single tokens. The model is challenged more on other named entities (i.e., location, person and other entities) because there are many more plausible candidates. However, named entities are still relatively easy to identify by their POS tag features. The model performs worst on other answer types, which together form 47.6% of the dataset. Humans have exceptional performance on dates, numbers and all named entities. Their performance on other answer types degrades only slightly.",
        "012345678Syntactic divergence2030405060708090100Preformance (%)Logistic Regression Dev F1Human Dev F1\fPerformance stratiﬁed by syntactic divergence. As discussed in Section 4, another challenging aspect of the dataset is the syntactic divergence between the question and answer sentence. Figure 5 shows that the more divergence there is, the lower the performance of the logistic regression model. Interestingly, humans do not seem to be sensitive to syntactic divergence, suggesting that deep understanding is not distracted by superﬁcial differences. Measuring the degree of degradation could therefore be useful in determining the extent to which a model is generalizing in the right way.",
        "Towards the end goal of natural language understanding, we introduce the Stanford Question Answering Dataset, a large reading comprehension dataset on Wikipedia articles with crowdsourced question-answer pairs. SQuAD features a diverse range of question and answer types. The performance of our logistic regression model, with 51.0% F1, against the human F1 of 86.8% suggests ample opportunity for improvement. We have made our dataset freely available to encourage exploration of more expressive models. Since the release of our dataset, we have already seen considerable interest in building models on this dataset, and the gap between our logistic regression model and human performance has more than halved (Wang and Jiang, 2016). We expect that the remaining gap will be harder to close, but that such efforts will result in signiﬁcant advances in reading comprehension.",
        "All code, data, and experiments for this paper are available on the CodaLab platform: https://worksheets.codalab.org/worksheets/ 0xd53d03a48ef64b329c16b9baf0f99b0c/ .",
        "We would like to thank Durim Morina and Professor Michael Bernstein for their help in crowdsourcing the collection of our dataset, both in terms of funding and technical support of the Daemo platform.",
        "J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. FeiFei. 2009. ImageNet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition (CVPR), pages 248–255.",
        "Y. Yang, W. Yih, and C. Meek. 2015. WikiQA: A challenge dataset for open-domain question answering. In Empirical Methods in Natural Language Processing (EMNLP), pages 2013–2018.",
        "M. Richardson, C. J. Burges, and E. Renshaw. 2013. Mctest: A challenge dataset for the open-domain machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP), pages 193– 203."
    ]
}