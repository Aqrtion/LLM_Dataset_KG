{
    "title_author_abstract_introduction": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 6,\nJUNE 2018\nPlaces: A 10 Million Image Database for Scene Recognition\nBolei Zhou , Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba\nAbstract—The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classiﬁcation performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classiﬁcation CNNs (Places-CNNs) as baselines, that signiﬁcantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classiﬁcation. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.\nIndex Terms—Scene classiﬁcation, visual recognition, deep learning, deep feature, image dataset\n1 INTRODUCTION\nIF a current state-of-the-art visual recognition system\nwould send you a text to describe what it sees, the text might read something like: “There is a sofa facing a TV set. A person is sitting on the sofa holding a remote control. The TV is on and a talk show is playing”. Reading this, you would likely imagine a living-room. However, that scenery can very well happen in a resort by the beach.\nFor an agent acting into the world, there is no doubt that object and event recognition should be a primary goal of its visual system. But knowing the place or context in which the objects appear is as equally important for an intelligent system to understand what might have happened in the past and what may happen in the future. For instance, a table inside a kitchen can be used to eat or prepare a meal, while a table inside a classroom is intended to support a notebook or a laptop to take notes.\nA key aspect of scene recognition is to identify the place in which the objects seat (e.g., beach, forest, corridor, ofﬁce, street, ...). Although one can avoid using the place category by providing a more exhaustive list of the objects in the picture and a description of their spatial relationships, a place category provides the appropriate level of abstraction to avoid such a long and complex description. Note that one could avoid using object categories in a description by only\n(cid:1) B. Zhou, A. Khosla, A. Oliva, and A. Torralba are with the Computer Science and Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA 02139. E-mail: zhoubolei@gmail.com, khosla@csail.mit.edu, {oliva, torralba}@mit.edu.\n(cid:1) A. Lapedriza is with the Universitat Oberta de Catalunya, Barcelona\n08018, Spain. E-mail: alapedriza@uoc.edu.\nManuscript received 8 Feb. 2017; revised 29 June 2017; accepted 29 June 2017. Date of publication 3 July 2017; date of current version 14 May 2018. (Corresponding author: Bolei Zhou.) Recommended for acceptance by T. Berg. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identiﬁer below. Digital Object Identiﬁer no. 10.1109/TPAMI.2017.2723009\nlisting parts (i.e., two eyes on top of a mouth for a face). Like objects, places have functions and attributes. They are composed of parts and some of those parts can be named and correspond to objects, just like objects are composed of parts, some of which are nameable as well (e.g., legs, eyes).\nWhereas most datasets have focused on object categories (providing labels, bounding boxes or segmentations), here we describe the Places database, a quasi-exhaustive repository of 10 million scene photographs, labeled with 434 scene semantic categories, comprising about 98 percent of the type of places a human can encounter in the world. Image samples are shown in Fig. 1 while Fig. 2 shows the number of images per category, sorted in decreasing order.\nDeparting from Zhou et al. [1], we describe in depth the construction of the Places Database, and evaluate the performance of several state-of-the-art Convolutional Neural Networks (CNNs) for place recognition. We compare how the features learned in a CNN for scene classiﬁcation behave when used as generic features in other visual recognition tasks. Finally, we visualize the internal representations of the CNNs and discuss one major consequence of training a deep learning model to perform scene recognition: object detectors emerge as an intermediate representation of the network [2]. Therefore, while the Places database does not contain any object labels or segmentations, it can be used to train new object classiﬁers.\n1.1 The Rise of Multi-Million Datasets What does it take to reach human-level performance with a machine-learning algorithm? In the case of supervised learning, the problem is two-fold. First, the algorithm must be suitable for the task, such as Convolutional Neural Networks in the large scale visual recognition [1], [3] and Recursive Neural Networks for natural language processing [4], [5]. Second, it must have access to a training dataset of appropriate coverage (quasi-exhaustive representation of\n0162-8828 (cid:1) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See ht_tp://www.ieee.org/publications_standards/publications/rights/index.html for more information. Authorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 03:40:18 UTC from IEEE Xplore.  Restrictions apply.\nZHOU ET AL.: PLACES: A 10 MILLION IMAGE DATABASE FOR SCENE RECOGNITION\nFig. 1. Image samples from various categories of the Places Database (two samples per category). The dataset contains three macro-classes: Indoor, Nature, and Urban.\nclasses and variety of exemplars) and density (enough samples to cover the diversity of each class). The optimal space for these datasets is often task-dependent, but the rise of multi-million-item sets has enabled unprecedented performance in many domains of artiﬁcial intelligence.\nThe successes of Deep Blue in chess, Watson in “Jeopardy!”, and AlphaGo in Go against their expert human opponents may thus be seen as not just advances in algorithms, but the increasing availability of very large datasets: 700,000, 8.6 million, and 30 million items, respectively [6], [7], [8]. Convolutional Neural Networks [3], [9] have likewise achieved near human-level visual recognition, trained on 1.2 million object [10], [11], [12] and 2.5 million scene images [1]. Expansive coverage of the space of classes and samples allows getting closer to the right ecosystem of data that a natural system, like a human, would experience. The history of image datasets for scene recognition also sees the rapid growingintheimagesamplesasfollows.\n1.2 Scene-Centric Datasets The ﬁrst benchmark for scene recognition was the Scene15 database [13], extended from the initial 8 scene dataset in [14]. This dataset contains only 15 scene categories with a\nfew hundred images per class, and current classiﬁers are saturated, reaching near human performance with 95 percent. The MIT Indoor67 database [15] with 67 indoor categories and the SUN (Scene Understanding, with 397 categories and 130,519 images) database [16] provided a larger coverage of place categories, but failed short in term of quantity of data needed to feed deep learning algorithms. To complement large object-centric datasets such as ImageNet [11], we build thePlacesdatasetdescribedhere.\nMeanwhile, the Pascal VOC dataset [17] is one of the earliest image dataset with diverse object annotations in scene context. The Pascal VOC challenge has greatly advanced the development of models for object detection and segmentation tasks. Nowadays, COCO dataset [18] focuses on collecting object instances both in polygon and bounding box annotations for images depicting everyday scenes of common objects. The recent Visual Genome dataset [19] aims at collecting dense annotations of objects, attributes, and their relationships. ADE20K [20] collects precise dense annotation of scenes, objects, parts of objects with a large and open vocabulary. Altogether, annotated datasets further enable artiﬁcial systems to learn visual knowledge linking parts, objects and scene context.\nFig. 2. Sorted distribution of image number per category in the Places Database. Places contains 10,624,928 images from 434 categories. Category names are shown for every six intervals.\nAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 03:40:18 UTC from IEEE Xplore.  Restrictions apply.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 40, NO. 6,\nJUNE 2018\nFig. 3. Image samples from four scene categories grouped by queries to illustrate the diversity of the dataset. For each query we show nine annotated images.",
    "data_related_paragraphs": [
        "2 PLACES DATABASE 2.1 Coverage of the Categorical Space The ﬁrst asset of a high-quality dataset is an expansive coverage of the categorical space to be learned. The strategy of Places is to provide an exhaustive list of the categories of environments encountered in the world, bounded by spaces where a human body would ﬁt (e.g., closet, shower). The Scene UNderstanding dataset [16] provided that initial list of semantic categories. The SUN dataset was built around a quasi-exhaustive list of scene categories with different functionalities, namely categories with unique identities in discourse. Through the use of WordNet [21], the SUN database team selected 70,000 words and concrete terms that described scenes, places and environments that can be used to complete the phrase “I am in a place”, or “let’s go to the/a place”. Most of the words referred to basic and entry-level names ([22]), resulting in a corpus of 900 different scene categories after bundling together synonyms, and separating classes described by the same word but referring to different environments (e.g., inside and outside views of churches). Details about the building of that initial corpus can be found in [16]. Places Database has inherited the same list of scene categories from the SUN dataset, with a few changes that are described in Section 2.2.4.",
        "2.2 Construction of the Database The construction of the Places Database is composed of four steps, from querying and downloading images, labeling images with ground truth category, to scaling up the dataset using a classiﬁer, and further improving the separation of similar classes. The detail of each step is introduced in the following sections.",
        "The data collection process of the Place Database is similar to the image collection in other common datasets, like ImageNet and COCO. The deﬁnition of categories for the ImageNet dataset [11] is based on the synset of WordNet [21]. Candidate images are queried from several Image search engines using the set of WordNet synonyms. Images are cleaned up through AMT in the format of the binary task similar to the ours. Quality control is done by multiple users annotating the same image. There are about 500-1200 ground-truth images per synset. On the other hand, COCO",
        "dataset [18] focuses on annotating the object instances inside the images with more scene context. The candidate images are mainly collected from Flickr, in order to include less iconic images commonly returned by image search engines. The image annotation process of COCO is split into category labeling, instance spotting, and instance segmentation, with all the tasks done by AMT workers. COCO has 80 object categories with more than 2 million object instances.",
        "From online image search engines (Google Images, Bing Images, and Flickr), candidate images were downloaded using a query word from the list of scene classes provided by the SUN database [16]. In order to increase the diversity of visual appearances in the Places dataset, each scene class query was combined with 696 common English adjectives1 (e.g., messy, spare, sunny, desolate, etc.). In Fig. 3) we show some examples of images in Places grouped by queries. About 60 million images (color images of at least 200 (cid:3) 200 pixels size) with unique URLs were identiﬁed. Importantly, the Places and SUN datasets are complementary: PCAbased duplicate removal was conducted within each scene category in both databases, so that they do not contain the same images.",
        "ZHOU ET AL.: PLACES: A 10 MILLION IMAGE DATABASE FOR SCENE RECOGNITION",
        "task. The experimental interface displayed a central image, ﬂanked by smaller version of images the worker had just responded (on the left), and the images the worker will respond to next (on the right). Information gleaned from the construction of the SUN dataset suggests that in the ﬁrst iteration of labeling more than 50 percent of the the downloaded imagesarenottrueexemplarsofthecategory.Forthisreason the default answer in the interface the default answer was set to NO (notice that all the smaller versions of the images in the left are marked with a bold red contour, which denotes that the image do not belong to the category). Thus, if the worker just presses the space bar to move, images will keep the default NO label. Whenever a true exemplar appears in the center, the worker can press a speciﬁc key to mark it as a positive exemplar (responding YES). As the response is set to YES the bold contour of the image turns to green. The interface also allows moving backwards to revise previous annotations. Each AMT HIT (Human Intelligence Task, one assignmentforoneworker),consistedof750imagesformanual annotation. A control set of 30 positive samples and 30 negative samples with ground-truth category labelsfrom the SUN database were intermixed in the HIT as well. As a quality controlmeasure,only workerHITswithanaccuracyof90 percentor higheronthesecontrolimageswerekept.",
        "After completing this third round of AMT annotation, the distribution of the number of images per category ﬂattened out:401 scene categorieshad more than 5,000images per category and 240 scene categories had more than 20,000 images. Intotal,about3 million imageswereaddedintothedataset.",
        "Despite the initial effort to bundle synonyms from WordNet, the scene list from the SUN database still contained a few categories with very close synonyms (e.g., ‘ski lodge’ and ‘ski resort’, or ‘garbage dump’ and ‘landﬁll’). We manually identiﬁed 46 synonym pairs like these and merged their images into a single category.",
        "2.2.3 Step 3: Scaling Up the Dataset Using a Classiﬁer As a result of the previous round of image annotation, there were 53 million remaining downloaded images not assigned to any of the 476 scene categories (e.g., a bedroom picture could have been downloaded when querying images for living-room category, but marked as negative by the AMT worker). Therefore, a third annotation task was designed to re-classify then re-annotate those images, using a semi-automatic bootstrapping approach.",
        "After this fourth annotation step, the Places database was ﬁnalized with over 10 millions labeled exemplars (10,624,928 images) from 434 place categories.",
        "Here we describe four subsets of Places database as benchmarks. Places205 and Places88 are from [1]. Two new benchmarks have been added: from the 434 categories, we selected 365 categories with more than 4,000 images each to create Places365-Standard and Places365-Challenge. The details of each benchmark are the following:",
        "Places365-Standard has 1,803,460 training images with the image number per class varying from 3,068 to 5,000. The validation set has 50 images per class and the test set has 900 images per class. Note that the experiments in this paper are reported on Places365-Standard. Places365-Challenge contains the same categories as Places365-Standard, but the training set is signiﬁcantly larger with a total of 8 million training images. The validation set and testing set are the same as the Places365-Standard. This subset was released for the Places Challenge 20162 held in conjunction with the European Conference on Computer Vision (ECCV) 2016, as part of the ILSVRC Challenge. Places205. Places205, described in [1], has 2.5 million images from 205 scene categories. The image number per class varies from 5,000 to 15,000. The training set has 2,448,873 total images, with 100 images per category for the validation set and 200 images per category for the test set. Places88. Places88 contains the 88 common scene categories among the ImageNet [12], SUN [16] and Places205 databases. Places88 contains only the images obtained in round 2 of annotations, from the ﬁrst version of Places used in [1]. We call the other two corresponding subsets ImageNet88 and SUN88",
        "respectively. These subsets are used to compare performances across different scene-centric databases, as the three datasets contain different exemplars per category (i.e., none of these three datasets contain common images). Note that ﬁnding correspondences between the classes deﬁned in ImageNet and Places brings some challenges. ImageNet follows the WordNet deﬁnitions, but some WordNet deﬁnitions are not always appropriate for describing places. For instance, the class ’elevator’ in ImageNet refers to an object. In Places, ‘elevator’ takes different meanings depending on the location of the observer: elevator door, elevator interior, or elevator lobby. Many categories in ImageNet do not differentiate between indoor and outdoor (e.g., ice-skating rink) while in Places, indoor and outdoor versions are separated as they do not necessarily afford the same function.",
        "4 COMPARING SCENE-CENTRIC DATASETS",
        "Scene-centric datasets correspond to images labeled with a scene, or place name, as opposed to object-centric datasets, where images are labeled with object names. In this section we use the Places88 benchmark to compare Places dataset with the tow other biggest scene datasets: ImageNet88 and SUN88. Fig. 7 illustrates the differences among the number of images found in the different categories for Places88, ImageNet88 and SUN88. Notice that Places Database is the largest scene-centric image dataset so far. The next section presents a comparison of these three datasets in terms of image diversity.",
        "4.1 Dataset Diversity Given the types of images found on the internet, some categories will be more biased than others in terms of viewpoints, types of objects, or even image style [23]. However, bias can be compensated with a high diversity of images, with many appearances represented in the dataset. In this section we describe a measure of dataset diversity to compare how diverse images from three scene-centric datasets (Places88, SUN88 and ImageNet88) are.",
        "Comparing datasets is an open problem. Even datasets covering the same visual classes have notable differences providing different generalization performances when used to train a classiﬁer [23]. Beyond the number of images and categories, there are aspects that are important but difﬁcult to quantify, like the variability in camera poses, in decoration styles or in the type of objects that appear in the scene.",
        "Although the quality of a database is often task dependent, it is reasonable to assume that a good database should be dense (with a high degree of data concentration), and diverse (it should include a high variability of appearances and viewpoints). Imagine, for instance, a dataset composed of 100,000 images all taken within the same bedroom. This dataset would have a very high density but a very low diversity as all the images will look very similar. An ideal dataset, expected to generalize well, should have high diversity as well. While one can achieve high density by collecting a large number of images, diversity is not an obvious quantity to estimate in image sets, as it assumes some notion of similarity between images. One way to estimate",
        "ZHOU ET AL.: PLACES: A 10 MILLION IMAGE DATABASE FOR SCENE RECOGNITION",
        "Fig. 7. Comparison of the number of images per scene category for the common 88 scene categories in Places, ImageNet, and SUN datasets.",
        "similarity is to ask the question are these two images similar? However, similarity in the wild is a subjective and loose concept, as two images can be viewed as similar if they contain similar objects, and/or have similar spatial conﬁgurations, and/or have similar decoration styles and so on. A way to circumvent this problem is to deﬁne relative measures of similarity for comparing datasets.",
        "Several measures of diversity have been proposed, particularly in biology for characterizing the richness of an ecosystem (see [24] for a review). Here, we propose to use a measure inspired by the Simpson index of diversity [25]. The Simpson index measures the probability that two random individuals from an ecosystem belong to the same species. It is a measure of how well distributed the individuals across different species are in an ecosystem, and it is related to the entropy of the distribution. Extending this measure for evaluating the diversity of images within a category is non-trivial if there are no annotations of sub-categories. For this reason, we propose to measure the relative diversity of image datasets A and B based on the following idea: if set A is more diverse than set B, then two random images from set B are more likely to be visually similar than two random samples from A. Then, the diversity of A with respect to B can be deﬁned as",
        "For an arbitrary number of datasets, A1;...;AN, the diversity of A1 with respect to A2;...;AN can be deﬁned as",
        "We measured the relative diversities between SUN88, ImageNet88 and Places88 using AMT. Workers were presented with different pairs of images and they had to select the pair that contained the most similar images. The pairs were randomly sampled from each database. Each trial was composed of 4 pairs from each database, giving a total of 12 pairs to choose from. We used 4 pairs per database to",
        "increase the chances of ﬁnding a similar pair and avoiding users having to skip trials. AMT workers had to select the most similar pair on each trial. We ran 40 trials per category and two observers per trial, for the 88 categories in common between ImageNet88, SUN88 and Places88 databases. Figs. 8a and 8b shows some examples of pairs from the diversity experiments for the scene categories playground (a) and bedroom (b). In the ﬁgure only one pair from each database is shown. We observed that different annotators",
        "Fig. 8. Examples of pairs for the diversity experiment for a) playground and b) bedroom. Which pair shows the most similar images? The bottom pairs were chosen in these examples. c) Histogram of relative diversity per each category (88 categories) and dataset. Places88 (in blue line) contains the most diverse set of images, then ImageNet88 (in red line) and the lowest diversity is in the SUN88 database (in yellow line) as most images are prototypical of their class.",
        "Fig. 8c shows the histograms of relative diversity for all the 88 scene categories common to the three databases. If the three datasets were identical in terms of diversity, the average diversity should be 2/3 for the three datasets. Note that this measure of diversity is a relative measure between the three datasets. In the experiment, users selected pairs from the SUN database to be the closest to each other 50 percent of the time, while the pairs from the Places database were judged to be the most similar only on 17 percent of the trials. ImageNet88 pairs were selected 33 percent of the time.",
        "The results show that there is a large variation in terms of diversity among the three datasets, showing Places to be the most diverse of the three datasets. The average relative diversity on each dataset is 0.83 for Places, 0.67 for ImageNet88 and 0.50 for SUN. The categories with the largest variation in diversity across the three datasets were playground, veranda and waiting room.",
        "4.2 Cross Dataset Generalization As discussed in [23], training and testing across different datasets generally results in a drop of performance due to the dataset bias problem. In this case, the bias between datasets is due, among other factors, to the differences in the diversity between the three datasets. Fig. 9 shows the classiﬁcation results obtained from the training and testing on different permutations of the 3 datasets. For these results we use the features extracted from a pre-trained ImageNetCNN and a linear SVM. In all three cases training and testing on the same dataset provides the best performance for a ﬁxed number of training examples. As the Places database is very large, it achieves the best performance on two of the test sets when all the training data is used.",
        "Fig. 9. Cross dataset generalization of training on the 88 common scenes between Places, SUN and ImageNet then testing on the 88 common scenes from: a) SUN, b) ImageNet and c) Places database.",
        "ZHOU ET AL.: PLACES: A 10 MILLION IMAGE DATABASE FOR SCENE RECOGNITION",
        "The rule of the challenge is that each team can only use the provided data in the Places365-Challenge to train their networks. Standard CNN models trained on Imagenet1.2 million and previous Places are allowed to use. Each team can submit at most 5 prediction results. Ranks are based on the top-5 classiﬁcation error of each submission. Winners teams are then invited to give talks at the ILSVRCCOCO Joint Workshop at ECCV’16.",
        "We further used the activation from the trained PlacesCNNs as generic features for visual recognition tasks using different image classiﬁcation benchmarks. Activations from the higher-level layers of a CNN, also termed deep features, have proven to be effective generic features with state-ofthe-art performance on various image datasets [33], [34]. But most of the deep features are from the CNNs trained on ImageNet, which is mostly an object-centric dataset.",
        "ZHOU ET AL.: PLACES: A 10 MILLION IMAGE DATABASE FOR SCENE RECOGNITION",
        "TABLE 3 Classiﬁcation Accuracy/Precision on Scene-Centric Databases (the First Four Datasets) and Object-Centric Databases (the Last Four Datasets) for the Deep Features of Various Places-CNNs and ImageNet-CNNs",
        "Places-CNN and ImageNet-CNN have complementary strengths on scene-centric tasks and object-centric tasks, as expected from the type of the datasets used to train these networks. On the other hand, the deep features from the Places365-VGG achieve the best performance (63.24 percent) on the most challenging scene classiﬁcation dataset SUN397, while the deep features of Places205-VGG performs the best on the MIT Indoor67 dataset. As far as we know, they are the state-of-the-art scores achieved by a single feature + linear SVM on those two datasets. Furthermore, we merge the 1,000 classes from the ImageNet and the 365 classes from the Places365-Standard to train a VGG (Hybrid1365-VGG). The deep feature from the Hybrid1365- VGG achieves the best score averaged over all the eight image datasets.",
        "training set size is 50 images per category. Experiments were run on 5 splits of the training set and test set given in the dataset. In the MIT Indoor67 experiment [15], the training set size is 100 images per category. The experiment is run on the split of the training set and test set given in the dataset. In the Scene15 experiment [13], the training set size is 50 images per category. Experiments are run on 10 random splits of the training set and test set. In the SUN Attribute experiment [35], the training set size is 150 images per attribute. The reported result is the average precision. The splits of the training set and test set are given in the paper. In Caltech101 and Caltech256 experiment [36], [37], the training set size is 30 images per category. The experiments are run on 10 random splits of the training set and test set. In the Stanford Action40 experiment [38], the training set size is 100 images per category. Experiments are run on 10 random splits of the training set and test set. The reported result is the classiﬁcation accuracy. In the UIUC Event8 experiment [39], the training set size is 70 images per category and the test set size is 60 images per category. The experiments are run on 10 random splits of the training set and test set.",
        "Places-CNNs and ImageNet-CNNs have the same network architectures for AlexNet, GoogLeNet, and VGG, but they are trained on scene-centric data (Places) and objectcentric data (ImageNet) respectively. For AlexNet and VGG, we used the 4,096-dimensional feature vector from the activation of the Fully Connected Layer (fc7) of the CNN. For GoogLeNet, we used the 1,024-dimensional feature vector from the response of the global average pooling layer before softmax producing the class predictions. The classiﬁer in all of the experiments is a linear SVM with the default parameter for all of the features.",
        "Table 3 summarizes the classiﬁcation accuracy on various datasets for the deep features of Places-CNNs and the deep features of the ImageNet-CNNs. Fig. 14 plots the classiﬁcation accuracy for different visual features on the SUN397 database over different numbers of training samples per category. The classiﬁer is a linear SVM with the same default parameters for the two deep feature layers (C ¼ 1) [40]. The Places-CNN features show impressive performance on scene-related datasets, outperforming the ImageNet-CNN features. On the other hand, the ImageNetCNN features show better performance on object-related image datasets. Importantly, our comparison shows that",
        "Fig. 14. Classiﬁcation accuracy on the SUN397 Dataset. We compare the deep features of Places365-VGG, Places205-AlexNet (result to hand-designed features reported in [1]), and ImageNet-AlexNet, (HOG, gist, etc). The deep features of Places365-VGG outperforms other deep features and hand-designed features by a large margin. Results of other hand-designed features/kernels are fetched from [16].",
        "ZHOU ET AL.: PLACES: A 10 MILLION IMAGE DATABASE FOR SCENE RECOGNITION",
        "Thus the specialty of the units in the object-centric CNN and scene-centric CNN yield very different performances of generic visual features on a variety of recognition benchmarks (object-centric datasets versus scene-centric datasets) in Table 3.",
        "From the Tiny Image dataset [42], to ImageNet [11] and Places [1], the rise of multi-million-item dataset initiatives and other densely labeled datasets [18], [20], [43], [44] have enabled data-hungry machine learning algorithms to reach near-human semantic classiﬁcation of visual patterns, like objects and scenes. With its category coverage and highdiversity of exemplars, Places offers an ecosystem of visual context to guide progress on scene understanding problems. Such problems could include determining the actions happening in a given environment, spotting inconsistent objects or human behaviors for a particular place, and predicting future events or the cause of events given a scene.",
        "Research and Engineering and funded by the Ofﬁce of Naval Research through grant N00014-16-1-3116 to A.O.; the MIT Big Data Initiative at CSAIL, the Toyota Research Institute / MIT CSAIL Joint Research Center, Google, Xerox and Amazon Awards, and a hardware donation from NVIDIA Corporation, to A.O and A.T. B.Z is supported by a Facebook Fellowship.",
        "[1] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva, “Learning deep features for scene recognition using places database,” in Proc. Advances Neural Inf. Process. Syst., 2014, pp. 487–495. [2] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Object detectors emerge in deep scene CNNs,” Int. Conf. Learn. Representations, 2015.",
        "[10] K. He, X. Zhang, S. Ren, and J. Sun, “Delving deep into rectiﬁers: Surpassing human-level performance on ImageNet classiﬁcation,” in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 1026–1034. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A large-scale hierarchical image database,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009, pp. 248–255. [12] O. Russakovsky, et al., “ImageNet large scale visual recognition",
        "Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2009, pp. 413–420. J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, “SUN database: Large-scale scene recognition from abbey to zoo,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2010, pp. 3485–3492.",
        "[20] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba, “Scene paring through ADE20K dataset,” Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017.",
        "[21] G. A. Miller, “WordNet: A lexical database for english,” Commun.",
        "[23] A. Torralba and A. A. Efros, “Unbiased look at dataset bias,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2011, pp. 1521– 1528.",
        "[35] G. Patterson and J. Hays, “SUN attribute database: Discovering, annotating, and recognizing scene attributes,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 2751–2758.",
        "[42] A. Torralba, R. Fergus, and W. T. Freeman, “80 million tiny images: A large data set for nonparametric object and scene recognition,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 11, pp. 1958–1970, Nov. 2008.",
        "[44] M. Cordts, et al., “The cityscapes dataset for semantic urban scene understanding,” Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 3213–3223."
    ]
}