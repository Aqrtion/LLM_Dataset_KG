{
    "title_author_abstract_introduction": "Describing Textures in the Wild\nMircea Cimpoi1, Subhransu Maji2, Iasonas Kokkinos3, Sammy Mohamed4, and Andrea Vedaldi1\n1Department of Engineering Science, University of Oxford 2Toyota Technological Institute, Chicago (TTIC) 3Center for Visual Computing, Ecole Centrale Paris 4Stony Brook University\nAbstract\nPatterns and textures are deﬁning characteristics of many natural objects: a shirt can be striped, the wings of a butterﬂy can be veined, and the skin of an animal can be scaly. Aiming at supporting this analytical dimension in image understanding, we address the challenging problem of describing textures with semantic attributes. We identify a rich vocabulary of forty-seven texture terms and use them to describe a large dataset of patterns collected “in the wild”. The resulting Describable Textures Dataset (DTD) is the basis to seek for the best texture representation for recognizing describable texture attributes in images. We port from object recognition to texture recognition the Improved Fisher Vector (IFV) and show that, surprisingly, it outperforms specialized texture descriptors not only on our problem, but also in established material recognition datasets. We also showthatthedescribableattributesareexcellenttexturedescriptors, transferring between datasets and tasks; in particular, combinedwithIFV,theysigniﬁcantlyoutperformthe state-of-the-art by more than 8% on both FMD and KTHTIPS-2b benchmarks. We also demonstrate that they produceintuitivedescriptionsofmaterialsandInternetimages.\n1. Introduction\nRecently visual attributes have raised signiﬁcant interest in the community [6, 11, 17, 25]. A “visual attribute” is a property of an object that can be measured visually and has a semantic connotation, such as the shape of a hat or the color of a ball. Attributes allow characterizing objects in far greater detail than a category label and are therefore the key to several advanced applications, including understanding complex queries in semantic search, learning about objects from textual description, and accounting for the content of\nFigure 1: Both the man-made and the natural world are an abundant source of richly textured objects. The textures of objects shown above can be described (in no particular order) as dotted, striped, chequered, cracked, swirly, honeycombed, and scaly. We aim at identifying these attributes automatically and generating descriptions based on them.\nimagesingreatdetail. Texturalpropertieshaveanimportant role in object descriptions, particularly for those objects that are best qualiﬁed by a pattern, such as a shirt or the wing of bird or a butterﬂy as illustrated in Fig. 1. Nevertheless, so far the attributes of textures have been investigated only tangentially. In this paper we address the question of whether there exists a “universal” set of attributes that can describe a wide range of texture patterns, whether these can be reliably estimated from images, and for what tasks they are useful.\nThe study of perceptual attributes of textures has a long history starting from pre-attentive aspects and grouping [16], to coarse high-level attributes [1, 2, 33], to some recent work aimed at discovering such attributes by automatically mining descriptions of images from the Internet [3, 12]. However, the texture attributes investigated so far are rather few or too generic for a detailed description most “real world” patterns. Our work is motivated by the one of Bhusan et al. [5] who studied the relationship between commonly used English words and the perceptual properties of textures, identifying a set of words sufﬁcient to describing a wide variety of texture patterns. While they study the psychological aspects of texture perception, the\nTuesday, October 29, 13\nbanded\nblotchy\nbraided\nbubbly\nchequered\ncobwebbed\ncracked\ncrosshatched crystalline\ndotted\nﬁbrous\nﬂecked\nfreckled\nfrilly\ngrooved\nhoneycombed interlaced\nknitted\nlacelike\nmarbled\nmatted\nmeshed\npaisley\nperforated\npitted\npleated\npolka-dotted porous\npotholed\nsmeared\nspiralled\nsprinkled\nstained\nstratiﬁed\nstriped\nstudded\nswirly\nveined\nwafﬂed\nwrinkled\nzigzagged\nFigure 2: The 47 texture words in the describable texture dataset introduced in this paper. Two examples of each attribute are shown to illustrate the signiﬁcant amount of variability in the data.\nfocus of this paper is the challenge of estimating such properties from images automatically.\nOur ﬁrst contribution is to select a subset of 47 describable texture attributes, based on the work of Bhusan et al., that capture a wide variety of visual properties of textures and to introduce a corresponding describable texture dataset consisting of 5,640 texture images jointly annotated with the 47 attributes (Sect. 2). In an effort to support directly real world applications, and inspired by datasets such as ImageNet [10] and the Flickr Material Dataset (FMD) [30], our images are captured “in the wild” by downloading them from the Internet rather than collecting them in a laboratory. We also address the practical issue of crowd-sourcing this large set of joint annotations efﬁciently accounting for the co-occurrence statistics of attributes, the appearance of the textures, and the reliability of annotators (Sect. 2.1).\nOur second contribution is to identify a gold standard texture representation that achieves optimal recognition of the describable texture attributes in challenging real-world conditions. Texture classiﬁcation has been widely studied in the context of recognizing materials supported by datasets such as CUReT [9], UIUC [18], UMD [39], Outex [23], Drexel Texture Database [24], and KTH-TIPS [7, 14]. These datasets address material recognition under variable occlusion, viewpoint, and illumination and have motivated the creation of a large number of specialized texture representations that are invariant or robust to these factors [19, 23, 35, 36]. In contrast, generic object recognition features such as SIFT was shown to work the best for material recognition in FMD, which, like DTD, was collected “in the wild”. Our ﬁndings are similar, but we also ﬁnd that Fisher vectors [26] computed on SIFT features and certain color features can signiﬁcantly boost performance. Surprisingly, these descriptors outperform specialized state-of-theart texture representations not only in recognizing our de-\nscribable attributes, but also in a variety of datasets for material recognition, achieving an accuracy of 63.3% on FMD and 67.5% on KTH-TIPS2-b dataset (Sect. 3, 4.1).\nOur third contribution consists in several applications of the proposed describable attributes. These can serve a complimentary role for recognition and description in domains where the material is not-important or is known ahead of time, such as fabrics or wallpapers. However, can these attributes improve other texture analysis tasks such as material recognition? We answer this question in the afﬁrmative in a series of experiments on the challenging FMD and KTH datasets. We show that estimates of these properties when used a features can boost recognition rates even more for material classiﬁcation achieving an accuracy of 53.1% on FMD and 64.6% on KTH when used alone as a 47 dimensional feature, and 65.4% on FMD and 74.6% on KTH when combined with SIFT and simple color descriptors (Sect. 4.2). These represent more than an absolute gain of 8% in accuracy over previous state of the art. Our 47 dimensional feature contributed with 2.2 to 7% to the gain. Furthermore, these attribute are easy to describe by design, hence they can serve as intuitive dimensions to explore large collections of texture patterns – for e.g., product catalogs (wallpapers or bedding sets) or material datasets. We present several such visualizations in the paper (Sect. 4.3).",
    "data_related_paragraphs": [
        "2. The describable texture dataset",
        "This section introduces the Describable Textures Dataset (DTD), a collection of real-world texture images annotated with one or more adjectives selected in a vocabulary of 47 English words. These adjectives, or describable texture attributes, are illustrated in Fig. 2 and include words such as banded, cobwebbed, freckled, knitted, and zigzagged.",
        "considered in existing datasets such as CUReT, KTH, and FMD. While describable attributes are correlated with materials, attributes do not imply materials (e.g. veined may equally apply to leaves or marble) and materials do not imply attributes (not all marbles are veined). Describable attributes can be combined to create rich descriptions (Fig. 3; marble can be veined, stratiﬁed and cracked at the same time), whereas a typical assumption is that textures are made of a single material. Describable attributes are subjective properties that depend on the imaged object as well as on human judgments, whereas materials are objective. In short, attributes capture properties of textures beyond materials, supporting human-centric tasks where describing textures is important. At the same time, they will be shown to behelpfulinmaterialrecognitionaswell(Sect.3.2and4.2). DTD contains textures in the wild, i.e. texture images extracted from the web rather than begin captured or generated in a controlled setting. Textures ﬁll the images, so we can study the problem of texture description independently of texture segmentation. With 5,640 such images, this dataset aims at supporting real-world applications were the recognition of texture properties is a key component. Collecting images from the Internet is a common approach in categorization and object recognition, and was adopted in materialrecognition inFMD. Thischoice trades-off the systematic sampling of illumination and viewpoint variations existing in datasets such as CUReT, KTH-TIPS, Outex, and Drexel datasets for a representation of real-world variations, shortening the gap with applications. Furthermore, the invariance of describable attributes is not an intrinsic property as for materials, but it reﬂects invariance in the human judgments, which should be captured empirically.",
        "Related work. Apart from material datasets, there have been numerous attempts at collecting attributes of textures at a smaller scale, or in controlled settings. Our work is related to the work of [22], where they analyzed images in the Outex dataset [23] using a subset of the attributes we consider. Their attributes were demonstrated to perform better than several low-level descriptors, but these were trained and evaluated on the same dataset. Hence it is not clear if their learned attributes generalize well to other settings. In contrast, we show that: (i) our texture attributes trained on DTD outperform their semantic attributes on Outex and (ii) they can signiﬁcantly boost performance on a number of other material and texture benchmarks (Sect. 4.2).",
        "2.1. Dataset design and collection",
        "This section discusses how DTD was designed and collected, including: selecting the 47 attributes, ﬁnding at least 120 representative images for each attribute, collecting a full set of multiple attribute labels for each image in the dataset, and addressing annotation noise.",
        "Given the DTD dataset developed in Sect. 2, this section moves on to the problem of designing a system that can automatically recognize the attributes of textures. Given a texture image (cid:96) the ﬁrst step is to compute a representation φ((cid:96)) ∈ Rd of the image; the second step is to use a classiﬁer such as a Support Vector Machine (SVM) (cid:104)w,φ((cid:96))(cid:105) to score how strongly the (cid:96) matches a given perceptual category. We propose two such representations: a gold-standard low-level texture descriptor based on the improved Fisher Vector (Sect. 3.1) and a mid-level texture descriptor consisting of the describable attributes themselves (Sect. 3.2). The details of the classiﬁers are discussed in Sect. 4.",
        "Table 1: Comparison of local descriptors and kernels on the DTD data, averaged over ten splits.",
        "exof the SVM kernels: linear several perimented with K(x(cid:48),x(cid:48)(cid:48)) = (cid:104)x(cid:48),x(cid:48)(cid:48)(cid:105), Hellinger’s (cid:80)d ix(cid:48)(cid:48) i , additive-χ2 (cid:80)d i ), and exponential-χ2 i + x(cid:48)(cid:48) i /(x(cid:48) −λ(cid:80)d i + x(cid:48)(cid:48) i )2/(x(cid:48) exp i ) signkernels extended as in [37]. In the latter case, λ is selected as one over the mean of the kernel matrix on the training set. The data is normalized so that K(x(cid:48),x(cid:48)(cid:48)) = 1 as this is often found to improve performance. Learning uses a standard non-linear SVM solver and validation in order to select the parameter C in the range {0.1,1,10,100} (the choice of C was found to have little impact on the result).",
        "Encoding comparisons on DTD. This experiment compares three encodings: BoVW, VLAD [15] and IFV. VLAD is similar to IFV, but uses K-means for quantization and stores only ﬁrst-order statistics of the descriptors. Dense SIFT is used as a baseline descriptor and performance is evaluated on ten splits of DTD in Tab. 2. IFV (256 Gaussian modes) and VLAD (512 K-means centers) performs similarly (about 60% mAP) and signiﬁcantly better than BoVW (53.82% mAP). As we will see next, however, IFV signiﬁcantly outperforms VLAD in other texture datasets. We also experimented with the state-of-the-art descriptor of [32] which we did not ﬁnd to be competitive with IFV on FMD and DTD (Tab. 2); unfortunately could not obtain an implementation of [29] to try on our data – however IFVSIFT outperforms it on material recognition.",
        "Local descriptor comparisons on DTD. This experiments compares local descriptors and kernels on DTD. All comparison use the bag-of-visual-word pooling/encoding scheme using K-means for vector quantization the descriptors. TheDTDdataisusedasabenchmarkaveragingtheresults on the ten train-val-test splits. K was cross-validated, ﬁnding an optimal setting of 1024 visual words for SIFT and color patches, 512 for LBP-VQ, 470 for the ﬁlter banks. Tab. 1, reports the mean Average Precision (mAP) for 47 SVM attribute classiﬁers. As expected, the best kernel is exp-χ2, followed by additive χ2 and Hellinger, and then linear. Dense SIFT (53.82% mAP) outperforms the best specialized texture descriptor on the DTD data (39.67% mAP for LM). Fig. 4 shows AP for each attribute: concepts like chequered achieve nearly perfect classiﬁcation, while oth-",
        "State-of-the-art material classiﬁcation. This experiments evaluates the encodings on several material recognition datasets: CUReT [9], UMD [39], UIUC [18], KTHTIPS [14], KTH-TIPS2(a and b) [7], and FMD [30]. Tab. 2 compares with the existing state-of-the-art [31, 32, 34] on each of them. For saturated datasets such as CUReT, UMD, UIUC, KTH-TIPS the performance of most methods is above to 99% mean accuracy and there is little difference between them. In harder datasets the advantage of IFV is evident: KTH-TIPS-2a (+5%), KTH-TIPS-2b (+3%), and FMD (+1%). In particular, while FMD includes manual segmentations of the textures, these are not used here here. Furthermore, IFV is conceptually simpler than the multiple specialized features used in [31] for material recognition.",
        "Dataset",
        "Finally, we compared the semantic attributes of [22] with DTDLIN on the Outex data. Using IFVSIFT as an underlying representation for our attributes, we obtain 49.82% mAP on the retrieval experiment of [22], which is is not as good as their result with LBPu (63.3%). However, LBPu was developed on the Outex data, and it is therefore not surprising",
        "that it works so well. To verify this, we retrained our DTD attributes with IFV using LBPu as local descriptor, obtaining a score of 64.52% mAP. This is remarkable considering that their retrieval experiment contains the data used to traintheirownattributes(targetset), whileourattributesare trained on a completely different data source. Tab. 1 shows that LBPu is not competitive on DTD.",
        "In what follows, we experimented with describing images from a challenging material dataset, FMD and encouraged by the good results, we applied the same technique to images from the wild, from some online catalog.",
        "OurexperimentsillustratehowtheDTDattributescanbe used to ﬁnd “semantic structures” in a dataset such as FMD, for example by distinguishing between “knitted vs pleated fabric”, “gauzy vs crystalline glass”, “veined vs frilly foliage” etc. To do so, FDM images for each material were clustered based on the 47 attribute vectors using K-means into 3-5 clusters each. Examples of the most meaningful clusters are shown in Fig. 8 along with the dominant attributes in each.",
        "Figure 5: Descriptions of materials from KTH-TIPS-2b dataset. These words are the most frequent top scoring texture attributes (from the list of 47 we proposed), when classifying the images from the KTH-TIPS-2b dataset.",
        "As an additional application of our describable texture attributes we compute them on a large dataset of 10,000 wallpapers and bedding sets (about 5,000 for each of the two categories) from houzz.com. The 47 attribute classiﬁers are learned as explained in Sect. 4.1 using the IFVSIFT representation and them apply them to the 10,000 images to predict the strength of association of each attribute and image. Classiﬁers scores are recalibrated on a subset of the target data and converted to probabilities using Platt’s method [28], for each individual attribute. Fig. 11 and",
        "Figure 6: Per class AP results on FMD dataset using DTD classiﬁcation scores as features.",
        "We introduced a large dataset of 5,640 images collected “in the wild” jointly labeled with 47 describable texture attributes and used it to study the problem of extracting semantic properties of textures and patterns, addressing realworld human-centric applications. Looking for the best representation to recognize such describable attributes in natural images, we have ported IFV, an object recognition representation, to the texture domain. Not only IFV works best inrecognizingdescribableattributes, butitalsooutperforms specialized texture representation on a number of challenging material recognition benchmarks. We have shown that the describable attributes, while not being designed to do so, are good predictors of materials as well, and that, when combined with IFV, signiﬁcantly outperform the state-ofthe-art on the FMD and KTH-TIPS recognition tasks.",
        "characterization from noisy web data. ECCV, 2010.",
        "[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009.",
        "ysis with semantic data. In CVPR, June 2013.",
        "[25] G. Patterson and J. Hays. Sun attribute database: Discovering, anno-"
    ]
}