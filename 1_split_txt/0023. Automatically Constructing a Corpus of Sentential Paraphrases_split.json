{
    "title_author_abstract_introduction": "Automatically Constructing a Corpus of Sentential Paraphrases\nWilliam B. Dolan and Chris Brockett Natural Language Processing Group Microsoft Research Redmond, WA, 98052, USA {billdol,chrisbkt}@microsoft.com\nAbstract\nAn  obstacle  to  research  in  automatic paraphrase  identification  and  generation is the lack of large-scale, publiclyavailable labeled corpora of sentential paraphrases.  This  paper  describes  the creation of the recently-released Microsoft  Research  Paraphrase  Corpus, which  contains  5801  sentence  pairs, each hand-labeled with a binary judgment as to whether the pair constitutes a  paraphrase. The  corpus was  created using heuristic extraction techniques in conjunction with an SVM-based classifier to select likely sentence-level paraphrases  from  a  large  corpus  of  topicclustered news data. These pairs were then submitted to human judges, who confirmed  that  67%  were  in  fact  semantically  equivalent.  In  addition  to describing the corpus itself, we explore a number of issues that arose in defining guidelines for the human raters.\nIntroduction\navailable\ndownload\nThe  Microsoft  Research  Paraphrase  Corpus (MSRP), at http://research.microsoft.com/research/nlp/msr_ paraphrase.htm,  consists  of  5801  pairs  of  sentences, each accompanied by a binary judgment indicating whether human raters considered the pair of sentences to be similar enough in meaning to be considered close paraphrases. This data has been published for the purpose of encouraging research in areas relating to paraphrase and sentential synonymy and inference, and to help\nestablish a discourse on the proper construction of paraphrase corpora for training and evaluation.  It is hoped that by releasing this corpus, we will stimulate the publication of similar corpora by others and help move the field toward adoption of a shared dataset that will permit useful comparisons of results across research efforts.\n(cid:0)",
    "data_related_paragraphs": [
        "The success of Statistical Machine Translation (SMT) has sparked a successful line of investigation  that  treats  paraphrase  acquisition  and generation essentially as a monolingual machine translation problem (e.g., Barzilay & Lee, 2003; Pang et al., 2003; Quirk et al., 2004; Finch et al., 2004). However, a lack of standardly-accepted corpora on which to train and evaluate models is a major stumbling block to the successful application of SMT models or other machine learning algorithms  to  paraphrase  tasks.    Since  paraphrase  is  not  apparently  a  common  “natural” task—under normal circumstances people do not attempt to create extended paraphrase texts—the field  lacks  a  large  readily  identifiable  dataset comparable to, for example, the Canadian Hansard corpus in SMT that can serve as a standard against  which  algorithms  can  be  trained  and evaluated.",
        "What paraphrase data is currently available is usually too small to be viable for either training or  testing,  or  exhibits  narrow  topic  coverage, limiting  its  broad-domain  applicability.  One class of paraphrase data that is relatively widely available is multiple translations of sentences in a second language. These, however, tend to be rather restricted in their domain (e.g. the ATR English-Chinese paraphrase corpus, which con-",
        "sists of translations of travel phrases (Zhang & Yamamoto,  2002)),  are  limited  to  short  handcrafted  predicates  (e.g.  the  ATR  JapaneseEnglish corpus (Shirai, et al., 2002)), or exhibit quality  problems  stemming  from  insufficient command of the target language by the translators of the documents in question, e.g. the Linguistic Data Consortium’s Multiple-Translation Chinese Corpus (Huang et al., 2002).  Multiple translations  of  novels,  such  as  those  used  in (Barzilay  &  McKeown,  2001)  provide  a  relatively limited dataset to work with, and – since these usually involve works that are out of copyright –  usually exhibit older styles of language that  have  little  in  common  with  modern  language resources or application requirements.",
        "Likewise, the data made available by (Barzilay  &  Lee,  2003:  http://www.cs.cornell.edu/ Info/Projects/NLP/statpar.html),  while  invaluable  in  understanding  and  evaluating  their  results, is too limited in size and domain coverage to serve as either training or test data.",
        "Attempting to evaluate models of paraphrase acquisition and generation under limitations can thus be an exercise in frustration. Accordingly, we have tried to create a reasonably large corpus of naturally-occurring, non-handcrafted sentence pairs,  along  with  accompanying  human  judgments, that can be used as a resource for training or testing purposes. Since the search space for identifying any two sentence pairs occurring “in the  wild”  is  huge,  and  provides  far  too  many negative examples for humans to wade through, clustered  news  articles  were  used  to  constrain the initial search space to data that was likely to yield paraphrase pairs.",
        "Source Data",
        "The  Microsoft  Research  Paraphrase  Corpus from  a  database  of (MSRP) 13,127,938  sentence  pairs,  extracted from 9,516,684  sentences  in  32,408  news  clusters collected from the World Wide Web over a 2- year period, The methods and assumptions used in building this initial data set are discussed in Quirk et al. (2004) and Dolan et al. (2004). Two heuristics based on shared lexical properties and sentence  position  in  the  document  were  employed to construct the initial database:",
        "Within  this  initial  dataset  we  were  able  to automatically identify the names of both authors and copyright holders of 61,618 articles.1  Limiting ourselves only to sentences found in those articles, we further narrowed the range of candidate pairs using the following criteria:",
        "This enabled us extract a set of 49,375 initial candidate  sentence  pairs  whose  author  was known,    The  purpose  of  these  heuristics  was two-fold: 1) to narrow the search space for subsequent  application  of  the  classifier  algorithm and human evaluation, and 2) to ensure at least some diversity among the sentences. In particular, we sought to exclude the large number of sentence  pairs  whose  differences  might  be  attributable only to typographical errors, variance between  British  and  American  spellings,  and minor editorial variations. Lexical distance was computed by constructing an alphabetized list of unique vocabulary items from each of the sentences and measuring the number of insertions and deletions. Note that the number of sentence pairs collected in this first pass was relatively small compared with the overall size of the dataset; the requirement of author identification significantly circumscribed the available dataset.",
        "A separate set of 10,000 sentence pairs had previously been extracted from randomly heldout clusters and hand-tagged by two annotators according to whether the sentence pairs constituted  paraphrases.  This  yielded  a  set  of  2968 positive examples and 7032 negative examples. The sentences represented a random mixture of held  out  sentences;  no  attempt  was  made  to match their characteristics to those of the candidate data set.",
        "WordNet  Lexical  Mappings:  314,924  word synonyms  and  hypernym  pairs  were  extracted  from  WordNet,  (Fellbaum,  1998; http://www.cogsci.princeton.edu/~wn/). Only pairs identified as occurring in either training data or the corpus to be classified were included in the final classifier.",
        "Composite  Features:  Additional,  more  abstract  features  summarized  the  frequency with which each feature or class of features occurred  in  the  training  data,  both  independently, and in correlation with other features or feature classes.",
        "Since our purpose was not to evaluate the potential effectiveness of the classifier itself, but to identify a reasonably large set of both positive and  plausible  “near-miss”  negative  examples, the classifier was applied with output probabilities overidentification,  i.e.,  towards  Type  1  errors,  assuming  non-paraphrase  (0)  as  null  hypothesis. This yielded 20,574 pairs out the initial 49,375- pair data set, from which 5801 pairs were then further randomly selected for human assessment.",
        "If a full paraphrase relationship can be described  as  “bidirectional  entailment”,  then  the majority of the “equivalent” pairs in this dataset exhibit “mostly bidirectional entailments”, with one sentence containing information that differs from or is not contained in the other. Our decision to adopt this relatively loose tagging criterion was ultimately a practical one: insisting on complete sets of bidirectional entailments would have  limited  the  dataset  to  pairs  of  sentences that are practically identical at the string level, as in the following examples.",
        "Such pairs are commonplace in the raw data, reflecting the tendency of news agencies to publish and republish the same articles, with editors inexplicable small  and  often introducing changes (is “however” really better than “but”?) along  the  way.  The  resulting  alternations  are useful sources of information about synonymy and local syntactic changes, but our goal was to produce a richer type of corpus; one that provides information about the large-scale alternations that typify complex paraphrases.4",
        "4 Recall that in an effort to focus on sentence pairs that are not simply trivial variants of some original single source, we restricted our original dataset by removing all pairs with a minimum word-based Levenshtein distance of (cid:149) 8.",
        "Some sentence pairs in the news data capture",
        "Far  more  frequently,  however,  interesting paraphrases in the data are accompanied by at least minor differences in content:",
        "Several  classes  of  named  entities  were  replaced by generic tags in sentences presented to bethe that came  %%DAY%%, became “%%MONEY%%,  and  so  on.  In  the  released version  of  the  dataset,  however,  these  placeholders were replaced by the original strings.",
        "be  recalled  that  parameters  were  deliberately distorted to yield imprecise results that included positive  and  a  large  number  of  “near-miss” negatives.  Coverage is hard to estimate reliably. we calculate that fewer than 30% of the pairs in a  set  of  matched  first-two  sentences  extracted from  clustered  news  data,  after  application  of simple heuristics, are paraphrases (Dolan et al., 2004).  It  seems  reasonable  to  assume  that  the reduction to 10% seen in the initial data set still leaves many valid paraphrase pairs uncaptured in the corpus. The need to limit the corpus to those  sentences  for  which  authorship  can  be verified, and more specifically. to no more than a  single  sentence  extracted  from  each  article. further constrains the coverage in ways whose consequences are not yet known. In addition, the three-shared-words  heuristic  further  guarantees that an entire class of paraphrases in which no words  are  shared  in  common  have  been  excluded from the data. It has been observed that the mean lexical overlap in the corpus is a relatively high 0.7 (Weeds et al, 2005), suggesting that more lexically divergent examples will be needed.  In these respects, as Wu (2005) points out, the corpus is far from distributionally neutral. This is a matter that we hope to remedy in the future, since in many ways this excluded set of pairs is the most interesting of all.",
        "ever  encounter  a  “naturally  occurring”  paraphrase corpus on the scale of any of these bilingual  corpora.    Moreover,  whatever  extraction technique is employed to identify paraphrases in other kinds of data will be apt to reflect the implicit biases of the methodology employed.",
        "Here we would like to put forward a proposal. The  paraphrase  research  community  might  be able to construct a “virtual paraphrase corpus” that would be adequately large for both training and  testing  purposes  and  minimize  selectional biases. This could be achieved in something like the  following  manner.  Research  groups  could compile  their  own  labeled  paraphrase  corpora, applying  whatever  learning  techniques  they choose to select their initial data. If enough interested  groups  were  to  release  a  sufficiently large  number  of  reasonably-sized  corpora,  it might be possible to achieve some sort consensus, in a manner analogous to the division of the Penn Treebank into sections, whereby classifiers and other tools are conventionally trained on one subset  of  corpora,  and  tested  against  another subset. Though this would present issues of its own, it would obviate many of the problems of extraction bias inherent in automated extraction, and  allow  better  cross  comparison  across  systems.",
        "For our part we plan to expand the MSRP, both by extending the number of sentence pairs, and also improving the balance of positive and negative examples. We anticipate using multiple classifiers to reduce inherent biases in candidate corpus selection, and with better author identification to ensure proper attribution, to be able to draw on a larger dataset for consideration by our judges.",
        "In future releases we expect to make available more information about individual evaluator  judgments.  Burger  &  Ferro  (2005)  have suggested that this data may allow researchers greater  freedom  to  construct  models  based  on the  judgments  of  specific  judges  or  combinations of judges, permitting more fine-grained use of the corpus.",
        "One other path that we are concurrently exploring  is  collection  and  validation  of  paraphrase  data  by  volunteers  on  the  web.  Some initial efforts using game formats for elicitation are presented in Chklovski (2005) and Brockett & Dolan (2005). It is our hope that web volunteers  will  prove  a  useful  source  of  colloquial paraphrases  of  written  text,  and–if  paraphrase identification can be effectively embedded in the game–of paraphrase judgments.",
        "We would like to thank Monica Corston-Oliver, Jeff Stevenson, Amy Muia and David Rojas of Butler  Hill  Group  LLC  for  their  assistance  in annotating  the  Microsoft  Research  Paraphrase Corpus and in preparing the seed data used for training.  This  paper  has  also  benefited  from feedback  from  several  anonymous  reviewers. All errors and omissions are our own.(cid:0)",
        "tronic Lexical Database. The MIT Press.",
        "Shudong Huang, David Graff, and George Doddington  (eds.)  2002.  Multiple-Translation  Chinese Corpus. Linguistic Data Consortium."
    ]
}