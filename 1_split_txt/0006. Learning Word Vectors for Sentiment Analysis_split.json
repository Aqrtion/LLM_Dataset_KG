{
    "title_author_abstract_introduction": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 142–150, Portland, Oregon, June 19-24, 2011. c(cid:13)2011 Association for Computational Linguistics\nLearningWordVectorsforSentimentAnalysisAndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopherPottsStanfordUniversityStanford,CA94305[amaas,rdaly,ptpham,yuze,ang,cgpotts]@stanford.eduAbstractUnsupervisedvector-basedapproachestose-manticscanmodelrichlexicalmeanings,buttheylargelyfailtocapturesentimentinforma-tionthatiscentraltomanywordmeaningsandimportantforawiderangeofNLPtasks.Wepresentamodelthatusesamixofunsuper-visedandsupervisedtechniquestolearnwordvectorscapturingsemanticterm–documentin-formationaswellasrichsentimentcontent.Theproposedmodelcanleveragebothcon-tinuousandmulti-dimensionalsentimentin-formationaswellasnon-sentimentannota-tions.Weinstantiatethemodeltoutilizethedocument-levelsentimentpolarityannotationspresentinmanyonlinedocuments(e.g.starratings).Weevaluatethemodelusingsmall,widelyusedsentimentandsubjectivitycor-poraandﬁnditout-performsseveralprevi-ouslyintroducedmethodsforsentimentclas-siﬁcation.Wealsointroducealargedatasetofmoviereviewstoserveasamorerobustbenchmarkforworkinthisarea.1IntroductionWordrepresentationsareacriticalcomponentofmanynaturallanguageprocessingsystems.Itiscommontorepresentwordsasindicesinavocab-ulary,butthisfailstocapturetherichrelationalstructureofthelexicon.Vector-basedmodelsdomuchbetterinthisregard.Theyencodecontinu-oussimilaritiesbetweenwordsasdistanceoranglebetweenwordvectorsinahigh-dimensionalspace.Thegeneralapproachhasprovenusefulintaskssuchaswordsensedisambiguation,namedentityrecognition,partofspeechtagging,anddocumentretrieval(TurneyandPantel,2010;CollobertandWeston,2008;Turianetal.,2010).Inthispaper,wepresentamodeltocapturebothsemanticandsentimentsimilaritiesamongwords.Thesemanticcomponentofourmodellearnswordvectorsviaanunsupervisedprobabilisticmodelofdocuments.However,inkeepingwithlinguisticandcognitiveresearcharguingthatexpressivecontentanddescriptivesemanticcontentaredistinct(Ka-plan,1999;Jay,2000;Potts,2007),weﬁndthatthisbasicmodelmissescrucialsentimentinforma-tion.Forexample,whileitlearnsthatwonderfulandamazingaresemanticallyclose,itdoesn’tcap-turethefactthatthesearebothverystrongpositivesentimentwords,attheoppositeendofthespectrumfromterribleandawful.Thus,weextendthemodelwithasupervisedsentimentcomponentthatiscapableofembracingmanysocialandattitudinalaspectsofmeaning(Wil-sonetal.,2004;Almetal.,2005;AndreevskaiaandBergler,2006;PangandLee,2005;GoldbergandZhu,2006;SnyderandBarzilay,2007).Thiscomponentofthemodelusesthevectorrepresen-tationofwordstopredictthesentimentannotationsoncontextsinwhichthewordsappear.Thiscauseswordsexpressingsimilarsentimenttohavesimilarvectorrepresentations.Thefullobjectivefunctionofthemodelthuslearnssemanticvectorsthatareimbuedwithnuancedsentimentinformation.Inourexperiments,weshowhowthemodelcanleveragedocument-levelsentimentannotationsofasortthatareabundantonlineintheformofconsumerreviewsformovies,products,etc.Thetechniqueissufﬁ-\f143\ncientlygeneraltoworkalsowithcontinuousandmulti-dimensionalnotionsofsentimentaswellasnon-sentimentannotations(e.g.,politicalafﬁliation,speakercommitment).Afterpresentingthemodelindetail,wepro-videillustrativeexamplesofthevectorsitlearns,andthenwesystematicallyevaluatetheapproachondocument-levelandsentence-levelclassiﬁcationtasks.Ourexperimentsinvolvethesmall,widelyusedsentimentandsubjectivitycorporaofPangandLee(2004),whichpermitsustomakecomparisonswithanumberofrelatedapproachesandpublishedresults.Wealsoshowthatthisdatasetcontainsmanycorrelationsbetweenexamplesinthetrainingandtestingsets.Thisleadsustoevaluateon,andmakepubliclyavailable,alargedatasetofinformalmoviereviewsfromtheInternetMovieDatabase(IMDB).2RelatedworkThemodelwepresentinthenextsectiondrawsin-spirationfrompriorworkonbothprobabilistictopicmodelingandvector-spacedmodelsforwordmean-ings.LatentDirichletAllocation(LDA;(Bleietal.,2003))isaprobabilisticdocumentmodelthatas-sumeseachdocumentisamixtureoflatenttop-ics.ForeachlatenttopicT,themodellearnsaconditionaldistributionp(w|T)fortheprobabilitythatwordwoccursinT.Onecanobtainak-dimensionalvectorrepresentationofwordsbyﬁrsttrainingak-topicmodelandthenﬁllingthematrixwiththep(w|T)values(normalizedtounitlength).Theresultisaword–topicmatrixinwhichtherowsaretakentorepresentwordmeanings.However,becausetheemphasisinLDAisonmodelingtop-ics,notwordmeanings,thereisnoguaranteethattherow(word)vectorsaresensibleaspointsinak-dimensionalspace.Indeed,weshowinsection4thatusingLDAinthiswaydoesnotdeliverro-bustwordvectors.ThesemanticcomponentofourmodelsharesitsprobabilisticfoundationwithLDA,butisfactoredinamannerdesignedtodiscoverwordvectorsratherthanlatenttopics.SomerecentworkintroducesextensionsofLDAtocapturesen-timentinadditiontotopicalinformation(Lietal.,2010;LinandHe,2009;Boyd-GraberandResnik,2010).LikeLDA,thesemethodsfocusonmodel-ingsentiment-imbuedtopicsratherthanembeddingwordsinavectorspace.Vectorspacemodels(VSMs)seektomodelwordsdirectly(TurneyandPantel,2010).LatentSeman-ticAnalysis(LSA),perhapsthebestknownVSM,explicitlylearnssemanticwordvectorsbyapply-ingsingularvaluedecomposition(SVD)tofactoraterm–documentco-occurrencematrix.ItistypicaltoweightandnormalizethematrixvaluespriortoSVD.Toobtainak-dimensionalrepresentationforagivenword,onlytheentriescorrespondingtotheklargestsingularvaluesaretakenfromtheword’sba-sisinthefactoredmatrix.Suchmatrixfactorization-basedapproachesareextremelysuccessfulinprac-tice,buttheyforcetheresearchertomakeanumberofdesignchoices(weighting,normalization,dimen-sionalityreductionalgorithm)withlittletheoreticalguidancetosuggestwhichtoprefer.Usingtermfrequency(tf)andinversedocumentfrequency(idf)weightingtotransformthevaluesinaVSMoftenincreasestheperformanceofre-trievalandcategorizationsystems.Deltaidfweight-ing(MartineauandFinin,2009)isasupervisedvari-antofidfweightinginwhichtheidfcalculationisdoneforeachdocumentclassandthenonevalueissubtractedfromtheother.MartineauandFininpresentevidencethatthisweightinghelpswithsen-timentclassiﬁcation,andPaltoglouandThelwall(2010)systematicallyexploreanumberofweight-ingschemesinthecontextofsentimentanalysis.ThesuccessofdeltaidfweightinginpreviousworksuggeststhatincorporatingsentimentinformationintoVSMvaluesviasupervisedmethodsishelp-fulforsentimentanalysis.Weadoptthisinsight,butweareabletoincorporateitdirectlyintoourmodel’sobjectivefunction.(Section4comparesourapproachwitharepresentativesampleofsuchweightingschemes.)3OurModelTocapturesemanticsimilaritiesamongwords,wederiveaprobabilisticmodelofdocumentswhichlearnswordrepresentations.Thiscomponentdoesnotrequirelabeleddata,andsharesitsfoundationwithprobabilistictopicmodelssuchasLDA.Thesentimentcomponentofourmodelusessentimentannotationstoconstrainwordsexpressingsimilar\f144\nsentimenttohavesimilarrepresentations.Wecanefﬁcientlylearnparametersforthejointobjectivefunctionusingalternatingmaximization.3.1CapturingSemanticSimilaritiesWebuildaprobabilisticmodelofadocumentus-ingacontinuousmixturedistributionoverwordsin-dexedbyamulti-dimensionalrandomvariableθ.Weassumewordsinadocumentareconditionallyindependentgiventhemixturevariableθ.Weassignaprobabilitytoadocumentdusingajointdistribu-tionoverthedocumentandθ.Themodelassumeseachwordwi∈disconditionallyindependentoftheotherwordsgivenθ.Theprobabilityofadocu-mentisthusp(d)=Zp(d,θ)dθ=Zp(θ)NYi=1p(wi|θ)dθ.(1)WhereNisthenumberofwordsindandwiistheithwordind.WeuseaGaussianprioronθ.Wedeﬁnetheconditionaldistributionp(wi|θ)us-ingalog-linearmodelwithparametersRandb.Theenergyfunctionusesawordrepresentationma-trixR∈R(βx|V|)whereeachwordw(representedasaone-onvector)inthevocabularyVhasaβ-dimensionalvectorrepresentationφw=Rwcorre-spondingtothatword’scolumninR.Therandomvariableθisalsoaβ-dimensionalvector,θ∈Rβwhichweightseachoftheβdimensionsofwords’representationvectors.Weadditionallyintroduceabiasbwforeachwordtocapturedifferencesinover-allwordfrequencies.TheenergyassignedtoawordwgiventhesemodelparametersisE(w;θ,φw,bw)=−θTφw−bw.(2)Toobtainthedistributionp(w|θ)weuseasoftmax,p(w|θ;R,b)=exp(−E(w;θ,φw,bw))Pw′∈Vexp(−E(w′;θ,φw′,bw′))(3)=exp(θTφw+bw)Pw′∈Vexp(θTφw′+bw′).(4)Thenumberoftermsinthedenominator’ssum-mationgrowslinearlyin|V|,makingexactcom-putationofthedistributionpossible.Foragivenθ,awordw’soccurrenceprobabilityisrelatedtohowcloselyitsrepresentationvectorφwmatchesthescalingdirectionofθ.Thisideaissimilartothewordvectorinnerproductusedinthelog-bilinearlanguagemodelofMnihandHinton(2007).Equation1resemblestheprobabilisticmodelofLDA(Bleietal.,2003),whichmodelsdocumentsasmixturesoflatenttopics.Onecouldviewtheen-triesofawordvectorφasthatword’sassociationstrengthwithrespecttoeachlatenttopicdimension.Therandomvariableθthendeﬁnesaweightingovertopics.However,ourmodeldoesnotattempttomodelindividualtopics,butinsteaddirectlymodelswordprobabilitiesconditionedonthetopicmixturevariableθ.Becauseofthelog-linearformulationoftheconditionaldistribution,θisavectorinRβandnotrestrictedtotheunitsimplexasitisinLDA.WenowderivemaximumlikelihoodlearningforthismodelwhengivenasetofunlabeleddocumentsD.Inmaximumlikelihoodlearningwemaximizetheprobabilityoftheobserveddatagiventhemodelparameters.Weassumedocumentsdk∈Darei.i.d.samples.ThusthelearningproblembecomesmaxR,bp(D;R,b)=Ydk∈DZp(θ)NkYi=1p(wi|θ;R,b)dθ.(5)Usingmaximumaposteriori(MAP)estimatesforθ,weapproximatethislearningproblemasmaxR,bYdk∈Dp(ˆθk)NkYi=1p(wi|ˆθk;R,b),(6)whereˆθkdenotestheMAPestimateofθfordk.WeintroduceaFrobeniousnormregularizationtermforthewordrepresentationmatrixR.Thewordbi-asesbarenotregularizedreﬂectingthefactthatwewantthebiasestocapturewhateveroverallwordfre-quencystatisticsarepresentinthedata.Bytakingthelogarithmandsimplifyingweobtaintheﬁnalob-jective,ν||R||2F+Xdk∈Dλ||ˆθk||22+NkXi=1logp(wi|ˆθk;R,b),(7)whichismaximizedwithrespecttoRandb.Thehyper-parametersinthemodelaretheregularization\f145\nweights(λandν),andthewordvectordimension-alityβ.3.2CapturingWordSentimentThemodelpresentedsofardoesnotexplicitlycap-turesentimentinformation.Applyingthisalgorithmtodocumentswillproducerepresentationswherewordsthatoccurtogetherindocumentshavesim-ilarrepresentations.However,thisunsupervisedapproachhasnoexplicitwayofcapturingwhichwordsarepredictiveofsentimentasopposedtocontent-related.Muchpreviousworkinnaturallan-guageprocessingachievesbetterrepresentationsbylearningfrommultipletasks(CollobertandWeston,2008;FinkelandManning,2009).Followingthisthemeweintroduceasecondtasktoutilizelabeleddocumentstoimproveourmodel’swordrepresenta-tions.Sentimentisacomplex,multi-dimensionalcon-cept.Dependingonwhichaspectsofsentimentwewishtocapture,wecangivesomebodyoftextasentimentlabelswhichcanbecategorical,continu-ous,ormulti-dimensional.Toleveragesuchlabels,weintroduceanobjectivethatthewordvectorsofourmodelshouldpredictthesentimentlabelusingsomeappropriatepredictor,ˆs=f(φw).(8)Usinganappropriatepredictorfunctionf(x)wemapawordvectorφwtoapredictedsentimentlabelˆs.Wecanthenimproveourwordvectorφwtobetterpredictthesentimentlabelsofcontextsinwhichthatwordoccurs.Forsimplicityweconsiderthecasewherethesen-timentlabelsisascalarcontinuousvaluerepre-sentingsentimentpolarityofadocument.Thiscap-turesthecaseofmanyonlinereviewswheredoc-umentsareassociatedwithalabelonastarratingscale.Welinearlymapsuchstarvaluestotheinter-vals∈[0,1]andtreatthemasaprobabilityofpos-itivesentimentpolarity.Usingthisformulation,weemployalogisticregressionasourpredictorf(x).Weusew’svectorrepresentationφwandregressionweightsψtoexpressthisasp(s=1|w;R,ψ)=σ(ψTφw+bc),(9)whereσ(x)isthelogisticfunctionandψ∈Rβisthelogisticregressionweightvector.Weadditionallyintroduceascalarbiasbcfortheclassiﬁer.Thelogisticregressionweightsψandbcdeﬁnealinearhyperplaneinthewordvectorspacewhereawordvector’spositivesentimentprobabilityde-pendsonwhereitlieswithrespecttothishyper-plane.Learningoveracollectionofdocumentsre-sultsinwordsresidingdifferentdistancesfromthishyperplanebasedontheaveragepolarityofdocu-mentsinwhichthewordsoccur.GivenasetoflabeleddocumentsDwhereskisthesentimentlabelfordocumentdk,wewishtomaximizetheprobabilityofdocumentlabelsgiventhedocuments.Weassumedocumentsinthecollec-tionandwordswithinadocumentarei.i.d.samples.Bymaximizingthelog-objectiveweobtain,maxR,ψ,bc|D|Xk=1NkXi=1logp(sk|wi;R,ψ,bc).(10)Theconditionalprobabilityp(sk|wi;R,ψ,bc)iseasilyobtainedfromequation9.3.3LearningThefulllearningobjectivemaximizesasumofthetwoobjectivespresented.Thisproducesaﬁnalob-jectivefunctionof,ν||R||2F+|D|Xk=1λ||ˆθk||22+NkXi=1logp(wi|ˆθk;R,b)+|D|Xk=11|Sk|NkXi=1logp(sk|wi;R,ψ,bc).(11)|Sk|denotesthenumberofdocumentsinthedatasetwiththesameroundedvalueofsk(i.e.sk<0.5andsk≥0.5).Weintroducetheweighting1|Sk|tocombatthewell-knownimbalanceinratingspresentinreviewcollections.Thisweightingpreventstheoveralldistributionofdocumentratingsfromaffect-ingtheestimateofdocumentratingsinwhichapar-ticularwordoccurs.Thehyper-parametersofthemodelaretheregularizationweights(λandν),andthewordvectordimensionalityβ.MaximizingtheobjectivefunctionwithrespecttoR,b,ψ,andbcisanon-convexproblem.Weusealternatingmaximization,whichﬁrstoptimizesthe\f146\nwordrepresentations(R,b,ψ,andbc)whileleav-ingtheMAPestimates(ˆθ)ﬁxed.ThenweﬁndthenewMAPestimateforeachdocumentwhileleav-ingthewordrepresentationsﬁxed,andcontinuethisprocessuntilconvergence.Theoptimizationalgo-rithmquicklyﬁndsaglobalsolutionforeachˆθkbe-causewehavealow-dimensional,convexproblemineachˆθk.BecausetheMAPestimationproblemsfordifferentdocumentsareindependent,wecansolvethemonseparatemachinesinparallel.Thisfacilitatesscalingthemodeltodocumentcollectionswithhundredsofthousandsofdocuments.4ExperimentsWeevaluateourmodelwithdocument-levelandsentence-levelcategorizationtasksinthedomainofonlinemoviereviews.Fordocumentcategoriza-tion,wecompareourmethodtopreviouslypub-lishedresultsonastandarddataset,andintroduceanewdatasetforthetask.Inbothtaskswecom-pareourmodel’swordrepresentationswithseveralbagofwordsweightingmethods,andalternativeap-proachestowordvectorinduction.4.1WordRepresentationLearningWeinducewordrepresentationswithourmodelus-ing25,000moviereviewsfromIMDB.Becausesomemoviesreceivesubstantiallymorereviewsthanothers,welimitedourselvestoincludingatmost30reviewsfromanymovieinthecollection.Webuildaﬁxeddictionaryofthe5,000mostfre-quenttokens,butignorethe50mostfrequenttermsfromtheoriginalfullvocabulary.Traditionalstopwordremovalwasnotusedbecausecertainstopwords(e.g.negatingwords)areindicativeofsenti-ment.Stemmingwasnotappliedbecausethemodellearnssimilarrepresentationsforwordsofthesamestemwhenthedatasuggestsit.Additionally,be-causecertainnon-wordtokens(e.g.“!”and“:-)”)areindicativeofsentiment,weallowtheminourvo-cabulary.RatingsonIMDBaregivenasstarvalues(∈{1,2,...,10}),whichwelinearlymapto[0,1]touseasdocumentlabelswhentrainingourmodel.Thesemanticcomponentofourmodeldoesnotrequiredocumentlabels.Wetrainavariantofourmodelwhichuses50,000unlabeledreviewsinaddi-tiontothelabeledsetof25,000reviews.Theunla-beledsetofreviewscontainsneutralreviewsaswellasthosewhicharepolarizedasfoundinthelabeledset.Trainingthemodelwithadditionalunlabeleddatacapturesacommonscenariowheretheamountoflabeleddataissmallrelativetotheamountofun-labeleddataavailable.Forallwordvectormodels,weuse50-dimensionalvectors.Asaqualitativeassessmentofwordrepresen-tations,wevisualizethewordsmostsimilartoaquerywordusingvectorsimilarityofthelearnedrepresentations.Givenaquerywordwandan-otherwordw′weobtaintheirvectorrepresentationsφwandφw′,andevaluatetheircosinesimilarityasS(φw,φw′)=φTwφw′||φw||·||φw′||.Byassessingthesimi-larityofwwithallotherwordsw′,wecanﬁndthewordsdeemedmostsimilarbythemodel.Table1showsthemostsimilarwordstogivenquerywordsusingourmodel’swordrepresentationsaswellasthoseofLSA.Allofthesevectorscap-turebroadsemanticsimilarities.However,bothver-sionsofourmodelseemtodobetterthanLSAinavoidingaccidentaldistributionalsimilarities(e.g.,screwballandgrantassimilartoromantic)Acom-parisonofthetwoversionsofourmodelalsobeginstohighlighttheimportanceofaddingsentimentin-formation.Ingeneral,wordsindicativeofsentimenttendtohavehighsimilaritywithwordsofthesamesentimentpolarity,soeventhepurelyunsupervisedmodel’sresultslookpromising.However,theyalsoshowmoregenreandcontenteffects.Forexam-ple,thesentimentenrichedvectorsforghastlyaretrulysemanticalternativestothatword,whereasthevectorswithoutsentimentalsocontainsomecontentwordsthattendtohaveghastlypredicatedofthem.Ofcourse,thisisonlyanimpressionisticanalysisofafewcases,butitishelpfulinunderstandingwhythesentiment-enrichedmodelprovessuperioratthesentimentclassiﬁcationresultswereportnext.4.2OtherWordRepresentationsForcomparison,weimplementedseveralalternativevectorspacemodelsthatareconceptuallysimilartoourown,asdiscussedinsection2:LatentSemanticAnalysis(LSA;Deerwesteretal.,1990)WeapplytruncatedSVDtoatf.idfweighted,cosinenormalizedcountmatrix,whichisastandardweightingandsmoothingschemefor\f147\nOurmodelOurmodelSentiment+SemanticSemanticonlyLSAmelancholybittersweetthoughtfulpoeticheartbreakingwarmthlyricalhappinesslayerpoetrytendernessgentleprofoundcompassionatelonelinessvividghastlyembarrassinglypredatorshideoustritehideousineptlaughablytubeseverelyatrociousbafﬂedgrotesqueappallingsmackunsuspectinglacklusterlamepassableuninspiredlaughableunconvincingﬂatunimaginativeamateurishblanduninspiredclich´edforgettableawfulinsipidmediocreromanticromanceromanceromancelovecharmingscrewballsweetdelightfulgrantbeautifulsweetcomediesrelationshipchemistrycomedyTable1:Similarityoflearnedwordvectors.Eachtargetwordisgivenwithitsﬁvemostsimilarwordsusingcosinesimilarityofthevectorsdeterminedbyeachmodel.Thefullversionofourmodel(left)capturesbothlexicalsimilarityaswellassimilarityofsentimentstrengthandorientation.Ourunsupervisedsemanticcomponent(center)andLSA(right)capturesemanticrelations.VSMinduction(TurneyandPantel,2010).LatentDirichletAllocation(LDA;Bleietal.,2003)Weusethemethoddescribedinsec-tion2forinducingwordrepresentationsfromthetopicmatrix.Totrainthe50-topicLDAmodelweusecodereleasedbyBleietal.(2003).Weusethesame5,000termvocabularyforLDAasisusedfortrainingwordvectormodels.WeleavetheLDAhyperparametersattheirdefaultvalues,thoughsomeworksuggestsoptimizingoverpriorsforLDAisimportant(Wallachetal.,2009).WeightingVariantsWeevaluatebothbinary(b)termfrequencyweightingwithsmootheddeltaidf(∆t’)andnoidf(n)becausethesevariantsworkedwellinpreviousexperimentsinsentiment(Mar-tineauandFinin,2009;Pangetal.,2002).Inallcases,weusecosinenormalization(c).PaltoglouandThelwall(2010)performanextensiveanalysisofsuchweightingvariantsforsentimenttasks.4.3DocumentPolarityClassiﬁcationOurﬁrstevaluationtaskisdocument-levelsenti-mentpolarityclassiﬁcation.Aclassiﬁermustpre-dictwhetheragivenreviewispositiveornegativegiventhereviewtext.Givenadocument’sbagofwordsvectorv,weobtainfeaturesfromourmodelusingamatrix-vectorproductRv,wherevcanhavearbitrarytf.idfweighting.Wedonotcosinenormalizev,insteadapplyingcosinenormalizationtotheﬁnalfeaturevectorRv.ThisprocedureisalsousedtoobtainfeaturesfromtheLDAandLSAwordvectors.Inpreliminaryexperiments,wefound‘bnn’weightingtoworkbestforvwhengeneratingdocumentfea-turesviatheproductRv.Inallexperiments,weusethisweightingtogetmulti-wordrepresentations\f148\nFeaturesPL04OurDatasetSubjectivityBagofWords(bnc)85.4587.8087.77BagofWords(b∆t’c)85.8088.2385.65LDA66.7067.4266.65LSA84.5583.9682.82OurSemanticOnly87.1087.3086.65OurFull84.6587.4486.19OurFull,AdditionalUnlabeled87.0587.9987.22OurSemantic+BagofWords(bnc)88.3088.2888.58OurFull+BagofWords(bnc)87.8588.3388.45OurFull,Add’lUnlabeled+BagofWords(bnc)88.9088.8988.13BagofWordsSVM(PangandLee,2004)87.15N/A90.00ContextualValenceShifters(KennedyandInkpen,2006)86.20N/AN/Atf.∆idfWeighting(MartineauandFinin,2009)88.10N/AN/AAppraisalTaxonomy(Whitelawetal.,2005)90.20N/AN/ATable2:Classiﬁcationaccuracyonthreetasks.Fromlefttorightthedatasetsare:Acollectionof2,000moviereviewsoftenusedasabenchmarkofsentimentclassiﬁcation(PangandLee,2004),50,000reviewswegatheredfromIMDB,andthesentencesubjectivitydatasetalsoreleasedby(PangandLee,2004).Alltasksarebalancedtwo-classproblems.fromwordvectors.4.3.1PangandLeeMovieReviewDatasetThepolaritydatasetversion2.0introducedbyPangandLee(2004)1consistsof2,000moviereviews,whereeachisassociatedwithabinarysentimentpo-laritylabel.Wereport10-foldcrossvalidationre-sultsusingtheauthors’publishedfoldstomakeourresultscomparablewithothersintheliterature.Weusealinearsupportvectormachine(SVM)classiﬁertrainedwithLIBLINEAR(Fanetal.,2008),andsettheSVMregularizationparametertothesamevalueusedbyPangandLee(2004).Table2showstheclassiﬁcationperformanceofourmethod,otherVSMsweimplemented,andpre-viouslyreportedresultsfromtheliterature.Bagofwordsvectorsaredenotedbytheirweightingnota-tion.Featuresfromwordvectorlearneraredenotedbythelearnername.Asacontrol,wetrainedver-sionsofourmodelwithonlytheunsupervisedse-manticcomponent,andthefullmodel(semanticandsentiment).Wealsoincluderesultsforaversionofourfullmodeltrainedwith50,000additionalunla-beledexamples.Finally,totestwhetherourmod-els’representationscomplementastandardbagofwords,weevaluateperformanceofthetwofeaturerepresentationsconcatenated.1http://www.cs.cornell.edu/people/pabo/movie-review-dataOurmethod’sfeaturesclearlyoutperformthoseofotherVSMs,andperformbestwhencombinedwiththeoriginalbagofwordsrepresentation.Thevari-antofourmodeltrainedwithadditionalunlabeleddataperformedbest,suggestingthemodelcaneffec-tivelyutilizelargeamountsofunlabeleddataalongwithlabeledexamples.Ourmethodperformscom-petitivelywithpreviouslyreportedresultsinspiteofourrestrictiontoavocabularyofonly5,000words.Weextractedthemovietitleassociatedwitheachreviewandfoundthat1,299ofthe2,000reviewsinthedatasethaveatleastoneotherreviewofthesamemovieinthedataset.Of406movieswithmultiplereviews,249havethesamepolaritylabelforalloftheirreviews.Overall,thesefactssuggestthat,rela-tivetothesizeofthedataset,therearehighlycorre-latedexampleswithcorrelatedlabels.Thisisanat-uralandexpectedpropertyofthiskindofdocumentcollection,butitcanhaveasubstantialimpactonperformanceindatasetsofthisscale.Intherandomfoldsdistributedbytheauthors,approximately50%ofreviewsineachvalidationfold’stestsethaveareviewofthesamemoviewiththesamelabelinthetrainingset.Becausethedatasetissmall,alearnermayperformwellbymemorizingtheassociationbe-tweenlabelandwordsuniquetoaparticularmovie(e.g.,characternamesorplotterms).Weintroduceasubstantiallylargerdataset,which\f149\nusesdisjointsetsofmoviesfortrainingandtesting.Thesestepsminimizetheabilityofalearnertorelyonidiosyncraticword–classassociations,therebyfocusingattentionongenuinesentimentfeatures.4.3.2IMDBReviewDatasetWeconstructedacollectionof50,000reviewsfromIMDB,allowingnomorethan30reviewspermovie.Theconstructeddatasetcontainsanevennumberofpositiveandnegativereviews,sorandomlyguessingyields50%accuracy.Followingpreviousworkonpolarityclassiﬁcation,weconsideronlyhighlypo-larizedreviews.Anegativereviewhasascore≤4outof10,andapositivereviewhasascore≥7outof10.Neutralreviewsarenotincludedinthedataset.Intheinterestofprovidingabenchmarkforfutureworkinthisarea,wereleasethisdatasettothepublic.2Weevenlydividedthedatasetintotrainingandtestsets.Thetrainingsetisthesame25,000la-beledreviewsusedtoinducewordvectorswithourmodel.Weevaluateclassiﬁerperformanceaftercross-validatingclassiﬁerparametersonthetrainingset,againusingalinearSVMinallcases.Table2showsclassiﬁcationperformanceonoursubsetofIMDBreviews.Ourmodelshowedsuperiorper-formancetootherapproaches,andperformedbestwhenconcatenatedwithbagofwordsrepresenta-tion.Againthevariantofourmodelwhichutilizedextraunlabeleddataduringtrainingperformedbest.Differencesinaccuracyaresmall,but,becauseourtestsetcontains25,000examples,thevarianceoftheperformanceestimateisquitelow.Forex-ample,anaccuracyincreaseof0.1%correspondstocorrectlyclassifyinganadditional25reviews.4.4SubjectivityDetectionAsasecondevaluationtask,weperformedsentence-levelsubjectivityclassiﬁcation.Inthistask,aclas-siﬁeristrainedtodecidewhetheragivensentenceissubjective,expressingthewriter’sopinions,orob-jective,expressingpurelyfacts.WeusedthedatasetofPangandLee(2004),whichcontainssubjectivesentencesfrommoviereviewsummariesandobjec-tivesentencesfrommovieplotsummaries.Thistask2Datasetandfurtherdetailsareavailableonlineat:http://www.andrew-maas.net/data/sentimentissubstantiallydifferentfromthereviewclassiﬁca-tiontaskbecauseitusessentencesasopposedtoen-tiredocumentsandthetargetconceptissubjectivityinsteadofopinionpolarity.Werandomlysplitthe10,000examplesinto10foldsandreport10-foldcrossvalidationaccuracyusingtheSVMtrainingprotocolofPangandLee(2004).Table2showsclassiﬁcationaccuraciesfromthesentencesubjectivityexperiment.OurmodelagainprovidedsuperiorfeatureswhencomparedagainstotherVSMs.Improvementoverthebag-of-wordsbaselineisobtainedbyconcatenatingthetwofeaturevectors.5DiscussionWepresentedavectorspacemodelthatlearnswordrepresentationscaptuingsemanticandsentimentin-formation.Themodel’sprobabilisticfoundationgivesatheoreticallyjustiﬁedtechniqueforwordvectorinductionasanalternativetotheoverwhelm-ingnumberofmatrixfactorization-basedtechniquescommonlyused.Ourmodelisparametrizedasalog-bilinearmodelfollowingrecentsuccessinus-ingsimilartechniquesforlanguagemodels(Bengioetal.,2003;CollobertandWeston,2008;MnihandHinton,2007),anditisrelatedtoprobabilisticlatenttopicmodels(Bleietal.,2003;SteyversandGrif-ﬁths,2006).Weparametrizethetopicalcomponentofourmodelinamannerthataimstocapturewordrepresentationsinsteadoflatenttopics.Inourex-periments,ourmethodperformedbetterthanLDA,whichmodelslatenttopicsdirectly.Weextendedtheunsupervisedmodeltoincor-poratesentimentinformationandshowedhowthisextendedmodelcanleveragetheabundanceofsentiment-labeledtextsavailableonlinetoyieldwordrepresentationsthatcapturebothsentimentandsemanticrelations.Wedemonstratedtheutil-ityofsuchrepresentationsontwotasksofsenti-mentclassiﬁcation,usingexistingdatasetsaswellasalargeronethatwereleaseforfutureresearch.Thesetasksinvolverelativelysimplesentimentin-formation,butthemodelishighlyﬂexibleinthisregard;itcanbeusedtocharacterizeawidevarietyofannotations,andthusisbroadlyapplicableinthegrowingareasofsentimentanalysisandretrieval.\f150\nAcknowledgmentsThisworkissupportedbytheDARPADeepLearn-ingprogramundercontractnumberFA8650-10-C-7020,anNSFGraduateFellowshipawardedtoAM,andONRgrantNo.N00014-10-1-0109toCP.ReferencesC.O.Alm,D.Roth,andR.Sproat.2005.Emotionsfromtext:machinelearningfortext-basedemotionpredic-tion.InProceedingsofHLT/EMNLP,pages579–586.A.AndreevskaiaandS.Bergler.2006.MiningWord-Netforfuzzysentiment:sentimenttagextractionfromWordNetglosses.InProceedingsoftheEuropeanACL,pages209–216.Y.Bengio,R.Ducharme,P.Vincent,andC.Jauvin.2003.aneuralprobabilisticlanguagemodel.JournalofMa-chineLearningResearch,3:1137–1155,August.D.M.Blei,A.Y.Ng,andM.I.Jordan.2003.Latentdirichletallocation.JournalofMachineLearningRe-search,3:993–1022,May.J.Boyd-GraberandP.Resnik.2010.Holisticsentimentanalysisacrosslanguages:multilingualsupervisedla-tentDirichletallocation.InProceedingsofEMNLP,pages45–55.R.CollobertandJ.Weston.2008.Auniﬁedarchitecturefornaturallanguageprocessing.InProceedingsoftheICML,pages160–167.S.Deerwester,S.T.Dumais,G.W.Furnas,T.K.Lan-dauer,andR.Harshman.1990.Indexingbylatentse-manticanalysis.JournaloftheAmericanSocietyforInformationScience,41:391–407,September.R.E.Fan,K.W.Chang,C.J.Hsieh,X.R.Wang,andC.J.Lin.2008.LIBLINEAR:Alibraryforlargelin-earclassiﬁcation.TheJournalofMachineLearningResearch,9:1871–1874,August.J.R.FinkelandC.D.Manning.2009.Jointparsingandnamedentityrecognition.InProceedingsofNAACL,pages326–334.A.B.GoldbergandJ.Zhu.2006.Seeingstarswhentherearen’tmanystars:graph-basedsemi-supervisedlearningforsentimentcategorization.InTextGraphs:HLT/NAACLWorkshoponGraph-basedAlgorithmsforNaturalLanguageProcessing,pages45–52.T.Jay.2000.WhyWeCurse:ANeuro-Psycho-SocialTheoryofSpeech.JohnBenjamins,Philadel-phia/Amsterdam.D.Kaplan.1999.Whatismeaning?ExplorationsinthetheoryofMeaningasUse.Briefversion—draft1.Ms.,UCLA.A.KennedyandD.Inkpen.2006.Sentimentclas-siﬁcationofmoviereviewsusingcontextualvalenceshifters.ComputationalIntelligence,22:110–125,May.F.Li,M.Huang,andX.Zhu.2010.Sentimentanalysiswithglobaltopicsandlocaldependency.InProceed-ingsofAAAI,pages1371–1376.C.LinandY.He.2009.Jointsentiment/topicmodelforsentimentanalysis.InProceedingofthe18thACMConferenceonInformationandKnowledgeManage-ment,pages375–384.J.MartineauandT.Finin.2009.Deltatﬁdf:animprovedfeaturespaceforsentimentanalysis.InProceedingsofthe3rdAAAIInternationalConferenceonWeblogsandSocialMedia,pages258–261.A.MnihandG.E.Hinton.2007.Threenewgraphicalmodelsforstatisticallanguagemodelling.InProceed-ingsoftheICML,pages641–648.G.PaltoglouandM.Thelwall.2010.Astudyofinforma-tionretrievalweightingschemesforsentimentanaly-sis.InProceedingsoftheACL,pages1386–1395.B.PangandL.Lee.2004.Asentimentaleducation:sentimentanalysisusingsubjectivitysummarizationbasedonminimumcuts.InProceedingsoftheACL,pages271–278.B.PangandL.Lee.2005.Seeingstars:exploitingclassrelationshipsforsentimentcategorizationwithrespecttoratingscales.InProceedingsofACL,pages115–124.B.Pang,L.Lee,andS.Vaithyanathan.2002.Thumbsup?sentimentclassiﬁcationusingmachinelearningtechniques.InProceedingsofEMNLP,pages79–86.C.Potts.2007.Theexpressivedimension.TheoreticalLinguistics,33:165–197.B.SnyderandR.Barzilay.2007.Multipleaspectrank-ingusingthegoodgriefalgorithm.InProceedingsofNAACL,pages300–307.M.SteyversandT.L.Grifﬁths.2006.Probabilistictopicmodels.InT.Landauer,DMcNamara,S.Dennis,andW.Kintsch,editors,LatentSemanticAnalysis:ARoadtoMeaning.J.Turian,L.Ratinov,andY.Bengio.2010.Wordrep-resentations:Asimpleandgeneralmethodforsemi-supervisedlearning.InProceedingsoftheACL,page384394.P.D.TurneyandP.Pantel.2010.Fromfrequencytomeaning:vectorspacemodelsofsemantics.JournalofArtiﬁcialIntelligenceResearch,37:141–188.H.Wallach,D.Mimno,andA.McCallum.2009.Re-thinkingLDA:whypriorsmatter.InProceedingsofNIPS,pages1973–1981.C.Whitelaw,N.Garg,andS.Argamon.2005.Usingap-praisalgroupsforsentimentanalysis.InProceedingsofCIKM,pages625–631.T.Wilson,J.Wiebe,andR.Hwa.2004.Justhowmadareyou?Findingstrongandweakopinionclauses.InProceedingsofAAAI,pages761–769.",
    "data_related_paragraphs": []
}