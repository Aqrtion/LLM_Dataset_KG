{
    "title_author_abstract_introduction": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\nKhurram Soomro, Amir Roshan Zamir and Mubarak Shah\nCRCV-TR-12-01\nNovember 2012\nKeywords: Action Dataset, UCF101, UCF50, Action Recognition\nCenter for Research in Computer Vision\nUniversity of Central Florida\n4000 Central Florida Blvd.\nOrlando, FL 32816-2365 USA\nUCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\nKhurram Soomro, Amir Roshan Zamir and Mubarak Shah Center for Research in Computer Vision, Orlando, FL 32816, USA {ksoomro, aroshan, shah}@cs.ucf.edu http://crcv.ucf.edu/data/UCF101.php\nAbstract\nWe introduce UCF101 which is currently the largest dataset of human actions. It consists of 101 action classes, over 13k clips and 27 hours of video data. The database consists of realistic user-uploaded videos containing camera motion and cluttered background. Additionally, we provide baseline action recognition results on this new dataset using standard bag of words approach with overall performance of 44.5%. To the best of our knowledge, UCF101 is currently the most challenging dataset of actions due to its large number of classes, large number of clips and also unconstrained nature of such clips.\n1. Introduction\nThe majority of existing action recognition datasets suffer from two disadvantages: 1) The number of their classes is typically very low compared to the richness of performed actions by humans in reality, e.g. KTH [11], Weizmann [3], UCF Sports [10], IXMAS [12] datasets includes only 6, 9, 9, 11 classes respectively. 2) The videos are recorded in unrealistically controlled environments. For instance, KTH, Weizmann, IXMAS are staged by actors; HOHA [7] and UCF Sports are composed of movie clips captured by professional ﬁlming crew. Recently, web videos have been used in order to utilize unconstrained user-uploaded data to alleviate the second issue [6, 8, 9, 5]. However, the ﬁrst disadvantage remains unresolved as the largest existing dataset does not include more than 51 actions while several works showedthatthenumberofclassesplayacrucialroleinevaluating an action recognition method [4, 9]. Therefore, we have compiled a new dataset with 101 actions and 13320 clips which is nearly twice bigger than the largest existing dataset in terms of number of actions and clips. (HMDB51 [5] and UCF50 [9] are the currently the largest ones with 6766 clips of 51 actions and 6681 clips of 50 actions respectively.)\nThe dataset\nis composed of web videos which are recorded in unconstrained environments and typically in-\nFigure 1. Sample frames for 6 action classes of UCF101.\nclude camera motion, various lighting conditions, partial occlusion, low quality frames, etc. Fig. 1 shows sample frames of 6 action classes from UCF101.",
    "data_related_paragraphs": [
        "2. Dataset Details",
        "The videos are downloaded from YouTube [2] and the irrelevant ones are manually removed. All clips have ﬁxed frame rate and resolution of 25 FPS and 320 × 240 respectively. The videos are saved in .avi ﬁles compressed using DivX codec available in k-lite package [1]. The audio is preserved for the clips of the new 51 actions. Table 1 summarizes the characteristics of the dataset.",
        "Naming Convention: The zipped ﬁle of the dataset http://crcv.ucf.edu/data/ (available UCF101.php ) includes 101 folders each containing the clips of one action class. The name of each clip has the following form:",
        "From each clip, we extracted Harris3D corners (using the implementation by [7]) and computed 162 dimensional HOG/HOF descriptors for each. We clustered a randomly selected set of 100,000 space-time interest points (STIP) using k-means to build the codebook. The size of our codebook is k=4000 which is shown to yield good results over a wide range of datasets. The descriptors were assigned to their closest video words using nearest neighbor classiﬁer, and each clip was represented by a 4000-dimensional histogram of its words. Utilizing a leave-one-group-out 25- fold cross validation scenario, a SVM was trained using",
        "024681012141605001000150020002500ApplyEyeMakeupApplyLipstickArcheryBabyCrawlingBalanceBeamBandMarchingBaseballPitchBasketballBasketballDunkBenchPressBikingBilliardsBlowDryHairBlowingCandlesBodyWeightSquatsBowlingBoxingPunchingBagBoxingSpeedBagBreastStrokeBrushingTeethCleanAndJerkCliffDivingCricketBowlingCricketShotCuttingInKitchenDivingDrummingFencingFieldHockeyPenaltyFloorGymnasticsFrisbeeCatchFrontCrawlGolfSwingHaircutHammeringHammerThrowHandstandPushupsHandstandWalkingHeadMassageHighJumpHorseRaceHorseRidingHulaHoopIceDancingJavelinThrowJugglingBallsJumpingJackJumpRopeKayakingKnittingTime (sec) Total TimeAverage Clip Duration024681012141605001000150020002500LongJumpLungesMilitaryParadeMixingMoppingFloorNunchucksParallelBarsPizzaTossingPlayingCelloPlayingDafPlayingDholPlayingFlutePlayingGuitarPlayingPianoPlayingSitarPlayingTablaPlayingViolinPoleVaultPommelHorsePullUpsPunchPushUpsRaftingRockClimbingIndoorRopeClimbingRowingSalsaSpinShavingBeardShotputSkateBoardingSkiingSkijetSkyDivingSoccerJugglingSoccerPenaltyStillRingsSumoWrestlingSurfingSwingTableTennisShotTaiChiTennisSwingThrowDiscusTrampolineJumpingTypingUnevenBarsVolleyballSpikingWalkingWithDogWallPushupsWritingOnBoardYoYoTime (sec) Total TimeAverage Clip Duration\fDataset KTH [11] Weizmann [3] UCF Sports [10] IXMAS [12] UCF11 [6] HOHA [7] Olympic [8] UCF50 [9] HMDB51 [5] UCF101",
        "Table 2. Summary of Major Action Recognition Datasets",
        "[5] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: A large video database for human motion recognition, 2011. International Conference on Computer Vision (ICCV). 2, 6",
        "We recommend a 25-fold cross validation experimental setup using all the videos in the dataset to keep consistency of the reported tests on UCF101; the baseline results provided in this section were computed using the same scenario.",
        "4. Related Datasets",
        "UCF Sports, UCF11, UCF50 and UCF101 are the four action datasets compiled by UCF in chronological order; each one includes its precursor. We made two minor modiﬁcations in the portion of UCF101 which includes UCF50 videos: the number of groups is ﬁxed to 25 for all the actions, and each group includes up to 7 clips. Table 2 shows a list of existing action recognition datasets with detailed characteristics of each. Note that UCF101 is remarkably larger than the rest.",
        "We introduced UCF101 which is the most challenging dataset for action recognition compared to the existing ones. It includes 101 action classes and over 13k clips which makes it outstandingly larger than other datasets. UCF101 is composed of unconstrained videos downloaded from YouTube which feature challenges such as poor lighting, cluttered background and severe camera motion. We provided baseline action recognition results on this new dataset using standard bag of words method with overall accuracy of 44.5%."
    ]
}