{
    "title_author_abstract_introduction": "Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera\nTimo von Marcard1, Roberto Henschel1, Michael J. Black2, Bodo Rosenhahn1, and Gerard Pons-Moll3\n1 Leibniz Universit¨at Hannover, Germany 2 MPI for Intelligent Systems, T¨ubingen, Germany 3 MPI for Informatics, Saarland Informatics Campus, Germany {marcard,henschel,rosenhahn}@tnt.uni-hannover.de, black@tue.mpg.de, gpons@mpi-inf.mpg.de\nAbstract. In this work, we propose a method that combines a single hand-held camera and a set of Inertial Measurement Units (IMUs) attached at the body limbs to estimate accurate 3D poses in the wild. This poses many new challenges: the moving camera, heading drift, cluttered background, occlusions and many people visible in the video. We associate 2D pose detections in each image to the corresponding IMUequipped persons by solving a novel graph based optimization problem that forces 3D to 2D coherency within a frame and across long range frames. Given associations, we jointly optimize the pose of a statistical body model, the camera pose and heading drift using a continuous optimization framework. We validated our method on the TotalCapture dataset, which provides video and IMU synchronized with ground truth. We obtain an accuracy of 26mm, which makes it accurate enough to serve as a benchmark for image-based 3D pose estimation in the wild. Using our method, we recorded 3D Poses in the Wild (3DPW), a new dataset consisting of more than 51,000 frames with accurate 3D pose in challenging sequences, including walking in the city, going up-stairs, having coﬀee or taking the bus. We make the reconstructed 3D poses, video, IMU and 3D models available for research purposes at http://virtualhumans.mpi-inf.mpg.de/3DPW.\nKeywords: Human Pose, Video, IMUs, Sensor Fusion, 2D to 3D, People Tracking, 3D Pose Dataset\nIntroduction\nThis paper addresses two inter-related goals. First, we propose a method capable of accurately reconstructing 3D human pose in outdoor scenes, with multiple people interacting with the environment, see Fig. 1. Our method combines data coming from IMUs (attached at the person’s limbs) with video obtained from a hand-held phone camera. This allows us to achieve the second goal, which is collecting the ﬁrst dataset with accurate 3D reconstructions in the wild. Since\nT. v. Marcard, R. Henschel, M. J. Black, B. Rosenhahn, G. Pons-Moll\nFig.1. We propose Video Inertial Poser (VIP), which enables accurate 3D human pose capture in natural environments. VIP combines video obtained from a handheld smartphone camera with data coming from body-worn inertial measurement units (IMUs). With VIP we collected 3D Poses in the Wild, a new dataset of accurate 3D human poses in natural video, containing variations in person identity, activity and clothing.\nour system works with a moving camera, we can record people in their everyday environments, for example, walking in the city, having coﬀee or taking the bus. 3D human pose estimation from un-constrained single images and videos has been a longstanding goal in computer vision. Recently, there has been a significant progress, particularly in 2D human pose estimation [23,4]. This progress has been possible thanks to the availability of large training datasets and benchmarks to compare research methods. While obtaining manual 2D pose annotations in the wild is fairly easy, collecting 3D pose annotations manually is almost impossible. This is probably the main reason there exist very limited datasets with accurate 3D pose in the wild. Datasets such as HumanEva [32] and H3.6M [8] have facilitated progress in the ﬁeld by providing ground truth 3D poses obtained using a marker-based motion capture system synchronized with video. These datasets, while useful and necessary, are restricted to indoor scenarios with static backgrounds, little variation in clothing and no environmental occlusions. As a result, evaluations of 3D human pose estimation methods in challenging images have been made mainly qualitatively, so far. There exist several options to record humans in outdoor scenes, none of which is satisfactory. Marker-based capture outdoors is limited. Depth sensors like Kinect do not work under strong illumination and can only capture objects near the camera. Using multiple cameras as in [21] requires time consuming set-up and calibration. Most importantly, the ﬁxed recording volume severely limits the kind of activities that can be captured.\nIMU-based systems hold promise because they are not bound to a ﬁxed space since they are worn by the person. In practice, however, accuracy is limited by a number of factors. Inaccuracies in the initial pose introduce sensor-tobone misalignments. In addition, during continuous operation, IMUs suﬀer from heading drift, see Fig. 2. This means, that after some time, each IMU does\nAccurate 3D Human Pose Using IMUs and a Moving Camera\nFig.2. For accurate motion capture in the wild we have to solve several challenges: IMU heading drift has accumulated after a longer recording session and the obtained 3D pose is completely oﬀ (left image pair). In order to estimate the heading drift, we combine IMU data and 2D poses detected in the camera view. This requires the association of 2D poses to the person wearing IMUs, which is diﬃcult when several people are in the scene (middle image). Also, 2D pose candidates might be inaccurate and should be automatically rejected during the assignment step (right image pair).\nnot measure relative to the same world coordinate frame. Rather, each sensor provides readings relative to independent coordinate frames that slowly drift away from the world frame. Furthermore, global position can not be accurately obtained due to positional drift. Moreover, IMU systems do not provide 3D pose synchronized and aligned with image data.\nTherefore, we propose a new method, called Video Inertial Poser (VIP), that jointly estimates the pose of people in the scene by using 6 to 17 IMUs attached at the body limbs and a single hand-held moving phone camera. Using IMUs makes the task less ambiguous but many challenges remain. First, the persons need to be detected in the video and associated with the IMU data, see Fig. 2. Second, IMUs are inaccurate due to heading drift. Third, the estimated 3D poses need to align with the images of the moving camera. Furthermore, the scenes we tackle in this work include complete occlusions, multiple people, tracked persons falling out of the camera view and camera motion. To address these diﬃculties, we deﬁne a novel graph-based association method, and a continuous pose optimization scheme that integrates the measurements from all frames in the sequence. To deal with noise and incomplete data, we exploit SMPL [14], which incorporates anthropometric and kinematic constraints.\nSpeciﬁcally, our approach has three steps: initialization, association and data fusion. During initialization, we compute initial 3D poses by ﬁtting SMPL to the IMU orientations. The association step automatically associates the 3D poses with 2D person detections for the full sequence by solving a single binary quadratic optimization problem. Given those associations, in the data fusion step, we deﬁne an objective function and jointly optimize for the 3D poses of the full sequence, the per-sensor heading errors, the camera pose and translation. Speciﬁcally, the objective is minimized when (i) the model orientation and acceleration is close to the IMU readings and (ii) the projected 3D joints of SMPL are close to 2D CNN detections [4] in the image. To further improve results, we repeat association and joint optimization once.\nT. v. Marcard, R. Henschel, M. J. Black, B. Rosenhahn, G. Pons-Moll\nWith VIP we can accurately estimate 3D human poses in challenging natural scenes. To validate the accuracy of VIP, we use the recently released 3D dataset Total Capture [39] because it provides video synchronized with IMU data. VIP obtains an average 3D pose error of 26mm, which makes it accurate enough to benchmark methods that tackle in-the-wild data. Using VIP we created 3D Poses in the Wild (3DPW): a dataset consisting of hand-held video with ground-truth 3D human pose and shape in natural videos.\nWe make 3DPW publicly available for research purposes, including 60 video sequences (51,000 frames or 1700 seconds of video captured with a phone at 30Hz), IMU data, 3D scans and 3D people models with 18 clothing variations, and the accurate 3D pose reconstruction results of VIP in all sequences. We anticipate that the dataset will stimulate novel research by providing a platform to quantitatively evaluate and compare methods for 3D human pose estimation.",
    "data_related_paragraphs": [
        "Pose Estimation using IMUs. There exist commercial solutions for MoCap with IMUs. The approach of [30] integrates 17 IMUs in a Kalman Filter to estimate pose. The seminal work of [41] uses a custom made suit to capture pose in everyday surroundings. These approaches require many sensors and do not align the reconstructions with video; therefore they suﬀer from drift. The approach of [42] ﬁts the SMPL body model to 5-6 IMUs over a full sequence, obtaining realistic results. The method, however, is applied to only 1 person at a time and the motion is not aligned with video. To compensate for drift, 4-8 cameras and 5 IMUs are combined in [17,25]. Using particle-based optimization, in [24] they use 4 cameras and IMUs to sample from a manifold of constrained poses. Other works combine depth data with IMUs [6,47]. In [39] a CNN-based approach fuses information from 8 camera views and IMU data to regress pose. Since these approaches also use multiple static cameras, recordings are restricted to a ﬁxed recording volume. A recent approach [16] also combines IMUs and 2D poses detected in one or two cameras but expects only a single person visible in the cameras and does not account for heading drift.",
        "3D Pose Datasets. The most commonly used datasets for 3D human pose evaluation are HumanEva [32] and H3.6M [8], which provide synchronized video with MoCap. These datasets however are limited to indoor scenes, static backgrounds and limited clothing and activity variation. Recently, a dataset of single people, including outdoor scenes, has been introduced [19]. The approach uses commercial marker-less motion capture from multiple cameras (the accuracy of the marker-less MoCap software used is not reported). The sequences show variation in clothing, but again, since it uses a multi-camera setup, the activities are restricted to a ﬁxed recording volume. Another recent dataset is TotalCapture [39], which features synchronized video, marker-based ground-truth poses and IMUs. In order to collect 3D poses in the wild, in [11] they ask users to pick “acceptable” results obtained using an automatic 3D pose estimation method. The problem is that it is diﬃcult to judge a correct pose visually and it is not",
        "clear how accurate automatic methods are with in-the-wild images. We do not see our proposed dataset as an alternative to existing datasets; rather 3DPW complements existing ones with new, more challenging, sequences.",
        "Fig.3. Method overview: By ﬁtting the SMPL body model to the measured IMU orientations we obtain initial 3D poses ˆΘ. Given all 2D poses V detected in the images we search for a globally consistent assignment of 2D to 3D poses. We jointly optimize camera poses Ψ, heading angles Γ and 3D poses Θ with respect to associated IMU and image data. In a second iteration we feed back camera poses and heading angles which provides additional information further improving the assignment and tracking results.",
        "In the following, we will refer to this tracking approach simply as the inertial tracker (IT), which outputs initial 3D pose candidates θ∗ t,l for every tracked person l. Such initial 3D poses need to be associated with 2D detections in the video in order to eﬀectively fuse the data – this poses a challenging assignment problem.",
        "4.3 Video-Inertial Data Fusion",
        "To validate our approach quantitatively (Section 5.1 and Section 5.2), we use the recent TotalCapture [39] dataset, which is the only one including IMU data and video synchronized with ground-truth. In Section 5.3 we then provide details of the newly recorded 3DPW dataset, demonstrate 3D pose reconstruction of VIP in challenging scenes, and evaluate the accuracy of automatic 2D to 3D pose assignment in multiple-person scenes.",
        "Pose assignment: In the graph G, edges e ∈ E are created between any two nodes that are at most 30 frames apart. The weights mapping from features to costs are learned using 5 sequences from 3DPW dataset, which have been manually labeled for this purpose. Given the features f deﬁned in Section 4.2 and learned weights α from logistic regression, we turn features into costs via c = −hf,αi, making the optimization problem (5) probabilistically motivated [37]. Video Inertial Fusion: Diﬀerent weighting parameters in Eq. (4) and Eq. (12) produce good results as long as they are balanced. However, rather than setting them by hand, we used Bayesian Optimization [3] in the proposed training set of TotalCapture (seen subjects). The values found are wacc = 0.2, wimg = 0.0001 and wprior = 0.006 and are kept ﬁxed for all experiments. Note, that these are very few parameters and therefore, there is very little risk of over-ﬁtting, which is also reﬂected in the results.",
        "We quantitatively evaluate tracking accuracy on the TotalCapture dataset. The dataset consists of 5 subjects performing several activities such as walking, acting, range of motions and freestyle motions – which are recorded using 8 calibrated, static RGB-cameras and 13 IMUs attached to head, sternum, waist, upper arms, lower arms, upper legs, lower legs and feet. Ground-truth poses are obtained using a marker-baser motion capture system. All data is synchronized and operates at a framerate of 60Hz. The ground truth poses are provided as joint positions, which do not contain information about pronation and supination angles; i.e. rotations about the bone’s long axis. To obtain full degree of freedom pose, we ﬁt the SMPL model to the raw ground-truth markers using a method similar to [15].",
        "Comparisons to state-of-the-art: We outperform the learning-based approach introduced in the TotalCapture dataset [39] by 44mm – the approach uses all 8 cameras and fuses IMU data with a probabilistic visual hull. We also outperform [16], who report a mean MPJPE of 62mm using 8 cameras and all 13 IMUs. Admittedly, it is diﬃcult to compare approaches, since [39] and [16] process the data in a frame-by-frame manner which is an advantage w.r.t. VIP, which jointly optimizes over all frames simultaneously. However, VIP uses only a single camera with unknown pose whereas the competitors use 8 fully calibrated cameras. To understand better the inﬂuence of components of VIP we also report the tracking accuracy for ﬁve tracker variants in Table 1.",
        "Fewer sensors: We report the error of VIP using 6 IMUs similar to [42], denoted as VIP-IMU6. The combination of only 6 IMUs and 2D pose information achieves a MPJPE of 39.6mm, which is 13.6mm higher than VIP-13 IMUs but still very accurate. This demonstrates our approach could be used for applications where a minimal number of sensors is required. This quantitative evaluation demonstrates the accuracy of VIP. Ideally, we would evaluate VIP quantitatively also in challenging scenes, like the ones in 3DPW. However, there exists no dataset with a comparable setting and ground-truth, which was one of the main motivations of this work.",
        "3D Poses in the Wild Dataset",
        "VIP allowed us to achieve the second goal of this work: recording a dataset with accurate 3D pose in challenging outdoor scenes with a moving camera. A hand-held smartphone camera was used to record one or two IMU-equipped actors performing various activities such as shopping, doing sports, hugging, discussing, capturing selﬁes, riding bus, playing guitar, relaxing. The dataset includes 60 sequences, more than 51,000 frames and 7 actors in a total of 18 clothing styles. We also scanned subjects and non-rigidly ﬁtted SMPL to obtain 3D models similar to [27,46]. For single subject tracking, we attached 17 IMUs to all major bone segments. We used 9-10 IMUs per person to simultaneously track up to 2 subjects. During all recordings one additional IMU was attached to the smartphone. Video and inertial data was automatically synchronized by a clapping motion at the beginning of a sequence as in [24]. For every sequence, the subjects were asked to start in an upright pose with closed arms. In Fig. 6 we show tracking results illustrating the 3D model alignment with the images. Fig. 7 shows more tracking results, where we animated the 3D models with the reconstructed poses. 3DPW is the most challenging dataset (with 3D pose annotation) for state-of-the-art 3D pose estimation methods as evidenced by the results reported in the supplemental material.",
        "Fig.7. We show several example frames of sequences in the 3DPW. The dataset contains large variations in person identity, clothing and activities. For a couple of cases we also show animated, textured SMPL body models.",
        "Combining IMUs and a moving camera, we introduced the ﬁrst method that can robustly recover pose in challenging scenes. The main challenges we addressed are: person identiﬁcation and tracking in cluttered scenes, and joint recovery of 3D pose for 2 subjects, camera and IMU heading drift. We combined discrete optimization to ﬁnd associations, with continuous optimization to eﬀectively fuse the sensor information. Using our method, we collected the 3D Poses in the Wild dataset, including challenging sequences with accurate 3D poses that we make available for research purposes. With VIP it is possible to record people in natural video easily and we plan to keep adding to the dataset. We anticipate the proposed dataset will provide the means to quantitatively evaluate monocular methods in diﬃcult scenes and stimulate new research in this area.",
        "8. Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 36(7), 1325–1339 (2014)",
        "32. Sigal, L., Balan, A.O., Black, M.J.: Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. International Journal of Computer Vision (IJCV) 87(1-2) (2010)"
    ]
}