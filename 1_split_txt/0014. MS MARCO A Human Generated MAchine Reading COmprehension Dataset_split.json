{
    "title_author_abstract_introduction": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang Microsoft AI & Research\nAbstract\nWe introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions— sampled from Bing’s search query logs—each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages—extracted from 3,563,535 web documents retrieved by Bing—that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difﬁculty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and ﬁnally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.\nIntroduction\nBuilding intelligent agents with machine reading comprehension (MRC) or open-domain question answering (QA) capabilities using real world data is an important goal of artiﬁcial intelligence. Progress in developing these capabilities can be of signiﬁcant consumer value if employed in automated assistants—e.g., Cortana [Cortana], Siri [Siri], Alexa [Amazon Alexa], or Google Assistant [Google Assistant]—on mobile devices and smart speakers, such as Amazon Echo [Amazon Echo]. Many of these devices rely heavily on recent advances in speech recognition technology powered by neural models with deep architectures [Hinton et al., 2012, Dahl et al., 2012]. The rising popularity of spoken interfaces makes it more attractive for users to use natural language dialog for questionanswering and information retrieval from the web as opposed to viewing traditional search result pages on a web browser [Gao et al., 2018]. Chatbots and other messenger based intelligent agents are also becoming popular in automating business processes—e.g., answering customer service requests. All of these scenarios can beneﬁt from fundamental improvements in MRC models. However, MRC in the wild is extremely challenging. Successful MRC systems should be able to learn good representations from raw text, infer and reason over learned representations, and ﬁnally generate a summarized response that is correct in both form and content.\nThe public availability of large datasets has been instrumental in many AI research breakthroughs [Wissner-Gross, 2016]. For example, ImageNet’s [Deng et al., 2009] release of 1.5 million labeled\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nexamples with 1000 object categories led to the development of object classiﬁcation models that perform better than humans on the ImageNet task [He et al., 2015]. Similarly, the large speech database collected over 20 years by DARPA enabled new breakthroughs in speech recognition performance from deep learning models Deng and Huang [2004]. Several MRC and QA datasets have also recently emerged. However, many of these existing datasets are not sufﬁciently large to train deep neural models with large number of parameters. Large scale existing MRC datasets, when available, are often synthetic. Furthermore, a common characteristic, shared by many of these datasets, is that the questions are usually generated by crowd workers based on provided text spans or documents. In MS MARCO, in contrast, the questions correspond to actual search queries that users submitted to Bing, and therefore may be more representative of a “natural” distribution of information need that users may want to satisfy using, say, an intelligent assistant.\nReal-world text is messy: they may include typos or abbreviations—and transcription errors in case of spoken interfaces. The text from different documents may also often contain conﬂicting information. Most existing datasets, in contrast, often contain high-quality stories or text spans from sources such as Wikipedia. Real-world MRC systems should be benchmarked on realistic datasets where they need to be robust to noisy and problematic inputs.\nFinally, another potential limitation of existing MRC tasks is that they often require the model to operate on a single entity or a text span. Under many real-world application settings, the information necessary to answer a question may be spread across different parts of the same document, or even across multiple documents. It is, therefore, important to test an MRC model on its ability to extract information and support for the ﬁnal answer from multiple passages and documents.\nIn this paper, we introduce Microsoft MAchine Reading Comprehension (MS MARCO)—a large scale real-world reading comprehension dataset—with the goal of addressing many of the above mentioned shortcomings of existing MRC and QA datasets. The dataset comprises of anonymized search queries issued through Bing or Cortana. We annotate each question with segment information as we describe in Section 3. Corresponding to each question, we provide a set of extracted passages from documents retrieved by Bing in response to the question. The passages and the documents may or may not actually contain the necessary information to answer the question. For each question, we ask crowd-sourced editors to generate answers based on the information contained in the retrieved passages. In addition to generating the answer, the editors are also instructed to mark the passages containing the supporting information—although we do not enforce these annotations to be exhaustive. The editors are allowed to mark a question as unanswerable based on the passages provided. We include these unanswerable questions in our dataset because we believe that the ability to recognize insufﬁcient (or conﬂicting) information that makes a question unanswerable is important to develop for an MRC model. The editors are strongly encouraged to form answers in complete sentences. In total, the MS MARCO dataset contains 1,010,916 questions, 8,841,823 companion passages extracted from 3,563,535 web documents, and 182,669 editorially generated answers. Using this dataset, we propose three different tasks with varying levels of difﬁculty:\n(i) Predict if a question is answerable given a set of context passages, and extract relevant\ninformation and synthesize the answer.\n(ii) Generate a well-formed answer (if possible) based on the context passages that can be\nunderstood with the question and passage context.\n(iii) Rank a set of retrieved passages given a question.\nWe describe the dataset and the proposed tasks in more details in the rest of this paper and present some preliminary benchmarking results on these tasks.",
    "data_related_paragraphs": [
        "Machine reading comprehension and open domain question-answering are challenging tasks [Weston et al., 2015]. To encourage more rapid progress, the community has made several different datasets and tasks publicly available for benchmarking. We summarize some of them in this section.",
        "The Stanford Question Answering Dataset (SQuAD) Rajpurkar et al. [2016] consists of 107,785 question-answer pairs from 536 articles, where each answer is a text span. The key distinction between SQUAD and MS MARCO are:",
        "Table 1: Comparison of MS MARCO and some of the other MRC datasets.",
        "Segment Question Source Answer Dataset Crowd-sourced No NewsQA Crowd-sourced DuReader No Crowd-sourced NarrativeQA No Generated No SearchQA Crowd-sourced Multiple choice No RACE Multiple choice Generated No ARC Span of words Crowd-sourced SQuAD No Human generated 1M User logs MS MARCO Yes",
        "1. The MS MARCO dataset is more than ten times larger than SQuAD—which is an important",
        "NewsQA [Trischler et al., 2017] is a MRC dataset with over 100,000 question and span-answer pairs based off roughly 10,000 CNN news articles. The goal of the NewsQA task is to test MRC models on reasoning skills—beyond word matching and paraphrasing. Crowd-sourced editors created the questions from the title of the articles and the summary points (provided by CNN) without access to the article itself. A 4-stage collection methodology was employed to generate a more challenging MRC task. More than 44% of the NewsQA questions require inference and synthesis, compared to SQuAD’s 20%.",
        "DuReader [He et al., 2017] is a Chinese MRC dataset built with real application data from Baidu search and Baidu Zhidao—a community question answering website. It contains 200,000 questions and 420,000 answers from 1,000,000 documents. In addition, DuReader provides additional annotations of the answers—labelling them as either fact based or opinionative. Within each category, they are further divided into entity, yes/no, and descriptive answers.",
        "NarrativeQA [Kociský et al., 2017] dataset contains questions created by editors based on summaries of movie scripts and books. The dataset contains about 45,000 question-answer pairs over 1,567 stories, evenly split between books and movie scripts. Compared to the news corpus used in NewsQA, the collection of movie scripts and books are more complex and diverse—allowing the editors to create questions that may require more complex reasoning. The movie scripts and books are also longer documents than the news or wikipedia article, as is the case with NewsQA and SQuAD, respectively.",
        "RACE [Lai et al., 2017] contains roughly 100,000 multiple choice questions and 27,000 passages from standardized tests for Chinese students learning English as a foreign language. The dataset is split up into: RACE-M, which has approximately 30,000 questions targeted at middle school students aged 12-15, and RACE-H, which has approximately 70,000 questions targeted at high school students aged 15 to 18. Lai et al. [2017] claim that current state of the art neural models at the time of their publishing were performing at 44% accuracy while the ceiling human performance was 95%.",
        "ReCoRD [Zhang et al., 2018] contains 12,000 Cloze-style question-passage pairs extracted from CNN/Daily Mail news articles. For each pair in this dataset, the question and the passage are selected from the same news article such that they have minimal text overlap—making them unlikely to be paraphrases of each other—but refer to at least one common named entity. The focus of this dataset is on evaluating MRC models on their common-sense reasoning capabilities.",
        "3 The MS Marco dataset",
        "To generate the 1,010,916 questions with 1,026,758 unique answers we begin by sampling queries from Bing’s search logs. We ﬁlter out any non-question queries from this set. We retrieve relevant documents for each question using Bing from its large-scale web index. Then we automatically extract relevant passages from these documents. Finally, human editors annotate passages that contain useful and necessary information for answering the questions—and compose a well-formed natural language answers summarizing the said information. Figure 1 shows the user interface for a web-based tool that the editors use for completing these annotation and answer composition tasks. During the editorial annotation and answer generation process, we continuously audit the data being generated to ensure accuracy and quality of answers—and verify that the guidelines are appropriately followed.",
        "As previously mentioned, the questions in MS MARCO correspond to user submitted queries from Bing’s query logs. The question formulations, therefore, are often complex, ambiguous, and may even contain typographical and other errors. An example of such a question issued to Bing is: “in what type of circulation does the oxygenated blood ﬂow between the heart and the cells of the body?”. We believe that these questions, while sometimes not well-formatted, are more representative of human information seeking behaviour. Another example of a question from our dataset includes: “will I qualify for osap if i’m new in Canada”. As shown in ﬁgure 1, one of the relevant passages include: “You must be a 1. Canadian citizen, 2. Permanent Resident or 3. Protected person”. When auditing our editorial process, we observe that even the human editors ﬁnd the task of answering these questions to be sometimes difﬁcult—especially when the question is in a domain the editor is unfamiliar with. We, therefore, believe that the MS MARCO presents a challenging dataset for benchmarking MRC models.",
        "The MS MARCO dataset that we are publishing consists of six major components:",
        "excluded from our dataset. This ﬁltering of question queries is performed automatically by a machine learning based classiﬁer trained previously on human annotated data. Selected questions are further annotated by editors based on whether they are answerable using the passages provided.",
        "3. Answers: For each question, the dataset contains zero, or more answers composed manually by the human editors. The editors are instructed to read and understand the questions, inspect the retrieved passages, and then synthesize a natural language answer with the correct information extracted strictly from the passages provided.",
        "4. Well-formed Answers: For some question-answer pairs, the data also contains one or more answers that are generated by a post-hoc review-and-rewrite process. This process involves a separate editor reviewing the provided answer and rewriting it if: (i) it does not have proper grammar, (ii) there is a high overlap in the answer and one of the provided passages (indicating that the original editor may have copied the passage directly), or (iii) the answer can not be understood without the question and the passage context. e.g., given the question “tablespoon in cup” and the answer “16”, the well-formed answer should be “There are 16 tablespoons in a cup.”.",
        "6. Question type: Each question is further automatically annotated using a machine learned classiﬁer with one of the following segment labels: (i) NUMERIC, (ii) ENTITY, (iii) LOCATION, (iv) PERSON, or (v) DESCRIPTION (phrase). Table 2 lists the relative size of the different question segments and compares it with the proportion of questions that explicitly contain words like “what” and “\"where”. Note that because the questions in our dataset are based on web search queries, we are may observe a question like “what is the age of barack obama” be expressed simply as “barack obama age” in our dataset.",
        "Table 3: The MS MARCO dataset format.",
        "Table 3 describes the ﬁnal dataset format for MS MARCO. Inspired by [Gebru et al., 2018] we also release our dataset’s datasheet on our website. Finally, we summarize the key distinguishing features of the MS MARCO dataset as follows:",
        "3.1 The passage ranking dataset",
        "To facilitate the benchmarking of ML based retrieval models that beneﬁt from supervised training on large datasets, we are releasing a passage collection—constructed by taking the union of all the passages in the MS MARCO dataset—and a set of relevant question and passage identiﬁer pairs. To identify the relevant passages, we use the is_selected annotation provided by the editors. As the editors were not required to annotate every passage that were retrieved for the question, this annotation should be considered as incomplete—i.e., there are likely passages in the collection that contain the answer to a question but have not been annotated as is_selected: 1. We use this dataset to propose a re-ranking challenge as described in Section 4. Additionally, we are organizing a “Deep Learning” track at the 2019 edition of TREC2 where we use these passage and question collections to setup an ad-hoc retrieval task.",
        "Using the MS MARCO dataset, we propose three machine learning tasks of diverse difﬁculty levels:",
        "ranking of the said passages based on how likely they are to contain information relevant to answer the question. This task is targeted to provide a large scale dataset for benchmarking emerging neural IR methods [Mitra and Craswell, 2018].",
        "We continue to develop and reﬁne the MS MARCO dataset iteratively. Presented at NIPS 2016 the V1.0 dataset was released and recieved with enthusiasm In January 2017, we publicly released the 1.1 version of the dataset. In Section 5.1, we present our initial benchmarking results based on this dataset. Subsequently, we release 2.0 the v2.1 version of the MS MARCO dataset in March 2018 and April 2018 respectively. Section 5.2 covers the experimental results on the update dataset. Finally, in October 2018, we released additional data ﬁles for the passage ranking task.",
        "5.1 Experimental results on v1.1 dataset",
        "The following experiments were run on the V1.1 dataset",
        "In Cloze-style tests, a model is required to predict missing words in a text sequence by considering contextual information in textual format. CNN and Daily Mail dataset [Hermann et al., 2015b] is an example of such a cloze-style QA dataset. In this section, we present the performance of two MRC models using both CNN test dataset and a MS MARCO subset. The subset is ﬁltered to numeric answer type category, to which cloze-style test is applicable.",
        "We show model accuracy numbers on both datasets in table 6, and precision-recall curves on MS MARCO subset in ﬁgure 2.",
        "5.2 Experimental results on v2.1 dataset",
        "The human baseline on our v1.1 benchmark was surpassed by competing machine learned models in approximately 15 months. For the v2.1 dataset, we revisit our approach to generating the human baseline. We select ﬁve top performing editors—based on their performance on a set of auditing questions—to create a human baseline task group. We randomly sample 1,427 questions from our evaluation set and ask each of these editors to produce a new assessment. Then, we compare all our editorial answers to the ground truth and select the answer with the best ROUGE-L score as the candidate answer. Table 7 shows the results. We evaluate the answer set on both the novice and the intermediate task and we include questions that have no answer.",
        "To provide a competitive experimental baseline for our dataset, we trained the model introduced in [Clark and Gardner, 2017]. This model uses recent ideas in reading comprehension research, like self-attention [Cheng et al., 2016] and bi-directional attention [Seo et al., 2016]. Our goal is to train this model such that, given a question and a passage that contains an answer to the question, the model identiﬁes the answer (or span) in the passage. This is similar to the task in SQuAD [Rajpurkar et al., 2016]. First, we select the question-passage pairs where the passage contains an answer to the question and the answer is a contiguous set of words from the passage. Then, we train the model to predict a span for each question-passage pair and output a conﬁdence score. To evaluate the model,",
        "for each question we chose our model generated answer that has the highest conﬁdence score among all passages available for that question. To compare model performance across datasets we run this exact setup (training and evaluation) on the original dataset and the new V2 Tasks. Table 7 shows the results. The results indicate that the new v2.1 dataset is more difﬁcult than the previous v1.1 version. On the novice task BiDaF cannot determine when the question is not answerable and thus performs substantially worse compared to on the v1.1 dataset. On the intermediate task, BiDaF performance once again drops because the model only uses vocabulary present in the passage whereas the well-formed answers may include words from the general vocabulary.",
        "The process of developing the MS MARCO dataset and making it publicly available has been a tremendous learning experience. Between the ﬁrst version of the dataset and the most recent edition, we have signiﬁcantly modiﬁed how we collect and annotate the data, the deﬁnition of our tasks, and even broadened our scope to cater to the neural IR community. The future of this dataset will depend largely on how the broader academic community makes use of this dataset. For example, we believe that the size and the underlying use of Bing’s search queries and web documents in the construction of the dataset makes it particularly attractive for benchmarking new machine learning models for MRC and neural IR. But in addition to improving these ML models, the dataset may also prove to be useful for exploring new metrics—e.g., ROUGE-2 [Ganesan, 2018] and ROUGE-AR[Maples, 2017]—and robust evaluation strategies. Similarly, combining MS MARCO with other existing MRC datasets may also be interesting in the context of multi-task and cross domain learning. We want to engage with the community to get their feedback and guidance on how we can make it easier to enable such new explorations using the MS MARCO data. If there is enough interest, we may also consider generating similar datasets in other languages in the future—or augment the existing dataset with other information from the web.",
        "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fe. Imagenet: Alarge-scalehierarchicalimagedatabas.",
        "M. Dunn, L. Sagun, M. Higgins, V. U. Güney, V. Cirik, and K. Cho. Searchqa: A new q&a dataset augmented",
        "2017/10/23/google-brain-chief-says-100000-examples-is-enough-data-for-deep-learning/, 2017.",
        "T. Gebru, J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. D. III, and K. Crawford. Datasheets for",
        "datasets. 2018.",
        "W. He, K. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, H. Wu, Q. She, X. Liu, T. Wu, and H. Wang. Dureader: a chinese machine reading comprehension dataset from real-world applications. CoRR, abs/1711.05073, 2017.",
        "P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management, pages 2333–2338. ACM, 2013.",
        "G. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy. Race: Large-scale reading comprehension dataset from",
        "data. 2016.",
        "comprehension dataset. In Rep4NLP@ACL, 2017.",
        "A. Wissner-Gross. Datasets over algorithms. Edge. com. Retrieved, 8, 2016."
    ]
}