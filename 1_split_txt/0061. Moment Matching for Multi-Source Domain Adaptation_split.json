{
    "title_author_abstract_introduction": "Moment Matching for Multi-Source Domain Adaptation\nXingchao Peng Boston University xpeng@bu.edu\nQinxun Bai Horizon Robotics qinxun.bai@horizon.ai\nXide Xia Boston University xidexia@bu.edu\nZijun Huang Columbia University zijun.huang@columbia.edu\nKate Saenko Boston University saenko@bu.edu\nBo Wang Vector Institute & Peter Munk Cardiac Center bowang@vectorinstitute.ai\nAbstract\nConventional unsupervised domain adaptation (UDA) assumes that training data are sampled from a single domain. This neglects the more practical scenario where training data are collected from multiple sources, requiring multi-source domain adaptation. We make three major contributions towards addressing this problem. First, we collect and annotate by far the largest UDA dataset, called DomainNet, which contains six domains and about 0.6 million images distributed among 345 categories, addressing the gap in data availability for multi-source UDA research. Second, we propose a new deep learning approach, Moment Matching for Multi-Source Domain Adaptation (M3SDA), which aims to transfer knowledge learned frommultiplelabeledsourcedomainstoanunlabeledtarget domain by dynamically aligning moments of their feature distributions. Third, we provide new theoretical insights speciﬁcally for moment matching approaches in both single and multiple source domain adaptation. Extensive experiments are conducted to demonstrate the power of our new dataset in benchmarking state-of-the-art multi-source domain adaptation methods, as well as the advantage of our proposed model. Dataset and Code are available at http://ai.bu.edu/M3SDA/\n1. Introduction\nGeneralizing models learned on one visual domain to novel domains has been a major obstacle in the quest for universal object recognition. The performance of the learnedmodelsdegradessigniﬁcantlywhentestingonnovel domains due to the presence of domain shift [36].\nRecently, transfer learning and domain adaptation methods have been proposed to mitigate the domain gap. For example, several UDA methods [27, 41, 25] incorporate Maximum Mean Discrepancy loss into a neural network to diminish the domain discrepancy; other models introduce different learning schema to align the source and target domains, including aligning second order correlation [39, 32],\nFigure 1. We address Multi-Source Domain Adaptation where source images come from multiple domains. We collect a large scale dataset, DomainNet, with six domains, 345 categories, and ∼0.6 million images and propose a model (M3SDA) to transfer knowledge from multiple source domains to an unlabeled target domain.\nmoment matching [47], adversarial domain confusion [40, 8, 38] and GAN-based alignment [50, 15, 23]. However, most of current UDA methods assume that source samples are collected from a single domain. This assumption neglects the more practical scenarios where labeled images are typically collected from multiple domains. For example, thetrainingimagescanbetakenunderdifferentweather or lighting conditions, share different visual cues, and even have different modalities (as shown in Figure 1).\nIn this paper, we consider multi-source domain adaptation (MSDA), a more difﬁcult but practical problem of knowledge transfer from multiple distinct domains to one unlabeled target domain. The main challenges in the research of MSDA are that: (1) the source data has multiple domains, which hampers the effectiveness of mainstream single UDA method; (2) source domains also possess domain shift with each other; (3) the lack of large-scale multidomain dataset hinders the development of MSDA models. In the context of MSDA, some theoretical analysis [1, 28, 4, 49, 14] has been proposed for multi-source domain\nsketchrealquickdrawpaintinginfographclipartairplaneclockaxeballbicyclebirdstrawberryflowerpizzabutterfly\nDataset Digit-Five Ofﬁce [37]\n4,110 2010 2,533 Ofﬁce-Caltech [11] 2012 2015 12,000 CAD-Pascal [33] Ofﬁce-Home [43] 2017 15,500 2017 9,991 2018 16,156 2018 280,157 569,010\nImages Classes Domains Description 5 3 4 6 4 4 - 3 6\ndigit ofﬁce ofﬁce animal,vehicle ofﬁce, home animal, stuff PACS [21] museum Open MIC [17] animal,vehicle Syn2Real [35] DomainNet (Ours) see Appendix Table 1. A collection of most notable datasets to evaluate domain adaptation methods. Speciﬁcally, “Digit-Five” dataset indicates ﬁve most popular digit datasets (MNIST [19], MNIST-M [8], Synthetic Digits [8], SVHN, and USPS) which are widely used to evaluate domain adaptation models. Our dataset is challenging as it containsmoreimages, categories, anddomainsthanotherdatasets. (see Table 10, Table 11, and Table 12 in Appendix for detailed categories.)\nadaptation (MSDA). Ben-David et al [1] pioneer this direction by introducing an H∆H-divergence between the weighted combination of source domains and target domain. More applied works [6, 45] use an adversarial discriminator to align the multi-source domains with the target domain. However, these works focus only on aligning the source domains with the target, neglecting the domain shift between the source domains. Moreover, H∆Hdivergence based analysis does not directly correspond to moment matching approaches.\nIn terms of data, research has been hampered due to the lack of large-scale domain adaptation datasets, as state-ofthe-art datasets contain only a few images or have a limited number of classes. Many domain adaptation models exhibit saturation when evaluated on these datasets. For example, many methods achieve ∼90 accuracy on the popular Ofﬁce [37] dataset; Self-Ensembling [7] reports ∼99% accuracy on the “Digit-Five” dataset and ∼92% accuracy on Syn2Real [35] dataset.\nIn this paper, we ﬁrst collect and label a new multidomain dataset called DomainNet, aiming to overcome benchmark saturation. Our dataset consists of six distinct domains, 345 categories and ∼0.6 million images. A comparison of DomainNet and several existing datasets is shown in Table 1, and example images are illustrated in Figure 1. We evaluate several state-of-the-art single domain adaptation methods on our dataset, leading to surprising ﬁndings (see Section 5). We also extensively evaluate our model on existing datasets and on DomainNet and show that it outperforms the existing single- and multi-source approaches.\nSecondly, we propose a novel approach called M3SDA to tackle MSDA task by aligning the source domains with the target domain, and aligning the source domains with each other simultaneously. We dispose multiple complex adversarial training procedures presented in [45], but di-\nrectly align the moments of their deep feature distributions, leading to a more robust and effective MSDA model. To our best knowledge, we are the ﬁrst to empirically demonstrate that aligning the source domains is beneﬁcial for MSDA tasks.\nFinally, we extend existing theoretical analysis [1, 14, 49] to the case of moment-based divergence between source and target domains, which provides new theoretical insight speciﬁcally for moment matching approaches in domain adaptation, including our approach and many others.",
    "data_related_paragraphs": [
        "Domain Adaptation Datasets Several notable datasets that can be utilized to evaluate domain adaptation approaches are summarized in Table 1. The Ofﬁce dataset [37] is a popular benchmark for ofﬁce environment objects. It contains 31 categories captured in three domains: ofﬁce environment images taken with a high quality camera (DSLR), ofﬁce environment images taken with a low quality camera (Webcam), and images from an online merchandising website (Amazon). The ofﬁce dataset and its extension, OfﬁceCaltech10 [11], have been used in numerous domain adaptation papers [25, 40, 27, 39, 45], and the adaptation performance has reached ∼90% accuracy. More recent benchmarks [43, 17, 34] are proposed to evaluate the effectiveness of domain adaptation models. However, these datasets are small-scale and limited by their speciﬁc environments, such as ofﬁce, home, and museum. Our dataset contains about 600k images, distributed in 345 categories and 6 distinct domains. We capture various object divisions, ranging from furniture, cloth, electronic to mammal, building, etc.",
        "Single-source UDA Over the past decades, various singlesource UDA methods have been proposed. These methods can be taxonomically divided into three categories. The ﬁrst category is the discrepancy-based DA approach, which utilizes different metric learning schemas to diminish the domain shift between source and target domains. Inspired by the kernel two-sample test [12], Maximum Mean Discrepancy (MMD) is applied to reduce distribution shift in various methods [27, 41, 9, 44]. Other commonly used methods include correlation alignment [39, 32], Kullback-Leibler (KL) divergence [51], and H divergence [1]. The second category is the adversarial-based approach [24, 40]. A domain discriminator is leveraged to encourage the domain confusion by an adversarial objective. Among these approaches, generative adversarial networks are widely used to learn domain-invariant features as well to generate fake source or target data. Other frameworks utilize only adversarial loss to bridge two domains. The third category is reconstruction-based, which assumes the data reconstruction helps the DA models to learn domain-invariant features. The reconstruction is obtained via an encoder-",
        "Figure 2. Statistics for our DomainNet dataset. The two plots show object classes sorted by the total number of instances. The top ﬁgure shows the percentages each domain takes in the dataset. The bottom ﬁgure shows the number of instances grouped by 24 different divisions. Detailed numbers are shown in Table 10, Table 11 and Table 12 in Appendix. (Zoom in to see the exact class names!)",
        "decoder [3, 10] or a GAN discriminator, such as dualGAN [46], cycle-GAN [50], disco-GAN [16], and CyCADA [15]. Though these methods make progress on UDA, few of them consider the practical scenario where training data are collected from multiple sources. Our paper proposes a model to tackle multi-source domain adaptation, which is a more general and challenging scenario. Multi-Source Domain Adaptation Compared with single source UDA, multi-source domain adaptation assumes that training data from multiple sources are available. Originated from the early theoretical analysis [1, 28, 4], MSDA has many practical applications [45, 6]. Ben-David et al [1] introduce an H∆H-divergence between the weighted combination of source domains and target domain. Crammer et al [4] establish a general bound on the expected loss of the model by minimizing the empirical loss on the nearest k sources. Mansour et al [28] claim that the target hypothesis can be represented by a weighted combination of source hypotheses. In the more applied works, Deep Cocktail Network (DCTN) [45] proposes a k-way domain discriminator and category classiﬁer for digit classiﬁcation and real-world object recognition. Hoffman et al [14] propose normalized solutions with theoretical guarantees for cross-entropy loss, aiming to provide a solution for the MSDA problem with very practical beneﬁts. Duan et al [6] propose Domain Selection Machine for event recognition in consumer videos by leveraging a large number of loosely labeled web images from different sources. Different from these methods, our model directly matches all the distributions by matching the moments. Moreover, we provide a concrete proof of why matching the moments of multiple distributions works for multi-source domain adaptation. Moment Matching The moments of distributions have been studied by the machine learning community for a long In order to diminish the domain discrepancy betime.",
        "3. The DomainNet dataset",
        "is well-known that deep models require massive amounts of training data. Unfortunately, existing datasets for visual domain adaptation are either small-scale or limited in the number of categories. We collect by far the largest domain adaptation dataset to date, DomainNet . The DomainNet contains six domains, with each domain containing 345 categories of common objects, as listed in Table 10, Table 11, and Table 12 (see Appendix). The domains include Clipart (clp, see Appendix, Figure 9): collection of clipart images; Infograph (inf, see Figure 10): infographic images with speciﬁc object; Painting (pnt, see Figure 11): artistic depictions of objects in the form of paintings; Quickdraw (qdr, see Figure 12): drawings of the worldwide players of game “Quick Draw!”1; Real (rel, see Figure 13): photos and real world images; and Sketch",
        "1https://quickdraw.withgoogle.com/data",
        "treegolf clubsquirreldogwhalespreadsheetsnowmantigertableshoewindmillsubmarinefeatherbirdspiderstrawberrynailbeardbreadtrainwatermelonzebrasheepelephantteapoteyemushroomsea turtleswordstreetlightlighthouseowlhorsepenguinpondsocksnorkelhelicoptersnakebutterflyumbrellariverfishvangrapeshot air balloonwine glassteddy-bearspeedboatsunswanbicyclebrainbracelettornadoflowerstairscupsteaktractorwristwatchtoothbrushsuitcasetriangleparrotzigzagice creammugbeachcatraccoongardenmonkeysharkanimal migrationlionsaxophoneasparagustentfiretruckhandspoonsquigglepalm treeoctopustoasterskateboarddumbbellmountainbottlecappigshovelwashing machinewine bottlestovecoffee cupleafgoateedrumsyogabowtiesailboatscissorsonionsnailbushouse plantmapmoonlobstercanoepineapplenecklacebasketbearenvelopebeegrassmotorbikebeddonutfacehatskullschool busdolphincruise shiptoothpasteblueberryshortseyeglassesbackpackbookbroccoliduckhamburgerhelmetcakerhinocerosladdertrombonehedgehogtelevisionscorpionpearflashlightbarnlegoceantelephonebenchpillowhot tubfenceflamingowaterslidecrocodilesweatermoustacherollerskatescirclegiraffesyringepoolcrabcandlecarrotsoccer ballbroomsandwichsnowflakeparachutecastlesleeping bagtoothbinocularskangarooriflewheelpickup truckhot dogpantspandascrewdriverpolice carcamellightningpencilarmmicrophonefireplacemegaphonepianolollipoptrumpetkeyboardpeasmosquitotennis racquettraffic lightflip flopsflying saucerdragonhousecowbeltThe Great Walldiamondbandageangelmermaidplierslaptoprainkneebathtubcrayoncactusgarden hosediving boardpursecouchhockey puckjacketfire hydrantblackberrybataxeswing setlipstickfrying pansinkbasketballt-shirtcookietoestereorakepaper clipboomerangalarm clockchurchbaseball batambulancemailboxpaintbrushsmiley facepostcardremote controlpotatoearhospitalThe Mona Lisaknifeforkantcamerapaint cancoolerjailstring beanlinemousepicture framehourglasslighterfloor lamphurricanedishwashercellocomputerbushmarkerpeanutclarinetradiobucketovenstitchesoctagonbaseballfanchandelieranvilchairaircraft carrierhammerstarcrownmicrowaveerasercompasskeystop signcannoncalculatormatchessawcamouflagerainbowdressercalendarceiling fan010002000Number of Instancesclipartinfographpaintingquickdrawrealsketchtableteapotstreetlightumbrellawine glassstairsvasetoothbrushsuitcasetoiletstovebedtoothpastesee sawladderbenchpillowhot tubfencedoorsleeping bagfireplacelanternbathtubcouchswing setsinkmailboxpostcardpicture framefloor lampchandelierchairdresserceiling fansquirreldogwhaletigerzebrasheepelephanthorsecatraccoonmonkeylionpigbeardolphinrabbitrhinoceroshedgehoggiraffekangaroopandacamelcowbatmousenailswordstethoscopeskateboarddumbbellbottlecapshovelbasketsyringebroomriflewheelscrewdriverbandagepliersaxerakeboomerangdrillpaint canpassportbucketstitchesanvilhammercompasskeysawshoesockbraceletwristwatchbowtienecklacehatshortseyeglasseshelmetsweaterrollerskatespantsflip flopsunderwearbeltdiamondpursejacketlipstickt-shirtcrowncamouflagespreadsheettoasterheadphoneswashing machinelight bulbtelevisionflashlighttelephonemicrophonemegaphonekeyboardlaptopstereopower outletremote controlcell phonecameracoolerdishwashercomputerradioovenfanmicrowavecalculatorgolf clubwindmilllighthousebridgepondgardententThe Eiffel TowersquareskyscraperbarnwaterslidepoolcastlehouseThe Great Wallgarden hosediving boardchurchhospitaljailnailcupmugcoffee cupscissorsmapenvelopebackpackbookcandlebinocularspencilclockbandagecrayonpaper clipalarm clockpaintbrushmarkererasercalendarbeardeyebrainhandgoateefaceskullnoselegmoustachefingertoothfootarmkneetoeelbowsmiley facemouthtrucktrainvanbicycletractorfiretruckbusmotorbikeschool buscarpickup truckpolice carbulldozerroller coasterambulancebreadsteakice creamdonutpopsiclehamburgercakepizzasandwichhot doglollipopcookiebirthday cakepeanutriversuntornadobeachmountainmoonoceansnowflakelightningcloudrainhurricanestarrainbowspidersea turtlesnakefishsharkoctopusfrogsnaillobsterscorpioncrocodilecrabsnowmanfeatherteddy-bearanimal migrationtraffic lightdragonangelmermaidcampfirefire hydrantThe Mona Lisastop signcannonsaxophonedrumsviolinguitartromboneharppianotrumpetcelloclarinetstrawberrywatermelongrapespineappleblueberrypearbananablackberryapplesnorkelyogasoccer ballhockey sticktennis racquetflying saucerhockey puckbasketballbaseball batbaseballtreeflowerpalm treeleafhouse plantgrasscactusbushbirdowlpenguinswanparrotduckflamingomushroomasparagusonionbroccolicarrotpeaspotatostring beantrianglezigzagsquigglehexagoncirclelineoctagonspoonwine bottlefrying panknifeforkhourglasslightermatchessubmarinespeedboatsailboatcanoecruise shipaircraft carrierhelicopterhot air balloonparachuteairplanebutterflybeemosquitoant010002000Number of Instancesfurniture (9.93%)mammal (8.22%)tool (7.33%)cloth (6.48%)electricity (6.45%)building (6.39%)office (5.76%)human body (5.52%)road transport (4.64%)food (4.04%)nature (3.93%)cold blooded (3.92%)other (3.60%)music (2.80%)fruit (2.79%)sport (2.66%)tree (2.54%)bird (2.40%)vegetable (2.31%)shape (2.04%)kitchen (1.97%)water transport (1.88%)sky transport (1.21%)insect (1.15%)\fFigure 3. The framework of Moment Matching for Multi-source Domain Adaptation (M3SDA). Our model consists of three components: i) feature extractor, ii) moment matching component, and iii) classiﬁers. Our model takes multi-source annotated training data as input and transfers the learned knowledge to classify the unlabeled target samples. Without loss of generality, we show the i-th domain and j-th domain as an example. The feature extractor maps the source domains into a common feature space. The moment matching component attempts to match the i-th and j-th domains with the target domain, as well as matching the i-th domain with the j-th domain. The ﬁnal predictions of target samples are based on the weighted outputs of the i-th and j-th classiﬁers. (Best viewed in color!)",
        "The images from clipart, infograph, painting, real, and sketch domains are collected by searching a category name combined with a domain name (e.g. “aeroplane painting”) in different image search engines. One of the main challenges is that the downloaded data contain a large portion of outliers. To clean the dataset, we hire 20 annotators to manually ﬁlter out the outliers. This process took around 2,500 hours (more than 2 weeks) in total. To control the annotation quality, we assign two annotators to each image, and only take the images agreed by both annotators. After the ﬁltering process, we keep 423.5k images from the 1.2 million images crawled from the web. The dataset has an average of around 150 images per category for clipart and infograph domain, around 220 per category for painting and sketch domain, and around 510 for real domain. A statistical overview of the dataset is shown in Figure 2.",
        "The quickdraw domain is downloaded directly from https://quickdraw.withgoogle.com/. The raw data are presented as a series of discrete points with temporal information. We use the B-spline [5] algorithm to connect all the points in each strike to get a complete drawing. We choose 500 images for each category to form the quickdraw domain, which contains 172.5k images in total. 4. Moment Matching for Multi-Source DA",
        "These three training steps are performed periodically until the whole network converges. Ensemble Schema In the testing phase, testing data from the target domain are forwarded through the feature generator and the N classiﬁers. We propose two schemas to combine the outputs of the classiﬁers:",
        "We perform an extensive evaluation on the following tasks: digit classiﬁcation (MNIST, SVHN, USPS, MNIST-M, Sythetic Digits), and image recognition (Ofﬁce-Caltech10, DomainNet dataset). In total, we conduct 714 experiments. The experiments are run on a GPU-cluster with 24 GPUs and the total running time is more than 21,440 GPU-hours. Due to space limitations, we only report major results; more implementation details are provided in the supplementary material. Throughout the experiments, we set the trade-off parameter λ in Equation 2 as 0.5. In terms of the parameter sensitivity, we have observed that the performance variation is not signiﬁcant if λ is between 0.1∼1. All of our experiments are implemented in the PyTorch3 platform.",
        "Five digit datasets are sampled from ﬁve different sources, namely MNIST [19], Synthetic Digits [8], MNISTM [8], SVHN, and USPS. Following DCTN [45], we sample 25000 images from training subset and 9000 from testing subset in MNIST, MINST-M, SVHN, and Synthetic Digits. USPS dataset contains only 9298 images in total, so we take",
        "the entire dataset as a domain. In all of our experiments, we take turns to set one domain as the target domain and the rest as the source domains.",
        "The results are shown in Table 2. Our model M3SDA achieves an 86.13% average accuracy, and M3SDA-β boosts the performance to 87.65%, outperforming other baselines by a large margin. One interesting observation is that the results on MNIST-M dataset is lower. This phenomenon is probably due to the presence of negative transfer [31]. For a fair comparison, all the experiments are based on the same network architecture. For each experiment, we run the same setting for ﬁve times and report the mean and standard deviation. (See Appendix for detailed experiment settings and analyses.)",
        "The Ofﬁce-Caltech10 [11] dataset is extended from the standard Ofﬁce31 [37] dataset. It consists of the same 10 object categories from 4 different domains: Amazon, Caltech, DSLR, and Webcam.",
        "The experimental results on Ofﬁce-Caltech10 dataset are shown in Table 4. Our model M3SDA gets a 96.1% average accuracy on this dataset, and M3SDA-β further boosts the performance to 96.4%. All the experiments are based on ResNet-101 pre-trained on ImageNet. As far as we",
        "skt Avg. clp N/A 9.1 23.2 13.7 37.6 28.6 22.4 clp 65.5 8.2 21.4 10.5 36.1 10.8 17.4 inf inf 17.9 N/A 16.4 2.1 27.8 13.3 15.5 32.9 27.7 23.8 2.2 26.4 13.7 19.8 pnt pnt 29.1 8.6 N/A 5.1 41.5 24.7 21.8 28.1 7.5 57.6 2.6 41.6 20.8 20.1 qdr qdr 16.8 1.8 4.8 N/A 9.3 10.2 8.6 13.4 1.2 2.5 68.0 5.5 7.1 5.9 rel rel 36.5 11.4 33.9 5.9 N/A 24.5 22.4 36.9 10.2 33.9 4.9 72.8 23.1 21.8 skt skt 35.5 7.1 21.9 11.8 30.8 56.3 21.4 37.9 8.2 26.3 12.2 35.3 N/A 24.0 29.4 6.8 20.7 6.4 28.1 15.1 17.8 Avg. 27.6 8.0 21.0 10.5 29.9 21.2 19.7 Avg. 26.3 7.1 22.2 10.2 30.8 19.5 19.4 Avg. 27.6 7.8 20.9 7.8 30.3 20.3 19.1 Avg. skt Avg. RTN clp inf pnt qdr rel SE clp N/A 9.7 12.2 2.2 33.4 23.1 16.1 clp N/A 8.1 21.1 13.1 36.1 26.5 21.0 inf inf 10.3 N/A 9.6 1.2 13.1 6.9 8.2 15.6 N/A 15.3 3.4 25.1 12.8 14.4 pnt pnt 17.1 9.4 N/A 2.1 28.4 15.9 14.6 26.8 8.1 N/A 5.2 40.6 22.6 20.7 qdr qdr 13.6 3.9 11.6 N/A 16.4 11.5 11.4 15.1 1.8 4.5 N/A 8.5 8.9 7.8 rel rel 31.7 12.9 19.9 3.7 N/A 26.3 18.9 35.3 10.7 31.7 7.5 N/A 22.9 21.6 skt skt 34.1 7.4 23.3 12.6 32.1 N/A 21.9 18.7 7.8 12.2 7.7 28.9 N/A 15.1 25.4 7.2 19.2 8.4 28.4 18.7 17.9 Avg. 28.2 9.3 20.1 8.4 31.1 21.7 19.8 Avg. 31.4 13.1 24.9 2.2 35.7 23.9 21.9 Avg. 18.3 8.7 13.1 3.4 24.1 16.7 14.1 Avg. Table 3. Single-source baselines on the DomainNet dataset. Several single-source adaptation baselines are evaluated on the DomainNet dataset, including AlexNet [18], DAN [25], JAN [27], DANN [8], RTN [26], ADDA [40], MCD [38], SE [7]. In each sub-table, the column-wise domains are selected as the source domain and the row-wise domains are selected as the target domain. The green numbers represent the average performance of each column or row. The red numbers denote the average accuracy for all the 30 (source, target) combinations.",
        "Table 4. Results on Ofﬁce-Caltech10 dataset. A,C,W and D represent Amazon, Caltech, Webcam and DSLR, respectively. All the experiments are based on ResNet-101 pre-trained on ImageNet.",
        "know, our models achieve the best performance among all the results ever reported on this dataset. We have also tried AlexNet, but it did not work as well as ResNet-101.",
        "Single-Source Adaptation To demonstrate the intrinsic difﬁculty of DomainNet, we evaluate multiple state-ofthe-art algorithms for single-source adaptation: Deep Alignment Network (DAN) [25], Joint Adaptation Network (JAN) [27], Domain Adversarial Neural Network (DANN) [8], Residual Transfer Network (RTN) [26], Adversarial Deep Domain Adaptation (ADDA) [40], Maximum Classiﬁer Discrepancy (MCD) [38], and SelfEnsembling (SE) [7]. As the DomainNet dataset contains 6 domains, experimentsfor30different(sources, target)combinationsareperformedforeachbaseline. Foreachdomain, wefollowa70%/30%splitschemetoparticipateourdataset into training and testing trunk. The detailed statistics can be viewed in Table 8 (see Appendix). All other experimental settings (neural network, learning rate, stepsize, etc.) are kept the same as in the original papers. Speciﬁcally, DAN, JAN, DANN, and RTN are based on AlexNet [18], ADDA and MCD are based on ResNet-101 [13], and SE is based on ResNet-152 [13]. Table 3 shows all the source-only and experimental results. (Source-only results for ResNet-101",
        "and ResNet-152 are in Appendix, Table 7). The results show that our dataset is challenging, especially for the infograph and quickdraw domain. We argue that the difﬁculty is mainly introduced by the large number of categories in our dataset. Multi-Source Domain Adaptation DomainNet contains six domains. Inspired by Xu et al [45], we introduce two MSDA standards: (1) single best, reporting the single bestperforming source transfer result on the test set, and (2) source combine, combining the source domains to a single domain and performing traditional single-source adaptation. The ﬁrst standard evaluates whether MSDA can improve the best single source UDA results; the second testify whether MSDA is necessary to exploit. Baselines For both single best and source combine experiment setting, we take the following state-of-the-art methods as our baselines: Deep Alignment Network (DAN) [25], Joint Adaptation Network (JAN) [27], Domain Adversarial Neural Network (DANN) [8], Residual Transfer Network (RTN) [26], Adversarial Deep Domain Adaptation (ADDA) [40], Maximum Classiﬁer Discrepancy (MCD) [38], and Self-Ensembling (SE) [7]. For multisource domain adaptation, we take Deep Cocktail Network (DCTN) [45] as our baseline. Results The experimental results of multi-source domain",
        "clp,inf,qdr, rel,skt→pnt 33.9 ± 0.62 33.3±0.62 31.7±0.82 32.5±0.65 33.9±0.60 29.1±0.78 19.9±0.75 42.6±0.98 38.1±0.45 36.2±0.58 35.3±0.59 35.4±0.50 37.0±0.69 36.7±0.53 12.7±0.35 45.7±0.63 48.8±0.63 50.5±0.45 51.6±0.44 52.3±0.55 57.6±0.49 66.3±0.67 68.1 ± 0.49 Table 5. Multi-source domain adaptation results on the DomainNet dataset. Our model M3SDA and M3SDA-β achieves 41.5% and 42.6% accuracy, signiﬁcantly outperforming all other baselines. M3SDA∗ indicates the normal average of all the classiﬁers. When the target domain is quickdraw, the multi-source methods perform worse than single-source and source only baselines, which indicates negative transfer [31] occurs in this case. (clp: clipart, inf: infograph, pnt: painting, qdr: quickdraw, rel: real, skt: sketch.)",
        "adaptation are shown in Table 5. We report the results of the two different weighting schemas and all the baseline results in Table 5. Our model M3SDA achieves an average accuracy of 41.5%, and M3SDA-β boosts the performance to 42.6%. The results demonstrate that our models designed for MSDA outperform the single best UDA results, the source combine results, and the multi-source baseline. From the experimental results, we make three interesting (1)The performance of M3SDA∗ is 40.8%. observations. After applying the weight vector W, M3SDAimproves the meanaccuracyby0.7percent. (2)Inclp,inf,pnt,rel,skt→qdr setting, the performances of our models are worse than source-only baseline, which indicates that negative transfer [31] occurs. (3) In the source combine setting, the performances of DAN [25], RTN [26], JAN [27], DANN [8] are lower than the source only baseline, indicating the negative transfer happens when the training data are from multiple source domains. Effect of Category Number To show how the number of categories affects the performance of state-of-the-art domainadaptationmethods, wechoosethepainting→realsetting in DomainNet and gradually increase the number of category from 20 to 345. The results are in Figure 4. An interesting observation is that when the number of categories is small (which is exactly the case in most domain adaptation benchmarks), all methods tend to perform well. However, their performances drop at different rates when the number of categories increases. For example, SE [7] per-",
        "In this paper, we have collected, annotated and evaluated by far the largest domain adaptation dataset named DomainNet. The dataset is challenging due to the presence of notable domain gaps and a large number of categories. We hope it will be beneﬁcial to evaluate future single- and multi-source UDA methods.",
        "We thank Ruiqi Gao, Yizhe Zhu, Saito Kuniaki, Ben Usman, Ping Hu for their useful discussions and suggestions. We thank anonymous annotators for their hard work to label the data. This work was partially supported by NSF and Honda Research Institute. The authors also acknowledge support from CIFAR AI Chairs Program.",
        "[31] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345–1359, 2010. 6, 8, 14",
        "[36] Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. Dataset Shift in Machine Learning. The MIT Press, 2009. 1",
        "The appendix is organized as follows: Section A shows the ablation study for source-source alignment. Section B introduces the formal deﬁnition of the cross-moment divergence; Section C gives the proof of Theorem 1 and further discussions; Section D provides the details of experiments on “Digit-Five” dataset; Section E shows feature visualization with t-SNE plot; Section F shows how the number of categories will affect the performance of the state-of-theart models; Section G and Section H introduce the ResNet baselines and Train/Test split of our DomainNet dataset, respectively; Section I and Section J show the image samples and the detailed statistics of our DomainNet dataset; Section K shows a toy experiment to demonstrate the importance of aligning the source domains; Section L shows the time consumption of our method, compared to baseline.",
        "Table 7. Single-source ResNet101 and ResNet152 [13] baselines on the DomainNet dataset. We provide ResNet baselines for Table 3. In each sub-table, the column-wise domains are selected as the source domain and the row-wise domains are selected as the target domain. The green numbers represent the average performance of each column or row. The red numbers denote the average accuracy for all the 30 (source, target) combinations. (clp: clipart, inf: infograph, pnt: painting, qdr: quickdraw, rel: real, skt: sketch.)",
        "We show the detailed number of images we used in our experiments in Table 8. For each domain, we follow a 70%/30% schema to split the dataset to training and testing trunk. The “Per-Class” row shows the average number of images that each category contains.",
        "J. Dataset Statistics",
        "Table 10, Table 11, and Table 12 show the detailed statistics of our DomainNet dataset. Our dataset contains 6 distinct domains, 345 categories and ∼0.6 million images. The categories are from 24 divisions, which are: Furniture,",
        "201001020201001020100102030201001020201001020302010010203020100102030201001020300102030405060Num of trainng epoches0.000.250.500.751.001.251.50Training errormnist-mmnistsvhnsyntheticMD0.650.700.750.800.850.900.95Accuracy\fFigure 9. Images sampled from clipart domain of the DomainNet dataset.",
        "Figure 10. Images sampled from infograph domain of the DomainNet dataset.",
        "Figure 11. Images sampled from painting domain of the DomainNet dataset.",
        "Figure 12. Images sampled from quickdraw domain of the DomainNet dataset.",
        "Figure 13. Images sampled from real domain of the DomainNet dataset.",
        "Figure 14. Images sampled from sketch domain of the DomainNet dataset.",
        "Table 10. Detailed statistics of the DomainNet dataset.",
        "Table 11. Detailed statistics of the DomainNet dataset.",
        "Table 12. Detailed statistics of the DomainNet dataset."
    ]
}