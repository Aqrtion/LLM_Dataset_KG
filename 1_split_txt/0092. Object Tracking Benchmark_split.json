{
    "title_author_abstract_introduction": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. 37, NO. 9, SEPTEMBER 2015\nObject Tracking Benchmark\nYi Wu, Member, IEEE, Jongwoo Lim, Member, IEEE, and Ming-Hsuan Yang, Senior Member, IEEE\nAbstract—Object tracking has been one of the most important and active research areas in the ﬁeld of computer vision. A large number of tracking algorithms have been proposed in recent years with demonstrated success. However, the set of sequences used for evaluation is often not sufﬁcient or is sometimes biased for certain types of algorithms. Many datasets do not have common ground-truth object positions or extents, and this makes comparisons among the reported quantitative results difﬁcult. In addition, the initial conditions or parameters of the evaluated tracking algorithms are not the same, and thus, the quantitative results reported in literature are incomparable or sometimes contradictory. To address these issues, we carry out an extensive evaluation of the state-of-the-art online object-tracking algorithms with various evaluation criteria to understand how these methods perform within the same framework. In this work, we ﬁrst construct a large dataset with ground-truth object positions and extents for tracking and introduce the sequence attributes for the performance analysis. Second, we integrate most of the publicly available trackers into one code library with uniform input and output formats to facilitate large-scale performance evaluation. Third, we extensively evaluate the performance of 31 algorithms on 100 sequences with different initialization settings. By analyzing the quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this ﬁeld.\nIndex Terms—Object tracking, benchmark dataset, performance evaluation\n1 INTRODUCTION\nOBJECT tracking is one of the most important problems in\nthe ﬁeld of computer vision with applications ranging from surveillance and human-computer interactions to medical imaging [13], [89]. Given the initial state (e.g., position and extent) of a target object in the ﬁrst image, the goal of tracking is to estimate the states of the target in the subsequent frames. Although object tracking has been studied for several decades and considerable progress has been made in recent years [5], [17], [32], [35], [36], [54], [65], it remains a challenging problem. Numerous factors affect the performance of a tracking algorithm, including illumination variation, occlusion, and background clutters, and there exists no single approach that successfully handles all scenarios. Therefore, it is crucial to thoroughly evaluate the state-ofthe-art tracking algorithms to demonstrate their strength and weakness, thereby identifying future research directions in this ﬁeld for more robust algorithms.\nFor a comprehensive performance evaluation, it is critical to collect a representative dataset. There exist several datasets for object tracking in surveillance scenarios, such as the VIVID [14], CAVIAR [24], and PETS [23] databases. However, in these surveillance sequences, the target objects are usually humans or small cars and the background is usually\n(cid:1) Y. Wu is with the Department of Computer Science, Nanjing University of\n(cid:1)\nInformation Science and Technology, Nanjing, Nanjing, China. E-mail: ywu.china@gmail.com. J. Lim is with the Department of Computer Science and Engineering, Hanyang University, Seoul, Seoul, Republic of Korea. E-mail: jlim@hanyang.ac.kr.\n(cid:1) M.-H. Yang is with Department of Electrical Engineering and Computer\nScience, University of California at Merced, Merced, CA 95344. E-mail: mhyang@ucmerced.edu.\nManuscript received 18 May 2014; revised 26 Sept. 2014; accepted 10 Oct. 2014. Date of publication 31 Dec. 2014; date of current version 7 Aug. 2015. Recommended for acceptance by K.M. Lee. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identiﬁer below. Digital Object Identiﬁer no. 10.1109/TPAMI.2014.2388226\nstatic. For generic scenes with various types of tracking targets, many of the available sequences do not provide the ground-truth target locations except a few datasets [5], [43], [65]. The reported tracking results on these unlabeled datasets are not directly comparable since different groundtruth annotations are used.\nRecently, signiﬁcant efforts have been made to make tracking code available for evaluation, e.g., OAB [27], IVT [65], MIL [5], L1 [53], and TLD [39] algorithms. These tracking algorithms accommodate different input formats (e.g., avi videos or raw image sequences) and motion models (e.g., 2D translation, similarity or afﬁne transforms) with various output formats (e.g., position or extent). Therefore, to evaluate the performance of these algorithms on a large number of image sequences, it is necessary to integrate them in a library for evaluation on a common platform. In this work, we integrate most of the publicly available trackers in a code library with uniform input and output formats for a performance evaluation. In addition, we provide a large benchmark dataset with ground-truth annotations and attributes (e.g., occlusion, fast motion, or illumination variation) such that the performance of the evaluated tracking algorithms can be better analyzed.\nOne common issue in assessing tracking algorithms is that the reported results are often based on a few sequences with different initializations or parameters. Inaccurate localization of the target occurs frequently as an object detector may be used for locating the object in the ﬁrst frame. In addition, an object detector may be used to recover from tracking failures by re-initializing the tracker. For a fair and comprehensive evaluation, we propose to perturb the initial object states spatially and temporally on the basis of the ground-truth target locations. With this evaluation methodology, the sensitivity of a tracking algorithm to initialization (i.e., robustness) can be better analyzed. While the robustness to initialization is a known problem in other ﬁelds, it has not been well addressed in the literature of object\nAuthorized licensed use limited to: SUN YAT-SEN UNIVERSITY. Downloaded on March 14,2025 at 07:00:33 UTC from IEEE Xplore.  Restrictions apply. 0162-8828 (cid:1) 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.\nWU ET AL.: OBJECT TRACKING BENCHMARK\ntracking. To the best of our knowledge, this is the ﬁrst work to comprehensively address and analyze the initialization problem of object tracking.\nThe contributions of this work are three-fold: Benchmark dataset. We construct a benchmark dataset with 100 fully annotated sequences1 to facilitate the performance evaluation.\nCode library. We integrate most of the publicly available trackers in one code library with uniﬁed input and output formats to facilitate a large-scale performance evaluation.\nPerformance evaluation. We propose novel metrics to evaluate tracking algorithms where the initial object states are perturbed spatially and temporally for the robustness analysis. Each algorithm is extensively evaluated by analyzing more than millions of tracking outputs.\nThis work2 mainly focuses on the performance evaluation of online3 tracking algorithms for single targets. The code library, annotated dataset, and all the tracking results are available at http://pami.visual-tracking.net.",
    "data_related_paragraphs": [
        "2.3 Model Update It has been shown that online update of target representation to account for appearance variations plays an important role for robust object tracking [27], [32], [37], [38], [65]. Matthews et al. [52] addressed the template update problem for the LK algorithm [50], where the template was updated with the combination of the ﬁxed reference template extracted from the ﬁrst frame and the result from the most recent frame. Effective update algorithms have also been proposed in the form of the online mixture model [37], online boosting [27], and incremental subspace update [65]. For the discriminative model, recently, considerable attention has been paid to draw samples effective for training online classiﬁers [5], [28], [32], [39]. In contrast to supervised discriminative object tracking, Grabner et al. [28] formulated the update problem as a semi-supervised task where the classiﬁer was updated with both labeled and unlabeled data. To handle ambiguously labeled positive and negative samples obtained online, Babenko et al. [5] focused on the tracking problem within the multiple instance learning framework and developed an online algorithm. To exploit the underlying structure of the unlabeled data, Kalal et al. [39] developed a tracking algorithm within the semi-supervised learning framework to select positive and negative samples for model update. In [32], the proposed tracking algorithm directly predicts the target location change between frames on the basis of structured learning. Yu et al. [91] presented a tracking method based on co-training to combine generative and discriminative models. While considerable progress has been made, it is still difﬁcult to develop an adaptive appearance model without drifts.",
        "Most recently, Smeulders et al. [70] have evaluated 19 trackers on 315 videos. Although it has been noted that poor initialization of a tracker signiﬁcantly decreases the tracking accuracy, further analysis based on comprehensive experimental evaluations is necessary and important to better understand the state-of-the-art algorithms. In [59], Pang and Ling used a ranking approach to analyze the reported results of object tracking methods. The main shortcoming of this approach is that it is not appropriate to rank the recently published trackers due to a lack of sufﬁcient experimental evaluations in terms of trackers, sequences, and metrics. In [40], 27 trackers were evaluated on 16 sequences, where the ranking of trackers were obtained by averaging the performance on image sequences using different metrics. The failure rate of a tracking method was computed by counting the number of frames in which a method fails to follow a target object. A tracker was re-initialized several frames after a failure occurs (when the overlap ratio is zero). If a different overlap threshold is used, each tracker needs to be re-evaluated on the entire dataset. In this work, we propose a virtual run to compute failure rates with different overlap thresholds.",
        "4. In addition, we implemented the tracking algorithms [17], [60]. We evaluated the trackers in the VIVID dataset [14], including the mean shift, template matching (TM), ratio shift (RS), and peak difference (PD) methods.",
        "4 DATASETS",
        "Recently, numerous benchmark datasets have been developed for various vision problems, such as the Berkeley segmentation [51], FERET face recognition [61], and optical ﬂow dataset [8]. There exist a few benchmark datasets for tracking in the surveillance scenarios, such as the VIVID [14] and CAVIAR [24] databases. For generic object tracking, several sequences have been used for the evaluation [5], [43], [65]. However, most image sequences are not appropriately provided with the ground-truth annotations, and thus, the reported quantitative results in the literature are different or inconsistent since the trackers are not initialized and evaluated on the same platform. To facilitate a fair performance evaluation, we collected and annotated most of the commonly used tracking sequences. This work expands the sequences that we collected in [83] to include 100 target objects in the tracking benchmark TB-100 dataset.6 Since some of the target objects are similar or less challenging, we also selected 50 difﬁcult and representative ones in the TB-50 dataset for an in-depth analysis. Note that",
        "as humans are the most important target objects in practice, the TB-100 dataset contains more sequences of this category (36 body and 26 face/head videos) than of the other types.",
        "In addition to the performance evaluation on the TB-100 dataset, we report tracking results of sequences with speciﬁc attributes. The characteristics of tracking algorithms can be better understood from the sequences with the same attributes. For example, to evaluate how well the tracker handles occlusion, one may use 49 sequences (29 in TB-50) annotated with the OCC attribute. Fig. 1 shows the ﬁrst frames of all 100 targets with ground-truth bounding boxes and attributes where the target objects of the TB-50 dataset are marked with green rectangles. The attribute distribution of the TB-100 dataset is shown in Table 3.",
        "The diagonal corresponds to the distribution over the entire dataset, and each row or column presents the distribution for the attribute subset.",
        "at different frames. However, it is impractical to evaluate all possible scenarios with different thresholds and parameters (and spatial perturbations in SRER) for every image the TB-50 or TB-100 benchmark datasets. sequence of Second, numerous tracking algorithms are distributed with binary code, and it is not possible to detect a failure and restart a tracker at some particular frames. As such, we use virtual runs to approximate speciﬁc parameter settings generated from a set of actual experiments. For each spatial perturbation d, each tracker is evaluated from every tth frame to the end of a sequence with N frames, and the set of , where each run rdd results is frdd k is a sequence of tracking outputs from frame ðtk þ 1Þ to N.",
        "In this work, each SRER consists of seven spatial perturbations (one ground-truth, four center shifts, and two scale variations: 0.9 and 1.1) and v ¼ 90 frames. We only evaluate on the TB-50 dataset with SRER due to the large number of experiments: for a sequence of 600 frames, one tracker requires 140 runs (7 variations (cid:5) 20 runs), processing 44,100 (7 (cid:5) ð600 þ 570 þ (cid:3)(cid:3)(cid:3) þ 30)) frames.",
        "6.1 Overall Performance Each tracker is evaluated on 58,897 frames of the entire TB100 dataset for OPE. For SRE, each tracking algorithm is evaluated on each sequence with 12 initial object states, where more than 700,000 tracking results per tracker are generated. For TRE, each sequence is partitioned into 20 segments and each method is tested with more than 610,000 frames per tracker. In terms of SRER, more than 80 million tracking results from the TB-50 sequences are generated. To the best of our knowledge, this is the largest-scale performance evaluation of object tracking. We report the most important ﬁndings in this manuscript, and further details can be found at http://pami.visual-tracking.net.",
        "The experimental results of OPE, SRE, and TRE on the TB-100 and TB-50 datasets are shown in Fig. 4. The scores from the TB-50 dataset are lower than those from the TB-100 set as the TB-50 set consists of more challenging sequences. For the sake of presentation clarity, the plots of the top-10 performing trackers, ordered by the AUC scores, are shown in color (plots of the other trackers are shown in gray).",
        "The average TRE scores from both datasets are higher than those of OPE in that the number of frames decreases from the ﬁrst to the last segment of TRE. As tracking algorithms tend to perform well in shorter sequences, the average scores of all the results in TRE tend to be higher. On the other hand, the average SRE scores are lower than those of OPE. As a result of imprecise initial appearance models induced in SRE, tracking methods tend to drift away from target objects at a faster pace than those in OPE.",
        "The evaluation results show that OPE is not the best performance indicator as it is one trial of SRE or TRE and does not take the initialization noise into account. The OPE, SRE, and TRE results are mostly consistent in the the top few performing tracking methods sense that according to one criterion also perform well in the other evaluations. However, these tracking methods are effective in different aspects. In the success plots for evaluations based on the TB-100 dataset, the SCM method in OPE outperforms the ASLA approach (by 3.65 percent) but is slightly worse in SRE (by 0.57 percent), which suggests that one of the considered algorithms is more robust to the spatial perturbation of the initial object states. The ranking of the TLD method in TRE is lower than that in",
        "the considered trackers on the TB-50 dataset. The top-ﬁve tracking methods in SRER perform signiﬁcantly better than the others in terms of both the average success rate and the average number of failures.",
        "6.3 Performance Analysis by Attributes By annotating each sequence, we construct subsets with different dominant attributes that facilitate the analysis of the performance of trackers for each challenging factor. Fig. 5 shows the SRER plots of 11 attributes on the TB-50 dataset. These plots show the different performance characteristics of the tracking algorithms.",
        "TABLE 4 SRER Evaluation Results on the TB-50 Dataset",
        "We note that considerable progress has recently been made [19], [25], [49] to improve the state-of-the-art. In [19], Danelljan et al. extended the CSK method [34] by using many color attributes/features and 41 color sequences were selected from the benchmark dataset [83] for the evaluation.",
        "on the benchmark dataset [83]. These trackers will be added to the library and made available on the evaluation website. In this work, the large-scale performance evaluation facilitated a better understanding of the state-of-the-art object tracking approaches, and provided a platform for gauging new algorithms. Our future work focuses on extending the datasets and code library to include more fully annotated sequences and trackers.",
        "[6] Y. Bai and M. Tang, “Robust tracking via weakly supervised ranking SVM,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 1854–1861. S. Baker and I. Matthews, “Lucas-Kanade 20 years on: A unifying framework,” Int. J. Comput. Vis., vol. 56, no. 3, pp. 221–255, 2004. S. Baker, S. Roth, D. Scharstein, M. J. Black, J. Lewis, and R. Szeliski, “A database and evaluation methodology for optical ﬂow,” in Proc. 11th IEEE Int. Conf. Comput. Vis., 2007, pp. 1–8.",
        "[21] M. Everingham, L. Gool, C. K. I. Williams, J. Winn, and A. Zisserman, “The Pascal Visual Object Classes (VOC) challenge,” Int. J. Comput. Vis., vol. 88, no. 2, pp. 303–338, 2010. J. Fan, Y. Wu, and S. Dai, “Discriminative spatial attention for robust tracking,” in Proc. 11th Eur. Conf. Comput. Vis., 2010, pp. 480–493. J. Ferryman and A. Shahrokni, “PETS 2009: Dataset and challenge,” in Proc. IEEE Int. Workshop Perform. Eval. Tracking Surveillance, 2009, pp. 1–6.",
        "[24] R. B. Fisher, “The PETS04 surveillance ground-truth data sets,” in Proc. IEEE Int. Workshop Perform. Eval. Tracking Surveillance, 2004, pp. 1–5. J. Gao, H. Ling, W. Hu, and J. Xing, “Transfer learning based visual tracking with Gaussian processes regression,” in Proc. 10th Eur. Conf. Comput. Vis., 2014, pp. 188–203."
    ]
}