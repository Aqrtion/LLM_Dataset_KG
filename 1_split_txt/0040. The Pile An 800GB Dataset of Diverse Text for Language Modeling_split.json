{
    "title_author_abstract_introduction": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling\nLeo Gao\nStella Biderman\nSid Black\nLaurence Golding\nTravis Hoppe\nCharles Foster\nJason Phang\nHorace He\nAnish Thite\nNoa Nabeshima\nShawn Presser\nConnor Leahy\nEleutherAI contact@eleuther.ai\nAbstract\nRecent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present the Pile: an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets—both existing and newly constructed—many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve signiﬁcantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.1\nIntroduction\nRecent breakthroughs in general-purpose language modeling have demonstrated the effectiveness of training massive models on large text corpora for downstream applications (Radford et al., 2019; Shoeybi et al., 2019; Raffel et al., 2019; Rosset, 2019; Brown et al., 2020; Lepikhin et al., 2020). As the ﬁeld continues to scale up language model training, the demand for high-quality massive text data will continue to grow (Kaplan et al., 2020).\nThe growing need for data in language modeling has caused most existing large-scale language models to turn to the Common Crawl for most or all of their data (Brown et al., 2020; Raffel et al., 2019). While training on the Common Crawl has been effective, recent work has shown that dataset di-\n1https://pile.eleuther.ai/\nversity leads to better downstream generalization capability (Rosset, 2019). Additionally, large-scale language models have been shown to effectively acquire knowledge in a novel domain with only relatively small amounts of training data from that domain (Rosset, 2019; Brown et al., 2020; Carlini et al., 2020). These results suggest that by mixing together a large number of smaller, high quality, diverse datasets, we can improve the general cross-domain knowledge and downstream generalization capabilities of the model compared to models trained on only a handful of data sources.\nTo address this need, we introduce the Pile: a 825.18 GiB English text dataset designed for training large scale language models. The Pile is composed of 22 diverse and high-quality datasets, including both established natural language processing datasets and several newly introduced ones. In addition to its utility in training large language models, the Pile can also serve as a broad-coverage benchmark for cross-domain knowledge and generalization ability of language models.\nWe introduce new datasets derived from the following sources: PubMed Central, ArXiv, GitHub, the FreeLaw Project, Stack Exchange, the US Patent and Trademark Ofﬁce, PubMed, Ubuntu IRC, HackerNews, YouTube, PhilPapers, and NIH ExPorter. We also introduce OpenWebText2 and BookCorpus2, which are extensions of the original OpenWebText (Gokaslan and Cohen, 2019) and BookCorpus (Zhu et al., 2015; Kobayashi, 2018) datasets, respectively.\nIn addition, we incorporate several existing highquality datasets: Books3 (Presser, 2020), Project Gutenberg (PG-19) (Rae et al., 2019), OpenSubtitles (Tiedemann, 2016), English Wikipedia, DM Mathematics (Saxton et al., 2019), EuroParl (Koehn,2005), andtheEnronEmailscorpus(Klimt and Yang, 2004). To supplement these, we also in-\nFigure 1: Treemap of Pile components by effective size.\ntroduce a new ﬁltered subset of Common Crawl, Pile-CC, with improved extraction quality.\n1.1 Contributions\nThe core contributions of this paper are:\nThrough our analyses, we conﬁrm that the Pile is signiﬁcantly distinct from pure Common Crawl data. Additionally, our evaluations show that the existing GPT-2 and GPT-3 models perform poorly on many components of the Pile, and that models trained on the Pile signiﬁcantly outperform both raw and ﬁltered Common Crawl models. To complement the performance evaluations, we also perform an exploratory analysis of the text within the Pile to provide a detailed picture of the data. We hope that our extensive documentation of the construction and characteristics of the Pile will help researchers make informed decisions about potential downstream applications.\nFinally, we make publicly available the preprocessing code for the constituent datasets of the Pile and the code for constructing alternative versions2. In the interest of reproducibility, we also document all processing performed on each dataset (and the Pile as a whole) in as much detail as possible. For further details about the processing of each dataset, see Section 2 and Appendix C.",
    "data_related_paragraphs": [
        "1. The introduction of a 825.18 GiB englishlanguage dataset for language modeling combining 22 diverse sources.",
        "2. The introduction of 14 new language modeling datasets, which we expect to be of independent interest to researchers.",
        "improvements across many domains by GPT-2- sized models trained on this new dataset, compared to training on CC-100 and raw Common Crawl.",
        "4. The investigation and documentation of this dataset, which we hope will better inform researchers about how to use it as well as motivate them to undertake similar investigations of their own data.",
        "2 The Pile Datasets",
        "ThePileiscomposedof22constituentsub-datasets, as shown in Table 1. Following Brown et al. (2020), we increase the weights of higher quality components, with certain high-quality datasets such as Wikipedia being seen up to 3 times (“epochs”) for",
        "Table 1: Overview of datasets in the Pile before creating the held out sets. Raw Size is the size before any up- or down-sampling. Weight is the percentage of bytes in the ﬁnal dataset occupied by each dataset. Epochs is the number of passes over each constituent dataset during a full epoch over the Pile. Effective Size is the approximate number of bytes in the Pile occupied by each dataset. Datasets marked with a † are used with minimal preprocessing from prior work.",
        "each full epoch over the Pile. Detailed information about the construction of each dataset is available in Appendix C.",
        "Common Crawl is a collection of website crawls from 2008 onwards, including raw web pages, metadata and text extractions. Due to the raw nature of the dataset, Common Crawl has the advantage of including text from diverse domains, but at the cost of varying quality data. Due to this, use of Common Crawl typically necessitates well-designed extraction and ﬁltering. Our Common Crawl-based dataset, Pile-CC, uses jusText (Endrédy and Novák, 2013) on Web Archive ﬁles (raw HTTP responses including page HTML) for extraction, which yields higher quality output than directly using the WET ﬁles (extracted plaintext).",
        "Books3 is a dataset of books derived from a copy of the contents of the Bibliotik private tracker made available by Shawn Presser (Presser, 2020). Bibliotik consists of a mix of ﬁction and nonﬁction books and is almost an order of magnitude",
        "larger than our next largest book dataset (BookCorpus2).We included Bibliotik because books are invaluable for long-range context modeling research and coherent storytelling.",
        "and other metadata, we focused speciﬁcally on court opinions due to an abundance of full-text entries. This data is entirely within the public domain.",
        "OpenWebText2 (OWT2) is a generalized web scrape dataset inspired by WebText (Radford et al., 2019) and OpenWebTextCorpus (Gokaslan and Cohen, 2019). Similar to the original WebText, we use net upvotes on Reddit submissions as a proxy for outgoing link quality. OpenWebText2 includes more recent content from Reddit submissions up until 2020, content from multiple languages, document metadata, multiple dataset versions, and open source replication code. We included OWT2 as a high quality general purpose dataset.",
        "GitHub is a large corpus of open-source code repositories. Motivated by the ability of GPT-3 (Brown et al., 2020) to generate plausible code completions despite its training data not containing any explicitly gathered code datasets, we included GitHub in the hopes that it would enable better downstream performance on code-related tasks.",
        "The Free Law Project is a US-registered non-proﬁt that provides access to and analytical tools for academic studies in the legal realm. CourtListener,3 part of the Free Law Project, provides bulk downloads for millions of legal opinions from federal and state courts. While the full dataset provides multiple modalities of legal proceedings, including dockets, bibliographic information on judges,",
        "The Stack Exchange Data Dump4 contains an anonymized set of all user-contributed content on the Stack Exchange network, a popular collection of websites centered around user-contributed questions and answers. It is one of the largest publicly available repositories of question-answer pairs, and covers a wide range of subjects—from programming, to gardening, to Buddhism. We included Stack Exchange in the hopes that it will improve the question answering capabilities of downstream models on diverse domains.",
        "USPTO Backgrounds is a dataset of background sections from patents granted by the United States Patent and Trademark Ofﬁce, derived from its published bulk archives5. A typical patent background lays out the general context of the invention, gives an overview of the technical ﬁeld, and sets up the framingoftheproblemspace. WeincludedUSPTO Backgrounds because it contains a large volume of technical writing on applied subjects, aimed at a non-technical audience.",
        "5https://bulkdata.uspto.gov/",
        "Project Gutenberg is a dataset of classic Western literature. The speciﬁc Project Gutenberg derived dataset we used, PG-19, consists of Project Gutenberg books from before 1919 (Rae et al., 2019), which represent distinct styles from the more modern Books3 and BookCorpus. Additionally, the PG19 dataset is already being used for long-distance context modeling.",
        "The OpenSubtitles dataset is an English language dataset of subtitles from movies and television shows gathered by Tiedemann (2016). Subtitles provide an important source of natural dialog, as well as an understanding of ﬁctional formats other than prose, which may prove useful for creative writing generation tasks such as screenwriting, speechwriting, and interactive storytelling.",
        "The DeepMind Mathematics dataset consists of a collection of mathematical problems from topics such as algebra, arithmetic, calculus, number theory, and probability, formatted as natural language prompts (Saxton et al., 2019). One major weakness of large language models has been performance on mathematical tasks (Brown et al., 2020), which may be due in part to a lack of math problems in the training set. By explicitly including a dataset of mathematical problems, we hope to improve the mathematical ability of language models trained on the Pile.",
        "BookCorpus2 is an expanded version of the original BookCorpus (Zhu et al., 2015), a widely used language modeling corpus consisting of books written by “as of yet unpublished authors.” BookCorpus is therefore unlikely to have signiﬁcant overlap with Project Gutenberg and Books3, which consist of published books. BookCorpus is also commonly used as dataset for training language models (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019).",
        "The Ubuntu IRC dataset is derived from the publicly available chatlogs6 of all Ubuntu-related channels on the Freenode IRC chat server. Chatlog data",
        "The YouTube Subtitles dataset is a parallel corpus of text gathered from human generated closedcaptions on YouTube. In addition to providing multilingual data, Youtube Subtitles is also a source of educational content, popular culture, and natural dialog.",
        "2.19 PhilPapers The PhilPapers7 dataset consists of open-access philosophy publications from an international database maintained by the Center for Digital Philosophy at the University of Western Ontario. We included PhilPapers because it spans a wide body of abstract, conceptual discourse, and its articles contain high quality academic writing.",
        "TheNIHGrantabstractsprovidesabulk-datarepository for awarded applications through the ExPORTER8 service covering the ﬁscal years 1985- present. Weincludedthedatasetbecauseitcontains examples of high-quality scientiﬁc writing.",
        "The Enron Emails dataset (Klimt and Yang, 2004) is a valuable corpus commonly used for research about the usage patterns of email. We included Enron Emails to aid in understanding the modality of email communications, which is typically not found in any of our other datasets.",
        "While the Pile was conceived as a training dataset for large-scale language models, its coverage of multiple disparate domains makes it also suitable as an evaluation dataset. In this section, we describe how the Pile can be used as a broad-coverage dataset for benchmarking language models.",
        "The Pile is provided as train, validation, and testing splits. The validation and testing components each contain 0.1% of the data, sampled uniformly at random. While this is a far smaller percentage than most datasets, the sheer size of the dataset results in over 1 GiB of validation and testing data each. We highlight that while we have made efforts to deduplicate documents within the Pile (See: Section D.2), it is still possible that some documents are duplicated across the train/validation/test splits.",
        "Our preferred metric is bits per UTF-8 encoded byte (BPB). Bits per byte is preferred over bits per character or perplexity when using Pile as a metric due to its invariance to different tokenization schemes and the ambiguity of measuring characters in Unicode. To compute bits per byte from a given negative log likelihood loss (cid:96), we compute BPB = (LT/LB)log2(e(cid:96)) = (LT/LB)(cid:96)/ln(2), where LT is the length of the dataset in tokens and LB is the length of the dataset in UTF-8 encoded bytes. We ﬁnd that LT/LB is 0.29335 GPT-2- tokens/byte across the Pile; dataset-speciﬁc values of LT/LB can be found in Table 7.",
        "We compute the test perplexity of the constituent datasets of the Pile using GPT-2 (Radford et al.,",
        "2019) and GPT-3 (Brown et al., 2020), shown in Figure 2. We use all available versions of GPT-2, and all four versions of GPT-3 available via the OpenAI API. Because of the cost associated with using the OpenAI API, we evaluate on one-tenth of the respective test sets for most of the constituent datasets. We report the perplexity converted to bits per UTF-8 encoded byte (BPB). Importantly, we compute perplexity by evaluating each document independently within each dataset, as opposed to concatenating all documents as is common practice for computing perplexity on large corpora.",
        "Figure2: Scalinglawforperformanceof GPT-2/3models. ‘Zero-shot’ refers to the fact that none of the models have been ﬁne-tuned on data from the Pile.",
        "Determining which components GPT-3 underperforms on provides information about which Pile components are most dissimilar to the distribution of text (web pages and books) that GPT-3 was trained on. These components would thus make especially good candidates for supplementing GPT-3 training data. These results are also valuable for determining which types of datasets to emphasize for future iterations of the Pile.",
        "Due to the difference in entropy of different datasets, directly comparing perplexity of GPT-3 on different Pile components is not an accurate indication of relative performance. Ideally we would train a GPT-3 model from scratch on the Pile and compare the difference in loss per dataset with that of the original GPT-3. Because of resource constraints, we instead use a GPT-2 model trained from scratch on the Pile (see Section 4) to construct a proxy measure. To construct our proxy, we ﬁrst measure the improvement from the GPT2-Pile model to GPT-3 on each component. Then, we normalize our results by setting the change on OpenWebText2 to be zero. This computation is shown in the equation below:",
        "Since GPT2-Pile was trained on both OWT2 and the dataset we are evaluating, we expect the second term in ∆set to reﬂect the difference in the intrinsic difﬁculty of the two datasets. Thus the total value of ∆set reﬂects how much harder the dataset we are evaluating was for GPT-3 than OWT2, minus the relative difﬁculty of the two tasks. As GPT-3 was trained on data very similar to OWT2, this gives us a proxy for how much better GPT-3 would do if it were trained on the Pile.",
        "The results are shown in Figure 3. As a sanity check, we observe that datasets that are contained in, or are extremely similar to, GPT-3’s training set (Books3, Wikipedia (en), Pile-CC and Project Gutenberg) score close to zero on our metric.",
        "GPT-3 appears to perform poorly on datasets pertaining to research or academic writing like PubMed Central, PubMed Abstracts, and ArXiv; domain-speciﬁc datasets like FreeLaw, HackerNews, and USPTO Backgrounds; and on datasets containing predominantly text distinct from natural language, like GitHub and DM Mathematics. In addition, the majority of datasets see less of an improvement than OpenWebText2. As such, we expectaGPT-3sizedmodeltrainedonPiletoperform signiﬁcantly better on research related tasks, software tasks, and symbol manipulation tasks than the base model. Additionally, this experiment provides evidence that the majority of Pile components are not redundant with the predominantly web-based GPT-3 training data.",
        "We note that this metric is only a proxy for similarity, and that it could be confounded by dataset speciﬁc scaling effects. Although our results largely accord with expectations, there are some puzzling results, like the datasets on which GPT-3 outperformed GPT-2 Pile. We hypothesize that GPT-3 learns to be so good at these datasets that training on them explicitly does not notably beneﬁt the model’s performance. We leave a more rigorous analysis of these effects for future work.",
        "To conﬁrm the effectiveness of the Pile for improving language modeling quality, we train architecturally-identical 1.3 billion parameter models based on those in Brown et al. (2020) on different datasets and evaluate on the WikiText and LAMBADA tasks as benchmarks of language modeling ability. We also report results on the Pile as a measure of more cross-domain generalization.",
        "To ensure a fair comparison across datasets of different sizes, we decontaminate any instances of the evaluation sets using the same 13-gram overlap ﬁltering as in Brown et al. (2020) and downsample to 40GB to control for dataset size. As we control for dataset size, we emphasize that our evaluation is generous to CC-100 (en), which is about 1/3 the size of the Pile in reality.",
        "Wecomparethefollowingdatasets: thePile, theEn-",
        "Table 2: Test perplexity of the Pile using GPT-2 and GPT-3, converted to bits per UTF-8 encoded byte (BPB). Evaluation is performed on one-tenth of the test data of the Pile, on a per-document basis. Bold indicates the best-performing model in each row.",
        "glish component of the CC-100 dataset11 (Wenzek et al., 2019; Conneau et al., 2020), and a sample of raw CC WET ﬁles ﬁltered for English-only.",
        "The magnitude of improvement over CC-100 per set is shown in Figure 4. Unsurprisingly, there is almost no improvement on Pile-CC. However, the model trained on the Pile performs signiﬁ- cantly better than either of the other models on academic datasets such as ArXiv, Pubmed Central, FreeLaw, and PhilPapers. It also improves signiﬁ-",
        "11The data was obtained from http://data.statmt.",
        "cantly on programming-related datasets like Github and StackExchange, on EuroParl, due to the lack of multilingual text in either other dataset, and on DM Mathematics, indicating a signiﬁcant improvement in mathematical ability.",
        "Surprisingly, raw Common Crawl performs better on the Pile BPB than CC-100, despite losing by a signiﬁcant margin on LAMBADA and WikiText. We hypothesize that this is due to the perplexity based ﬁltering used in CC-100, where a language model is trained on Wikipedia and all data with a perplexity too high or too low is discarded. This effectively discards any data too similar to or too different from Wikipedia, which severely limits the diversity of the collected data. This result suggests that future work using Common Crawl should take caution with ﬁltering to preserve its diversity.",
        "In this section, we cover the Structural Statistics of the dataset, which provide more coarse-grained and statistical information about the Pile. In Sec-",
        "Dataset Size Pile (val) Pile (test) WikiText LAMBADA LAMBADA",
        "Table 3: Size-controlled evaluation results. Each dataset is deduplicated against all evaluation metrics and subsampled to approximately 40GB to control for the effects of dataset size. For LAMBADA, we use the variant of the data introduced in Radford et al. (2019) and only evaluate the perplexity on the ﬁnal token rather than the ﬁnal word. For WikiText, we report the perplexity per GPT-2 token. † indicates that the size is an estimate.",
        "tion 6, we provide a closer investigation and documentation of the textual content within the Pile datasets.",
        "Each dataset consists of a large number of documents. We analyze the distribution of document lengths, as well as the number of bytes-per-token using the GPT-2 tokenizer in order to put our ablations in context.",
        "Since the GPT-2 BPE tokenizer is trained on WebText, the mean bytes per token is also a very rough indicator of how syntactically different each Pile component is from WebText. For instance, datasets like NIH ExPorter, OpenWebText2 and Books3",
        "While only 13% of the world’s population speaks English, the vast majority of NLP research is done on English. For the Pile, we took a similar approach to the dataset used by Brown et al. (2020) and focused predominantly on English, while also not explicitly ﬁltering out other languages when collecting our own data. When evaluating a multilingual dataset, our main criteria for inclusion was whether the English component of the dataset merited inclusion alone. We plan to create a fully multi-",
        "Dataset",
        "Table 4: Breakdown of BPB on Pile heldout test set. Columns indicate the dataset each model is trained on; rows indicate the evaluation dataset. Bold indicates the best performing model in each row.",
        "Investigating and Documenting the Datasets",
        "As the scale of machine learning research has grown, scrutiny has been placed on the ever larger datasets that models are trained on (Prabhu and Birhane, 2020; Biderman and Scheirer, 2020)",
        "While this issue has been raised within AI ethics and bias research (Hovy and Spruit, 2016; Hutchinson et al., 2020; Blodgett et al., 2020), it has not been a focal point of concern within the language modeling community. Despite the proliferation of work exploring and documenting issues with datasets (Gebru et al., 2018; Bender and Friedman,",
        "tives (Seck et al., 2018; Costa-jussà et al., 2020; Thieme et al., 2020). The second, the data statements methodology (Bender and Friedman, 2018), was proposed speciﬁcally for natural language processing and has been well received by the NLP community. Our datasheet and data statement will be featured in the GitHub repository where the code for the Pile is stored and will also be available as separate documents on arXiv (Biderman et al., 2021; Biderman, 2021).",
        "In addition to the datasheet and data statement, there is additional information that may be helpful to people training language models that these documents do not cover. In the rest of this section we investigate and document in greater detail some of this additional contextual information.",
        "Figure 6: Mean bytes per GPT-2-token for each dataset in the Pile. Error bars indicate standard deviation.",
        "2018; Jo and Gebru, 2020), no dataset intended to train massive language models has been seriously documented by its creators12. Therefore, our analyses serve two goals: to address ethical concerns about the Pile, and to promote and normalize the practice of engaging with the AI ethics literature.",
        "language processing technologies are Natural widely applicable and can be used in extremely different contexts. What is and is not appropriate data to train on can therefore vary wildly with the application context. In our view, the best approach istodocument ratherthaneliminatepotentiallyconcerning aspects of datasets13, particularly since the purpose of the Pile is to train general-purpose language models. The primary goal of our documentation, therefore, is to empower NLP researchers to make informed decisions.",
        "To document the Pile, we chose to implement two frameworks that have been proposed by methodologists and ethics researchers. The ﬁrst, the datasheets methodology (Gebru et al., 2018), is a general purpose methodology that is recommended by several methodologists (Raji and Yang, 2019; Biderman and Scheirer, 2020) and appears to be used more frequently by practitioners than alterna-",
        "12Brown et al. (2020) discusses ethical issues surrounding their model, but do not discuss those surrounding the training dataset itself.",
        "13That said, we did exclude several datasets, see Appendix",
        "sports and entertainment—the content clusters it misses become apparent when compared qualitatively to other components of the Pile. Notably, the data modes covering programming, logic, physics, and legal knowledge appear largely absent.",
        "Due to the wide diversity in origins, it is possible for the Pile to contain pejorative, sexually explicit, or otherwise objectionable content. As this content may not be desirable for some use cases, we break down profanity on a per-dataset level.",
        "We used the profanity-checker Python package (Zhou, 2019). This package includes a “toxicity model” trained on multiple profanity lists as well as the Wikidetox Toxic Comment Dataset (Wulczyn et al., 2016) and classiﬁes a given string as being profane or not profane.",
        "We considered only the English sentences in each dataset using the same language classiﬁer from Section 3.7. We did this since profanity-checker is built for English and other languages may improperly impact the results. For instance, the German nominative/accusative feminine/plural deﬁnite article \"die\" is ﬂagged as being profane regardless of context. We split each sentence into words and computed the percentage of words that are ﬂagged as profane for each component of the Pile. We emphasize that this methodology is only a proxy for profanity, given the complexity of determining whether a given word or phrase is profane in context.",
        "We also broke each dataset down on a sentence level, to allow profanity-checker to check entire sentences. Splitting datasets by sentence allows for additional context to be considered when determining whether content is pejorative. Our results are shown in Figure 12.",
        "Aslanguagemodelsmaypickupunexpectedbiases from the training data, we performed a preliminary analysis of the different components that make up the Pile. Because models with different characteristics may be trained on the Pile, we aimed to document the biases of the data and not a speciﬁc",
        "We focused our analysis on gender, religion, and race. Our goal is to provide users of this dataset with preliminary guidance on how the different components are biased so that they can make decisions on which components to train on.",
        "In addition, we computed the average sentiment (Baccianella et al., 2010) of words cooccurring with the gendered pronouns across each dataset in Figure 13. Generally, we ﬁnd no signiﬁcant sentiment bias towards men or women. This, of course, does not mean that the dataset is free of gender bias (as our co-occurrence tests show).",
        "6.5 Author Consent and Public Data",
        "In addition, we computed the average sentiment of co-occurrences across each of the constituent datasets in Figure 14. Over the entire dataset, we ﬁnd that “Buddhist” has the highest sentiment, followed by “Hindu”, “Christian”, “Atheist”, and “Muslim”. Notably, “Jew” is the lowest, perhaps reﬂecting its historical use as a pejorative.",
        "gation or a good measure to guard against misuse (Obar, 2020; Prabhu and Birhane, 2020). On the other hand, there is signiﬁcant disagreement surrounding the ethics of repurposing data protected by terms of service in research contexts (Vitak et al., 2016; Fiesler et al., 2020), particularly given the power asymmetries inherent in digital platforms, which often close off independent researchers from investigatingpublicdatawhilesimultaneouslycompelling users to consent to its private use (Halavais, 2019).",
        "While much of the Pile’s data comes from sources that have expressly consented to its wider dissemination and use in research, researchers often fail to clearly document where their data came from and under what terms its use was consented to. In light of this, we felt it appropriate to release the Pile with transparency around how the authors of its data have indicated that that data can be used.",
        "To provide needed nuance to our discussion of consent, we identiﬁed three tiers of availability for public use. Public data is data which is freely and readily available on the internet. This primarily excludes data which is pay-walled (regardless of how easy that paywall is to bypass) and data which cannot be easily obtained but can be obtained, e.g. through a torrent or on the dark web. Terms of Service (ToS) compliant data is data which is obtained and used in a fashion that is known to be consistent with the terms of service of the data host. Data with authorial consent is data for which the original authors of the work consented to the use of their data, or where a reasonable person could not assume that their data would not be used for purposes such as research. ToS compliant data and authorial consented data differ in two main ways: It is important to keep in mind that people typically do not read Terms of Service, and additionally that being ToS-compliant does not entail authorial consent. We adopted a strict model of consent, where ambiguous or unknown consent is treated as nonconsensual.",
        "Table 5 summarizes our understanding of the status of each of the datasets within the Pile. Datasets marked with a (cid:51)are compliant in the relevant respects, though a couple datasets are worth remarking on in particular. Book3 and OpenSubtitles are being used in a fashion that is consistent with the terms of service of the data host. However, this is somewhat misleading in that the data host is not",
        "authorized to post the data online by the parties that own it. The Enron Emails dataset was not collected with the permission of the authors, but was collected by the U.S. government as part of a criminal investigation. While the people whose emails are in the Enron dataset are aware of this fact, they were not given the ability to consent to its inclusion in any way.",
        "ThereareﬁvedatasetsincludedinthePilethatwere not collected and distributed in a ToS compliant fashion and for which the authors had no ability to consent to their data being used. Each of these datasets are widely used, both in the NLP literature and the world at large. With the exception of the YouTube Subtitles dataset, each of these datasets were published by researchers and are passed around freely on the internet. The YouTube Subtitles dataset was created by us for this project, using a very popular unofﬁcial API that is both widely used and easily obtainable on Pip, Conda, and GitHub, among other places. Given the processing applied and the difﬁculty of identifying particular ﬁles in the Pile, we feel that our use of these datasets does not constitute signiﬁcantly increased harm beyond that which has already been done by the widespread publication of these datasets.",
        "The Pile represents yet another stepping stone along the path of scaling models and datasets to ever larger sizes and capabilities. There are many serious concerns about how the emergence of progressively stronger AI systems will inﬂuence the wider world (Brundage et al., 2018; Amodei et al., 2016; Bostrom and Yudkowsky, 2014; Bostrom, 2014; Critch and Krueger, 2020), and we believe In this section that they merit serious thought. we discuss the legal ramiﬁcations of the Pile, and then consider the impact of the Pile to AI alignment from two angles: accelerating AI timelines and the dangers posed by unaligned language models.",
        "While the machine learning community has begun to discuss the issue of the legality of training models on copyright data, there is little acknowledgment of the fact that the processing and distribution of data owned by others may also be a violation of copyright law. As a step in that direc-",
        "Table 5: Types of consent for each dataset",
        "tion, we discuss the reasons we believe that our use of copyright data is in compliance with US copyright law.16",
        "Under pre (1984) (and afﬁrmed in subsequent rulings such as aff (2013); Google (2015)), noncommercial, not-for-proﬁt use of copyright media is preemptively fair use. Additionally, our use is transformative, in the sense that the original form of the data is ineffective for our purposes and our form of the data is ineffective for the purposes of the original documents. Although we use the full text of copyright works, this is not necessarily disqualifying when the full work is necessary (ful, 2003). In our case, the long-term dependencies in natural language require that the full text be used in order to produce the best results (Dai et al., 2019; Rae et al., 2019; Henighan et al., 2020; Liu et al., 2018).",
        "additional restrictions on some of these works in particular jurisdictions. To enable easier compliance with local laws, the Pile reproduction code is available and can be used to exclude certain components of the Pile which are inappropriate for the user. Unfortunately, we do not have the metadata necessary to determine exactly which texts are copyrighted, and so this can only be undertaken at the component level. Thus, this should be be taken to be a heuristic rather than a precise determination.",
        "With this in mind, we accept the reality that the Pile could potentially accelerate AI timelines. However, we hope our efforts to establish best practices, such as thoroughly documenting the contents of our data, will help encourage diligence for downstream researchers on alignment problems.",
        "Another concern is that training such models on huge datasets will almost inevitably require them to have undesirable content in their training sets, such as that promoting hateful stereotypes (Christian, 2020). Having models output undesirable content is, by deﬁnition, undesirable, but we believe that attacking this problem from the training set side is unproductive and ultimately leads us away from optimal solutions. If a person reads a racist piece of content, they do not then immediately adopt its racist views—they may be capable of doing so, but can decide not to. This capacity to understand undesirable content and then decide to ignore it is an essential future research direction. Not only would this allow models to use “dirtier\" data with less concern, but also to use their gained knowledge to better understand what not to do. We recognize that, despite recent progress in human-guided learning (Stiennon et al., 2020), the technology is not yet at this stage, and have thus made a number of editorial decisions as described in this paper. However, this approach seems essential to the future of these models and AI more broadly, and more research is needed.",
        "Self-supervised training of natural language processing models on large, unlabeled text corpora, has seen widespread adoption in the ﬁeld. Word representation models such as GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) were trained on datasets such as Wikipedia, Gigaword (Graff et al., 2003), or a non-public Google News corpus. More recently, language models (Radford et al., 2018, 2019; Brown et al., 2020;",
        "Rosset, 2019; Shoeybi et al., 2019) and masked language models (Devlin et al., 2019; Liu et al., 2019; Raffel et al., 2019) have been trained on datasets such as Wikipedia, BookCorpus (Zhu et al., 2015), RealNews (Zellers et al., 2019), CC-Stories (Trinh and Le, 2018), and other Internet scrape-derived datasets discussed below. Other datasets such as WikiText (Stephen et al., 2016) have also been used in similar self-supervised training.",
        "As data requirements for language modeling have grown, the ﬁeld has turned towards Internet scrapes for large-scale datasets (Gokaslan and Cohen, 2019), with Common Crawl being particularly prevalent. Works such as Brown et al. (2020); Wenzek et al. (2019); Suárez et al. (2019b); Raffel et al. (2019) have relied on Common Crawl to build training datasets for large-scale models. However, these works often highlight the difﬁculty of cleaning and ﬁltering the Common Crawl data, and often highlight the resulting data quality as a determining factor of model capability.",
        "It has also been increasingly common practice to combine multiple datasets when training language models. For instance, GPT (Radford et al., 2018) was trained on Wikipedia and BookCorpus, whereas GPT-3 (Brown et al., 2020) was trained on Wikipedia, two ﬁction datasets, and two webscraped datasets. The Pile continues the trend of combining large-scale web-scrapes with smaller, higher-quality datasets that capture knowledge we believe would be most beneﬁcial to training language models.",
        "The two most comparable publicly available datasets to the Pile are CC-100 (Wenzek et al., 2019) and C4/mC4 (Raffel et al., 2019). C4 is comparably-sized to the Pile, while mC4 and CC100 are larger, multilingual datasets. However, C4/mC4 require immense computational resources topreprocessthedata, withitsmaintainersevenrecommending the use of a distributed cloud service,17 setting a high bar of entry to using these datasets. CC-100 is directly downloadable and pre-cleaned; however, its English portion is much smaller than the Pile. Importantly, these three datasets are all derived entirely from Common Crawl—as discussed above, the current best practice in training largescalelanguagemodelsinvolveusingbothlargeweb scrapes and more targeted, higher-quality datasets,",
        "17https://www.tensorflow.org/datasets/",
        "Emily M Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–604.",
        "Stella Biderman. 2021. Data statement for the Pile. arXiv preprint arXiv.",
        "Stella Biderman, Kieran Bicheno, and Leo Gao. 2021. Datasheet for the Pile. arXiv preprint arXiv.",
        "Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2020. Extracting training data from large language models.",
        "Marta R Costa-jussà, Roger Creus, Oriol Domingo, Albert Domínguez, Miquel Escobar, Cayetana López, Marina Garcia, and Margarita Geleta. 2020. Mtadapted datasheets for datasets: Template and repository. arXiv preprint arXiv:2005.13156.",
        "Casey Fiesler, Nathan Beard, and Brian C Keegan. 2020. Norobots, spiders, orscrapers: Legalandethical regulation of data collection methods in social media terms of service. In Proceedings of the International AAAI Conference on Web and Social Media, volume 14, pages 187–196.",
        "Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé III, and Kate Crawford. 2018. Datasheets for datasets. arXiv preprint arXiv:1803.09010.",
        "David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data Consortium, Philadelphia, 4(1):34.",
        "Chris Hardin. 2018. How to shufﬂe a big dataset.",
        "Eun Seo Jo and Timnit Gebru. 2020. Lessons from archives: Strategies for collecting sociocultural data in In Proceedings of the 2020 Conmachine learning. ference on Fairness, Accountability, and Transparency, pages 306–316.",
        "Bryan Klimt and Yiming Yang. 2004. The Enron corpus: A new dataset for email classiﬁcation research. In European Conference on Machine Learning, pages 217–226. Springer.",
        "Jonathan A Obar. 2020. Sunlight alone is not a disinfectant: Consent and the futility of opening big data black boxes (without assistance). Big Data & Society, 7(1):2053951720935615.",
        "Vinay Uday Prabhu and Abeba Birhane. 2020. Large image datasets: A pyrrhic win for computer vision? arXiv preprint arXiv:2006.16923.",
        "Ismaïla Seck, Khouloud Dahmane, Pierre Duthon, and Gaëlle Loosli. 2018. Baselines and a datasheet arXiv preprint for arXiv:1806.04016.",
        "the Cerema AWP dataset.",
        "Jessica Vitak, Katie Shilton, and Zahra Ashktorab. 2016. Beyond the Belmont principles: Ethical challenges, practices, and beliefs in the online data research community. In Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work & Social Computing, pages 941–953.",
        "Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2019. CCNet: Extracting high quality monolingual datasets from web crawl data. arXiv preprint arXiv:1911.00359.",
        "Leo Gao led the project, implemented the main Pile codebase, contributed to the model training code, performed the evaluations and the language analysis, interpreted the perplexity analysis results, implemented the processing to create the ﬁnal data, and processed Pile-CC, PubMed Central, ArXiv, and Ubuntu IRC. Stella Biderman led the data analysis, the broader impact analysis, and the data documentation, and coordinated the project. She also wrote the analysis of structural statistics, authorial consent, and copyright law. Sid Black implemented the model training and evaluation code and processed YouTube Subtitles, Stack Exchange, and GitHub. Laurence Golding implemented deduplication, performed the n-gram analysis, and processed OpenWebText2. Travis Hoppe processed FreeLaw, Pubmed Abstracts, ExPorter, and PhilPapers. Charles Foster performed the topic modeling analysis, contributed to the discussion of authorial consent, and processed USPTO Backgrounds. JasonPhangimplementedandperformedtheGPT2/3 perplexity analysis and advised the project. Horace He performed the bias and sentiment analysis. Anish Thite implemented and performed the profanity analysis and processed Hacker News. Noa Nabeshima processed GitHub. Shawn Presser processed BookCorpus2. Connor Leahy wrote the alignment implication analysis and the model training code.",
        "B Excluded Datasets",
        "1. US Congressional Record. The ofﬁcial record of the United States Congress (1800 – today) records important points of debate at the highest levels of American government. It reﬂects the opinions and biases of the political class over the past 200 years, including segregationism and xenophobia. In particular, we found a large quantity of extremely racist content that we did not feel appropriate for a dataset intended for general-purpose language modeling.",
        "2. Fanﬁction. Hundreds of GiB of fanﬁction has been written and put online, primarily on the websites www.fanfiction.net and www.https://archiveofourown. org/. This represents a signiﬁcant untapped resource for language modeling as it is almost exclusively short-form ﬁction, a writing style that is not represented in most language modeling datasets. We ultimately decided to exclude fanﬁction on logistical grounds: we found other sources of data that were easier to obtain.",
        "3. Literotica. Literoticaisawebsitewhereusers can upload short-form erotic ﬁction. We had originally planned on including it in the Pile and even went as far as scraping and processing it. However we decided to not include it for several reasons. Firstly, once we decided to exclude fanﬁction, Literotica represented our sole source of short-form ﬁction, which would likely lead to undesirable biases in the trained model. Secondly, Literotica would require signiﬁcantly more investigation, assessment, and care than we spent on the other datasets. Thirdly, Literotica contains a signiﬁ- cant amount of stereotyping, including racial fetishes. While Literotica is likely usable for some tasks, we are not comfortable including it in the Pile.",
        "In the course of building the Pile, we considered including and ultimately decided to not use several datasets. We excluded several datasets on the grounds that they were too small to be worth spending time on or because the English component of the data did not merit inclusion on its own. However we also decided to exclude several data sets for other reasons, which we document here for",
        "C Dataset Details",
        "This section contains additional information about each dataset listed in Section 2, including how it was obtained, how it was processed, and any other details relevant for replication. The intent of this section is to provide as much detail as possible, so that Pile can be replicated in the future if necessary, and so that any future processing of these",
        "and similar datasets can use or improve on our methods. As such, all code created for processing has been made publicly available under permissive open source licenses and is referenced in footnotes where applicable.",
        "We extract Common Crawl using jusText (Endrédy and Novák, 2013). Our ﬁltering implementation uses a classiﬁer trained against the OpenWebText2 dataset. We process only a small fraction of the available Common Crawl data; we break the list of urls to individual WARC ﬁles from 2013 to 2020 into 3679 chunks and process 22 random chunks.",
        "C.1.1 WARC vs WET CommonCrawl data is available in two main formats: Web ARChive (WARC) ﬁles, which contain a full record of the crawl as well as the raw HTML of the webpage, and WET ﬁles, which contain preextracted versions of the contents of the WARC ﬁles. The WET ﬁles have poor quality, often containing large amounts of boilerplate text like menus and page footers, but due to the lower bandwidth and computation requirements necessary to use WET ﬁles, prior work based on CC have mainly focused on using WET ﬁles while applying cleaning such as document level ﬁltering (Brown et al., 2020; Wenzek et al., 2019), or n-sentence level deduplication with very aggressive heuristics (Raffel et al., 2019).",
        "C.1.2 Extraction In addition to jusText, we also considered Traﬁ- latura, Newspaper, Goose3, and DragNet. While we were originally intending on creating an extraction benchmark, this proved infeasible given our available resources, and we chose jusText based on visual inspection of the output. In inspection, we noticed that jusText has the characteristic that it discards more data than many other extractors, which is not a major drawback given the large volume of CC data available. This was as expected, given",
        "Due to the difﬁculty of maintaining an acceptable level of extraction quality across all languages, we decided to restrict the scope of the CC dataset to only English and leave a high-quality, fully multilingual, WARC-based CC-based dataset to future work. To ﬁlter for only English, we use the pycld2 library and only attempt to extract text from documents where English is the most common language.",
        "To ﬁlter CC for quality, we follow Brown et al. (2020) in training a classiﬁer to classify between a known high quality dataset and CC. We use fasttext with an n-gram size of 2. We ran experiments using both the entire Pile and just OpenWebText2 as the positive examples, with score distributions on unseen CC data as shown in Figure 9. We decided to use only OpenWebText2 for positive examples for our ﬁnal CC data because of the low sensitivity",
        "We use pandoc 1.19.2.4 (MacFarlane, 2006– 2020) to convert the JATS format data provided by PMC to markdown. Afterwards, we remove any line beginning with :::, which is used by pandoc to indicate html classes in markdown.",
        "To produce the dataset, URLs and their associated metadata were ﬁrst extracted from all Reddit submissions up to April 2020. URLs were deduplicated, with each unique URL featuring a list of associated submissions metadata, and an aggregate score. URLs with an aggregate score of less then 3 were removed. The links were then scraped and processed with Newspaper scraper. Deduplication was performed at the document level using in memory MinHashLSH through the DataSketch library.",
        "C.4.1 Extractor Choice We chose to use Newspaper instead of jusText for OpenWebText2 for consistency with OpenWebTextCorpus. Additionally, by using multiple different html extractors for different components of the Pile, we reduce the potential impact of systematic biases from any one extractor negatively impacting the dataset.",
        "all We downloaded the TEX sources of papers 2020 up dump (the last ﬁle included in our data is arXiv_src_2007_068.tar) via arXiv’s S3 Bulk Source File Access18, and used pandoc 1.19.2.4 to convert these source ﬁles to Markdown, discarding any papers which had errors during the conversion process. This yielded a total of 1,264,405 papers.",
        "We separate the data gathering process into two steps:",
        "their metadata",
        "2. Extracting all text data useful for language",
        "For the ﬁrst step, mirroring the approach of the WebText dataset, we use GitHub ‘stars’ as a proxy for quality, and choose to gather only repositories with more than 100 stars. For practical reasons, we also limit the list of repositories gathered to repositories with less than 1GB of ﬁles. Since Github’s API limits the number of search results to 1000, in order to comprehensively gather all repositories we need to create many small queries that each return fewer than 1000 results in such a way that every repository of interest will be returned by at least one of our queries. To achieve this, we bound our initial search by size to return only repositories between a lower bound of 0 and 5 bytes. At the time of writing, this returns 965 results. For the next step, we set our lower bound one above our previous upper bound, and decide on a new upper bound that should also return fewer than 1000 results by",
        "18https://arxiv.org/help/bulk_data_s3",
        "Figure 9: Score distribution of documents from Common Crawl given different classiﬁer training data.",
        "Because we wanted to limit the size of the overall Pile, we randomly sampled 95.0 GiB of the 630.64 GiB of Github data we collected in total and leave quality ﬁltering to future work.",
        "However, we believe code generation will be an increasingly important component of language models as they continue to scale up and increase in their ability to generalize. As such, we hope to extend this dataset in future work.",
        "We download the court opinions data in bulk from CourtListener,19 and extract the raw text using BeautifulSoup.",
        "To construct the dataset, we download and parse every Stack Exchange database dump to plaintext ﬁles. We opt to extract the top three answers with at least three upvotes, discarding all other responses. We only include the plain text question and response and do not incorporate any metadata. Motivated by large-scale language models’ few-shot ability (Brown et al., 2020), we provide context by prepending all questions and answers with Q:\\n\\n and A:\\n\\n respectively.",
        "The resulting dataset contains a total of 15,622,475 documents across a total of 365 Stack Exchanges and Meta-Stack Exchanges, the bulk of which is from StackOverﬂow.",
        "Using the gathered list of repositories, we clone each one, extract any text-based ﬁles, and discard the rest. Because some repositories took an impractical amount of time to clone and/or extract, we set a hard time limit of 300 seconds for both the git cloning and text extraction steps. As such, some larger repositories may only be partially extracted. We also impose a ﬁle size limit of 100kB on extracted ﬁles, as we found that the majority of ﬁles over that size were typically very repetitive autogenerated source ﬁles or data ﬁles, and that setting this ﬁle size limit was an effective cleaning step to limit the data to code.",
        "text of all patents granted in the US from 1976 to September 2020. From these archives, we extract the Background sections, along with key grantspeciﬁc metadata, such as the inventor, assignee, and classiﬁcation information.",
        "TheﬁleformatusedforstoringbulktextUSpatents has changed over time. Prior to 2002, all of the datasets are in a specialized format called APS (Automated Patent System). Since 2002, the data is XML encoded. Partially as a function of this change, the location of the \"Background\" section has also shifted. Our converter accounts for these structural shifts and extracts the raw text from each patent’s Background.",
        "About one-third of the articles in the dataset were missing or contained a malformed title or abstract and were excluded. Additionally, PubMed Central (see Section 2.2) contains full-text resources to many recent publications; any publications which already appear in PMC are excluded from this set. To process the data, we concatenated the title and abstract and removed any copyright information. The remaining dataset contains 15,518,009 titles and abstracts.",
        "To create the text dataset, we simply extract the subtitle text from each XML ﬁle in the English language dataset provided by Tiedemann (2016), discarding any provided metadata.",
        "We use the wikipedia/20200301.en dataset from TensorFlow Datasets.20 We prepend the title to the body of each article, separated by two newlines.",
        "20https://www.tensorflow.org/datasets/",
        "To process the data, all system messages, such as joins, disconnects, nick changes, etc. were discarded, but actions (i.e using /me) were kept. Timestamps were removed, and all logs for the same channel in a given week were concatenated into a single document, with each the logs for each day prepended with the date if that day’s log is non-empty.",
        "• Correctly renders tables of data, whereas by default html2txt produces poor-quality results for tables,",
        "C.17 EuroParl We download the data in bulk from 21. We remove all basic tag information and only retain the name of each document as a title. For example, <SPEAKER ID=77 LANGUAGE=\"NL\" NAME=\"Pronk\"> becomes Pronk, and then extract the body of each document, discarding those that are shorter than 200 characters.",
        "We ﬁrst use the Hackernews BigQuery dataset to obtain a list of all story ids in our date range. For the Pile we use the ﬁrst Hacker News post (1) to post number 24531712. This corresponds to a date range of approximately 10/09/2006 to 09/20/2020. We use the BigQuery dataset to gather story ids for efﬁciency purposes. However, the BigQuery dataset was lacking some information for stories, so we used the ofﬁcial Hacker News API for story and comment text retrieval.",
        "Hacker News displays and stores comments in a tree-like manner, with children comments replying to parent comments. However, most language models require input data to be in a sequential form. Considering each path through the comment tree as a sequence could be detrimental, since there will be a large amount of near-duplicate comment sequences. In addition, only taking one path through the comment tree for each story leaves out a large portion of the comment data. Therefore, we parsed comments in a hybrid form. For every top-level comment(commentsthathavenoparentcomment), we create a sequence of comments by traversing down the comment tree from the top-level comment. We choose the next comment by taking the child comment with the highest number of children comments (a cheap attempt at taking a long path through the comment tree, note that it does not take the longest possible path).",
        "We construct the dataset in three stages:",
        "The PhilPapers (PP) are indexed using OAI-MPH, the Open Archives Initiative Protocol for Metadata Harvesting. As such, the ﬁrst step to collect the data is to get the XML for all links. This was done using pyoaiharvester.23",
        "From that, each publication is downloaded. Some entries do not exist, or have been removed by the authors. Papers with text are extracted using pdfbox, and papers with non-machine readable text are ignored. Non-English language publications are kept, and the metadata reﬂects the language reported by the OAI-MPH XML. The text is ﬁltered with pdf_filter.py from PDFextract, and we discard any papers with less than 1000 characters.24",
        "The NIH provides a bulk-data repository for awarded applications through the ExPORTER service covering the ﬁscal years 1985–present. These datacomefromtheNIH,butalsootherotherHealth and Human Services agencies (ACF, AHRQ, CDC, HRSA, FDA), and the VA. Additionally, the NIH",
        "provides a legacy data format named CRISP for awarded applications during the ﬁscal years 1970– 2009.",
        "We merged both the ExPORTER and CRISP data to form a consolidated dataset of awarded applications. Entries were deduplicated based off their application ID, and excluded if their abstract text was missing or too short. Small grants, especially administrative ones, consisted solely of short boilerplate. For this reason, we further deduplicated on abstract text. All grants types were considered, including new applications (Application Type Code 1) and renewals (Application Type Code 2) as the text differed enough to provide novel input. The text was then minimally parsed to remove administrative boilerplate, (ex. most old awards contain some variation of “description: (provided by applicant)\"). In total, there were 939,668 grant application abstracts added.",
        "To extract the data, we used the mailparser package25 to extract the body of each email as a document.",
        "D General Data Processing",
        "This section discusses any processes applied across multiple datasets.",
        "To combine the constituent datasets, we iterate until the size of the output dataset is the desired size, drawing documents from datasets at random, weighted by the number of documents in each dataset times the number of epochs desired on that dataset. Because the number of documents involved is high, by the law of large numbers, the number of copies of each dataset present in the Pile is approximately equal to its epoch count.",
        "Shufﬂing a dataset posed a major problem due to our limited memory and computational budget. We follow Hardin (2018), a method descended from Rao (1961), and interleave our output to produce 30 output piles.",
        "We hold out approximately 10GiB of data from the Pile, of which 2GiB are used to create the validation and test splits, and the remainder is held in reserve. From the training set, we remove any",
        "elements that are also present verbatim in any of the held out data, to prevent leakage.",
        "Similar to Brown et al. (2020), we increase the weight of certain components such that the number of epochs elapsed on data we consider high quality is greater than one. Our choice of weights was primarily informed by the source of the data and the size of the dataset; we attempted to upweight academictextsthe most, whichwe feltprovidedthe highest quality data, as well as smaller sets, such that they would have a more pronounced impact on the data. We strictly disallowed any data more than 3 epochs and avoided having any data with more than 2 epochs.",
        "The same technique was used for both OpenWebText2 and Common Crawl—MinHashLSH with the Python Datasketch library.26 We used 10 hash functions for each Minhash and an approximate Jaccard similarity of 0.5. This produced a duplicate rate of 28% in OpenWebText2 and 26% for Common Crawl.",
        "The main challenge here was computational, leading us on a journey through the various LSH persistence options. A simple quadratic Minhash comparison of all documents would have taken several hundred thousand years, motivating the use of LSH. Initially, we did not have sufﬁcient RAM for inmemory LSH and chose to use the Cassandra backend when de-duplicating OpenWebText2. This was reasonably fast, but the same method resulted in a corrupted database about 3 4 of the way through processing Common Crawl. After the Cassandra corruption, we brieﬂy tested the experimental Mongo implementation; however this was quite slow due to the nature of Mongo itself. In the end, we ran in-memory LSH on a machine with enough RAM for Common Crawl, taking several days.",
        "26https://github.com/ekzhu/datasketch",
        "To avoid leakage of data from downstream evaluations, recent work (Radford et al., 2019; Brown et al., 2020; Shoeybi et al., 2019) has removed any data in the training set that may overlap with the evaluation metrics. We decided not to perform any such removal, because it is impossible to anticipate all potential downstream evaluation metrics, and so any particular selection of metrics would inevitably either become obsolete as the choice of benchmarks in the ﬁeld changes, or potentially hinder thedevelopment ofnewbenchmarks formodels trained on Pile.",
        "For models trained on Pile and evaluated on metrics other than Pile’s own validation and test sets, we encourage authors to remove overlaps between Pile and the validation data of these additional downstream evaluations. We do not anticipate that such leakage removal will hurt model performance, as the validation sets of most benchmarks are very small in relation to the size of the Pile, and so choosing to evaluate on more metrics will not be a disadvantage for any model.",
        "E Investigating data",
        "As part of our exploratory analysis, we calculated the counts of all 13-grams across Common Crawl. We chose n = 13 due to its use in prior work (Brown et al., 2020). There were a total of 40,216,231,078 different 13-grams in this dataset. The 1000 most common range from 11 million occurrences down to 20k.",
        "To compute the perplexity for a given dataset, we tokenizeeachdocumentseparately, dividethedocumentintosegmentsofuptothemaximumsequence length of the model (1024 tokens for GPT-2, 2048 for GPT-3), and predict the logits of the each segment. The inputs to the model are the immediate prior tokens the e.g. for scoring tokens 1 to 1024, we provide tokens 0 to 1023 at the input context. The respective language model implementations handle the causal attention masking. This ensures that every token in the dataset is scored exactly once. This also means that some tokens will have more input context than others. We then aggregate over the whole dataset and compute the ﬁnal per-",
        "plexity score. The perplexity for the whole Pile is computed by aggregating over the constituent datasets (i.e. weighted by dataset size, not a simple average of dataset perplexities). Both GPT-2 and GPT-3 share the same tokenizer and vocabulary, making the perplexity scores directly comparable. We use the Hugging Face (Wolf et al., 2020) implementation of GPT-2, and the OpenAI API for GPT-3. The davinci model in the OpenAI API is presumed to correspond to a 175B parameter version of GPT-3.",
        "In Table 8 we show the test set perplexities (i.e. not normalized by UTF-8 length, as in Table 2). Because of the costs associated with using the OpenAI API,wecomputetestperplexitiesononlyone-tenth of the test set in Tables 8 and Table 2. Speciﬁcally, we randomly sample one-tenth of the documents of each dataset except for three: Ubuntu IRC, BookCorpus2, and PhilPapers. In Table 9, we show test perplexity computed on the full test set on all GPT-2 models.",
        "Figure 11: Test loss (log perplexity) over the Pile, bucketed by position in the input sequence based on the model’s maximum sequence length. To smooth out the lines, we bucket 4 positions per plotted datapoint. (e.g. positions 0–3, positions 2044–2047). Later tokens are predicted with more context and thus see lower perplexities.",
        "Initially we decided on separating pejorative content into 4 groups: sex-related terminology, slurs, neither of these categories, and both of these categories. We adapted a public \"naughty words\" list and broke them into these categories with the intern of looking at the proportion of each category in each dataset. However, this provided many issues.",
        "F Data Samples",
        "The following consists of two random, noncherrypicked 512-byte samples from each constituent dataset of the Pile, sampled from the validation split.",
        "Identiﬁcation of stakeholder community rate highly has proved informative in assisting us to describe a \"work ready *plus\"* medical imaging graduate for the New Zealand context. The results have provided data to the curriculum development team allowing them",
        "![Gamma-ray spectrum at Mt. Norikura (2.77 km a.s.l). The vertical axis is Flux$\\times Eˆ2$. Our data is at $<$ 100 GeV. Data above 300 GeV is from emulsion chamber experiments. For the latter, see Sec.\\[discuss\\] []{data-label=\"norispec\"}](norikura.eps){width=\"7.5cm\"}",
        "![The altitude variation of the ﬂux integrated over 6 GeV. The dpmjet3.03 and fritiof7.02 give almost the same feature consistent with the observation while the deviation of fritiof1.6 from the data is obvious. \\[trans",
        "l and behavioral research projects utilizing primates residing in a seminatural habitat. This population has the most extensive computerized demographic and genetics database available to researchers anywhere in the world. The population management program for CS has been designed to optimize the health and well-being of the monkeys, to enhance the value of the colony for research. In addition, the goal is to provide healthy animals to the scientiﬁc community for biomedical research, including AIDS and Sl",
        "Figure 13: The average sentiment co-occurrence with each gender across all datasets.",
        "Figure 14: The average sentiment co-occurrence with each religious word across all datasets. Each dataset’s sentiments have been normalized by the maximum norm sentiment for that dataset.",
        "Table 8: Test perplexity of the Pile using GPT-2 and GPT-3. Evaluation is performed on one-tenth of the test data of the Pile, on a per-document basis.",
        "Topic #1 Generic Cells Unknown US Politics Data Unknown Appeals Software Data Organ Transplant Unknown Unknown Education",
        "Topic #5 Leisure Cells Unknown Business Physics C/C++ Appeals Applications Data Ophthalmology",
        "Topic #13 Geography Cells Unknown Military Physics Java Legal HTML/CSS Data Clinical",
        "Topic #8 Entertainment Cells Unknown Generic Dynamics Unknown Appeals Users Data Fluids",
        "Topic #1 like time good use want cells data study cell results time said like new know said trump president house state case given time let data y d b abbr j court trial evidence case state run q server project use signal system invention memory line liver group acute transplantation renal said time little man great know right come got like category university school american college let pm minutes factor divided ubuntu like think bug need said like time know eyes european mr commission president europe like people work time use like know going think right theory case φ reduction paradox cells cell studies research study subject pm new time energy",
        "Topic #2 people said government war right cells cell data ﬁgure et like said time man way said court law case state let function case given order return function div var value court state case trial evidence data option pdf q rails invention data power voltage frequency activity nerve stimulation induced muscle said time little like man like know come right want people government category political chinese letters let replacement sequence prob ubuntu bug like think snap said like know way eyes european president commission mr union like people work time use know like going people time philosophy case moore physical theory cell studies research cells study pm subject enron cc know",
        "Topic #3 said like time got going study cells c data group like said time new right team season game said players function case let state model void f license v countries court defendant state states trial function server use thread client invention surface having present liquid species study studies associated risk said man time great men know oh right yeah like category players people football born collect terms positive assuming simplify like ubuntu know created ok said know like time right mr european president commission parliament like people work time think like going right time know case science theory set world cell research gene development study pm new enron time image",
        "Topic #4 system surface x high second study data patients cells analysis said like time know new people like said school life let set following number x return public int case null court district plaintiff defendant motion array int value like code pressure system invention cells use dose mg rats effects effect said time great man little know like oh got right championship category driver cars car let suppose solve nearest c ubuntu like think need yeah said like time know going european mr president commission energy people time like way work like going guest people think self case science theory analysis research determine health speciﬁc cells enron e mail new subject",
        "Topic #5 new dating art day city data study patients cells c said like time new way said government year market business phys data energy ﬁeld b const typename return void template court trial evidence defendant united device spring android app boot data system memory information devices retinal eye lens corneal laser said great like man little know right like oh want ﬁlm category ﬁlms new television let common calculate suppose highest ubuntu like think yeah know said like time know going european mr commission president parliament people like time think use like think people know going theory science physical de case cells speciﬁc cell study aim subject enron said sent image",
        "Topic #6 like time game good food patients cells study cell analysis said like time new good data use google system new let model ﬁeld system energy fa var span ﬁle key court defendant trial evidence states ﬁle image ﬁles echo path high air light invention temperature strains isolates resistance resistant bacteria said man time like day know like right got come category music album song released let solve suppose base calculate like ubuntu think good yeah said like know time head commission president european mr parliament like people work time think like know going look host case theory science s epistemic studies development patients analysis clinical subject pm enron database sent",
        "Topic #7 water plants food climate plant patients study data cells analysis said like way new time people law american god world let given case number theorem err return nil func error court defendant case law motion ﬁle line error import python al et invention present water p levels patients blood increased said man time little like know right yeah got let category railway station line new factors prime replacement letters list ubuntu like bug use know said like time know going european commission mr president council like people time think work like know think right going derivation reduction φ paradox ψ research study development speciﬁc use hou pm subject e time",
        "Topic #8 music like book new ﬁlm cells data cell analysis patients said new like time man like world time people day time case let set given import msgstr msgid insert license court state evidence defendant district q like use user set invention circuit data present signal activity acid high water concentration man said like time little know got right like oh system category energy work systems solve remainder divided calculate true ubuntu good snap use like said know time like going european commission mr union parliament people like work good use like people know going time case de set science theory study research clinical studies development ﬁnal enron schedule energy information",
        "Topic #9 students school work university research cells expression patients cell study said like time man good drug cannabis drugs marijuana women time function r model al string license def public import court defendant state trial plaintiff x y q d c image light data optical system women patients positive hiv cancer said little time man old know right got like oh align season points game right common let divided calculate factor ubuntu like time think snap said like know right time mr commission president european iran people like think time use like know people going time theory s case belief experience cells research speciﬁc studies role time new enron power subject",
        "Topic #10 said state government law court data cells patients study c said like time know way like time new game way let number model system theorem x z divide var y court â law case state text color width font height device invention layer ﬁlm power health data care based study said man little like time like know right think oh category new state united states let derivative wrt second ﬁnd ubuntu like need use snap said like know time going european mr president commission parliament like people time think use like time know going think theory science set de philosophy research cells cell studies study subject pm friday sent october",
        "Topic #11 home room house hotel area cells study cell data ﬁgure said like time way new city new unlockable building said let set space model given end values list color table court plaintiff state case evidence code n use int include invention material light method high bone asthma vaccine study sperm said man little time great know like come right good category game ﬁlm series video derivative wrt ﬁnd express rearrange ubuntu think like yeah use said like know eyes time mr european commission parliament president people like data time work like host guest know look φ reduction theory paradox derivation disease research study cells cell hou enron subject cc na",
        "Topic #12 use business data information new study data p ﬁg cells said like time new know said police people man old let theorem given case x deﬁne software copyright include endif court defendant states plaintiff case b q class n k invention present device object provide patients treatment group clinical patient said great little man like know come got oh right category population species age oil let suppose solve b c like ubuntu think use need said like time know going european mr commission president parliament use like work time think like think know people going theory case order space new cells cell study disease human image enron pm hou subject",
        "Topic #13 said new city years police study p cells c patients said like time people man game war party military said model order energy let phys void value public return class court district states case united div page function var form data network system user information patients disease study cases age tr said man time little know right like yeah come category county district references village let suppose value b simplify ubuntu like think work time said like know going time european commission mr parliament president like people time work data like think right know people epistemic science belief theory system cells speciﬁc development studies research subject message pm know cc",
        "Topic #14 game team season year said cells study data time group said like new people man ﬂight caption aircraft add water let phys order model case int struct return case static court plaintiff state case district string return public new class optical surface device invention system method artery surface energy optical said man great time like know right oh like come season team league player nﬂ base c common picked b ubuntu like think yeah yes said like eyes time going european mr commission parliament president people time like work good like know want going right theory case science physics theories research cells studies cell project hou enron subject cc gas",
        "Topic $15 health medical care treatment body patients study cells et data said time like man new study research time climate found x let time ﬁeld set return self size long string court trial state defendant case table select question like q image light device sheet display cells cell expression gene protein time said men man like know right like think come category century war new de digit terms collect thousands let ubuntu like think good à said like know time going european mr commission parliament president like people time use think like know going people look φ derivation reduction t paradox cell research cells speciﬁc studies space alias disk enron said",
        "Topic #16 people like know think life study cells patients cancer ﬁgure said time new people like v granada club m cent let model data set function var assert text label check court states district united trial android new public import void invention layer substituted et group binding protein receptor dna beta tr said man time little know like right want got category new states law american let suppose derivative c determine like ubuntu need ok juju said like know looked going european mr parliament commission president like time people think use like know people think going case φ s derivation theory research studies clinical determine cancer hou disk space alias e"
    ]
}