{
    "title_author_abstract_introduction": "Scalable Person Re-identiﬁcation: A Benchmark\nLiang Zheng†‡∗, Liyue Shen†∗, Lu Tian†∗, Shengjin Wang†, Jingdong Wang§, Qi Tian‡\n†Tsinghua University\n§Microsoft Research\n‡University of Texas at San Antonio\nAbstract\nThis paper contributes a new high quality dataset for person re-identiﬁcation, named “Market-1501”. Generally, current datasets: 1) are limited in scale; 2) consist of hand-drawn bboxes, which are unavailable under realistic settings; 3) have only one ground truth and one query image for each identity (close environment). To tackle these problems, the proposed Market-1501 dataset is featured in three aspects. First, it contains over 32,000 annotated bboxes, plus a distractor set of over 500K images, making it the largest person re-id dataset to date. Second, images in Market-1501 dataset are produced using the Deformable Part Model (DPM) as pedestrian detector. Third, our dataset is collected in an open system, where each identity has multiple images under each camera.\nAs a minor contribution, inspired by recent advances in large-scale image search, this paper proposes an unsupervised Bag-of-Words descriptor. We view person reidentiﬁcation as a special task of image search. In experiment, we show that the proposed descriptor yields competitive accuracy on VIPeR, CUHK03, and Market-1501 datasets, and is scalable on the large-scale 500k dataset.\n1. Introduction\nThis paper considers the task of person re-identiﬁcation. Given a probe image (query), our task is to search in a gallery (database) for images that contain the same person. Our work is motivated by two aspects. First, most existing person re-identiﬁcation datasets [10, 44, 4, 13, 22, 19] are ﬂawed either in the dataset scale or data richness. Speciﬁcally, the number of identities is often conﬁned in several hundred. This makes it infeasible to test the robustness of algorithms under large-scale data. Moreover, images of the same identity are usually captured by two cam-\n* Three authors contribute equally to this work. Dataset and code are available at http://www.liangzheng.com.cn.\neras; each identity has one image under each camera, so the number of queries and relevant images is very limited. Furthermore, in most datasets, pedestrians are well-aligned by hand-drawn bboxes (bboxes). But in reality, when pedestrian detectors are used, the detected persons may undergo misalignment or part missing (Fig. 1). On the other hand, pedestrian detectors, while producing true positive bboxes, also yield false alarms caused by complex background or occlusion (Fig. 1). These distractors may exert nonignorable inﬂuence on recognition accuracy. As a result, current methods may be biased toward ideal settings and their effectiveness may be impaired once the ideal dataset meets reality. To address this problem, it is important to introduce datasets that reach closer to realistic settings.\nSecond, local feature based approaches [11, 40, 38, 3] are proven to be effective in person re-identiﬁcation. Considering the “query-search” mode, this is potentially compatible with image search based on the Bag-of-Words (BoW) model. Nevertheless, some state-of-the-art methods in person re-identiﬁcation rely on brute-force featurefeature matching [39, 38]. Although good recognition rate is achieved, this line of methods suffer from low computational efﬁciency, which limits its potential in large-scale applications. In the BoW model, local features are quantized to visual words using a pretrained codebook. An image is thus represented by a visual word histogram weighted by TF-IDF scheme. Instead of performing exhaustive visual matching among images [39], in the BoW model, local features are aggregated into a global vector.\nConsidering the above two issues, this paper makes two contributions. The main contribution is the collection of a new person re-identiﬁcation dataset, named the “Market1501” (Fig. 1). It contains 1,501 identities collected by 6 cameras. We further add a distractor set composed of 500K irrelevant images. To our knowledge, Market-1501 is the largest person re-id dataset featured by 32,668+500K bboxes and 3,368 query images. It is distinguished from existing datasets in three aspects: DPM detected bboxes, the inclusion of distractor images, and multi-query, multi-ground truth per identity. This dataset thus provides a more real-\nFigure 1. Sample images of the Market-1501 dataset. All images are normalized to 128×64 (Top:) Sample images of three identities with distinctive appearance. (Middle:) We show three cases where three individuals have very similar appearance. (Bottom:) Some samples of the distractor images (left) as well as the junk images (right) are provided.\nistic benchmark. For accuracy evaluation, we propose to use mean average precision (mAP), a more comprehensive measurement compared to the commonly used Cumulated Matching Characteristics (CMC) curve [38, 39, 20].\nAs a minor contribution, inspired by the state-of-the-art image search systems, an unsupervised BoW representation is proposed. After generating a codebook on training data, each pedestrian image is represented as a visual word histogram. In this step, a number of techniques are integrated, e.g., root descriptor [2], negative evidences [14], burstiness weighting [16], avgIDF [41], etc. Moreover, several further improvements are adopted, i.e., weak geometric constraints, Gaussian Mask, multiple queries, and reranking. By simple dot product as similarity measurement, we show that the proposed BoW representation yields competitive recognition accuracy while enjoying a fast response time.",
    "data_related_paragraphs": [
        "3. The Market-1501 Dataset",
        "In this paper, a new person re-id dataset, the “Market1501” dataset, is introduced. During dataset collection, a total of six cameras were placed in front of a campus supermarket, including ﬁve 1280 1080 HD cameras, and one × 576 SD camera. Overlapping exists amongthese cam720",
        "Datasets # identities # BBoxes # distractors # cam. per ID DPM or Hand Evaluation",
        "632 1,264 0 2 hand CMC Table 1. Comparing Market-1501 with existing datasets [20, 10, 44, 22, 19, 4].",
        "eras. This dataset contains 32,668 bboxes of 1,501 identities. Due to the open environment, images of each identity are captured by at most six cameras. We make sure that each annotated identity is captured by at least two cameras, so that cross-camera search can be performed. Overall, our dataset has the following featured properties.",
        "First, while most existing datasets use hand-cropped bboxes, the Market-1501 dataset employs a state-of-the-art detector, i.e., the Deformable Part Model (DPM) [9]. Based on the “perfect” hand-drawn bboxes, current methods do not fully consider the misalignment of pedestrian images, a problem which always exists in DPM based bboxes. As is shown in Fig. 1, misalignment and part missing are common among the detected images.",
        "Second, in addition to the false positive bboxes, we also provide false alarms. We notice that the CUHK03 dataset [20]alsousestheDPMdetector, butthebboxesinCUHK03 are relatively good ones in terms of detector. In fact, a large number of the detected bboxes would be very “bad”. Considering this, for each detected bbox to be annotated, a hand-drawn ground truth bbox is provided (similar to [20]). Different from [20], for the detected and hand-drawn bboxes, the ratio of the overlapping area to the union area is calculated. In our dataset, if the area ratio is larger than 50%, the DPM bbox is marked as “good” (a routine in object detection [9]); if the ratio is smaller than 20%, the DPM bbox is marked as “distractor”; otherwise, the bbox is marked as “junk” [27], meaning that this image is of zero inﬂuence to re-id accuracy. Moreover, some obvious false alarm bboxes are also marked as “distractors”. In Fig. 1, examples of “good” images are shown in the top two rows, while “distractor” and “junk” images are in the bottom row. These images undergo extensive variations in pose, resolution, etc. Third, each identity may have multiple images under each camera. Therefore, during cross-camera search, there may be multiple queries and multiple ground truths for each identity. This is consistent with practical usage, especially where multiple queries can be fully exploited to obtain more discriminative information about the person of interest. In terms of performance evaluation, for a re-id system, a perfect method should be able to locate all instances of the query identity. In this sense, our dataset provides testbed for methods applied in open systems.",
        "Figure 2. Sample images of the distractor dataset.",
        "3.2. A Distractor Dataset",
        "We emphasize that scale is a vital problem for person re-id studies. Therefore, we further augment the Market1501 dataset with an additional distractor set. This dataset contains over 500,000 bboxes, consisting of false alarms on the background, as well as pedestrians not belonging to the 1,501 identities. Sample images are shown in Fig. 2. In the experiment, apart from the Market-1501 dataset, we will also report the results on the enlarged Market-1501 + 500K dataset.",
        "Astatisticscomparisonwithexistingdatasetsisshownin Table 1. Our dataset contains 1,501 identities, which is lower than CUHK02 [19]. With respect to this point, we plan to release version 2.0 to include more identities. The original dataset contains 32,668 fully annotated bboxes, making it the largest person re-id dataset to date. Since images containing a pedestrian are annotated with a hand-drawn bbox as well as an ID, this dataset can also be used for pedestrian detection. Moreover, our dataset is greatly enlarged by the 500K distractor images, and efﬁciency/scalability analysis can be reliably done. Compared with other benchmark datasets, Market-1501 is also featured by 6 cameras. In place of a close-system with 2 cameras only, our dataset serves as an ideal benchmark for metric learning methods, so that their generalization capacities can be evaluated for practical usages.",
        "Current datasets typically use the Cumulated Matching Characteristics (CMC) curve to evaluate the performance of person re-id algorithms. The CMC curve shows the probability that a query identity appears in different-sized candidate lists. This evaluation measurement is valid only if there is only one ground truth match for a given query (see",
        "For Market-1501 dataset,",
        "Our dataset is randomly divided into training and testing sets, containing 750 and 751 identities, respectively. During testing, for each identity, we select one query image in each camera. Note that, the selected queries are hand-drawn, instead of DPM-detected as in the gallery. The reason is that in reality, it is very convenient to interactively draw a bbox, which can yield higher recognition accuracy [20]. The search process is performed in a cross-camera mode, i.e., relevant images captured in the same camera as the query are viewed as “junk”. In this scenario, an identity has at most 6 queries, and there are 3368 query images in total. Queries of two sample identities are shown in Fig. 4.",
        "Figure 4. Sample query images. In Market-1501 dataset, queries are hand-drawn bboxes. Each identity has at most 6 queries, one for each camera.",
        "view, etc. We describe the individual steps below. Feature Extraction. We employ the Color Names (CN) descriptor [32]. Given a pedestrian image normalized to 128 4 are densely sampled. The sampling step is 4, so there is no overlapping between patches. For each patch, CN descriptors of all pixels are calculated, and are subsequently ℓ1 normalized followed by ) operator [2]. The mean vector is taken as the descrip√( tor of this patch (see Fig. 5). Codebook. For Market-1501, we generate a codebook on its training set. For other datasets, the codebook is trained on the independent TUD-Brussels dataset [35]. Standard k-means is used, so codebook size is k. Quantization. Given a local descriptor, we employ Multiple Assignment (MA) [15] to ﬁnd its near neighbors under Euclidean distance in the codebook. We set MA = 10, so a feature is represented by the indices of 10 visual words. TF-IDF. The visual word histogram is weighted by TF-IDF scheme. TF encodes the number of occurrences of a visual word, and IDF is calculated as log N ni, where N is the number of images in the gallery, and ni is the number of images containing visual word i. In this paper, we use the avgIDF [41] variant in place of the standard IDF. Burstiness. Burstiness refers to the phenomenon where a query feature ﬁnds multiple matches in a test image [16]. For CN descriptor, burstiness could be more prevalent due",
        "5.1. Datasets",
        "VIPeR dataset [10] is composed of 632 identities, and each has two images captured from two different cameras. All 48 pixels. VIPeR is ranimages are normalized to 128 domly divided into two equal halves, one for training, and the other for testing. Each half contains 316 identities. For each identity, we take an image from one camera as query, and perform cross-camera search. CUHK03 dataset [20] contains 13,164 DPM bboxes of 1,467 identities. Each identity is observed by two cameras and has 4.8 images in average for each view. Following the protocolin[20], forthetestset, werandomlyselect100persons. For each person, all the images are taken as query in turns, and a cross-camera search is performed. The test process is repeated 20 times. We report both the CMC scores and mAP for VIPeR and CUHK03 datasets.",
        "Codebook size k. In our experiment, codebooks of various sizes are constructed, and mAP on Market-1501 dataset is presented in Table 2. We set k = 350 where the peak value is achieved. Number of stripes M. Table 3 presents the performance of different numbers of stripes. As the stripe number increases, a ﬁner partition of the pedestrian image leads to a more discriminative representation. So the recognition accuracy increases, but recall may drop for a large M. As a trade-off",
        "Table 5. Results (rank-1, rank-20 matching rate, and mean Average Precision (mAP)) on three datasets by combining different methods, i.e., the BoW model (BoW), Weak Geometric Constraints (Geo), Background Suppression (Gauss), Multiple Queries by average (MultiQ avg) and max pooling (MultiQ max), and reranking (Rerank). Note that, here we use the Color Names descriptor for BoW.",
        "Figure 6. Performance of different method combinations on VIPeR and CUHK03 datasets.",
        "First, the baseline BoW vector produces a relatively low accuracy: rank-1 accuracy = 9.04%, 10.56%, and 5.35% on Market-1501, VIPeR, and CUHK03 datasets, respectively. Second, when we integrate geometric constraints by stripe matching, we observe consistent improvement in ac-",
        "curacy. On Market-1501 dataset, for example, mAP increases from 3.26% to 8.46% (+5.20%), and an even larger improvement can be seen from rank-1 accuracy, from 9.04% to 21.23% (+12.19%).",
        "Third, it is clear that the Gaussian mask works well on all three datasets. We observe +5.64% in mAP on Market1501 dataset. Therefore, the prior that pedestrian is roughly located in the center of the image is statistically sound.",
        "Then, we test multiple queries on CUHK03 and Market1501 datasets, where each query identity has multiple bboxes. Results suggest that the usage of multiple queries further improves recognition accuracy. The improvement is more prominent on Market-1501 dataset, where the query images take on more diverse appearance (see Fig. 4). Moreover, multi-query by max pooling is slightly superior to average pooling, probably because max pooling gives more weights to the rare but salient features and improves recall.",
        "Finally, we observe from Table 4 and Table 5 that reranking generates higher mAP. Nevertheless, one recurrent problem with reranking is the sensitivity to the quality of initial rank list. On Market-1501 and CUHK03 datasets, since a majority of queries DO NOT have a top-1 match, the improvement in mAP is relatively small. Results between camera pairs. To further understand the Market-1501 dataset, we provide the re-id results between all camera pairs in Fig. 7. We use the ”BoW+Geo+Gauss”",
        "representation. It is easy to tell that re-id within the same camera yields the highest accuracy. On the other hand, as expected, performance among different camera pairs varies a lot. For camera pairs 1-4 and 3-5, the BoW descriptor generates relatively good performance, mainly because the two camera pairs share more overlap. Moreover, camera 6 is a 720 576 SD camera, and captures distinct background with other HD cameras, so re-id accuracy between camera 6 and others are quite low. Similarly low result can be observed between camera pairs 5-1 and 5-2. We also compute the cross-camera average mAP and average rank-1 accuracy: 10.51% and 13.72%, respectively. We weight mAPs between different camera pairs according to their number of queries, and do not calculate the results on the diagonals. Compared with the ”BoW+Geo+Gauss” line in Table 5, both measurements are much lower than pooling images in all cameras as gallery. This indicates that re-id between camera pairs is very challenging on our dataset. Comparison with the state-of-the-arts. We compare our",
        "Table 7. Average query time of different steps on Market-1501 dataset. For fair comparison, Matlab implementation is used.",
        "On CUHK03, our method without multiple-query significantly outperforms almost all presented approaches. Compared with FPNN [20] which builds a deep learning architecture, our accuracy is slightly lower by 1.00%. But when multiple-query and HS feature are integrated, rank-1 matching rate exceeds [20] by +4.44% on CUHK03 dataset.",
        "Some sample results on Market-1501 dataset are provided in Fig. 9. Apart from the mAP increase with the method evolution, another ﬁnding which should be noticed is that the distractors detected by DPM on complex background or body parts severely affect re-identiﬁcation accuracy. Previous works typically focus on “good” bounding boxes with person only, and rarely study the detector errors. Large-scale experiments. First, on Market-1501, we compare our method with SDALF [8] and SDC [39] in two aspects, i.e., feature extraction and search time. We use the Matlab implementation by the authors and for fair comparison, run our algorithm in Matlab too. Evaluation is per-",
        "Figure 9. Sample results on Market-1501 dataset. Four rows correspond to four conﬁgurations, i.e., “BoW”, “BoW + Geo + Gauss”, “BoW + Geo + Gauss + MultiQ”, and “BoW + Geo + Gauss + MultiQ + Rerank”. The original query is in blue bbox, and the added multiple queries are in yellow. Images with the same identity as the query is in green box, otherwise red.",
        "Database Size (K)",
        "Database Size (K)",
        "(a) mAP vs database size",
        "(b) Query time vs database size",
        "Figure 10. mAP (a) and query time (b) in Market-1501+500K dataset. Dashed lines are obtained by exact NN search, while solid lines represent ANN search.",
        "Then, we experiment on the Market-1501+500K dataset. Images in the 500K dataset are treated as outliers. For efﬁciency, we use the Approximate Nearest Neighbor (ANN) algorithm proposed in [33]. During index construction, we build 4 kd-trees, and store 50 neighbors for each datum in the knn-graph. The number of neighbors returned is 1000",
        "Re-id performance on the large-scale dataset is presented in Fig. 10. As the database gets larger, accuracy drops. On Market-1501+500K dataset, when ANN is used, an mAP of 10.92% is achieved for “BoW + MultiQ max”. Compared with result on the original dataset, a relative drop of 69.7% is observed. As a result, database size has a signiﬁcantly negative effect on performance, which has been rarely discussed in literature. Moreover, although ANN marginally decreases re-id accuracy, the beneﬁt it brought obvious. With ANN, query time is 127.5ms on the 500K dataset, a 50x speedup compared with the NN case.",
        "This paper ﬁrstly introduces a large-scale re-id dataset, Market-1501 (+500k), which reaches closer to realistic settings. Then, a BoW descriptor is proposed in the attempt to bridge the gap between person re-id and image search. The new dataset will enable research possibilities in multiple directions, e.g., deep learning, large-scale metric learning, multiple query techniques, search reranking, etc. In the future, current test data will be treated as validation set, and new test IDs will be annotated and presented in a coming person re-id challenge."
    ]
}