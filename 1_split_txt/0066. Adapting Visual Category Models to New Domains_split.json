{
    "title_author_abstract_introduction": "Adapting Visual Category Models to New Domains\nKate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell\nUC Berkeley EECS and ICSI, Berkeley, CA {saenko,kulis,mfritz,trevor}@eecs.berkeley.edu\nAbstract. Domain adaptation is an important emerging topic in computer vision. In this paper, we present one of the ﬁrst studies of domain shift in the context of object recognition. We introduce a method that adapts object models acquired in a particular visual domain to new imaging conditions by learning a transformation that minimizes the eﬀect of domain-induced changes in the feature distribution. The transformation is learned in a supervised manner and can be applied to categories for which there are no labeled examples in the new domain. While we focus our evaluation on object recognition tasks, the transform-based adaptation technique we develop is general and could be applied to non-image data. Another contribution is a new multi-domain object database, freely available for download. We experimentally demonstrate the ability of our method to improve recognition on categories with few or no target domain labels and moderate to large changes in the imaging conditions.\n1 Introduction\nSupervised classiﬁcation methods, such as kernel-based and nearest-neighbor classiﬁers, have been shown to perform very well on standard object recognition tasks (e.g. [4], [17], [3]). However, many such methods expect the test images to come from the same distribution as the training images, and often fail when presented with a novel visual domain. While the problem of domain adaptation has received signiﬁcant recent attention in the natural language processing community, it has been largely overlooked in the object recognition ﬁeld. In this paper, we explore the issue of domain shift in the context of object recognition, and present a novel method that adapts existing classiﬁers to new domains where labeled data is scarce.\nOften, we wish to perform recognition in a target visual domain where we have very few labeled examples and/or only have labels for a subset of categories, but have access to a source domain with plenty of labeled examples in many categories. As Figure 1 shows, it is insuﬃcient to directly use object classiﬁers trained on the source domain, as their performance can degrade signiﬁ- cantly on the target domain. Even when the same features are extracted in both domains, and the necessary normalization is performed on the image and the feature vectors, the underlying cause of the domain shift can strongly aﬀect the feature distribution and thus violate the assumptions of the classiﬁer. Typical\nK. Daniilidis, P. Maragos, N. Paragios (Eds.): ECCV 2010, Part IV, LNCS 6314, pp. 213–226, 2010. c(cid:2) Springer-Verlag Berlin Heidelberg 2010\nK. Saenko et al.\nSVM-bow NBNN [3]\ntrain test source source 54 ± 2 20 ± 1 source target\nsource domain\ntarget domain\nFig.1. (a) Example of extreme visual domain shift. (b) Degradation of the performance of two object classiﬁcation methods (an SVM over a bag-of-words representation (SVMbow) and the Naive Bayes nearest neighbor (NBNN) classiﬁer of [3]) when trained and tested on these image domains (see Sec.4 for dataset descriptions). Classiﬁcation accuracy is averaged over 31 object categories, and over 5 random 80%-20% splits into train/test data.\ncauses of visual domain shift include changes in the camera, image resolution, lighting, background, viewpoint, and post-processing. In the extreme case, all of these changes take place, such as when shifting from typical object category datasets mined from internet search engines to images captured in real-world surroundings, e.g. by a mobile robot (see Figure 1).\nRecently, domain adaptation methods that attempt to transfer classiﬁers learned on a source domain to new domains have been proposed in the language community. For example, Blitzer et al. adapt sentiment classiﬁers learned on book reviews to electronics and kitchen appliances [2]. In this paper, we argue that addressing the problem of domain adaptation for object recognition is essential for two reasons: 1) while labeled datasets are becoming larger and more available, they still diﬀer signiﬁcantly from many interesting application domains, and 2) it is unrealistic to expect the user to collect many labels in each new domain, especially when one considers the large number of possible object categories. Therefore, we need methods that can transfer object category knowledge from large labeled datasets to new domains.\nInthispaper,weintroduceanoveldomainadaptationtechniquebasedoncrossdomain transformations. The key idea, illustrated in Figure 2, is to learn a regularized non-linear transformation that maps points in the source domain (green) closer to those in the target domain (blue), using supervised data from both domains.Theinputconsistsoflabeledpairsofinter-domainexamplesthatareknown to be either similar(black lines) or dissimilar(red lines). The output is the learned transformation,whichcanbe appliedtopreviouslyunseentestdatapoints.Oneof the key advantages of our transform-based approach is that it can be applied over novel test samples from categories seen at training time, and can also generalize to new categories which were not present at training time.\nWe develop a general framework for learning regularized cross-domain transformations, and then present an algorithm based on a speciﬁc regularizer which results in a symmetric transform. This special case of transformations has\nAdapting Visual Category Models to New Domains\n(a) Domain shift problem\n(b) Pairwise constraints\n(c) Invariant space\nFig.2. The key idea of our approach to domain adaptation is to learn a transformation that compensates for the domain-induced changes. By leveraging (dis)similarity constraints (b) we aim to reunite samples from two diﬀerent domains (blue and green) in a common invariant space (c) in order to learn and classify new samples more effectively across domains. The transformation can also be applied to new categories (lightly-shaded stars). This ﬁgure is best viewed in color.\npreviously been explored for metric learning, and we base the algorithm presented in this paper on the information theoretic metric learning method of [8]. Metric learning has been successfully applied to a variety of problems in vision and other domains (see [6,11,14] for some vision examples) but to our knowledge has not been applied to domain adaptation. In work subsequent to that reported in this paper, we have developed a variant of our method that learns regularized asymmetric transformations, which allows us to model more general types of domain shift1.\nRather than committing to a speciﬁc form of the classiﬁer, we only assume that it operates over (kernelized) distances between examples. Encoding the domain invariance into the feature representation allows our method to beneﬁt a broad range of classiﬁcation methods, from k-NN to SVM, as well as clustering methods. While we evaluate our technique on object recognition, it is a general adaptation method that could be applied to non-image data.\nIn the next section, we relate our approach to existing work on domain adaptation and transfer learning. Section 3 describes our general framework for domain adaptation and presents an algorithm based on symmetric transformations, i.e. metric learning.We evaluate our approach on a new dataset designed to study the problemofvisualdomainshift, whichisdescribedin Section4, andshowempirical results of object classiﬁer adaptation on several visual domains in Section 5.",
    "data_related_paragraphs": [
        "The goal is to learn the linear transformation given some form of supervision, and then to utilize the learned similarity function in a classiﬁcation or clustering algorithm. To avoid overﬁtting, we choose a regularization function for W, which we will denote as r(W) (choices of the regularizer are discussed below). Denote X = [x1,...,xnA] as the matrix of nA training data points (of dimensionality dA) from A and Y = [y1,...,ynB] as the matrix of nB training data points (of dimensionality dB) from B. We will discuss the exact form of supervision we propose for domain adaptation problems in Section 3.1, but for now assume that it is a function of the learned similarity values simW(x,y) (i.e., a function of the matrix XTWY ), so a general optimization problem would seek to minimize the regularizer subject to supervision constraints given by functions ci:",
        "In this section, we describe our speciﬁc algorithm in detail. In using symmetric positive deﬁnite matrices, the idea is that the shift can be approximated as an arbitrary linear scaling and rotation of the feature space. We aim to recover this transformation by leveraging labeled data consisting of similarity and dissimilarity constraints between points in the two domains. Since the matrix W corresponding to the metric is symmetric positive semi-deﬁnite, we can think of it as mapping samples coming from two diﬀerent domains into a common invariant space, in order to learn and classify instances more eﬀectively across domains. Note that by factorizing W as W = GTG, we can equivalently view the distance dW between points x and y as (Gx − Gy)T(Gx − Gy); that is, the distance is simply the squared Euclidean distance after applying the linear transformation speciﬁed by G. The transformation G therefore maps data points from both domains into an invariant space. Because a linear transformation may not be suﬃcient, we optionally kernelize the distance matrix to learn non-linear transformations.",
        "Generating Cross-Domain Constraints. Suppose that we want to recognize a total of n categories (tasks), with training data from category i denoted as Li and consisting of (x,l) pairs of input image observations x and category labels l. There are two cases that we consider. In the ﬁrst case, we have many",
        "2 Mathematically, to ensure that such constraints are a function of XTWY , we let X = Y be the concatenation of data points in both domains. This is possible since the dimensionalities of the domains are identical.",
        "labeled examples for each of the n categories in the source domain data, LA = LA ∪...∪LA n, and a few labeled examples for each category in the target domain 1 data, LB = LB n. In the second case, we have the same training data 1 LA, but only have labels for a subset of the categories in the target domain, LB = LB m, where m < n. Here, our goal is to adapt the classiﬁers 1 trained on tasks m+1,...,n, which only have source domain labels, to obtain new classiﬁers that reduce the predictive error on the target domain by accounting for the domain shift. We accomplish this by applying the transformation learned on the ﬁrst m categories to the features in the source domain training set of categories m + 1,...,n, and re-training the classiﬁer.",
        "We call these class-based constraints, and we use this procedure to construct a set S of pairs (i,j) of similarity constraints and D of dissimilarity constraints. Alternatively, we can generate constraints based not on class labels, but on information of the form “target domain sample xA is similar to source domain i sample xB j ”. This is particularly useful when the source and target data include images of the same object, as it allows us to best recover the structure of the domain shift, without learning anything about particular categories. We refer to these as correspondence constraints.",
        "In some cases, the dimensionality of the data is very high, or a linear transformation is not suﬃcient for the desired metric. In such cases, we can apply kernelization to the above algorithm in order to learn high-dimensional metrics and/or non-linear transformations. Let ¯X = [X Y ] be the concatenated matrix of data points from both domains. It is possible to show that the updates for ITML may be written in terms of the kernel matrix by multiplying the updates on the left by ¯XT and on the right by ¯X, yielding",
        "where eA is the standard basis vector corresponding to the index of xA i and Kt = i ¯XTWt ¯X. K0 = ¯XT ¯X corresponds to some kernel matrix over the concatenated input data when we map data points from both domains to a high-dimensional feature space. Furthermore, the learned kernel function may be computed over arbtirary points, and the method may be scaled for very large data sets; see [8,14] for details.",
        "4 A Database for Studying Eﬀects of Domain Shift in",
        "As detailed earlier, eﬀects of domain shift have been largely overlooked in previous object recognition studies. Therefore, one of the contributions of this paper is a database3 that allows researchers to study, evaluate and compare solutions to the domain shift problem by establishing a multiple-domain labeled dataset and benchmark. In addition to the domain shift aspects, this database also proposes a challenging oﬃce environment category learning task which reﬂects the diﬃculty of real-world indoor robotic object recognition, and may serve as a useful testbed for such tasks. It contains a total of 4652 images originating from the following three domains:",
        "Images from the web: The ﬁrst domain consists of images from the web downloaded from online merchants (www.amazon.com). This has become a very popular way to acquire data, as it allows for easy access to large amounts of data that lends itself to learning category models. These images are of products shot at medium resolution typically taken in an environment with studio lighting",
        "Fig.4. New dataset for investigating domain shifts in visual category recognition tasks. Images of objects from 31 categories are downloaded from the web as well as captured by a high deﬁnition and a low deﬁnition camera.",
        "conditions. We collected two datasets: amazon contains 31 categories4 with an average of 90 images each. The images capture the large intra-class variation of these categories, but typically show the objects only from a canonical viewpoint. amazonINS contains 17 object instances (e.g. can of Taster’s Choice instant coﬀee) with an average of two images each.",
        "Images from a digital SLR camera: The second domain consists of images that are captured with a digital SLR camera in realistic environments with natural lighting conditions. The images have high resolution (4288x2848) and low noise. We have recorded two datasets: dslr has images of the 31 object categories, with 5 diﬀerent objects for each, in an oﬃce environment. Each object was captured with on average 3 images taken from diﬀerent viewpoints, for a",
        "4 The 31 categories in the database are: backpack, bike, bike helmet, bookcase, bottle, calculator, desk chair, desk lamp, computer, ﬁle cabinet, headphones, keyboard, laptop, letter tray, mobile phone, monitor, mouse, mug, notebook, pen, phone, printer, projector, puncher, ring binder, ruler, scissors, speaker, stapler, tape, and trash can.",
        "Images from a webcam: The third domain consists of images of the 31 categories recorded with a simple webcam. The images are of low resolution (640x480) and show signiﬁcant noise and color as well as white balance artifacts. Many current imagers on robotic platforms share a similarly-sized sensor, and therefore also possess these sensing characteristics. The resulting webcam dataset contains the same 5 objects per category as in dSLR, for a total of 795 images.",
        "The database represents several interesting visual domain shifts. First of all, it allows us to investigate the adaptation of category models learned on the web to dSLR and webcam images, which can be thought of as in situ observations on a robotic platform in a realistic oﬃce or home environment. Second, domain transfer between the high-quality dSLR images to low-resolution webcam images allows for a very controlled investigation of category model adaptation, as the same objects were recorded in both domains. Finally, the amazonINS and dslrINS datasets allow us to evaluate adaptation of product instance models from web data to a user environment, in a setting where images of the same products are available in both domains.",
        "In this section, we evaluate our domain adaptation approach by applying it to k-nearest neighbor classiﬁcation of object categories and instances. We use the database described in the previous section to study diﬀerent types of domain shifts and compare our new approach to several baseline methods. First, we detail our image processing pipeline, and then describe the diﬀerent experimental settings and elaborate on our empirical ﬁndings.",
        "Image Processing: All images were resized to the same width and converted to grayscale. Local scale-invariant interest points were detected using the SURF [1] detector to describe the image. SURF features have been shown to be highly repeatable and robust to noise, displacement, geometric and photometric transformations. The blob response threshold was set to 1000, and the other parameters to default values. A 64-dimensional non-rotationally invariant SURF descriptor was used to describe the patch surrounding each detected interest point. After extracting a set of SURF descriptors for each image, vector quantization into visual words was performed to generate the ﬁnal feature vector. A codebook of size 800 was constructed by k-means clustering on a randomly chosen subset of the amazon database. All images were converted to histograms over the resulting visual words. No spatial or color information was included in the image representation for these experiments.",
        "We generate constraints between all cross-domain image pairs in the training set based on their class labels, as described in Section 3.1. Table 1 shows the results. In the ﬁrst result column, to illustrate the level of performance without the domain shift, we plot the accuracy of the Euclidean k-NN classiﬁer trained on the source domain A and tested on images from the same domain (knn AA). The next column shows the same classiﬁer, but trained on A and tested on B (knn AB). Here, the eﬀect of the domain shift is evident, as the performance drops for all domain pairs, dramatically so in the case of the amazon to webcam shift. We can also train k-NN using the few available B labels (knn BB, third column). The fourth and the ﬁfth columns show the metric learning baseline, trained either on all pooled training data from both domains (ITML(A+B)), or only on B labels (ITML(B)). Finally, the last two columns show the symmetric variant of our domain adaptation method presented in this paper (symm), and its asymmetric variant [15] (asymm). knn BB does not perform as well because of the limited amount of labeled examples we have available in B. Even the more powerful metric-learning based classiﬁer fails to perform as well as the k-NN classiﬁer using our domain-invariant transform.",
        "Fig.5. Examples where our method succeeds in ﬁnding images of the correct category despite the domain shift. The large image on the right of each group is a webcam query image, while the smaller images are of the 5 nearest neighbors retrieved from the amazon dataset, using either the knn AB baseline in Table 1 (top row of smaller images), or the learned cross-domain symm kernel (bottom row of smaller images).",
        "The shift between dslr and webcam domains represents a moderate amount of change, mostly due to the diﬀerences in the cameras, as the same objects were used to collect both datasets. Since webcam actually has more training images, the reverse webcam-to-dslr shift is probably better suited to adaptation. In both these cases, symm outperforms asym, possibly due to the more symmetric nature of the shift and/or lack of training data to learn a more general tranformation. The shift between the amazon and the dslr/webcam domains is the most drastic (bottom row of Table 1.) Even for this challenging problem, the",
        "New-category setting: In this setting, the test data belong to categories for which we only have labels in the source domain. We use the ﬁrst half of the categories to learn the transformation, forming correspondence constraints (Section 3.1) between images of the same object instances in roughly the same pose. We test the metric on the remaining categories. The results of adapting webcam to dslr are shown in the ﬁrst row of Table 2. Our approach clearly learns something about the domain shift, signiﬁcantly improving the performance over the baselines, with asymm beating symm. Note that the overall accuracies are higher as this is a 16-way classiﬁcation task. The last row shows results on an instance classiﬁcation task, tackling the shift from Amazon to user environment images. While the symmetric method does not improve on the baseline in this case (possibly due to limited training data, only 2 images per product in amazon), the asymmetric method is able to compensate for some of this domain shift.",
        "We presented a detailed study of domain shift in the context of object recognition, and introduced a novel adaptation technique that projects the features into a domain-invariant space via a transformation learned from labeled source and target domain examples. Our approach can be applied to adapt a wide range of visual models which operate over similarities or distances between samples, and works both on cases where we need to classify novel test samples from categories seen at training time, and on cases where the test samples come from new categories which were not seen at training time. This is especially useful for object recognition, as large multi-category object databases can be adapted to new domains without requiring labels for all of the possibly huge number of categories. Our results show the eﬀectiveness of our technique for adapting k-NN classiﬁers to a range of domain shifts."
    ]
}