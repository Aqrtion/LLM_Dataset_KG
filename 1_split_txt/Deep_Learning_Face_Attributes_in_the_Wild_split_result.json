{
    "title_abstract_introduction": "Deep Learning Face Attributes in the Wild∗\n\nZiwei Liu1\n\nPing Luo1 Xiaogang Wang2 Xiaoou Tang1\n\n1Department of Information Engineering, The Chinese University of Hong Kong 2Department of Electronic Engineering, The Chinese University of Hong Kong {lz013,pluo,xtang}@ie.cuhk.edu.hk, xgwang@ee.cuhk.edu.hk\n\nAbstract\n\nPredicting face attributes in the wild is challenging due to complex face variations. We propose a novel deep learning framework for attribute prediction in the wild. It cascades two CNNs, LNet and ANet, which are ﬁnetuned jointly with attribute tags, but pre-trained differently. LNet is pre-trained by massive general object categories for face localization, while ANet is pre-trained by massive face identities for attribute prediction. This framework not only outperforms the state-of-the-art with a large margin, but also reveals valuable facts on learning face representation. (1) It shows how the performances of face localization (LNet) and attribute prediction (ANet) can be improved by different pre-training strategies. (2) It reveals that although the ﬁlters of LNet are ﬁne-tuned only with imagelevel attribute tags, their response maps over entire images have strong indication of face locations. This fact enables training LNet for face localization with only image-level annotations, butwithoutfaceboundingboxesorlandmarks, which are required by all attribute recognition works. (3) It also demonstrates that the high-level hidden neurons of ANet automatically discover semantic concepts after pretraining with massive face identities, and such concepts are signiﬁcantly enriched after ﬁne-tuning with attribute tags. Each attribute can be well explained with a sparse linear combination of these concepts.\n\n1. Introduction\n\nFace attributes are beneﬁcial for multiple applications such as face veriﬁcation [15, 2, 25], identiﬁcation [20], and retrieval. Predicting face attributes from images in the wild is challenging, because of complex face variations such as poses, lightings, and occlusions as shown in Fig.1.\n\nAttribute recognition methods are generally categorized into two groups: global and local methods. Global methods extract features from the entire object, where accurate locations of object parts or landmarks are not required.\n\nFigure 1. (a) Inaccurate localization and alignment lead to prediction errors on attributes by existing methods (b) LNet localizes face regions by averaging the response maps of attribute ﬁlters. ANet predicts attributes without alignment (c) Face localization with the averaged response map when LNet is trained with different numbers of attributes. (Best viewed in color)\n\nThey are not robust to deformations of objects [23]. Recent local models [15, 4, 5, 2, 19, 32] ﬁrst detect object parts and extract features from each part. These local features are concatenated to train classiﬁers. For example, Kumar et al. [15] predicted face attributes by extracting hand-crafted features from ten face parts. Zhang et al. [32] recognized human attributes by employing hundreds of poselets [4] to align human body parts. These local methods may fail when unconstrained face images with complex variations are present, which makes face localization and alignment difﬁcult. As shown in Fig.1 (a), HOG+SVM fails because the faces or landmarks are wrongly localized or misaligned. Thus the features are extracted at wrong positions [26]. Recent research shows that face localization and alignment are still not well solved problems, especially in the wild condition, although much progress has been achieved in the past decade. It is also proved by our experimental result.\n\n∗This work has been accepted to appear in ICCV 2015. This is the preprinted version. Content may slightly change prior to the ﬁnal publication.\n\nThis work revisits global methods by proposing a novel deep learning framework, which integrates two CNNs,\n\nArched EyebrowsReceding HairlineSmilingMustacheYoung(a) HOG(landmarks)+SVM(b) Our Methodtruetruefalsetruetruetruetruefalsefalsetrue(c)5 attributes10 attributes20 attributes40 attributes\n\nLNet and ANet, where LNet locates the entire face region and ANet extracts high-level face representation from the located region. The novelties are in three aspects. Firstly, LNet is trained in a weakly supervised manner, i.e. only image-level attribute tags of training images are provided, making data preparation much easier. This is different from training face and landmark detectors, where face bounding boxes and landmark positions are required. LNet is pretrained by classifying massive general object categories, such that its pre-trained features have good generalization capability on handling large background clutters. LNet is then ﬁne-tuned by attributes tags. We demonstrate that features learned in this way are effective for face localization and also can distinguish subtle differences between human faces and analogous patterns, such as a cat face.\n\nSecondly, ANet extracts discriminative face representation, making attribute recognition from the entire face region possible. ANet is pre-trained by classifying massive face identities and is ﬁne-tuned by attributes. We show that the pre-training step enables ANet to account for complex variations in the unconstrained face images.\n\nThirdly, within the rough locations of face regions provided by LNet, averaging the predictions of multiple patches can improve the performance. A simple way is to evaluate the feed-forward pass for each single patch. However, it is slow and has a lot of redundant computation. A novel fast feed-forward scheme is proposed to replace It evaluates images with arbipatch-by-patch evaluation. trary sizes with only one-pass feed-forward operation. It becomes non-trivial if the ﬁlters are locally shared, while studies [28, 27] showed that locally shared ﬁlters perform better in face related tasks. This is solved by proposing an interweaved operation.\n\nBesides proposing new methods, our framework also revealsvaluablefactsonlearningfacerepresentation. They not only motivate this work but also beneﬁt future research on face and deep learning. (1) It shows how pre-training with massive object categories and massive identities can improve feature learning for face localization and attribute recognition, respectively. (2) It demonstrates that although ﬁltersofLNetareﬁne-tunedbyattributetags, theirresponse maps over the entire image have strong indication of face location. Good features for face localization should be able to capture rich face variations, and more supervised information on these variations improves the learning process. The examples in Fig. 1 (a) show that as the number of attributes decreases, the localization capability of learned neurons gets reduced dramatically. (3) ANet is pre-trained with massive face identities. It discloses that the pre-trained high-level hidden neurons of ANet implicitly learn and discover sematic concepts that are related to identity, such as race, gender, and age. It indicates that when a deep model is pre-trained for face recognition, it implicitly learns\n\nattributes. The performance of attribute prediction drops without this pre-training stage.\n\nThe main contributions are summarized as follows. (1) We propose a novel deep learning framework, which combinesmassiveobjectsandmassiveidentitiestopre-train two CNNs for face localization and attribute prediction, respectively. It achieves state-of-the-art attribute classiﬁ- cation results on both the challenging CelebFaces [27] and LFW [12] datasets, improving existing methods by 8 and 13 percent, respectively. (2) A novel fast feed-forward algorithm for CNN with locally shared ﬁlters is devised. (3) Our study reveals multiple valuable facts on leaning face representation by deep models. (4) We also contribute a large facial attribute database with more than eight million attribute labels and it is 20 times larger than the largest publicly available dataset.\n\n1.1. Related Work\n\nExtracting hand-crafted features at pre-deﬁned landmarks has become a standard step in attribute recognition [9, 15, 4, 2]. Kumar et al. [15] extracted HOG-like features on various face regions to tackle attribute classiﬁcation and face veriﬁcation. To improve the discriminativeness of hand-crafted features given a speciﬁc task, Bourdev et al. [4] built a three-level SVM system to extract higher-level information. Deep learning [23, 7, 19, 32, 31, 13, 33, 22, 3] recently achieved great success in attribute prediction, due to their ability to learn compact and discriminative features. Razavian et al. [23] and Donahue et al. [7] demonstrated thatoff-the-shelffeatureslearnedbyCNNofImageNet[13] can be effectively adapted to attribute classiﬁcation. Zhang et al. [32] showed that better performance can be achieved byensemblinglearnedfeaturesofmultiplepose-normalized CNNs. The main drawback of these methods is that they rely on accurate landmark detection and pose estimation in both training and testing steps. Even though a recent work [31] can perform automatic part localization during test, it still requires landmark annotations of the training data.\n",
    "data_related_paragraphs": [
        "Different from existing works that rely on accurate face and landmark annotations, LNet is trained in a weakly supervised manner with only image-level annotations. Specifically, it is pre-trained with one thousand object categories of ImageNet [6] and ﬁne-tuned by image-level attribute tags. The former step accounts for background clutters, while the latter step learns features robust to complex face variations. in this way not only signiﬁcantly reduces data labeling, but also improves the",
        "(ILSVRC) 2012 [6], containing 1.2 million training images and 50 thousands validation images. All the data is employed for pre-training except one third of the validation data for choosing hyper-parameters [13]. We augment data by cropping ten patches from each image, including one patch at the center and four at the corners, and their horizontal ﬂips. We adopt softmax for object classiﬁcation, which is optimized by stochastic gradient descent (SGD) with back-propagation (BP) [16]. As shown in Fig.3 (a.2), the averaged response map in C5 of LNeto already indicates locations of objects including human faces after pre-training.",
        "ground. To determine the threshold, we select 2000 images, each of which contains a single face, and 2000 background images from SUN dataset [29]. For each image, EdgeBox [34] is adopted to propose 500 candidate windows, each of which is measured by a score that sums over its response values normalized by its window size. A larger score indicates the localized pattern is more likely to be a face. Each image is then represented by the maximum score over all its windows. In Fig.3 (b), the histogram of the maximum scores shows that these scores clearly separate face images from background images. The threshold is chosen as the decision boundary as shown in Fig.3 (b). More results are given in Fig.6 (a), showing that the above strategy can preciselylocalizefacewithinasingletestimage. Sinceeach training image only contains one single face, we localize a face region using the window with the largest score during training.",
        "As shown in Fig.2 (c) and (d), ANet is learned to extract features and SVM classiﬁers are used to predict attributes. Speciﬁcally, in the pre-training stage, ANet is trained by classifying massive face identities. In the ﬁnetuningstage, weﬁrstextendthelocalizedfaceregion, which is properly resized, with a small factor to incorporate more context information. Then, multiple patches are cropped from the enlarged face region and utilized as inputs of ANet. ANet is ﬁne-tuned by attributes to learn the highlevel feature FC. Furthermore, as shown in Fig.2 (d), each feature vector is adopted to train SVM classiﬁer for attribute prediction. The above strategy is similar to the multiview data augmentation [13], increasing the robustness of attribute recognition. In the testing stage, attributes are predicted by averaging the SVM scores over all the patches. Pre-training of ANet We introduce how to learn discriminative features by pre-training ANet with a large number of identities. We select eight thousand face identities from the CelebFaces [27] dataset, where each identity has around twenty images. There are over 160 thousand training images in total. A simple way to train ANet is to classify eight thousand categories with the softmax loss. However, it is challenging because the number of samples of each identity is limited to maintain the intraclass invariance. To improve intra-class invariance, we employ the similarity loss similar to [27, 10]. It decreases the distances between samples of the same identity. We",
        "Large-scale Data Collection We construct two face attribute datasets, namely CelebA and LFWA, by labeling images selected from two challenging face datasets, CelebFaces [27] and LFW [12]. CelebA contains ten thousand identities, each of which has twenty images. There are two hundred thousand images in total. LFWA has 13,233 images of 5,749 identities. Each image in CelebA and LFWA is annotated with forty face attributes and ﬁve key",
        "bling multiple CNNs, each of which extracts features from a well-aligned human part. These features are concatenated to train SVM for attribute recognition. It is straightforward to adapt this method to face attributes, since face parts can be well-aligned by landmark points. Here, we consider two settings. PANDA-w obtains the face parts by applying the state-of-the-art face detection [17] and alignment [26] on wild images, while PANDA-l attains the face parts by using ground truth landmark points. For fair comparison, all the above methods are trained with the same data as ours.",
        "Performance on LFWA+ To further examine whether the proposed approach can be generalized to unseen attributes, we manually label 30 more attributes for the testing images on LFWA and denote this extended dataset as LFWA+. To test on these 30 attributes, we directly transfer weightslearnedbydeepmodelstoextractfeatures, andonly re-train SVMs using one third of the images. LNets+ANet leads to 8, 10, and 3 percent average gains over the other three approaches (FaceTracer, PANDA-w, and PANDA-l). It demonstrates that our method learns discriminative face representations and has good generalization ability.",
        "Size of Training Dataset We compare the attribute prediction accuracy of the proposed method with the accuracy of PANDA-l, regarding different sizes of training datasets. Only the training data of ANet is changed in our method for fair comparison. Fig.13 demonstrates that LNets+ANet performs well when dataset size is small, but the performance of PANDA-l drops signiﬁcantly.",
        "Figure 13. Performances of different training dataset sizes.",
        "60%65%70%75%80%85%90%95%100%FaceTracer[11]PANDA-w [26]PANDA-l [26]LNets+ANetAccuracy1k3k10k828588LNets+ANetDataset SizeAccuracyPANDA-l1k3k10k76801k3k10k77828486CelebALFWALFWA+\f[19] P. Luo, X. Wang, and X. Tang. A deep sum-product In ICCV,",
        "[29] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR, pages 3485–3492, 2010. 4",
        "[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248–255, 2009. 2, 3",
        "[12] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical Report 07-49, University of Massachusetts, Amherst, October 2007. 2, 5"
    ]
}