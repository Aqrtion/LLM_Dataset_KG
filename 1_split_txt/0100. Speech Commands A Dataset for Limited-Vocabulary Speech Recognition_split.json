{
    "title_author_abstract_introduction": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition\nPete Warden Google Brain Mountain View, California petewarden@google.com\nApril 2018\n1 Abstract\nDescribes an audio dataset[1] of spoken words designed to help train and evaluate keyword spotting systems. Discusses why this task is an interesting challenge, and why it requires a specialized dataset that’s diﬀerent from conventional datasets used for automatic speech recognition of full sentences. Suggests a methodology for reproducible and comparable accuracy metrics for this task. Describes how the data was collected and veriﬁed, what it contains, previous versions[2] and properties. Concludes by reporting baseline results of models trained on this dataset.",
    "data_related_paragraphs": [
        "Speech recognition researchhas traditionally required the resources of large organizations such as universities or corporations to pursue. People working in those organizations usually have free access to either academic datasets through agreements with groups like the Linguistic Data Consortium[3], or to proprietary commercial data.",
        "As speech technology has matured, the number of people who want to train and evaluate recognition models has grown beyond these traditional groups, but the availability of datasets hasn’t widened. As the example of ImageNet[4] and similar collections in computer vision has shown, broadening access to",
        "datasets encourages collaborations across groups and enables apples-for-apples comparisons between diﬀerent approaches, helping the whole ﬁeld move forward. The Speech Commands dataset is an attempt to build a standard training and evaluation dataset for a class of simple speech recognition tasks. Its primary goal is to provide a way to build and test small models that detect when a single word is spoken, from a set of ten or fewer target words, with as few false positives as possible from background noise or unrelated speech. This task is often known as keyword spotting.",
        "To reach a wider audience of researchers and developers, this dataset has been released under the Creative Commons BY 4.0 license[5]. This enables it to easily be incorporated in tutorials and other scripts where it can be downloaded and used without any user intervention required (for example to register on a website or email an administrator for permission). This license is also well known in commercial settings, and so can usually be dealt with quickly by legal teams where approval is required.",
        "Mozilla’s Common Voice dataset[6] has over 500 hours from 20,000 diﬀerent people, and is available under the Creative Commons Zero license (similar to public domain). This licensing makes it very easy It is aligned by sentence, and to build on top of.",
        "TIDIGITS[8] contains 25,000 digit sequences spoken by 300 diﬀerent speakers, recorded in a quiet room by paid contributors. The dataset is only available under a commercial license from the Language Data Consortium, and is stored in the NIST SPHERE ﬁle format, which proved hard to decode using modern software. Our initial experiments on keyword spotting were performed using this dataset. CHiME-5[9] has 50 hours of speech recorded in people’s homes, stored as 16 KHz WAV ﬁles, and available under a restricted license. It’s aligned at the sentence level.",
        "Many voice interfaces rely on keyword spotting to start interactions. For example you might say \"Hey Google\" or \"Hey Siri\"[10] to begin a query or command for your phone. Once the device knows that you want to interact, it’s possible to send the audio to a web service to run a model that’s only limited by commercial considerations, since it can run on a server whose resources are controlled by the cloud provider. The initial detection of the start of an interaction is impractical to run as a cloud-based service though, since it would require sending audio data over the web from all devices all the time. This would be very costly to maintain, and would increase the privacy risks of the technology.",
        "Instead, most voice interfaces run a recognition module locally on the phone or other device. This listens continuously to audio input from microphones, and rather than sending the data over the internet to a server, they run models that listen for the desired trigger phrases. Once a likely trigger is heard, the",
        "and general speech recognition models is quite different. There are some promising datasets to support general speech tasks, such as Mozilla’s Common Voice, but they aren’t easily adaptable to keyword spotting.",
        "This Speech Commands dataset aims to meet the special needs around building and testing on-device models, to enable model authors to demonstrate the accuracy of their architectures using metrics that are comparable to other models, and give a simple way for teams to reproduce baseline models by training on identical data. The hope is that this will speed up progress and collaboration, and improve the overall quality of models that are available.",
        "I also decided to focus on English. This was for pragmatic reasons, to limit the scope of the gathering process and make it easier for native speakers to perform quality control on the gathered data. I hope that transfer learning and other techniques will still make this dataset useful for other languages though, and I open-sourced the collection application to allow others to easily gather similar data in other languages. I did want to gather as wide a variety of accents as possible however, since we’re familiar from experience with the bias towards American English in many voice interfaces.",
        "I also wanted to avoid recording any personallyidentiﬁable information from contributors, since any such data requires handling with extreme care for privacy reasons. This meant that I wouldn’t ask for any attributes like gender or ethnicity, wouldn’t require a sign-in through a user ID that could link to personal data, and would need users to agree to a data-usage agreement before contributing.",
        "I wanted to have a limited vocabulary to make sure the capture process was lightweight, but still have enough variety for models trained on the data to potentially be useful for some applications. I also wanted the dataset to be usable in comparable ways to common proprietary collections like TIDIGITS. This led me to pick twenty common words as the core of our vocabulary. These included the digits zero to nine, and in version one, ten words that would be useful as commands in IoT or robotics applications; \"Yes\", \"No\", \"Up\", \"Down\", \"Left\", \"Right\", \"On\", \"Oﬀ\", \"Stop\", and \"Go\". In version 2 of the dataset, I added four more command words; “Backward”, “Forward”, “Follow”, and “Learn”. One of the most challenging problems for keyword recognition is ignoring speech that doesn’t contain triggers, so I also needed a set of words that could act as tests of that ability in the dataset. Some of these, such as “Tree”, were picked because they sound similar to target words and would be good tests of a model’s discernment. Others were chosen arbitrarily as short words that covered a lot of diﬀerent phonemes. The ﬁnal list was \"Bed\", \"Bird\", \"Cat\", \"Dog\", \"Happy\", \"House\", \"Marvin\", \"Sheila\", \"Tree\", and \"Wow\".",
        "The initial page that a new user sees when navigating to the application explains what the project is doing, and asks them to explicitly and formally agree to participating in the study. This process was designed to ensure that the resulting utterances could be freely redistributed as part of an open dataset, and that users had a clear understanding of what the application was doing. When a user clicks on “I Agree”, a session cookie is added to record their agreement. The recording portion of the application will only be shown if this session cookie is found, and all upload accesses are guarded by cross-site request forgery tokens, to ensure that only audio recorded from the application can be uploaded, and that utterances are from users who have agreed to the terms.",
        "The WebAudioAPI returns the audio data in OGGcompressed format, and this is what gets stored in the resulting ﬁles. The session ID is used as the preﬁx of each ﬁle name, and then the requested word is followed by a unique instance ID for the recording. This session ID has been randomly generated, and is not tied to an account or any other demographic information, since none has been generated. It does serve as a speaker identiﬁer for utterances however. To ensure there’s a good distribution of diﬀerent speakers, once a user has gone through this process once a cookie is added to the application that ensures they can’t access the recording page again.",
        "To gather volunteers for this process, I used appeals on social media to share the link and the aims of the project. I also experimented with using paid crowdsourcing for some of the utterances, though the majority of the dataset comes from the open site.",
        "With that complete, I then converted the OGG ﬁles into uncompressed WAV ﬁles containing PCM sample data at 16KHz, since this is any easier format for further processing:",
        "From manual inspection of the results, there were still large numbers of utterances that were too quiet or completely silent. The alignment of the spoken words within the 1.5 second ﬁle was quite arbitrary too, depending on the speed of the user’s response to the word displayed. To solve both these problems, I created a simple audio processing tool called Extract Loudest Section to examine the overall volume of the clips. As a ﬁrst stage, I summed the absolute diﬀerences of all the samples from zero (using a scale where -32768 in the 16-bit sample data was -1.0 as a ﬂoating-point number, and +32767 was 1.0), and looked at the mean average of that value to estimate the overall volume of the utterance. From experimentation, anything below 0.004 on this metric was likely to be to quiet to be intelligible, and so all of those clips were removed.",
        "These automatic processes caught technical problems with quiet or silent recordings, but there were still some utterances that were of incorrect words or were unintelligible for other reasons. To ﬁlter these out I turned to commercial crowdsourcing. The task asked workers to type in the word they heard from each clip, and gave a list of the expected words as examples. Each clip was only evaluated by a single worker, and any clips that had responses that didn’t match their expected labels were removed from the dataset.",
        "The ﬁnal dataset consisted of 105,829 utterances of 35 words, broken into the categories and frequencies shown in Table 1.",
        "Each utterance is stored as a one-second (or less) WAVE format ﬁle, with the sample data encoded as linear 16-bit single-channel PCM values, at a 16 KHz rate. There are 2,618 speakers recorded, each with a unique eight-digit hexadecimal identiﬁer assigned as described above. The uncompressed ﬁles take up approximately 3.8 GB on disk, and can be stored as a 2.7GB gzip-compressed tar archive.",
        "One of this dataset’s primary goals is to enable meaningful comparisons between diﬀerent models’ results, so it’s important to suggest some precise testing protocols. As a starting point, it’s useful to specify exactly which utterances can be used for training, and which must be reserved for testing, to avoid overﬁtting. The dataset download includes a text ﬁle called validation_list.txt, which contains a list of ﬁles that are expected to be used for validating results during training, and so can be used frequently to help adjust hyperparameters and make other model changes. The testing_list.txt ﬁle contains the names of audio clips that should only be used for measuring the results of trained models, not for training or validation. The set that a ﬁle belongs to is chosen using a hash function on its name. This is to ensure that ﬁles remain in the same set across releases, even as the total number changes, so avoid set crosscontamination when trying old models on the more recent test data. The Python implementation of the",
        "Figure 1: How many recordings of each word are present in the dataset",
        "set assignment algorithm is given in the TensorFlow tutorial code[12] that is a companion to the dataset.",
        "I’ve uploaded a standard set of test ﬁles[13] to make it easier to reproduce this metric. If you want to calculate the canonical Top-One error for a model, run inference on each audio clip, and compare the top predicted class against the ground truth label encoded in its containing subfolder name. The proportion of correct predictions will give you the Top-One error. There’s also a similar collection of test ﬁles[14] available for version one of the dataset.",
        "The example training code that accompanies the dataset[15] provides results of 88.2% on this metric for the highest-quality model when fully trained. This translates into a model that qualitatively gives a reasonable, but far from perfect response, so it’s expected that this will serve as a baseline to be ex-",
        "Top-One captures a single dimension of the perceived quality of the results, but doesn’t reveal much about other aspects of its performance in a real application. For example, models in products receive a continuous stream of audio data and don’t know when words start and end, whereas the inputs to Top One evaluations are aligned to the beginning of utterances. The equal weighting of each category in the overall score also doesn’t reﬂect the distribution of trigger words and silence in typical environments.",
        "To measure some of these more complex properties of models, I test them against continuous streams of audio and score them on multiple metrics. Here’s what the baseline model trained with V2 data produces:",
        "Version 1 of the dataset[2] was released August 3rd 2017, and contained 64,727 utterances from 1,881 speakers. Training the default convolution model from the TensorFlow tutorial (based on Convolutional Neural Networks for Small-footprint Keyword Spotting[19]) using the V1 training data gave a TopOne score of 85.4%, when evaluated against the test set from V1. Training the same model against version 2 of the dataset[1], documented in this paper,",
        "produces a model that scores 88.2% Top-One on the training set extracted from the V2 data. A model trained on V2 data, but evaluated against the V1 test set gives 89.7% Top-One, which indicates that the V2 training data is responsible for a substantial improvement in accuracy over V1. The full set of results are shown in Table 2.",
        "Data V1 Test V2 Test",
        "Figure 2: Top-One accuracy evaluations using diﬀerent training data",
        "python tensorflow/examples/speech_commands/ ֒→ train.py --data_url=\\protect\\vrule ֒→ width0pt\\protect\\href{http:// ֒→ download.tensorflow.org/data/ ֒→ speech_commands_v0.01.tar.gz}{http ֒→ ://download.tensorflow.org/data/ ֒→ speech_commands_v0.01.tar.gz}",
        "python tensorflow/examples/speech_commands/ ֒→ train.py --data_url=\\protect\\vrule ֒→ width0pt\\protect\\href{http:// ֒→ download.tensorflow.org/data/ ֒→ speech_commands_v0.02.tar.gz}{http ֒→ ://download.tensorflow.org/data/ ֒→ speech_commands_v0.02.tar.gz}",
        "The TensorFlow tutorial gives a variety of baseline models, but one of the goals of the dataset is to enable the creation and comparison of a wide range of models on a lot of diﬀerent platforms, and version one of has enabled some interesting applications. CMSISNN[21] covers a new optimized implementation of neural network operations for ARM microcontrollers, and uses Speech Commands to train and evaluate the results. Listening to the World[22] demonstrates how combining the dataset and UrbanSounds[23] can improve the noise tolerance of recognition models. Did you Hear That[24] uses the dataset to test adversarial attacks on voice interfaces. Deep Residual Learning for Small Footprint Keyword Spotting[25] shows how approaches learned from ResNet can produce more eﬃcient and accurate models. Raw Waveformbased Audio Classiﬁcation[26] investigates alternatives to traditional feature extraction for speech and music models. Keyword Spotting Through Image Recognition[27] looks at the eﬀect virtual adversarial training on the keyword task.",
        "The Speech Commands dataset has shown to be useful for training and evaluating a variety of models, and the second version shows improved results on equivalent test data, compared to the original.",
        "python tensorflow/examples/speech_commands/ ֒→ train.py --data_url=\\protect\\vrule ֒→ width0pt\\protect\\href{http:// ֒→ download.tensorflow.org/data/ ֒→ speech_commands_v0.01}{http:// ֒→ download.tensorflow.org/data/ ֒→ speech_commands_v0.0{1},1}.tar.gz --",
        "Massive thanks are due to everyone who donated recordings to this data set, I’m very grateful. I also couldn’t have put this together without the help and support of Billy Rutledge, Rajat Monga, Raziel Alvarez, Brad Krueger, Barbara Petit, Gursheesh Kour, Robert Munro, Kirsten Gokay, David Klein, Lukas Biewald, and all the AIY and TensorFlow teams.",
        "dataset",
        "http://download.tensorﬂow.org/data/speech_commands_v0.02.tar.gz",
        "dataset",
        "http://download.tensorﬂow.org/data/speech_commands_v0.01.tar.gz",
        "[3] (2018) Linguistic data consortium. [Online]. Available: https://www.ldc.upenn.edu/",
        "Image Database,” in CVPR09, 2009.",
        "[8] R. G. Leonard and G. R. Doddington. (1992) A speaker-independent connected-digit database.",
        "http://spandh.dcs.shef.ac.uk/chime_challenge/data.html",
        "https://github.com/tensorﬂow/tensorﬂow/blob/master/tensorﬂow/examples/speech_commands/input_data.py#L61",
        "dataset http://download.tensorﬂow.org/data/speech_commands_test_set_v0.02.tar.gz",
        "dataset http://download.tensorﬂow.org/data/speech_commands_test_set_v0.01.tar.gz",
        "dataset",
        "http://download.tensorﬂow.org/data/speech_commands_streaming_test_v0.02.tar.gz",
        "[23] J. Salamon, C. Jacoby, and J. P. Bello, “A dataset and taxonomy for urban sound research,” in Proceedings of the 22Nd ACM International Conference on Multimedia, ser. MM ’14. New York, NY, USA: ACM, 2014, pp. 1041–1044. [Online]. Available: http://doi.acm.org/10.1145/2647868.2655045"
    ]
}