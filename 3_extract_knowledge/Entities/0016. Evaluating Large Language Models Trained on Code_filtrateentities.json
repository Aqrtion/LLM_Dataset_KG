{
    "entities": [
        {
            "id": "20250318235155P1",
            "name": "Evaluating Large Language Models Trained on Code",
            "type": "Paper",
            "attributes": {
                "authors": [
                    "Mark Chen",
                    "Jerry Tworek",
                    "Heewoo Jun",
                    "Qiming Yuan",
                    "Henrique Ponde de Oliveira Pinto",
                    "Jared Kaplan",
                    "et al."
                ],
                "institution": "OpenAI"
            }
        },
        {
            "id": "20250318235155D1",
            "name": "HumanEval",
            "type": "Dataset",
            "attributes": {
                "size": "164 problems",
                "tests_per_problem": "average 7.7",
                "purpose": "measuring functional correctness for program synthesis"
            }
        },
        {
            "id": "20250318235155D2",
            "name": "APPS",
            "type": "Dataset",
            "attributes": {
                "size": "5000 training and 5000 test examples",
                "content": "coding problems from Codeforces"
            }
        },
        {
            "id": "20250318235155D3",
            "name": "The Pile",
            "type": "Dataset",
            "attributes": {
                "size": "800GB",
                "content": "diverse text including 8% GitHub code"
            }
        },
        {
            "id": "20250318235155D4",
            "name": "CodeSearchNet",
            "type": "Dataset",
            "attributes": {
                "content": "multi-language code corpus from GitHub"
            }
        },
        {
            "id": "20250318235155D5",
            "name": "CodeXGLUE",
            "type": "Dataset",
            "attributes": {
                "purpose": "aggregated programming benchmarks"
            }
        },
        {
            "id": "20250318235155D6",
            "name": "FlashFill",
            "type": "Dataset",
            "attributes": {
                "domain": "program synthesis benchmarks"
            }
        },
        {
            "id": "20250318235155R1",
            "name": "https://www.github.com/openai/human-eval",
            "type": "Repository",
            "attributes": {}
        },
        {
            "id": "20250318235155T1",
            "name": "program synthesis",
            "type": "Task",
            "attributes": {}
        },
        {
            "id": "20250318235155T2",
            "name": "code generation",
            "type": "Task",
            "attributes": {}
        },
        {
            "id": "20250318235155T3",
            "name": "functional correctness evaluation",
            "type": "Task",
            "attributes": {}
        },
        {
            "id": "20250318235155T4",
            "name": "docstring generation",
            "type": "Task",
            "attributes": {}
        },
        {
            "id": "20250318235155T5",
            "name": "code autocompletion",
            "type": "Task",
            "attributes": {}
        },
        {
            "id": "20250318235155T6",
            "name": "code understanding",
            "type": "Task",
            "attributes": {}
        }
    ]
}