{
  "entities": [
    {
      "id": "20250318214228P1",
      "name": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "type": "Paper",
      "attributes": {
        "authors": [
          "Alex Wang",
          "Amanpreet Singh",
          "Julian Michael",
          "Felix Hill",
          "Omer Levy",
          "Samuel R. Bowman"
        ],
        "institutions": [
          "New York University",
          "University of Washington",
          "DeepMind"
        ]
      }
    },
    {
      "id": "20250318214228D1",
      "name": "GLUE",
      "type": "Dataset",
      "attributes": {
        "number_of_tasks": 9,
        "domains": [
          "diverse"
        ],
        "diagnostic_suite": "included"
      }
    },
    {
      "id": "20250318214228D2",
      "name": "CoLA",
      "type": "Dataset",
      "attributes": {
        "size": "8.5k train, 1k test",
        "task": "acceptability classification",
        "metrics": "Matthews correlation",
        "domain": "miscellaneous"
      }
    },
    {
      "id": "20250318214228D3",
      "name": "SST-2",
      "type": "Dataset",
      "attributes": {
        "size": "67k train, 1.8k test",
        "task": "sentiment analysis",
        "metrics": "accuracy",
        "domain": "movie reviews"
      }
    },
    {
      "id": "20250318214228D4",
      "name": "MRPC",
      "type": "Dataset",
      "attributes": {
        "size": "3.7k train, 1.7k test",
        "task": "paraphrase detection",
        "metrics": "accuracy/F1",
        "domain": "news"
      }
    },
    {
      "id": "20250318214228D5",
      "name": "STS-B (Semantic Textual Similarity Benchmark)",
      "type": "Dataset",
      "attributes": {
        "size": "7k train, 1.4k test",
        "task": "sentence similarity",
        "metrics": "Pearson/Spearman correlation",
        "domain": "miscellaneous"
      }
    },
    {
      "id": "20250318214228D6",
      "name": "QQP (Quora Question Pairs)",
      "type": "Dataset",
      "attributes": {
        "size": "364k train, 391k test",
        "task": "paraphrase detection",
        "metrics": "accuracy/F1",
        "domain": "social"
      }
    },
    {
      "id": "20250318214228D7",
      "name": "MNLI (Multi-Genre Natural Language Inference Corpus)",
      "type": "Dataset",
      "attributes": {
        "size": "393k train",
        "task": "textual entailment",
        "metrics": "accuracy",
        "domain": "multiple"
      }
    },
    {
      "id": "20250318214228D8",
      "name": "QNLI (Question-answering NLI)",
      "type": "Dataset",
      "attributes": {
        "size": "105k train",
        "task": "textual entailment",
        "metrics": "accuracy",
        "domain": "Wikipedia"
      }
    },
    {
      "id": "20250318214228D9",
      "name": "RTE (Recognizing Textual Entailment)",
      "type": "Dataset",
      "attributes": {
        "size": "2.5k train",
        "task": "textual entailment",
        "metrics": "accuracy",
        "domain": "news, Wikipedia"
      }
    },
    {
      "id": "20250318214228D10",
      "name": "WNLI (Winograd NLI)",
      "type": "Dataset",
      "attributes": {
        "size": "634 train",
        "task": "textual entailment",
        "metrics": "accuracy",
        "domain": "fiction"
      }
    },
    {
      "id": "20250318214228D11",
      "name": "SNLI (Stanford Natural Language Inference)",
      "type": "Dataset",
      "attributes": {
        "size": "550k examples",
        "task": "textual entailment"
      }
    },
    {
      "id": "20250318214228D12",
      "name": "GLUE Diagnostic Dataset",
      "type": "Dataset",
      "attributes": {
        "purpose": "linguistic analysis",
        "phenomena": [
          "lexical semantics",
          "predicate-argument structure",
          "logic",
          "knowledge"
        ]
      }
    },
    {
      "id": "20250318214228R1",
      "name": "https://gluebenchmark.com",
      "type": "Repository",
      "attributes": {}
    },
    {
      "id": "20250318214228T1",
      "name": "natural language understanding",
      "type": "Task",
      "attributes": {}
    },
    {
      "id": "20250318214228T2",
      "name": "sentiment analysis",
      "type": "Task",
      "attributes": {}
    },
    {
      "id": "20250318214228T3",
      "name": "textual entailment",
      "type": "Task",
      "attributes": {}
    },
    {
      "id": "20250318214228T4",
      "name": "paraphrase detection",
      "type": "Task",
      "attributes": {}
    },
    {
      "id": "20250318214228T5",
      "name": "sentence similarity",
      "type": "Task",
      "attributes": {}
    },
    {
      "id": "20250318214228T6",
      "name": "acceptability classification",
      "type": "Task",
      "attributes": {}
    },
    {
      "id": "20250318214228T7",
      "name": "question answering",
      "type": "Task",
      "attributes": {}
    },
    {
      "id": "20250318214228T8",
      "name": "benchmarking",
      "type": "Task",
      "attributes": {}
    }
  ],
  "relations": [
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "stored_in",
      "tail_id": "20250318214228R1",
      "tail": "https://gluebenchmark.com",
      "tail_type": "Repository"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "introduced_in",
      "tail_id": "20250318214228P1",
      "tail": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "tail_type": "Paper"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D2",
      "tail": "CoLA",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D3",
      "tail": "SST-2",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D4",
      "tail": "MRPC",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D5",
      "tail": "STS-B (Semantic Textual Similarity Benchmark)",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D6",
      "tail": "QQP (Quora Question Pairs)",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D7",
      "tail": "MNLI (Multi-Genre Natural Language Inference Corpus)",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D8",
      "tail": "QNLI (Question-answering NLI)",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D9",
      "tail": "RTE (Recognizing Textual Entailment)",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D10",
      "tail": "WNLI (Winograd NLI)",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "composed_of",
      "tail_id": "20250318214228D12",
      "tail": "GLUE Diagnostic Dataset",
      "tail_type": "Dataset"
    },
    {
      "head_id": "20250318214228D2",
      "head": "CoLA",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T6",
      "tail": "acceptability classification",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D3",
      "head": "SST-2",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T2",
      "tail": "sentiment analysis",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D4",
      "head": "MRPC",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T4",
      "tail": "paraphrase detection",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D5",
      "head": "STS-B (Semantic Textual Similarity Benchmark)",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T5",
      "tail": "sentence similarity",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D6",
      "head": "QQP (Quora Question Pairs)",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T4",
      "tail": "paraphrase detection",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D7",
      "head": "MNLI (Multi-Genre Natural Language Inference Corpus)",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T3",
      "tail": "textual entailment",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D8",
      "head": "QNLI (Question-answering NLI)",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T3",
      "tail": "textual entailment",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D9",
      "head": "RTE (Recognizing Textual Entailment)",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T3",
      "tail": "textual entailment",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D10",
      "head": "WNLI (Winograd NLI)",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T3",
      "tail": "textual entailment",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D1",
      "head": "GLUE",
      "head_type": "Dataset",
      "relation": "used_for",
      "tail_id": "20250318214228T1",
      "tail": "natural language understanding",
      "tail_type": "Task"
    },
    {
      "head_id": "20250318214228D12",
      "head": "GLUE Diagnostic Dataset",
      "head_type": "Dataset",
      "relation": "introduced_in",
      "tail_id": "20250318214228P1",
      "tail": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
      "tail_type": "Paper"
    }
  ],
  "has_been_merged": true
}