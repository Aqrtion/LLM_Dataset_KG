{
    "entities": [
        {
            "name": "Visual Genome Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
            "type": "Paper",
            "attributes": {
                "authors": [
                    "Ranjay Krishna",
                    "Yuke Zhu",
                    "Oliver Groth",
                    "Justin Johnson",
                    "Kenji Hata",
                    "Joshua Kravitz",
                    "Stephanie Chen",
                    "Yannis Kalantidis",
                    "Li-Jia Li",
                    "David A. Shamma",
                    "Michael S. Bernstein",
                    "Li Fei-Fei"
                ],
                "institution": "Stanford University"
            }
        },
        {
            "name": "Visual Genome",
            "type": "Dataset",
            "attributes": {
                "size": "108,249 images",
                "objects_per_image": 21,
                "attributes_per_image": 18,
                "relationships_per_image": 18,
                "question_answer_pairs": "1.7 million",
                "canonicalization": "WordNet synsets"
            }
        },
        {
            "name": "MS-COCO",
            "type": "Dataset",
            "attributes": {
                "size": "328,000 images",
                "object_categories": 91,
                "captions_per_image": 5
            }
        },
        {
            "name": "YFCC100M",
            "type": "Dataset",
            "attributes": {
                "size": "100 million images"
            }
        },
        {
            "name": "LabelMe",
            "type": "Dataset",
            "attributes": {
                "size": "30,000 images",
                "segmented_images": "yes",
                "categories": 200
            }
        },
        {
            "name": "Lotus Hill dataset",
            "type": "Dataset",
            "attributes": {
                "segmented_images": "yes",
                "availability": "purchase required",
                "size": "50,000 images"
            }
        },
        {
            "name": "DAQUAR",
            "type": "Dataset",
            "attributes": {
                "focus": "indoor scenes",
                "QA_pairs": "toy-sized"
            }
        },
        {
            "name": "VQA",
            "type": "Dataset",
            "attributes": {
                "QA_pairs": "614,000"
            }
        },
        {
            "name": "Flickr30K",
            "type": "Dataset"
        },
        {
            "name": "SUN",
            "type": "Dataset"
        },
        {
            "name": "80 Million Tiny Images",
            "type": "Dataset",
            "attributes": {
                "size": "80 million",
                "resolution": "32x32"
            }
        },
        {
            "name": "PASCAL VOC",
            "type": "Dataset",
            "attributes": {
                "categories": 20,
                "size": "11,000 images"
            }
        },
        {
            "name": "ILSVRC",
            "type": "Dataset"
        },
        {
            "name": "NYU Depth v2",
            "type": "Dataset"
        },
        {
            "name": "CUB-2011",
            "type": "Dataset",
            "attributes": {
                "focus": "fine-grained attributes"
            }
        },
        {
            "name": "VisKE",
            "type": "Dataset",
            "attributes": {
                "relationships": "6,500"
            }
        },
        {
            "name": "Caltech-256",
            "type": "Dataset",
            "attributes": {
                "categories": 256
            }
        },
        {
            "name": "Caltech101",
            "type": "Dataset",
            "attributes": {
                "categories": 101
            }
        },
        {
            "name": "ImageNet",
            "type": "Dataset",
            "attributes": {
                "size": "14 million images",
                "structure": "WordNet synsets"
            }
        },
        {
            "name": "object recognition",
            "type": "Task"
        },
        {
            "name": "image classification",
            "type": "Task"
        },
        {
            "name": "visual question answering",
            "type": "Task"
        },
        {
            "name": "scene understanding",
            "type": "Task"
        },
        {
            "name": "part models learning",
            "type": "Task"
        },
        {
            "name": "benchmarking",
            "type": "Task"
        },
        {
            "name": "image search",
            "type": "Task"
        },
        {
            "name": "image understanding",
            "type": "Task"
        },
        {
            "name": "attribute classification",
            "type": "Task"
        },
        {
            "name": "relationship classification",
            "type": "Task"
        },
        {
            "name": "dense image captioning",
            "type": "Task"
        }
    ],
    "relations": [
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "introduced_in",
            "tail": "Visual Genome Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
            "tail_type": "Paper"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "object recognition",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "image classification",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "visual question answering",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "scene understanding",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "part models learning",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "benchmarking",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "image search",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "image understanding",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "attribute classification",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "relationship classification",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "used_for",
            "tail": "dense image captioning",
            "tail_type": "Task"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "composed_of",
            "tail": "MS-COCO",
            "tail_type": "Dataset"
        },
        {
            "head": "Visual Genome",
            "head_type": "Dataset",
            "relation": "composed_of",
            "tail": "YFCC100M",
            "tail_type": "Dataset"
        }
    ],
    "has_been_merged": false
}